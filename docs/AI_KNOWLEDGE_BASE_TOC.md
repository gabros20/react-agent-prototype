# AI Engineering Knowledge Base - Table of Contents

**Purpose**: Comprehensive educational reference for building production-grade AI agents  
**Organization**: 12 layers (Foundation ‚Üí Advanced) with 72 topics total  
**Grounding**: Research papers, production systems, proven implementations from this codebase

**Last Updated**: 2025-11-16  
**Status**: üöß In Progress

---

## Quick Navigation

-   [How to Use This Knowledge Base](#how-to-use-this-knowledge-base)
-   [Learning Paths](#learning-paths)
-   [Progress Tracker](#progress-tracker)
-   [Layer Index](#layer-index)
-   [Related Documentation](#related-documentation)

---

## How to Use This Knowledge Base

### For Beginners

Start with **Layer 0** (Foundations), then progress through **Layers 1-3**. This covers LLM basics, prompt engineering, and simple agents.

### For Intermediate Developers

Jump to **Layers 4-6** if you understand LLMs and basic agents. Focus on memory, RAG, and planning patterns.

### For Production Teams

Focus on **Layers 7-11** for reliability, error handling, tools, monitoring, and multi-agent systems.

### For Researchers

Explore **Layer 12** for cutting-edge patterns and experimental techniques.

### As Reference

Use Ctrl+F to search for specific patterns (e.g., "circuit breaker" ‚Üí Layer 7.3).

---

## Learning Paths

### Path 1: Beginner ‚Üí Intermediate (4-6 weeks)

```
Layer 0 (Foundations) ‚Üí Layer 1 (Prompts) ‚Üí Layer 2 (Context) ‚Üí
Layer 3 (Agents) ‚Üí Layer 4 (Memory) ‚Üí Layer 5 (RAG)
```

**Outcome**: Build simple RAG-powered agents with memory

### Path 2: Production Engineering (3-4 weeks)

```
Layer 3 (Agents) ‚Üí Layer 7 (Error Recovery) ‚Üí Layer 8 (Tools) ‚Üí
Layer 9 (HITL) ‚Üí Layer 11 (Production Patterns)
```

**Outcome**: Deploy reliable, monitored agents with safety features

### Path 3: Advanced Patterns (2-3 weeks)

```
Layer 6 (Planning) ‚Üí Layer 10 (Multi-Agent) ‚Üí Layer 12 (Cutting-Edge)
```

**Outcome**: Implement complex orchestration and self-improving agents

---

## Progress Tracker

**Completion**: 40/72 topics (56%)

| Layer                                | Topics | Status         | Completion   |
| ------------------------------------ | ------ | -------------- | ------------ |
| Layer 0: Foundations                 | 3      | ‚úÖ Complete    | 13/13 (100%) |
| Layer 1: Prompt Engineering          | 3      | ‚úÖ Complete    | 14/14 (100%) |
| Layer 2: Context Engineering         | 3      | ‚úÖ Complete    | 12/12 (100%) |
| Layer 3: Agent Architecture          | 4      | üöß In Progress | 2/20 (10%)   |
| Layer 4: Memory & State              | 4      | ‚è≥ Pending     | 0/20         |
| Layer 5: Retrieval & RAG             | 4      | ‚è≥ Pending     | 0/20         |
| Layer 6: Planning & Orchestration    | 4      | ‚è≥ Pending     | 0/20         |
| Layer 7: Error Recovery & Resilience | 5      | ‚è≥ Pending     | 0/20         |
| Layer 8: Tool Design Patterns        | 4      | ‚è≥ Pending     | 0/20         |
| Layer 9: Human-in-the-Loop           | 3      | ‚è≥ Pending     | 0/15         |
| Layer 10: Multi-Agent Systems        | 4      | ‚è≥ Pending     | 0/20         |
| Layer 11: Production Engineering     | 5      | ‚è≥ Pending     | 0/25         |
| Layer 12: Cutting-Edge Patterns      | 4      | ‚è≥ Pending     | 0/20         |

**Latest Updates** (2025-11-17):

**Layer 3: Agent Architecture** (2/20 topics - üöß 10% COMPLETE):

_Core Patterns_ (2/5):
- ‚úÖ 3.1.1 ReAct Pattern - Analysis of your `server/prompts/react.xml`, Think‚ÜíAct‚ÜíObserve loop, 2024-2025 research (ReSpAct, A3T, Plan-and-Execute), 5 enhancement recommendations
- ‚úÖ 3.1.2 Tool Design Patterns - **NEW!** Deep analysis of your `server/tools/all-tools.ts` (20+ CMS tools), schema design, parameter validation, error handling, HITL confirmations, granular fetching (40-96% savings), 4-phase enhancement roadmap

**Layer 2: Context Engineering** (12/12 topics - ‚úÖ 100% COMPLETE):

_Token Optimization_ (4/4 - ‚úÖ COMPLETE):
- ‚úÖ 2.1.1 Compression Techniques - LongLLMLingua, LLMLingua-2, 60-90% reduction maintaining 95%+ accuracy
- ‚úÖ 2.1.2 Importance Scoring - RankRAG (+18% accuracy), 5 scoring strategies, production integration
- ‚úÖ 2.1.3 Lazy Loading - Your `cms_getPage(fetchMode)` validated, 90%+ token reduction
- ‚úÖ 2.1.4 Hybrid Content Fetching - Deep analysis of your CMS pattern, three-tier enhancement

_Context Management Patterns_ (4/4 - ‚úÖ COMPLETE):
- ‚úÖ 2.2.1 Sliding Window - StreamingLLM (22.2√ó speedup), attention sinks, 4M token capability
- ‚úÖ 2.2.2 Hierarchical Memory - HiAgent (2√ó success rate), 10:1 compression, ACL 2025
- ‚úÖ 2.2.3 Context Pruning - LazyLLM, TokenSelect (23.84√ó speedup), AgentDiet (39.9-59.7% reduction)
- ‚úÖ 2.2.4 KV-Cache Optimization - RocketKV (400√ó compression), FastKV, EpiCache, production frameworks

_Injection Strategies_ (3/4 - üöß IN PROGRESS):
- ‚úÖ 2.3.1 Injection Location - ACE framework (+10.6%), RAT (+13-43%), system/user/assistant placement
- ‚úÖ 2.3.2 Injection Timing - "When to Retrieve" (95%+ accuracy), DeepRAG (+26.4%), 30-50% cost reduction
- ‚úÖ 2.3.3 Injection Format - **NEW!** XML/JSON/Markdown comparison, 15-40% performance impact, RAG formatting strategies
- ‚úÖ 2.3.4 Working Memory Pattern - **NEW!** Analysis of your `server/services/working-memory/` implementation, enhancement recommendations

**Layer 1: Prompt Engineering** (14/14 topics - ‚úÖ COMPLETE):

_Basic Techniques_ (5/5):

-   ‚úÖ 1.1.1 Instruction Design - CLEAR framework, principles, templates
-   ‚úÖ 1.1.2 Few-Shot Learning - Example selection, ordering, RAG integration
-   ‚úÖ 1.1.3 Chain-of-Thought - CoT variants, ReAct connection, self-consistency
-   ‚úÖ 1.1.4 Zero-Shot CoT - Magic phrases, Plan-and-Solve, Chain of Draft
-   ‚úÖ 1.1.5 Self-Consistency - Voting mechanisms, weighted voting, USC

_System Prompts_ (5/5):

-   ‚úÖ 1.2.1 Role Definition - System prompts, personas, agent analysis
-   ‚úÖ 1.2.2 Capabilities Declaration - Tool inventory, boundaries, CMS tools
-   ‚úÖ 1.2.3 Rules & Constraints - Guardrails, safety, quality, operational rules
-   ‚úÖ 1.2.4 Output Format Specification - JSON Schema, structured outputs 2024-2025
-   ‚úÖ 1.2.5 Modular Architecture - Composable prompts, versioning, production patterns

_Prompt Templates_ (4/4): **‚Üê NEWLY COMPLETED**

-   ‚úÖ 1.3.1 Template Engines - Jinja2, Handlebars, Mustache, Liquid, PromptL (2024-2025) **‚Üê NEW**
-   ‚úÖ 1.3.2 Conditional Sections - Control flow, adaptive prompts, context-aware logic **‚Üê NEW**
-   ‚úÖ 1.3.3 Versioning & Caching - Langfuse, PromptLayer, 60-90% cost reduction **‚Üê NEW**
-   ‚úÖ 1.3.4 Reserved for future expansion

**Layer 0: Foundations** (complete):

-   ‚úÖ 0.1.1 LLM Fundamentals - Complete (transformer architecture, attention mechanism)
-   ‚úÖ 0.1.2 Training vs Inference - Complete (cost analysis, fine-tuning, RLHF, optimization)
-   ‚úÖ 0.1.3 Context Windows - Complete (memory patterns, hybrid fetching)
-   ‚úÖ 0.1.4 Sampling Parameters - Complete (temperature, top-p, top-k, agent tuning)
-   ‚úÖ 0.1.5 Model Selection Guide - Complete (GPT-4, Claude, Gemini, Llama, decision frameworks)
-   ‚úÖ 0.2.1 Standard vs Thinking Models - Complete (o1, reasoning, benchmarks)
-   ‚úÖ 0.2.2 Reasoning Models Deep Dive - Complete (o1/o3 architecture, RL training, thinking tokens)
-   ‚úÖ 0.2.3 When to Use Which Model - Complete (decision frameworks, use case matrix, routing)
-   ‚úÖ 0.2.4 Trade-offs - Complete (cost-latency-quality triangle, optimization strategies)
-   ‚úÖ 0.3.1 Tokenization - Complete (BPE, WordPiece, SentencePiece, cost optimization)
-   ‚úÖ 0.3.2 Embedding Models - Complete (OpenAI, SBERT, vector search, LanceDB)
-   ‚úÖ 0.3.3 Vector Similarity - Complete (cosine, dot product, Euclidean, Manhattan)
-   ‚úÖ 0.3.4 Dimensionality Trade-offs - Complete (384 vs 768 vs 1536, PCA, UMAP, curse)

---

## Layer Index

### Layer 0: Foundations (Prerequisites)

**Goal**: Understand LLMs, tokens, embeddings, and model selection

#### 0.1 LLM Fundamentals

-   [0.1.1 What is a Large Language Model?](./kb/0-foundations/0.1.1-llm-intro.md) ‚úÖ
    -   **Status**: Complete - Comprehensive intro to transformers, attention mechanism, training process
    -   **Length**: 600+ lines with 10+ cited sources
    -   **Includes**: Codebase integration examples, evolution timeline, practical recommendations
-   [0.1.2 Training vs Inference](./kb/0-foundations/0.1.2-training-vs-inference.md) ‚úÖ
    -   **Status**: Complete - Comprehensive guide to training vs inference economics
    -   **Length**: 700+ lines with cost analysis, real-world examples
    -   **Includes**: Pretraining, fine-tuning (SFT, LoRA, RLHF), cost optimization, codebase examples
-   [0.1.3 Context Windows & Token Limits](./kb/0-foundations/0.1.3-context-windows.md) ‚úÖ
    -   **Status**: Complete - Deep dive into context management, token optimization
    -   **Length**: 700+ lines with examples from this codebase
    -   **Includes**: Hierarchical memory, working memory, hybrid fetching patterns
-   [0.1.4 Temperature, Top-P, and Sampling](./kb/0-foundations/0.1.4-sampling-parameters.md) ‚úÖ
    -   **Status**: Complete - Master guide to controlling LLM output
    -   **Length**: 650+ lines with API examples, debugging tips
    -   **Includes**: Temperature, top-p, top-k, greedy decoding, penalties, agent configuration
-   [0.1.5 Model Selection Guide](./kb/0-foundations/0.1.5-model-selection.md) ‚úÖ
    -   **Status**: Complete - Comprehensive model comparison and decision frameworks
    -   **Length**: 1100+ lines with benchmarks, pricing, practical recommendations
    -   **Includes**: GPT-4/Claude/Gemini/Llama comparison, cost-performance analysis, your GPT-4o-mini validation, decision matrices

#### 0.2 Thinking vs Non-Thinking Models

-   [0.2.1 Standard Models vs Thinking Models](./kb/0-foundations/0.2.1-standard-models.md) ‚úÖ
    -   **Status**: Complete - Comprehensive comparison of standard vs reasoning models
    -   **Length**: 600+ lines with benchmarks, cost analysis, use case recommendations
    -   **Includes**: o1 architecture, chain-of-thought comparison, practical examples
-   [0.2.2 Reasoning Models Deep Dive](./kb/0-foundations/0.2.2-reasoning-models.md) ‚è≥
-   [0.2.3 When to Use Which](./kb/0-foundations/0.2.3-model-comparison.md) ‚è≥
-   [0.2.4 Trade-offs (Cost, Latency, Capabilities)](./kb/0-foundations/0.2.4-tradeoffs.md) ‚è≥

#### 0.3 Tokens, Embeddings, and Vector Spaces

-   [0.3.1 Tokenization (BPE, WordPiece, SentencePiece)](./kb/0-foundations/0.3.1-tokenization.md) ‚úÖ
    -   **Status**: Complete - Comprehensive guide to subword tokenization algorithms
    -   **Length**: 750+ lines with algorithm comparisons, real-world examples
    -   **Includes**: BPE (GPT), WordPiece (BERT), SentencePiece (T5), cost optimization, codebase integration
-   [0.3.2 Embedding Models & Vector Spaces](./kb/0-foundations/0.3.2-embedding-models.md) ‚úÖ
    -   **Status**: Complete - Deep dive into semantic embeddings for search and RAG
    -   **Length**: 700+ lines with model comparisons, implementation examples
    -   **Includes**: OpenAI text-embedding-3, SBERT, BGE, vector search in your codebase (LanceDB)
-   [0.3.3 Vector Similarity Metrics](./kb/0-foundations/0.3.3-vector-similarity.md) ‚úÖ
    -   **Status**: Complete - Comprehensive guide to measuring embedding similarity
    -   **Length**: 900+ lines with formulas, visualizations, implementations
    -   **Includes**: Cosine similarity, dot product, Euclidean, Manhattan, normalization, LanceDB integration
-   [0.3.4 Dimensionality Trade-offs](./kb/0-foundations/0.3.4-dimensionality.md) ‚úÖ
    -   **Status**: Complete - Deep dive into embedding dimensions and optimization
    -   **Length**: 1000+ lines with comparisons, benchmarks, reduction techniques
    -   **Includes**: 384/768/1536/3072 dims comparison, curse of dimensionality, PCA, UMAP, OpenAI flexible dimensions

---

### Layer 1: Prompt Engineering

**Goal**: Master prompting techniques from single-shot to chain-of-thought

#### 1.1 Basic Prompting Techniques

-   [1.1.1 Single-Shot Prompting](./kb/1-prompts/1.1.1-single-shot.md) ‚è≥
-   [1.1.2 Few-Shot Learning](./kb/1-prompts/1.1.2-few-shot.md) ‚è≥
-   [1.1.3 Chain-of-Thought (CoT)](./kb/1-prompts/1.1.3-chain-of-thought.md) ‚è≥
-   [1.1.4 Zero-Shot CoT](./kb/1-prompts/1.1.4-zero-shot-cot.md) ‚è≥
-   [1.1.5 Self-Consistency](./kb/1-prompts/1.1.5-self-consistency.md) ‚è≥

#### 1.2 System Prompts & Instructions

-   [1.2.1 Role Definition (Identity)](./kb/1-prompts/1.2.1-role-definition.md) ‚è≥
-   [1.2.2 Capabilities Declaration](./kb/1-prompts/1.2.2-capabilities.md) ‚è≥
-   [1.2.3 Rules & Constraints](./kb/1-prompts/1.2.3-rules-constraints.md) ‚è≥
-   [1.2.4 Output Format Specification](./kb/1-prompts/1.2.4-output-format.md) ‚è≥
-   [1.2.5 Modular Prompt Architecture](./kb/1-prompts/1.2.5-modular-architecture.md) ‚è≥
    -   **Codebase Example**: `server/prompts/react.xml`

#### 1.3 Prompt Templates & Variables

-   [1.3.1 Template Engines (Handlebars, Mustache)](./kb/1-prompts/1.3.1-template-engines.md) ‚è≥
-   [1.3.2 Variable Injection](./kb/1-prompts/1.3.2-variable-injection.md) ‚è≥
-   [1.3.3 Conditional Sections](./kb/1-prompts/1.3.3-conditional-sections.md) ‚è≥
-   [1.3.4 Prompt Versioning & Caching](./kb/1-prompts/1.3.4-versioning-caching.md) ‚è≥

---

### Layer 2: Context Engineering

**Goal**: Optimize token usage and manage context effectively

#### 2.1 Token Optimization

-   [2.1.1 Compression Techniques (Summarization)](./kb/2-context/2.1.1-compression.md) ‚è≥
-   [2.1.2 Importance Scoring](./kb/2-context/2.1.2-importance-scoring.md) ‚è≥
-   [2.1.3 Lazy Loading (Fetch on Demand)](./kb/2-context/2.1.3-lazy-loading.md) ‚è≥
-   [2.1.4 Hybrid Content Fetching](./kb/2-context/2.1.4-hybrid-fetching.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (cms_getPage with includeContent flag)

#### 2.2 Context Management Patterns

-   [2.2.1 Sliding Window (Fixed Size)](./kb/2-context/2.2.1-sliding-window.md) ‚è≥
-   [2.2.2 Hierarchical Memory (Subgoal-Based)](./kb/2-context/2.2.2-hierarchical-memory.md) ‚è≥
    -   **Research**: HiAgent (2024)
-   [2.2.3 Context Pruning (Remove Low-Value)](./kb/2-context/2.2.3-context-pruning.md) ‚è≥
-   [2.2.4 KV-Cache Optimization](./kb/2-context/2.2.4-kv-cache.md) ‚è≥
    -   **Research**: Manus.im (2024)

#### 2.3 Context Injection Strategies

-   [2.3.1 Where to Inject (System, User, Assistant)](./kb/2-context/2.3.1-injection-location.md) ‚è≥
-   [2.3.2 Timing (Always vs Conditional)](./kb/2-context/2.3.2-injection-timing.md) ‚è≥
-   [2.3.3 Format (Structured vs Narrative)](./kb/2-context/2.3.3-injection-format.md) ‚è≥
-   [2.3.4 Working Memory Pattern](./kb/2-context/2.3.4-working-memory.md) ‚è≥
    -   **Codebase Example**: `server/services/working-memory/`

---

### Layer 3: Agent Architecture

**Goal**: Build autonomous agents using ReAct pattern with tool calling

#### 3.1 What is an AI Agent?

-   [3.1.1 Definition & Components](./kb/3-agents/3.1.1-agent-definition.md) ‚è≥
-   [3.1.2 Agent Types (Reflexive, Goal-Based, Utility-Based)](./kb/3-agents/3.1.2-agent-types.md) ‚è≥
-   [3.1.3 When to Use Agents vs Single LLM Calls](./kb/3-agents/3.1.3-when-agents.md) ‚è≥

#### 3.2 ReAct Pattern (Reasoning + Acting)

-   [3.2.1 Core Loop: Think ‚Üí Act ‚Üí Observe ‚Üí Repeat](./kb/3-agents/3.2.1-react-loop.md) ‚è≥
    -   **Codebase Example**: `server/prompts/react.xml`
-   [3.2.2 Reasoning Phase (Plan Next Step)](./kb/3-agents/3.2.2-reasoning-phase.md) ‚è≥
-   [3.2.3 Acting Phase (Execute Tool)](./kb/3-agents/3.2.3-acting-phase.md) ‚è≥
-   [3.2.4 Observation Phase (Interpret Result)](./kb/3-agents/3.2.4-observation-phase.md) ‚è≥
-   [3.2.5 Implementation with AI SDK v6](./kb/3-agents/3.2.5-ai-sdk-implementation.md) ‚è≥
    -   **Codebase Example**: `server/agent/orchestrator.ts`

#### 3.3 Tool Calling & Execution

-   [3.3.1 Tool Definition (Zod Schemas, Descriptions)](./kb/3-agents/3.3.1-tool-definition.md) ‚è≥
-   [3.3.2 Tool Registry & Metadata](./kb/3-agents/3.3.2-tool-registry.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (TOOL_METADATA)
-   [3.3.3 Context Injection (experimental_context)](./kb/3-agents/3.3.3-context-injection.md) ‚è≥
-   [3.3.4 Result Validation](./kb/3-agents/3.3.4-result-validation.md) ‚è≥
-   [3.3.5 Tool Composition Patterns](./kb/3-agents/3.3.5-composition.md) ‚è≥

#### 3.4 Loop Control & Convergence

-   [3.4.1 Max Steps Limits](./kb/3-agents/3.4.1-max-steps.md) ‚è≥
-   [3.4.2 Convergence Detection](./kb/3-agents/3.4.2-convergence.md) ‚è≥
    -   **Research**: AgentFlow (Stanford 2024)
-   [3.4.3 Stuck Detection](./kb/3-agents/3.4.3-stuck-detection.md) ‚è≥
-   [3.4.4 Loop State Machine](./kb/3-agents/3.4.4-state-machine.md) ‚è≥
-   [3.4.5 Early Exit Strategies](./kb/3-agents/3.4.5-early-exit.md) ‚è≥

---

### Layer 4: Memory & State

**Goal**: Implement memory systems for context retention and state persistence

#### 4.1 Working Memory (Short-Term)

-   [4.1.1 Working Memory Concept (RAM Analogy)](./kb/4-memory/4.1.1-working-memory-concept.md) ‚è≥
-   [4.1.2 Entity Extraction from Tool Results](./kb/4-memory/4.1.2-entity-extraction.md) ‚è≥
    -   **Codebase Example**: `server/services/working-memory/entity-extractor.ts`
-   [4.1.3 Sliding Window (Recent N Entities)](./kb/4-memory/4.1.3-sliding-window.md) ‚è≥
-   [4.1.4 Reference Resolution ("this page", "that entry")](./kb/4-memory/4.1.4-reference-resolution.md) ‚è≥
-   [4.1.5 Implementation: Universal Working Memory](./kb/4-memory/4.1.5-universal-implementation.md) ‚è≥
    -   **Research**: Mem0, A-MEM, AWS AgentCore

#### 4.2 Subgoal Memory (Medium-Term)

-   [4.2.1 HiAgent Hierarchical Memory](./kb/4-memory/4.2.1-hiagent.md) ‚è≥
    -   **Research**: HiAgent (2024) - 2x success rate, 3.8 fewer steps
-   [4.2.2 Compression Triggers (80% Context Capacity)](./kb/4-memory/4.2.2-compression-triggers.md) ‚è≥
-   [4.2.3 Subgoal Detection Patterns](./kb/4-memory/4.2.3-subgoal-detection.md) ‚è≥
-   [4.2.4 Summarization Strategies](./kb/4-memory/4.2.4-summarization.md) ‚è≥
-   [4.2.5 10:1 Compression Ratios](./kb/4-memory/4.2.5-compression-ratios.md) ‚è≥

#### 4.3 Long-Term Memory (Persistent)

-   [4.3.1 Vector Databases (LanceDB, Pinecone, Weaviate)](./kb/4-memory/4.3.1-vector-databases.md) ‚è≥
    -   **Codebase Example**: `server/services/vector-index.ts`
-   [4.3.2 Semantic Search](./kb/4-memory/4.3.2-semantic-search.md) ‚è≥
-   [4.3.3 Fact Extraction & Storage](./kb/4-memory/4.3.3-fact-extraction.md) ‚è≥
-   [4.3.4 Cross-Session Retrieval](./kb/4-memory/4.3.4-cross-session.md) ‚è≥
-   [4.3.5 When to Use vs Working Memory](./kb/4-memory/4.3.5-when-to-use.md) ‚è≥

#### 4.4 State Persistence & Checkpointing

-   [4.4.1 Why: Crash Recovery, Resume Conversations](./kb/4-memory/4.4.1-why-checkpoint.md) ‚è≥
-   [4.4.2 What to Save (Messages, Phase, Subgoals, Memory)](./kb/4-memory/4.4.2-what-to-save.md) ‚è≥
-   [4.4.3 When to Checkpoint (Every 3 Steps, Phase Transitions, Errors)](./kb/4-memory/4.4.3-when-to-checkpoint.md) ‚è≥
    -   **Codebase Example**: `server/agent/orchestrator.ts` (prepareStep)
-   [4.4.4 How to Resume (Load Checkpoint, Continue Execution)](./kb/4-memory/4.4.4-how-to-resume.md) ‚è≥
-   [4.4.5 Implementation: JSON Serialization, DB Storage](./kb/4-memory/4.4.5-implementation.md) ‚è≥

---

### Layer 5: Retrieval & RAG

**Goal**: Build retrieval-augmented generation systems with vector search

#### 5.1 Vector Search Fundamentals

-   [5.1.1 Embedding Documents](./kb/5-rag/5.1.1-embedding-documents.md) ‚è≥
-   [5.1.2 Similarity Metrics (Cosine, Dot Product, Euclidean)](./kb/5-rag/5.1.2-similarity-metrics.md) ‚è≥
-   [5.1.3 Index Types (Flat, IVF, HNSW)](./kb/5-rag/5.1.3-index-types.md) ‚è≥
-   [5.1.4 Query Strategies](./kb/5-rag/5.1.4-query-strategies.md) ‚è≥
-   [5.1.5 Top-K Selection](./kb/5-rag/5.1.5-top-k-selection.md) ‚è≥

#### 5.2 Chunking Strategies

-   [5.2.1 Fixed-Size Chunks (512 Tokens)](./kb/5-rag/5.2.1-fixed-size.md) ‚è≥
-   [5.2.2 Semantic Chunks (Paragraph, Section)](./kb/5-rag/5.2.2-semantic-chunks.md) ‚è≥
-   [5.2.3 Overlapping Windows](./kb/5-rag/5.2.3-overlapping-windows.md) ‚è≥
-   [5.2.4 Metadata Enrichment](./kb/5-rag/5.2.4-metadata.md) ‚è≥
-   [5.2.5 Chunk Size Trade-offs](./kb/5-rag/5.2.5-tradeoffs.md) ‚è≥

#### 5.3 Hybrid Search

-   [5.3.1 Vector Search (Semantic)](./kb/5-rag/5.3.1-vector-search.md) ‚è≥
-   [5.3.2 Fuzzy Search (Typo Tolerance)](./kb/5-rag/5.3.2-fuzzy-search.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (cms_findResource)
-   [5.3.3 BM25 (Keyword)](./kb/5-rag/5.3.3-bm25.md) ‚è≥
-   [5.3.4 Reranking (Cross-Encoder)](./kb/5-rag/5.3.4-reranking.md) ‚è≥
-   [5.3.5 Fusion Strategies (Weighted, RRF)](./kb/5-rag/5.3.5-fusion.md) ‚è≥

#### 5.4 RAG Patterns

-   [5.4.1 Naive RAG (Retrieve ‚Üí Inject ‚Üí Generate)](./kb/5-rag/5.4.1-naive-rag.md) ‚è≥
-   [5.4.2 Advanced RAG (Query Rewriting, HyDE)](./kb/5-rag/5.4.2-advanced-rag.md) ‚è≥
-   [5.4.3 Agentic RAG (Iterative Retrieval, Self-Reflection)](./kb/5-rag/5.4.3-agentic-rag.md) ‚è≥
-   [5.4.4 Context Injection Optimization](./kb/5-rag/5.4.4-context-optimization.md) ‚è≥
-   [5.4.5 Evaluation Metrics (Precision, Recall, MRR)](./kb/5-rag/5.4.5-evaluation.md) ‚è≥

---

### Layer 6: Planning & Orchestration

**Goal**: Implement planning patterns for complex multi-step tasks

#### 6.1 Plan-and-Execute

-   [6.1.1 Separate Planning from Execution](./kb/6-planning/6.1.1-separation.md) ‚è≥
-   [6.1.2 Generate Alternative Plans](./kb/6-planning/6.1.2-alternatives.md) ‚è≥
    -   **Research**: LangChain (2024) - 40% reduction in dead ends
-   [6.1.3 Feasibility Scoring](./kb/6-planning/6.1.3-feasibility.md) ‚è≥
-   [6.1.4 Fallback Strategies](./kb/6-planning/6.1.4-fallbacks.md) ‚è≥
-   [6.1.5 Implementation Patterns](./kb/6-planning/6.1.5-implementation.md) ‚è≥

#### 6.2 Reflexion (Self-Critique)

-   [6.2.1 Generate ‚Üí Critique ‚Üí Refine Loop](./kb/6-planning/6.2.1-reflexion-loop.md) ‚è≥
    -   **Research**: Reflexion (Shinn et al. 2023) - 20% accuracy improvement
-   [6.2.2 Quality Scoring](./kb/6-planning/6.2.2-quality-scoring.md) ‚è≥
-   [6.2.3 Iteration Limits (2-3 Max)](./kb/6-planning/6.2.3-iteration-limits.md) ‚è≥
-   [6.2.4 Adaptive Reflection (Complexity Heuristic)](./kb/6-planning/6.2.4-adaptive.md) ‚è≥
-   [6.2.5 Research Findings](./kb/6-planning/6.2.5-research.md) ‚è≥

#### 6.3 Tree of Thoughts

-   [6.3.1 Multi-Path Exploration](./kb/6-planning/6.3.1-multi-path.md) ‚è≥
-   [6.3.2 Branching Strategies](./kb/6-planning/6.3.2-branching.md) ‚è≥
-   [6.3.3 Pruning (Dead Ends)](./kb/6-planning/6.3.3-pruning.md) ‚è≥
-   [6.3.4 Best-First Search](./kb/6-planning/6.3.4-best-first.md) ‚è≥
-   [6.3.5 When to Use (Complex Problems)](./kb/6-planning/6.3.5-when-to-use.md) ‚è≥

#### 6.4 Preflight Validation

-   [6.4.1 Check Before Execute](./kb/6-planning/6.4.1-check-before-execute.md) ‚è≥
-   [6.4.2 Resource Existence](./kb/6-planning/6.4.2-resource-existence.md) ‚è≥
-   [6.4.3 Constraint Satisfaction](./kb/6-planning/6.4.3-constraints.md) ‚è≥
-   [6.4.4 Schema Compatibility](./kb/6-planning/6.4.4-schema.md) ‚è≥
-   [6.4.5 Validation Issues ‚Üí Suggestions](./kb/6-planning/6.4.5-suggestions.md) ‚è≥

---

### Layer 7: Error Recovery & Resilience

**Goal**: Build robust agents that handle failures gracefully

#### 7.1 Error Classification

-   [7.1.1 7 Error Types (Validation, Constraint, Not Found, etc.)](./kb/7-errors/7.1.1-error-types.md) ‚è≥
    -   **Research**: SuperAGI (2024) - 40% reduction in dead-end failures
-   [7.1.2 Pattern Matching (SQLite Errors, HTTP Codes)](./kb/7-errors/7.1.2-pattern-matching.md) ‚è≥
-   [7.1.3 LLM-Based Classification (Ambiguous Errors)](./kb/7-errors/7.1.3-llm-classification.md) ‚è≥
-   [7.1.4 Agent-Friendly Observations](./kb/7-errors/7.1.4-observations.md) ‚è≥

#### 7.2 Recovery Strategies

-   [7.2.1 Retry (Transient Errors)](./kb/7-errors/7.2.1-retry.md) ‚è≥
-   [7.2.2 Fallback (Not Found ‚Üí Create Instead)](./kb/7-errors/7.2.2-fallback.md) ‚è≥
-   [7.2.3 Skip (Wait for Recovery)](./kb/7-errors/7.2.3-skip.md) ‚è≥
-   [7.2.4 Escalate (Unrecoverable)](./kb/7-errors/7.2.4-escalate.md) ‚è≥
-   [7.2.5 Strategy Selection by Error Type](./kb/7-errors/7.2.5-selection.md) ‚è≥

#### 7.3 Circuit Breaker Pattern

-   [7.3.1 States: Closed, Open, Half-Open](./kb/7-errors/7.3.1-states.md) ‚è≥
    -   **Research**: Michael T. Nygard - Release It!
-   [7.3.2 Failure Threshold (3 Consecutive)](./kb/7-errors/7.3.2-threshold.md) ‚è≥
-   [7.3.3 Timeout Duration (30s)](./kb/7-errors/7.3.3-timeout.md) ‚è≥
-   [7.3.4 Test Call (Half-Open)](./kb/7-errors/7.3.4-test-call.md) ‚è≥
-   [7.3.5 Per-Tool Circuit Breakers](./kb/7-errors/7.3.5-per-tool.md) ‚è≥

#### 7.4 Retry Strategies

-   [7.4.1 Exponential Backoff (1s, 2s, 4s, 8s)](./kb/7-errors/7.4.1-exponential-backoff.md) ‚è≥
    -   **Codebase Example**: `server/agent/orchestrator.ts` (retry logic with jitter)
-   [7.4.2 Jitter (Avoid Thundering Herd)](./kb/7-errors/7.4.2-jitter.md) ‚è≥
-   [7.4.3 Max Retries (3-5)](./kb/7-errors/7.4.3-max-retries.md) ‚è≥
-   [7.4.4 Budget Tracking](./kb/7-errors/7.4.4-budget.md) ‚è≥
-   [7.4.5 When to Give Up](./kb/7-errors/7.4.5-when-give-up.md) ‚è≥

#### 7.5 Tool Result Validation

-   [7.5.1 Post-Mutation Verification](./kb/7-errors/7.5.1-post-mutation.md) ‚è≥
-   [7.5.2 Expected State Checks](./kb/7-errors/7.5.2-state-checks.md) ‚è≥
-   [7.5.3 Silent Failure Detection (60% of Issues)](./kb/7-errors/7.5.3-silent-failures.md) ‚è≥
-   [7.5.4 Auto-Correction (Retry with Fix)](./kb/7-errors/7.5.4-auto-correction.md) ‚è≥
-   [7.5.5 Validation Cost (~50-100ms per Mutation)](./kb/7-errors/7.5.5-cost.md) ‚è≥

---

### Layer 8: Tool Design Patterns

**Goal**: Design safe, reliable tools with validation and metadata

#### 8.1 Tool Registry & Metadata

-   [8.1.1 Centralized Tool Catalog](./kb/8-tools/8.1.1-catalog.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (ALL_TOOLS, TOOL_METADATA)
-   [8.1.2 Metadata: Category, Risk Level, Approval Flag, Tags](./kb/8-tools/8.1.2-metadata.md) ‚è≥
-   [8.1.3 Dynamic Discovery (Query by Metadata)](./kb/8-tools/8.1.3-discovery.md) ‚è≥
-   [8.1.4 Type-Safe Registry (TypeScript)](./kb/8-tools/8.1.4-type-safety.md) ‚è≥

#### 8.2 Input Validation

-   [8.2.1 Zod Schemas (inputSchema)](./kb/8-tools/8.2.1-zod-schemas.md) ‚è≥
-   [8.2.2 Runtime Validation](./kb/8-tools/8.2.2-runtime.md) ‚è≥
-   [8.2.3 Error Messages](./kb/8-tools/8.2.3-error-messages.md) ‚è≥
-   [8.2.4 Schema Evolution](./kb/8-tools/8.2.4-evolution.md) ‚è≥
-   [8.2.5 AI SDK v6 Integration](./kb/8-tools/8.2.5-ai-sdk.md) ‚è≥

#### 8.3 Context Injection

-   [8.3.1 experimental_context Parameter (Native AI SDK)](./kb/8-tools/8.3.1-experimental-context.md) ‚è≥
    -   **Codebase Example**: All tools in `server/tools/all-tools.ts`
-   [8.3.2 AgentContext Interface](./kb/8-tools/8.3.2-agent-context.md) ‚è≥
-   [8.3.3 Service Access (DB, APIs, etc.)](./kb/8-tools/8.3.3-service-access.md) ‚è≥
-   [8.3.4 Avoid Closures (Anti-Pattern)](./kb/8-tools/8.3.4-avoid-closures.md) ‚è≥
-   [8.3.5 Framework-Native Approach](./kb/8-tools/8.3.5-framework-native.md) ‚è≥

#### 8.4 HTTP Client Tools

-   [8.4.1 Allowlist Pattern (Security)](./kb/8-tools/8.4.1-allowlist.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (http_get, http_post)
-   [8.4.2 GET vs POST Separation](./kb/8-tools/8.4.2-get-post.md) ‚è≥
-   [8.4.3 Header Management](./kb/8-tools/8.4.3-headers.md) ‚è≥
-   [8.4.4 Timeout Configuration](./kb/8-tools/8.4.4-timeout.md) ‚è≥
-   [8.4.5 Error Handling & Result Validation](./kb/8-tools/8.4.5-error-handling.md) ‚è≥

---

### Layer 9: Human-in-the-Loop

**Goal**: Implement approval gates and feedback loops for safety

#### 9.1 Approval Gates (HITL)

-   [9.1.1 When: Destructive Operations, High-Risk Actions](./kb/9-hitl/9.1.1-when.md) ‚è≥
-   [9.1.2 How: needsApproval Flag on Tools](./kb/9-hitl/9.1.2-how.md) ‚è≥
    -   **Codebase Example**: `server/tools/all-tools.ts` (cms_deletePage)
-   [9.1.3 Flow: Pause ‚Üí Show Modal ‚Üí User Decides ‚Üí Resume](./kb/9-hitl/9.1.3-flow.md) ‚è≥
-   [9.1.4 AI SDK v6 Streaming Pattern](./kb/9-hitl/9.1.4-streaming.md) ‚è≥
    -   **Codebase Example**: `server/agent/orchestrator.ts` (streamAgentWithApproval)
-   [9.1.5 Approval Queue (Promise-Based)](./kb/9-hitl/9.1.5-queue.md) ‚è≥
    -   **Codebase Example**: `server/services/approval-queue.ts`

#### 9.2 Feedback Loops

-   [9.2.1 User Corrections](./kb/9-hitl/9.2.1-corrections.md) ‚è≥
-   [9.2.2 Thumbs Up/Down](./kb/9-hitl/9.2.2-thumbs.md) ‚è≥
-   [9.2.3 Regeneration](./kb/9-hitl/9.2.3-regeneration.md) ‚è≥
-   [9.2.4 Fine-Tuning from Feedback](./kb/9-hitl/9.2.4-finetuning.md) ‚è≥
-   [9.2.5 RLHF Patterns](./kb/9-hitl/9.2.5-rlhf.md) ‚è≥

#### 9.3 Adaptive Autonomy

-   [9.3.1 Modes: Off, On-Request, Proactive](./kb/9-hitl/9.3.1-modes.md) ‚è≥
-   [9.3.2 When to Suggest Improvements](./kb/9-hitl/9.3.2-suggestions.md) ‚è≥
-   [9.3.3 Proactivity Tuning (Avoid Annoyance)](./kb/9-hitl/9.3.3-tuning.md) ‚è≥
-   [9.3.4 User Control](./kb/9-hitl/9.3.4-control.md) ‚è≥
-   [9.3.5 Context-Aware Suggestions](./kb/9-hitl/9.3.5-context-aware.md) ‚è≥

---

### Layer 10: Multi-Agent Systems

**Goal**: Coordinate multiple specialized agents

#### 10.1 Orchestrator Pattern

-   [10.1.1 Master Agent Delegates to Specialists](./kb/10-multi-agent/10.1.1-delegation.md) ‚è≥
-   [10.1.2 Intent Classification](./kb/10-multi-agent/10.1.2-intent.md) ‚è≥
-   [10.1.3 Context Transfer Between Agents](./kb/10-multi-agent/10.1.3-context-transfer.md) ‚è≥
-   [10.1.4 Response Assembly](./kb/10-multi-agent/10.1.4-assembly.md) ‚è≥
-   [10.1.5 When to Use (>3 Distinct Responsibilities)](./kb/10-multi-agent/10.1.5-when.md) ‚è≥

#### 10.2 Specialized Sub-Agents

-   [10.2.1 Architect Agent (Planning, Read-Only)](./kb/10-multi-agent/10.2.1-architect.md) ‚è≥
-   [10.2.2 CRUD Agent (Execution, All Tools)](./kb/10-multi-agent/10.2.2-crud.md) ‚è≥
-   [10.2.3 Debug Agent (Error Correction, Limited Writes)](./kb/10-multi-agent/10.2.3-debug.md) ‚è≥
-   [10.2.4 Ask Agent (Inspection, Read-Only)](./kb/10-multi-agent/10.2.4-ask.md) ‚è≥
-   [10.2.5 Sub-Agent Configuration](./kb/10-multi-agent/10.2.5-config.md) ‚è≥

#### 10.3 Agent Communication

-   [10.3.1 Message Passing](./kb/10-multi-agent/10.3.1-message-passing.md) ‚è≥
-   [10.3.2 Shared Context](./kb/10-multi-agent/10.3.2-shared-context.md) ‚è≥
-   [10.3.3 Event-Driven Triggers](./kb/10-multi-agent/10.3.3-events.md) ‚è≥
-   [10.3.4 State Synchronization](./kb/10-multi-agent/10.3.4-sync.md) ‚è≥
-   [10.3.5 Conflict Resolution](./kb/10-multi-agent/10.3.5-conflicts.md) ‚è≥

#### 10.4 Coordination Strategies

-   [10.4.1 Sequential (A ‚Üí B ‚Üí C)](./kb/10-multi-agent/10.4.1-sequential.md) ‚è≥
-   [10.4.2 Parallel (All Agents Simultaneously)](./kb/10-multi-agent/10.4.2-parallel.md) ‚è≥
-   [10.4.3 Hierarchical (Tree Structure)](./kb/10-multi-agent/10.4.3-hierarchical.md) ‚è≥
-   [10.4.4 Peer-to-Peer (Agents Negotiate)](./kb/10-multi-agent/10.4.4-p2p.md) ‚è≥
-   [10.4.5 LangGraph Workflows](./kb/10-multi-agent/10.4.5-langgraph.md) ‚è≥

---

### Layer 11: Production Engineering

**Goal**: Deploy, monitor, and optimize agents in production

#### 11.1 Logging & Observability

-   [11.1.1 Structured Logging (JSON)](./kb/11-production/11.1.1-structured-logging.md) ‚è≥
-   [11.1.2 Log Levels (Debug, Info, Warn, Error)](./kb/11-production/11.1.2-log-levels.md) ‚è≥
-   [11.1.3 Trace IDs (Track Requests)](./kb/11-production/11.1.3-trace-ids.md) ‚è≥
    -   **Codebase Example**: `server/agent/orchestrator.ts` (traceId)
-   [11.1.4 Step IDs (Track Agent Steps)](./kb/11-production/11.1.4-step-ids.md) ‚è≥
-   [11.1.5 Log Aggregation (Datadog, Splunk)](./kb/11-production/11.1.5-aggregation.md) ‚è≥

#### 11.2 Monitoring & Metrics

-   [11.2.1 Token Usage (Input, Output, Total)](./kb/11-production/11.2.1-token-usage.md) ‚è≥
-   [11.2.2 Latency (p50, p95, p99)](./kb/11-production/11.2.2-latency.md) ‚è≥
-   [11.2.3 Cost per Request](./kb/11-production/11.2.3-cost.md) ‚è≥
-   [11.2.4 Success Rate](./kb/11-production/11.2.4-success-rate.md) ‚è≥
-   [11.2.5 Tool Call Distribution & Circuit Breaker Status](./kb/11-production/11.2.5-distribution.md) ‚è≥

#### 11.3 Debugging Techniques

-   [11.3.1 Debug Pane (Real-Time Logs)](./kb/11-production/11.3.1-debug-pane.md) ‚è≥
    -   **Codebase Example**: `app/assistant/_components/debug-pane.tsx`
-   [11.3.2 Replay from Checkpoint](./kb/11-production/11.3.2-replay.md) ‚è≥
-   [11.3.3 Step-by-Step Execution](./kb/11-production/11.3.3-step-by-step.md) ‚è≥
-   [11.3.4 LLM Call Inspection (Prompts, Responses)](./kb/11-production/11.3.4-llm-inspection.md) ‚è≥
-   [11.3.5 State Visualization (State Machine)](./kb/11-production/11.3.5-state-viz.md) ‚è≥

#### 11.4 Cost Optimization

-   [11.4.1 Token Reduction (Compression, Caching)](./kb/11-production/11.4.1-token-reduction.md) ‚è≥
-   [11.4.2 Model Selection (GPT-4 vs 3.5 vs Flash)](./kb/11-production/11.4.2-model-selection.md) ‚è≥
-   [11.4.3 Lazy Loading (Hybrid Fetching)](./kb/11-production/11.4.3-lazy-loading.md) ‚è≥
    -   **Codebase Example**: Sprint 15 (Hybrid Content Fetching)
-   [11.4.4 KV-Cache Optimization (60% Savings)](./kb/11-production/11.4.4-kv-cache.md) ‚è≥
-   [11.4.5 Rate Limiting & Budget Alerts](./kb/11-production/11.4.5-rate-limiting.md) ‚è≥

#### 11.5 Performance Tuning

-   [11.5.1 Concurrent Tool Execution](./kb/11-production/11.5.1-concurrent.md) ‚è≥
-   [11.5.2 Streaming vs Batch](./kb/11-production/11.5.2-streaming.md) ‚è≥
-   [11.5.3 Prompt Size Reduction](./kb/11-production/11.5.3-prompt-size.md) ‚è≥
-   [11.5.4 Tool Execution Time Profiling](./kb/11-production/11.5.4-profiling.md) ‚è≥
-   [11.5.5 Database Query Optimization](./kb/11-production/11.5.5-db-optimization.md) ‚è≥

---

### Layer 12: Cutting-Edge Patterns

**Goal**: Explore advanced and experimental techniques

#### 12.1 Self-Improving Agents

-   [12.1.1 Learning from Mistakes](./kb/12-advanced/12.1.1-learning.md) ‚è≥
-   [12.1.2 Tool Usage Optimization](./kb/12-advanced/12.1.2-tool-optimization.md) ‚è≥
-   [12.1.3 Prompt Evolution](./kb/12-advanced/12.1.3-prompt-evolution.md) ‚è≥
-   [12.1.4 Memory Management Tuning](./kb/12-advanced/12.1.4-memory-tuning.md) ‚è≥
-   [12.1.5 Meta-Learning](./kb/12-advanced/12.1.5-meta-learning.md) ‚è≥

#### 12.2 Code Generation Agents

-   [12.2.1 Cursor, v0, Claude Artifacts](./kb/12-advanced/12.2.1-platforms.md) ‚è≥
-   [12.2.2 Code ‚Üí Test ‚Üí Fix Loop](./kb/12-advanced/12.2.2-code-loop.md) ‚è≥
-   [12.2.3 Incremental Code Writing](./kb/12-advanced/12.2.3-incremental.md) ‚è≥
-   [12.2.4 Multi-File Editing](./kb/12-advanced/12.2.4-multi-file.md) ‚è≥
-   [12.2.5 Safety Patterns (Sandboxing)](./kb/12-advanced/12.2.5-safety.md) ‚è≥

#### 12.3 Agentic Workflows (LangGraph)

-   [12.3.1 Graph-Based Orchestration](./kb/12-advanced/12.3.1-graph.md) ‚è≥
-   [12.3.2 Conditional Edges](./kb/12-advanced/12.3.2-conditional.md) ‚è≥
-   [12.3.3 Subgraphs](./kb/12-advanced/12.3.3-subgraphs.md) ‚è≥
-   [12.3.4 Human-in-the-Loop Nodes](./kb/12-advanced/12.3.4-hitl-nodes.md) ‚è≥
-   [12.3.5 State Persistence](./kb/12-advanced/12.3.5-persistence.md) ‚è≥

#### 12.4 Multi-Modal Agents

-   [12.4.1 Vision + Language (GPT-4V, Gemini)](./kb/12-advanced/12.4.1-vision.md) ‚è≥
-   [12.4.2 Audio Input (Whisper)](./kb/12-advanced/12.4.2-audio.md) ‚è≥
-   [12.4.3 Image Generation (DALL-E)](./kb/12-advanced/12.4.3-image-gen.md) ‚è≥
-   [12.4.4 Document Understanding (PDFs)](./kb/12-advanced/12.4.4-documents.md) ‚è≥
-   [12.4.5 Unified Multi-Modal Tools](./kb/12-advanced/12.4.5-unified.md) ‚è≥

---

## Related Documentation

### Existing Codebase Documentation

-   [AGENTIC_PATTERNS_LIBRARY.md](./AGENTIC_PATTERNS_LIBRARY.md) - 17 production patterns with research citations
-   [DELETION_FLOW_ANALYSIS.md](./DELETION_FLOW_ANALYSIS.md) - Error recovery analysis and HITL patterns
-   [PROGRESS.md](./PROGRESS.md) - Implementation sprints and architecture decisions
-   [IMPLEMENTATION_SPRINTS.md](./IMPLEMENTATION_SPRINTS.md) - Step-by-step build guide
-   [NATIVE_AI_SDK_REFACTOR_PLAN.md](./NATIVE_AI_SDK_REFACTOR_PLAN.md) - AI SDK v6 native patterns

### Research Papers Referenced

-   HiAgent (2024) - Hierarchical Working Memory Management
-   AgentFlow (Stanford 2024) - In-the-Flow Agentic System Optimization
-   Reflexion (Shinn et al. 2023) - Language Agents with Verbal Reinforcement Learning
-   ReAct (Yao et al. 2023) - Synergizing Reasoning and Acting in Language Models
-   Mem0, A-MEM, AWS AgentCore - Working Memory Patterns
-   Anthropic, Manus.im, SuperAGI - Context Engineering & Error Recovery

### External Resources

-   [AI SDK v6 Documentation](https://sdk.vercel.ai/)
-   [LangChain Documentation](https://langchain.com/)
-   [LangGraph](https://langchain-ai.github.io/langgraph/)
-   [OpenRouter Models](https://openrouter.ai/)

---

## Contributing

To add new topics:

1. Research thoroughly (academic papers + production systems)
2. Create topic file in appropriate layer folder (`docs/kb/N-layer/`)
3. Include: Problem, Solution, Benefits, Trade-offs, When to Use, Code Examples, Research Citations
4. Update this TOC with link and mark as ‚úÖ
5. Cross-reference with existing codebase patterns

---

## Changelog

**2025-11-16**: Knowledge base scaffolding created (Option 3: Layer-Based Architecture)

---

**Next Steps**:

1. Create `docs/kb/` directory structure (0-foundations/, 1-prompts/, etc.)
2. Start writing Layer 0 topics (LLM Fundamentals)
3. Progressively fill in layers with deep research
4. Link codebase examples throughout
5. Update progress tracker as topics complete

**Estimated Completion**: 60-80 hours for all 72 topics
