# 3.2.1 Core Loop: Think → Act → Observe → Repeat

**Goal**: Understand the fundamental cycle that powers autonomous AI agents.

The ReAct (Reasoning and Acting) pattern is built on a simple yet powerful loop: **Think → Act → Observe**. This iterative process allows an agent to break down complex problems, interact with its environment (via tools), and dynamically adjust its course based on new information.

-   **Think**: The LLM analyzes the current goal and all available information (user query, conversation history, previous observations) to formulate a plan. It decides whether it has enough information to answer directly or if it needs to use a tool. This is the "reasoning" part of the loop.
-   **Act**: If the agent decides to use a tool, it generates the precise tool call, including the tool's name and the required parameters. This is the "acting" part.
-   **Observe**: The agent executes the tool call and receives an observation—the output or result from the tool. This new information is added to the agent's working memory, feeding into the next "Think" step.

This cycle repeats until the agent determines it has successfully achieved the user's goal and can provide a final answer.

---

## Benefits of the ReAct Loop

-   **Dynamic Problem Solving**: Instead of a fixed, pre-determined chain of actions, the agent can adapt its plan based on the results it observes.
-   **Tool Use**: Enables the agent to break out of the LLM's knowledge limitations and interact with the outside world (e.g., search the web, query a database, read a file).
-   **Error Correction**: If a tool fails or returns an unexpected result, the agent can observe the error and attempt a different approach in the next loop.
-   **Transparency**: The "Think" step, often exposed as a "thought" or reasoning log, provides a clear, step-by-step view into the agent's decision-making process, which is invaluable for debugging.

---

## Implementation with LangChain.js and Vercel AI SDK

In our stack, the ReAct loop is primarily orchestrated on the server-side within our Next.js API routes, leveraging the power of **LangChain.js** for agent logic and **Vercel AI SDK** for frontend streaming and UI integration.

This mirrors the pattern found in `server/agent/orchestrator.ts`.

### Conceptual Code Example

Here is a practical, step-by-step implementation of a ReAct agent in a Next.js API route. This example uses `langchain` for the agent runtime and `zod` for robust tool input validation, a best practice seen in our existing tool definitions (`server/tools/all-tools.ts`).

```typescript
// In: /app/api/agent/route.ts (or a similar server-side file)

import { NextRequest, NextResponse } from 'next/server';
import { ChatOpenAI } from '@langchain/openai';
import { AgentExecutor, createReactAgent } from 'langchain/agents';
import { pull } from 'langchain/hub';
import { DynamicTool } from '@langchain/core/tools';
import { z } from 'zod';

/**
 * 1. DEFINE TOOLS (The "Act" part)
 * These are the actions the agent can perform. Each tool has:
 * - A clear `name`.
 * - A `description` that tells the LLM *when* to use it.
 * - A `schema` (using Zod) for type-safe input validation.
 * - A `func` that contains the actual implementation.
 */
const searchTool = new DynamicTool({
  name: 'web_search',
  description: 'Useful for finding current information or facts on the internet.',
  schema: z.object({
    query: z.string().describe('The search query to execute.'),
  }),
  func: async ({ query }: { query: string }) => {
    console.log(`[Tool] Executing web search for: "${query}"`);
    // In a real application, this would call an external search API.
    // For example, using the Brave Search API or a Google Search connector.
    return `Mock search result for "${query}": The capital of France is Paris.`;
  },
});

const calculatorTool = new DynamicTool({
  name: 'calculator',
  description: 'Useful for performing mathematical calculations.',
  schema: z.object({
    expression: z.string().describe('The mathematical expression to evaluate.'),
  }),
  func: async ({ expression }: { expression: string }) => {
    console.log(`[Tool] Calculating: ${expression}`);
    try {
      // WARNING: eval() is unsafe in production. Use a dedicated math library like `mathjs`.
      const result = eval(expression);
      return `The result of "${expression}" is ${result}.`;
    } catch (error) {
      return `Error: Could not evaluate the expression. Invalid format.`;
    }
  },
});

const tools = [searchTool, calculatorTool];

/**
 * 2. INITIALIZE THE CORE AGENT COMPONENTS
 */

// The LLM that will power the agent's "Think" step.
const llm = new ChatOpenAI({
  modelName: 'gpt-4o-mini', // Cost-effective and capable
  temperature: 0,
});

// Pull a battle-tested ReAct prompt template from LangChain Hub.
// This prompt structures the Think/Act/Observe flow for the LLM.
const promptPromise = pull<any>('hwchase17/react');

/**
 * 3. CREATE THE AGENT EXECUTOR
 * The AgentExecutor is the runtime that wraps the agent and tools,
 * responsible for running the "Think -> Act -> Observe" loop until completion.
 */
async function getAgentExecutor() {
    const prompt = await promptPromise;
    const agent = createReactAgent({ llm, tools, prompt });
    return new AgentExecutor({
        agent,
        tools,
        verbose: true, // Logs the full loop to the console for debugging
        handleParsingErrors: true, // Gracefully handles LLM output errors
    });
}

const agentExecutorPromise = getAgentExecutor();


/**
 * 4. DEFINE THE API ROUTE
 * This route receives user input and invokes the agent loop.
 */
export async function POST(req: NextRequest) {
  try {
    const { message } = await req.json();

    if (!message) {
      return NextResponse.json({ error: 'Message is required' }, { status: 400 });
    }

    const agentExecutor = await agentExecutorPromise;

    // Invoke the agent loop with the user's message.
    const result = await agentExecutor.invoke({
      input: message,
    });

    // The `result.output` contains the agent's final answer.
    return NextResponse.json({ output: result.output });

  } catch (error) {
    console.error('[Agent Error]', error);
    return NextResponse.json({ error: 'Agent failed to process request.' }, { status: 500 });
  }
}
```

### How the Loop Executes

When a request hits the `POST` endpoint with `{ "message": "What is the square root of the capital of France's population?" }`:

1.  **Think 1**: The agent receives the input. It thinks: "The user is asking a multi-step question. First, I need to find the population of the capital of France. I need to use the `web_search` tool for that."
2.  **Act 1**: The agent generates a tool call: `web_search(query="population of Paris")`.
3.  **Observe 1**: The `searchTool` is executed. It returns the observation: "Mock search result for 'population of Paris': The population of Paris is approximately 2.1 million."
4.  **Think 2**: The agent receives the observation. It thinks: "Okay, the population is 2.1 million. Now I need to find the square root of this number. I should use the `calculator` tool."
5.  **Act 2**: The agent generates a tool call: `calculator(expression="Math.sqrt(2100000)")`.
6.  **Observe 2**: The `calculatorTool` is executed. It returns the observation: "The result of 'Math.sqrt(2100000)' is 1449.137674618944."
7.  **Think 3**: The agent receives the new observation. It thinks: "I have the final answer. I can now respond to the user."
8.  **Final Answer**: The agent synthesizes the final response: "The square root of the population of Paris (approximately 2.1 million) is about 1449.14." The loop terminates, and this answer is returned in the API response.

This example demonstrates how the agent autonomously uses tools in sequence, with each step of the loop informing the next, to solve a problem that a simple LLM call could not.