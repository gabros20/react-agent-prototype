# 3.1.3 - When to Use Agents vs Single LLM Calls

## Overview

One of the most critical architectural decisions when building AI systems is choosing between **autonomous agents** and **single LLM calls**. This decision impacts cost, latency, complexity, and success rates. While agents offer powerful capabilities for complex, multi-step tasks, they introduce overhead that may be unnecessary for simpler use cases.

This guide provides a **decision framework** grounded in 2024-2025 production research, helping you choose the right approach for your specific needs.

**Key Research Findings (2024-2025)**:

- **Start simple**: 70% of use cases can be solved with well-crafted prompts + RAG (Anthropic, Dec 2024)
- **Cost impact**: Agents incur 2-5× higher costs due to multiple LLM calls and reasoning overhead
- **Latency impact**: AgentX achieves competitive latency with direct calls but requires optimization (Aug 2025)
- **Success rate**: Agents excel at complex tasks (96-100% in AlfWorld) but overkill for linear workflows
- **ROI barrier**: 60% of agent projects fail due to poor Agentic ROI assessment (May 2025)
- **Framework overhead**: 40% of developers abandon LangChain due to abstraction complexity (2024-2025)

**Date Verified**: November 19, 2025

---

## Decision Framework

### The Three-Tier Decision Tree

```
┌─────────────────────────────────────────────────────────┐
│  TIER 1: Can this be solved with a single prompt?      │
│  ├─ YES → Use Single LLM Call                          │
│  └─ NO → Continue to Tier 2                            │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│  TIER 2: Can this be solved with a fixed workflow?     │
│  ├─ YES → Use Workflow (predefined steps)              │
│  └─ NO → Continue to Tier 3                            │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│  TIER 3: Does this require dynamic decision-making?    │
│  ├─ YES → Use Agent (ReAct, Plan-and-Execute)          │
│  └─ NO → Reconsider requirements                       │
└─────────────────────────────────────────────────────────┘
```

**Golden Rule**: Start with the simplest solution (Tier 1). Only increase complexity when the simpler tier demonstrably fails.

---

## When to Use Single LLM Calls

### Characteristics of Single-Call Tasks

**Use single LLM calls when**:

1. **The task is well-defined** with clear inputs and outputs
2. **No external information** is needed beyond the prompt
3. **Latency is critical** (real-time chat, live components)
4. **Cost constraints** are tight (high-volume operations)
5. **The process is linear** with no branching decisions

### Examples: Single LLM Call Excellence

#### 1. Text Classification
```typescript
// ✅ GOOD: Single call with structured output
const classifyMessage = async (message: string) => {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      category: z.enum(['support', 'sales', 'billing', 'other']),
      urgency: z.enum(['low', 'medium', 'high']),
      sentiment: z.enum(['positive', 'neutral', 'negative']),
    }),
    prompt: `Classify this customer message: "${message}"`,
  });
  return object;
};

// Cost: ~$0.0001 per call (150 input + 50 output tokens)
// Latency: 200-400ms
// Success rate: 95%+ with good examples
```

#### 2. Content Generation
```typescript
// ✅ GOOD: Single call for blog post generation
const generateBlogPost = async (topic: string, keywords: string[]) => {
  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Write a 500-word blog post about "${topic}". 
             Include keywords: ${keywords.join(', ')}.
             Use an engaging, professional tone.`,
  });
  return text;
};

// Cost: ~$0.015 per call (500 input + 600 output tokens)
// Latency: 3-5 seconds
// Quality: High with good prompts
```

#### 3. Data Extraction
```typescript
// ✅ GOOD: Single call for structured data extraction
const extractEntities = async (text: string) => {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      people: z.array(z.string()),
      organizations: z.array(z.string()),
      locations: z.array(z.string()),
      dates: z.array(z.string()),
    }),
    prompt: `Extract all entities from this text: "${text}"`,
  });
  return object;
};

// Cost: ~$0.0002 per call
// Latency: 300-500ms
// Accuracy: 90%+ with clear schemas
```

#### 4. Simple Q&A with RAG
```typescript
// ✅ GOOD: Single call with retrieval context
const answerQuestion = async (question: string) => {
  // Retrieve relevant context (vector search)
  const context = await vectorDb.search(question, { limit: 3 });
  
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: `Context: ${context.join('\n\n')}
             
             Question: ${question}
             
             Answer based only on the provided context.`,
  });
  return text;
};

// Cost: ~$0.0003 per call (vector search + LLM)
// Latency: 400-700ms
// Accuracy: 85%+ with good retrieval
```

### Performance Characteristics

| Metric              | Single LLM Call | Agent (ReAct)    | Difference     |
| ------------------- | --------------- | ---------------- | -------------- |
| **Cost per request**| $0.0001-0.015   | $0.005-0.20      | **5-20× more** |
| **Latency (p95)**   | 200-5000ms      | 3-30 seconds     | **10-15× more**|
| **Success rate**    | 85-95%          | 70-100%          | Task-dependent |
| **Predictability**  | High            | Medium           | -              |
| **Debugging**       | Easy            | Complex          | -              |

**Key Insight (Anthropic, Dec 2024)**: "For many applications, a well-crafted prompt with retrieval and examples is sufficient. Don't reach for agents until you've exhausted simpler approaches."

---

## When to Use Workflows (Predefined Paths)

### Characteristics of Workflow Tasks

**Use workflows when**:

1. **The steps are known** and can be defined upfront
2. **Branching is predictable** (if-else logic suffices)
3. **Consistency is critical** (same process every time)
4. **Debugging is important** (clear execution trace)
5. **Human approval gates** are needed at specific steps

### Examples: Workflow Excellence

#### 1. Order Processing Pipeline
```typescript
// ✅ GOOD: Fixed workflow with conditional logic
const processOrder = async (orderId: string) => {
  // Step 1: Validate order
  const order = await validateOrder(orderId);
  if (!order.valid) {
    return { status: 'invalid', reason: order.reason };
  }
  
  // Step 2: Check inventory
  const inventory = await checkInventory(order.items);
  if (!inventory.available) {
    return { status: 'out_of_stock', items: inventory.missing };
  }
  
  // Step 3: Process payment
  const payment = await processPayment(order.total);
  if (!payment.success) {
    return { status: 'payment_failed', error: payment.error };
  }
  
  // Step 4: Generate shipping label
  const shipping = await generateShippingLabel(order);
  
  // Step 5: Send confirmation
  await sendConfirmationEmail(order.customer, shipping.trackingNumber);
  
  return { status: 'completed', trackingNumber: shipping.trackingNumber };
};

// Cost: $0.001-0.01 (mostly API calls, minimal LLM usage)
// Latency: 1-5 seconds (predictable)
// Success rate: 99%+ (deterministic logic)
```

#### 2. Document Review with Approval
```typescript
// ✅ GOOD: Workflow with human-in-the-loop
const reviewDocument = async (docId: string, reviewerId: string) => {
  // Step 1: Extract content
  const doc = await fetchDocument(docId);
  
  // Step 2: AI analysis
  const { object: analysis } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      compliance: z.object({
        passed: z.boolean(),
        issues: z.array(z.string()),
      }),
      readability: z.object({
        score: z.number(),
        suggestions: z.array(z.string()),
      }),
      sentiment: z.enum(['positive', 'neutral', 'negative']),
    }),
    prompt: `Analyze this document for compliance and readability: ${doc.content}`,
  });
  
  // Step 3: Save analysis
  await saveAnalysis(docId, analysis);
  
  // Step 4: Human approval (if issues found)
  if (analysis.compliance.issues.length > 0) {
    const approval = await requestApproval(reviewerId, {
      document: doc,
      issues: analysis.compliance.issues,
    });
    
    if (!approval.approved) {
      return { status: 'rejected', reason: approval.reason };
    }
  }
  
  // Step 5: Finalize
  await markAsApproved(docId);
  return { status: 'approved', analysis };
};

// Cost: $0.05-0.10 per document
// Latency: 5-30 seconds (+ human time if needed)
// Success rate: 95%+ (predictable path)
```

#### 3. Customer Support Routing
```typescript
// ✅ GOOD: Workflow with deterministic routing
const routeTicket = async (ticketId: string) => {
  // Step 1: Classify ticket
  const ticket = await fetchTicket(ticketId);
  const { object: classification } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      category: z.enum(['billing', 'technical', 'account', 'other']),
      urgency: z.enum(['low', 'medium', 'high', 'critical']),
      requiresSpecialist: z.boolean(),
    }),
    prompt: `Classify this support ticket: ${ticket.content}`,
  });
  
  // Step 2: Route based on classification
  if (classification.category === 'billing') {
    return await routeToBilling(ticketId, classification);
  } else if (classification.category === 'technical' && classification.requiresSpecialist) {
    return await routeToTechnicalSpecialist(ticketId, classification);
  } else if (classification.urgency === 'critical') {
    return await escalateToManager(ticketId, classification);
  } else {
    return await routeToGeneralSupport(ticketId, classification);
  }
};

// Cost: $0.0002 per ticket (classification only)
// Latency: 300-500ms
// Success rate: 92%+ (deterministic routing)
```

### Workflow vs Agent: Trade-offs

| Aspect              | Workflow                     | Agent                        |
| ------------------- | ---------------------------- | ---------------------------- |
| **Flexibility**     | Low (fixed paths)            | High (dynamic decisions)     |
| **Cost**            | Low ($0.001-0.10)            | Medium-High ($0.01-1.00)     |
| **Latency**         | Predictable (1-10s)          | Variable (3-60s)             |
| **Debugging**       | Easy (clear execution trace) | Hard (non-deterministic)     |
| **Maintenance**     | Easy (explicit code)         | Medium (prompt engineering)  |
| **When to use**     | Known steps, fixed logic     | Unknown steps, adaptive logic|

**Key Insight (Toward Data Science, Jun 2025)**: "Workflows are boring but reliable. Agents are exciting but unpredictable. Choose workflows when predictability matters more than flexibility."

---

## When to Use Agents (ReAct, Plan-and-Execute)

### Characteristics of Agent Tasks

**Use agents when**:

1. **The solution path is unknown** upfront
2. **Dynamic decision-making** is required based on intermediate results
3. **Exploration is needed** (research, discovery, analysis)
4. **Flexibility is critical** (adapting to changing conditions)
5. **Multi-tool coordination** is complex

### Examples: Agent Excellence

#### 1. Research Assistant
```typescript
// ✅ GOOD: Agent for open-ended research
const researchAgent = createAgent({
  model: openai('gpt-4o'),
  systemPrompt: `You are a research assistant. Given a research question, 
                 gather information from multiple sources, synthesize findings, 
                 and provide a comprehensive answer with citations.`,
  tools: {
    web_search: tool({
      description: 'Search the web for information',
      parameters: z.object({ query: z.string() }),
      execute: async ({ query }) => searchWeb(query),
    }),
    fetch_url: tool({
      description: 'Fetch content from a URL',
      parameters: z.object({ url: z.string() }),
      execute: async ({ url }) => fetchUrl(url),
    }),
    search_papers: tool({
      description: 'Search academic papers',
      parameters: z.object({ query: z.string() }),
      execute: async ({ query }) => searchPapers(query),
    }),
  },
});

// Example execution:
// Question: "What are the latest advances in quantum computing?"
// Agent workflow (dynamic):
// 1. THINK: Need to search recent papers and news
// 2. ACT: web_search("quantum computing advances 2025")
// 3. OBSERVE: Found 3 relevant articles
// 4. THINK: Need more technical details
// 5. ACT: search_papers("quantum computing 2025")
// 6. OBSERVE: Found 5 recent papers
// 7. THINK: Should fetch full content of top paper
// 8. ACT: fetch_url("https://arxiv.org/...")
// 9. OBSERVE: Retrieved paper content
// 10. THINK: Have enough information to synthesize
// 11. FINISH: Generate comprehensive answer with citations

// Cost: $0.15-0.50 per research query (5-10 tool calls)
// Latency: 10-30 seconds
// Quality: High (comprehensive, cited)
```

#### 2. CMS Content Management (Your System)
```typescript
// ✅ GOOD: Agent for complex CMS operations
const cmsAgent = createAgent({
  model: openai('gpt-4o'),
  systemPrompt: `You are a CMS agent. Use ReAct pattern to complete tasks.`,
  tools: {
    cms_getPage: tool({
      description: 'Get page details by ID or slug',
      parameters: z.object({
        identifier: z.string(),
        fetchMode: z.enum(['summary', 'full']).default('summary'),
      }),
      execute: async ({ identifier, fetchMode }, context) => {
        return await context.cmsService.getPage(identifier, fetchMode);
      },
    }),
    cms_updatePage: tool({
      description: 'Update page content or metadata',
      parameters: z.object({
        pageId: z.string(),
        updates: z.object({
          title: z.string().optional(),
          content: z.string().optional(),
          metadata: z.record(z.any()).optional(),
        }),
      }),
      execute: async ({ pageId, updates }, context) => {
        return await context.cmsService.updatePage(pageId, updates);
      },
    }),
    cms_findResource: tool({
      description: 'Search for pages/resources',
      parameters: z.object({
        query: z.string(),
        filters: z.record(z.any()).optional(),
      }),
      execute: async ({ query, filters }, context) => {
        return await context.cmsService.search(query, filters);
      },
    }),
  },
});

// Example execution:
// Request: "Add a hero section to the about page with our mission statement"
// Agent workflow (dynamic):
// 1. THINK: Need to find the about page
// 2. ACT: cms_findResource({ query: "about", filters: { type: "page" } })
// 3. OBSERVE: Found page ID: "page-123"
// 4. THINK: Need to fetch current content
// 5. ACT: cms_getPage({ identifier: "page-123", fetchMode: "full" })
// 6. OBSERVE: Current content has no hero section
// 7. THINK: Should add hero section with mission statement
// 8. ACT: cms_updatePage({ 
//       pageId: "page-123", 
//       updates: { 
//         content: "<hero>Our mission...</hero>\n" + currentContent 
//       } 
//    })
// 9. OBSERVE: Update successful
// 10. FINISH: "Hero section added successfully"

// Cost: $0.02-0.10 per task (3-8 tool calls)
// Latency: 5-15 seconds
// Success rate: 85-95% (your system's performance)
```

#### 3. Debugging Assistant
```typescript
// ✅ GOOD: Agent for error diagnosis and fixing
const debugAgent = createAgent({
  model: openai('gpt-4o'),
  systemPrompt: `You are a debugging assistant. Analyze errors, 
                 investigate code, and suggest fixes.`,
  tools: {
    read_file: tool({
      description: 'Read file contents',
      parameters: z.object({ path: z.string() }),
      execute: async ({ path }) => readFile(path),
    }),
    search_code: tool({
      description: 'Search codebase for pattern',
      parameters: z.object({ pattern: z.string() }),
      execute: async ({ pattern }) => searchCode(pattern),
    }),
    run_tests: tool({
      description: 'Run test suite',
      parameters: z.object({ 
        testPath: z.string().optional(),
        filter: z.string().optional(),
      }),
      execute: async ({ testPath, filter }) => runTests(testPath, filter),
    }),
    suggest_fix: tool({
      description: 'Generate code fix suggestion',
      parameters: z.object({
        file: z.string(),
        error: z.string(),
        context: z.string(),
      }),
      execute: async ({ file, error, context }) => 
        generateFix(file, error, context),
    }),
  },
});

// Example execution:
// Error: "TypeError: Cannot read property 'map' of undefined at UserList.tsx:23"
// Agent workflow (dynamic):
// 1. THINK: Need to read the file with error
// 2. ACT: read_file({ path: "UserList.tsx" })
// 3. OBSERVE: Line 23 is: users.map(user => ...)
// 4. THINK: 'users' is undefined, need to find where it's defined
// 5. ACT: search_code({ pattern: "const users" })
// 6. OBSERVE: Found 3 definitions, one in UserList.tsx:15
// 7. THINK: Line 15 shows: const users = fetchUsers()
// 8. THINK: fetchUsers might return null/undefined
// 9. ACT: suggest_fix({ 
//       file: "UserList.tsx",
//       error: "users is undefined",
//       context: "Line 15: const users = fetchUsers()" 
//    })
// 10. OBSERVE: Suggested fix: Add null check and default value
// 11. THINK: Should verify fix works
// 12. ACT: run_tests({ testPath: "UserList.test.tsx" })
// 13. OBSERVE: Tests pass with suggested fix
// 14. FINISH: "Error fixed. Add: const users = fetchUsers() || []"

// Cost: $0.10-0.30 per debugging session (6-12 tool calls)
// Latency: 15-45 seconds
// Success rate: 70-85% (requires iteration)
```

### Performance Characteristics

| Metric              | Simple Task | Complex Task | Multi-Step Task |
| ------------------- | ----------- | ------------ | --------------- |
| **Tool calls**      | 1-2         | 3-8          | 8-20            |
| **Cost per request**| $0.01-0.05  | $0.05-0.20   | $0.20-1.00      |
| **Latency (p95)**   | 3-8s        | 8-20s        | 20-60s          |
| **Success rate**    | 90-95%      | 80-90%       | 70-85%          |

**Key Insight (AgentX paper, Aug 2025)**: "AgentX achieves competitive latency and 14.9% higher success rates than ReAct through in-the-flow optimization, but requires careful design."

---

## Cost-Benefit Analysis

### Total Cost of Ownership (TCO)

#### Single LLM Call TCO
```
Direct costs:
- API calls: $0.0001-0.015 per request
- Infrastructure: Minimal (stateless)

Indirect costs:
- Development: Low (simple prompts)
- Maintenance: Low (stable prompts)
- Debugging: Low (straightforward)

TOTAL: $0.0001-0.02 per request + minimal overhead
```

#### Workflow TCO
```
Direct costs:
- API calls: $0.001-0.10 per execution
- Infrastructure: Medium (state management)

Indirect costs:
- Development: Medium (explicit code)
- Maintenance: Medium (code changes)
- Debugging: Low (clear traces)

TOTAL: $0.001-0.15 per execution + medium overhead
```

#### Agent TCO
```
Direct costs:
- API calls: $0.01-1.00 per task (5-20 calls)
- Infrastructure: High (state, memory, checkpointing)

Indirect costs:
- Development: High (prompt engineering, tools)
- Maintenance: High (prompt drift, tool updates)
- Debugging: High (non-deterministic behavior)

TOTAL: $0.01-1.50 per task + high overhead
```

### ROI Calculation Framework

**From "The Real Barrier to LLM Agent Usability is Agentic ROI" (May 2025)**:

```
Agentic ROI = (Value Generated - Total Cost) / Total Cost

Value Generated = (Time Saved × Hourly Rate) + Quality Improvements
Total Cost = Direct Costs + Indirect Costs + Opportunity Cost
```

#### Example: Customer Support Automation

**Scenario**: 10,000 support tickets/month

**Option 1: Human-only**
- Cost: 10,000 tickets × 15 min × $25/hr = $62,500/month
- Quality: High (90% satisfaction)
- Speed: Medium (24-hour response time)

**Option 2: Single LLM Call (Classification + Routing)**
- Direct cost: 10,000 × $0.0002 = $2/month
- Human handling (complex): 3,000 × 15 min × $25/hr = $18,750/month
- Infrastructure: $500/month
- TOTAL: $19,252/month
- **ROI: 68% cost reduction**
- Quality: High (88% satisfaction, -2%)
- Speed: Fast (1-hour response time)

**Option 3: Agent (Full Automation)**
- Direct cost: 10,000 × $0.05 = $500/month
- Human handling (escalations): 1,000 × 20 min × $25/hr = $8,333/month
- Infrastructure: $2,000/month
- Development/maintenance: $5,000/month
- TOTAL: $15,833/month
- **ROI: 75% cost reduction**
- Quality: Medium (82% satisfaction, -8%)
- Speed: Very fast (immediate response)

**Decision**:
- **Best ROI**: Option 2 (Single LLM + Workflow) - simpler, almost as effective
- **Best Speed**: Option 3 (Agent) - but higher complexity
- **Best Quality**: Option 1 (Human) - but not scalable

**Winner**: Option 2 for most organizations (68% savings, minimal complexity)

---

## Latency Considerations

### Latency Comparison

| Approach           | Best Case | Average | p95    | p99    |
| ------------------ | --------- | ------- | ------ | ------ |
| **Single call**    | 200ms     | 800ms   | 2s     | 5s     |
| **Workflow**       | 1s        | 3s      | 8s     | 15s    |
| **Agent (simple)** | 3s        | 8s      | 20s    | 45s    |
| **Agent (complex)**| 10s       | 30s     | 60s    | 120s   |

### Latency Optimization Strategies

#### For Single Calls
```typescript
// ✅ GOOD: Use streaming for perceived speed
const generateResponse = async (prompt: string) => {
  const { textStream } = await streamText({
    model: openai('gpt-4o-mini'),
    prompt,
  });
  
  for await (const chunk of textStream) {
    process.stdout.write(chunk); // Stream to user immediately
  }
};

// Perceived latency: 200-500ms (first token)
// Total latency: 2-5s (full response)
// User experience: Much better than waiting 5s
```

#### For Workflows
```typescript
// ✅ GOOD: Parallelize independent steps
const processDocument = async (docId: string) => {
  const doc = await fetchDocument(docId);
  
  // Run analysis tasks in parallel
  const [compliance, readability, sentiment] = await Promise.all([
    analyzeCompliance(doc.content),
    analyzeReadability(doc.content),
    analyzeSentiment(doc.content),
  ]);
  
  return { compliance, readability, sentiment };
};

// Sequential: 3 × 2s = 6s
// Parallel: max(2s, 2s, 2s) = 2s
// Speedup: 3×
```

#### For Agents
```typescript
// ✅ GOOD: Use lightweight models for reasoning
const agentWithHybridModels = createAgent({
  model: openai('gpt-4o-mini'), // Fast, cheap for reasoning
  tools: {
    complex_analysis: tool({
      description: 'Perform deep analysis (uses GPT-4o)',
      parameters: z.object({ content: z.string() }),
      execute: async ({ content }) => {
        // Use expensive model only for complex operations
        return await generateText({
          model: openai('gpt-4o'), // Slower but smarter
          prompt: `Deep analysis: ${content}`,
        });
      },
    }),
  },
});

// Reasoning steps: GPT-4o-mini (200-400ms each)
// Complex operations: GPT-4o (2-5s each)
// Total: 3-8s instead of 10-20s with GPT-4o everywhere
```

**Key Insight (AgentX paper, Aug 2025)**: "AgentX achieves 4.7× throughput improvement through batch query processing and GPU optimization, making agents viable for production latency requirements."

---

## Framework Considerations

### When NOT to Use Frameworks

**From "Why we no longer use LangChain" (Jan 2025, Octomind)**:

**Problems with high-level abstractions**:
1. **Complexity**: Multiple layers of abstraction make debugging hard
2. **Rigidity**: Difficult to customize for unique use cases
3. **Churn**: Fast-evolving landscape makes frameworks outdated quickly
4. **Over-engineering**: Simple tasks become complex

**Better approach**: Modular building blocks

```typescript
// ❌ BAD: Over-abstraction with framework
import { ConversationalRetrievalQAChain } from 'langchain/chains';

const chain = ConversationalRetrievalQAChain.fromLLM(
  llm,
  retriever,
  { returnSourceDocuments: true }
);
const result = await chain.call({ question, chat_history });

// ✅ GOOD: Direct implementation with Vercel AI SDK
const answerQuestion = async (question: string, chatHistory: Message[]) => {
  // Retrieve context
  const context = await vectorDb.search(question, { limit: 3 });
  
  // Generate answer
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    messages: [
      ...chatHistory,
      {
        role: 'user',
        content: `Context: ${context.join('\n\n')}\n\nQuestion: ${question}`,
      },
    ],
  });
  
  return { answer: text, sources: context };
};

// Benefit: 
// - Clear control flow
// - Easy to debug
// - Customizable
// - No framework lock-in
```

### When Frameworks Add Value

**Use frameworks when**:
1. **Rapid prototyping** (fast exploration)
2. **Standard patterns** (your use case matches framework's design)
3. **Team experience** (team already knows the framework)
4. **Community support** (active ecosystem with plugins)

**Examples of good framework usage**:

#### LangGraph for Complex Workflows
```typescript
// ✅ GOOD: LangGraph for state machine workflows
import { StateGraph } from "@langchain/langgraph";

const workflow = new StateGraph({
  channels: {
    messages: { value: (x, y) => x.concat(y) },
    currentStep: { value: (x, y) => y },
  },
})
  .addNode("analyze", analyzeNode)
  .addNode("plan", planNode)
  .addNode("execute", executeNode)
  .addNode("verify", verifyNode)
  .addConditionalEdges("verify", shouldRetry, {
    retry: "execute",
    done: END,
  });

// Benefit: Visual workflow, built-in state management
```

#### Vercel AI SDK for Production Agents
```typescript
// ✅ GOOD: Vercel AI SDK for production-ready agents
import { createAgent } from 'ai';
import { openai } from '@ai-sdk/openai';

const agent = createAgent({
  model: openai('gpt-4o'),
  tools: {
    // Tools defined inline with experimental_context
  },
  maxSteps: 10,
});

// Benefit:
// - Native streaming support
// - Built-in tool calling
// - Context injection (experimental_context)
// - Framework-native patterns (your system uses this)
```

---

## Decision Matrix

### Quick Reference Table

| Factor                    | Single LLM | Workflow | Agent |
| ------------------------- | ---------- | -------- | ----- |
| **Task complexity**       | Low        | Medium   | High  |
| **Steps required**        | 1          | 2-5      | 5-20  |
| **Decision points**       | None       | Few      | Many  |
| **Cost per request**      | $0.0001    | $0.01    | $0.10 |
| **Latency**               | 200ms-2s   | 2-10s    | 10-60s|
| **Development time**      | Hours      | Days     | Weeks |
| **Debugging difficulty**  | Easy       | Medium   | Hard  |
| **Predictability**        | High       | High     | Low   |
| **Best for**              | Simple Q&A | Pipelines| Research|

### Real-World Use Case Mapping

| Use Case                          | Recommendation   | Reasoning                                    |
| --------------------------------- | ---------------- | -------------------------------------------- |
| **Chatbot (FAQ)**                 | Single LLM + RAG | Fixed knowledge base, fast responses         |
| **Customer support routing**      | Workflow         | Classification + deterministic routing       |
| **Code review automation**        | Workflow         | Fixed checks (linting, security, style)      |
| **Research assistant**            | Agent            | Open-ended exploration, multi-source synthesis|
| **CMS content management**        | Agent            | Dynamic decisions, multi-step operations     |
| **Email classification**          | Single LLM       | Simple categorization, high volume           |
| **Invoice processing**            | Workflow         | Extract → Validate → Store (fixed pipeline)  |
| **Debugging assistant**           | Agent            | Iterative investigation, adaptive strategies |
| **Content generation (blog)**     | Single LLM       | Self-contained task, no external data        |
| **Data pipeline orchestration**   | Workflow         | Defined steps, error handling at checkpoints |
| **Personalized recommendations**  | Single LLM + RAG | User data + product catalog → suggestions    |
| **Multi-agent collaboration**     | Agent            | Complex coordination, multiple specialists   |

---

## Anti-Patterns to Avoid

### 1. Using Agents for Simple Tasks

```typescript
// ❌ BAD: Agent overkill for simple classification
const classifyWithAgent = createAgent({
  model: openai('gpt-4o'),
  tools: {
    classify: tool({
      description: 'Classify message',
      parameters: z.object({ message: z.string() }),
      execute: async ({ message }) => {
        // Just calling one tool... why use an agent?
        return classifyMessage(message);
      },
    }),
  },
});

// Cost: $0.01-0.05 (agent overhead)
// Latency: 3-8s (agent loop)

// ✅ GOOD: Direct LLM call
const classifyDirect = async (message: string) => {
  return await generateObject({
    model: openai('gpt-4o-mini'),
    schema: classificationSchema,
    prompt: `Classify: ${message}`,
  });
};

// Cost: $0.0001 (100× cheaper)
// Latency: 200-500ms (15× faster)
```

### 2. Using Single Calls for Multi-Step Tasks

```typescript
// ❌ BAD: Trying to do everything in one prompt
const analyzeAndFixBug = async (error: string) => {
  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Given this error: "${error}"
             1. Read the relevant files
             2. Identify the root cause
             3. Suggest a fix
             4. Write test cases
             
             Do all of this and give me the complete solution.`,
  });
  // Problem: Model can't actually read files or run tests!
  return text;
};

// ✅ GOOD: Agent with actual file access
const debugAgent = createAgent({
  model: openai('gpt-4o'),
  tools: {
    read_file: tool({ /* ... */ }),
    search_code: tool({ /* ... */ }),
    run_tests: tool({ /* ... */ }),
  },
});
```

### 3. Using Workflows for Unknown Paths

```typescript
// ❌ BAD: Trying to predict all branches
const handleCustomerRequest = async (request: string) => {
  if (request.includes('refund')) {
    return processRefund(request);
  } else if (request.includes('exchange')) {
    return processExchange(request);
  } else if (request.includes('complaint')) {
    return processComplaint(request);
  } else {
    // What about: "I'm unhappy with my order"?
    // Doesn't match any keyword but needs routing!
    return "Sorry, I don't understand";
  }
};

// ✅ GOOD: Agent for dynamic routing
const supportAgent = createAgent({
  model: openai('gpt-4o'),
  tools: {
    process_refund: tool({ /* ... */ }),
    process_exchange: tool({ /* ... */ }),
    process_complaint: tool({ /* ... */ }),
    gather_info: tool({ /* ... */ }),
  },
});
// Agent can understand intent and choose appropriate action
```

---

## Migration Paths

### Graduating from Single Call to Agent

**Signals you need an agent**:

1. **Prompt is becoming huge** (>2000 tokens)
2. **You're trying to simulate tools** in the prompt
3. **Results are inconsistent** due to lack of real data
4. **You're hitting context limits** with all the information
5. **Users ask follow-up questions** that require memory

**Migration strategy**:

```typescript
// Phase 1: Single call with RAG
const answerV1 = async (question: string) => {
  const context = await search(question);
  return generateText({ prompt: `${context}\n${question}` });
};

// Phase 2: Add working memory (still single call)
const answerV2 = async (question: string, history: Message[]) => {
  const context = await search(question);
  return generateText({ 
    messages: [...history, { role: 'user', content: question }],
    systemPrompt: `Context: ${context}`,
  });
};

// Phase 3: Convert to agent when tools are needed
const answerV3 = createAgent({
  model: openai('gpt-4o'),
  tools: {
    search: tool({ /* ... */ }),
    fetch_detail: tool({ /* ... */ }),
    // Add more tools as needed
  },
});
```

### Downgrading from Agent to Workflow

**Signals you can simplify**:

1. **Agent always uses same sequence** of tools
2. **No branching decisions** are being made
3. **Latency is a complaint** from users
4. **Cost is higher** than expected for simple tasks

**Migration strategy**:

```typescript
// Before: Agent with 3 tool calls in fixed order
const processDocumentAgent = createAgent({
  tools: {
    extract: tool({ /* ... */ }),
    validate: tool({ /* ... */ }),
    store: tool({ /* ... */ }),
  },
});

// After: Workflow with explicit steps
const processDocumentWorkflow = async (doc: Document) => {
  const extracted = await extractData(doc);
  const validated = await validateData(extracted);
  const stored = await storeData(validated);
  return stored;
};

// Result: 3× faster, 5× cheaper, more reliable
```

---

## Best Practices Checklist

### For Single LLM Calls

- [ ] Task is well-defined with clear inputs/outputs
- [ ] No external data needed beyond prompt
- [ ] Latency requirement < 5 seconds
- [ ] Cost constraint < $0.01 per request
- [ ] Prompt is < 2000 tokens
- [ ] Success rate > 90% with good prompt
- [ ] No branching logic required

### For Workflows

- [ ] Steps are known upfront
- [ ] Branching is predictable (if-else logic)
- [ ] Each step can be independently tested
- [ ] Error handling is at step boundaries
- [ ] Execution trace is important for debugging
- [ ] Human approval gates are at specific steps
- [ ] Cost budget is $0.01-0.20 per execution

### For Agents

- [ ] Solution path is unknown upfront
- [ ] Dynamic decision-making is required
- [ ] Multiple tools must be coordinated
- [ ] Exploration/discovery is needed
- [ ] Flexibility > predictability
- [ ] Cost budget is $0.10-1.00 per task
- [ ] Latency tolerance is 10-60 seconds
- [ ] Team has agent debugging experience

---

## Research Citations

1. **Anthropic** - "Building Effective Agents" (Dec 2024)  
   https://www.anthropic.com/research/building-effective-agents

2. **AgentX Paper** - "Towards Orchestrating Robust Agentic Workflow Patterns with FaaS" (Aug 2025)  
   https://arxiv.org/html/2509.07595v1

3. **AgentFlow Paper** - "In-the-Flow Agentic System Optimization" (Stanford, Oct 2025)  
   https://arxiv.org/abs/2510.05592

4. **Agentic ROI Paper** - "The Real Barrier to LLM Agent Usability is Agentic ROI" (May 2025)  
   https://arxiv.org/abs/2505.17767

5. **Cost-of-Pass Framework** - "An Economic Framework for Evaluating Language Models" (Mar 2025)  
   https://arxiv.org/html/2504.13359v1

6. **Toward Data Science** - "A Developer's Guide to Building Scalable AI: Workflows vs Agents" (Jun 2025)  
   https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents

7. **Octomind** - "Why we no longer use LangChain for building our AI agents" (Jan 2025)  
   https://octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents

8. **LLM Watch** - "When to Use AI Agents: A Simple Flowchart" (Mar 2025)  
   https://www.llmwatch.com/p/when-to-use-ai-agents-a-simple-flowchart

9. **Comet** - "AI Agents: The Definitive Guide to Engineering for Production Scale" (Nov 2025)  
   https://www.comet.com/site/blog/ai-agents/

10. **LinkedIn** - "When (and When Not) to Use Agents" by Gurrpreet Sinngh (Dec 2024)  
    https://www.linkedin.com/pulse/when-use-agents-gurrpreet-sinngh-jkcsc

---

## Summary

### The Three-Tier Decision Framework

1. **Start simple**: Single LLM call (70% of use cases)
2. **Add structure**: Workflow for predictable multi-step (20% of use cases)
3. **Go dynamic**: Agent for complex, adaptive tasks (10% of use cases)

### Key Metrics

| Metric     | Single LLM | Workflow  | Agent     |
| ---------- | ---------- | --------- | --------- |
| **Cost**   | $0.0001    | $0.01     | $0.10     |
| **Latency**| 500ms      | 5s        | 20s       |
| **Success**| 90%        | 95%       | 85%       |
| **ROI**    | Best       | Good      | Moderate  |

### Golden Rules

1. **Prefer simplicity**: Start with the simplest solution that could work
2. **Measure ROI**: Calculate total cost of ownership, not just API costs
3. **Prototype quickly**: Test with single calls before building agents
4. **Optimize progressively**: Add complexity only when simpler approaches fail
5. **Choose frameworks wisely**: Direct implementation often beats high-level abstractions
6. **Monitor production**: Track cost, latency, and success rates continuously

**Final Insight**: Most production systems use a **hybrid approach** - single calls for classification, workflows for execution, and agents only for truly complex tasks that require dynamic decision-making.

---

**Next Steps**:

- Read [3.2.1 - ReAct Loop](./3.2.1-react-loop.md) to understand agent implementation
- Read [3.3.1 - Tool Definition](./3.3.1-tool-definition.md) to design effective tools
- Read [11.4.1 - Cost Optimization](../11-production/11.4.1-token-reduction.md) for production strategies
