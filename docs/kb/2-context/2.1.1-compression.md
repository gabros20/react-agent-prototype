# 2.1.1 Token Optimization: Compression Techniques

## Overview

Context compression reduces token usage by distilling large amounts of information into concise, semantically-preserved summaries. Instead of feeding entire documents or conversation histories into the LLM, you compress them to their essential meaning—maintaining accuracy while dramatically reducing costs and latency. This is essential for production systems where context windows are expensive and performance matters.

**Key Insight** (2024-2025): Context compression isn't about losing information—it's about removing redundancy while preserving semantic relevance.

**Current Date**: November 17, 2025

## Why Context Compression Matters

### The Token Economics Problem

**Without Compression**:
```
Long Document: 10,000 tokens
Cost per 1M tokens (GPT-4o-mini input): $0.15
Single request cost: $0.0015
100K requests/month: $150

Long conversation history: 5,000 tokens
Multi-turn dialogue (10 turns): 50,000 tokens
Cost per conversation: $0.0075
10K conversations/month: $75
```

**With 80% Compression**:
```
Compressed Document: 2,000 tokens (80% reduction)
Cost per request: $0.0003
100K requests/month: $30 (80% savings!)

Compressed history: 1,000 tokens per turn
Multi-turn dialogue: 10,000 tokens
Cost per conversation: $0.0015
10K conversations/month: $15 (80% savings!)
```

**Annual Savings**: $2,100 → From just these two use cases

### The Context Rot Problem (2024-2025 Research)

**Key Finding**: LLMs experience "context rot" — performance degrades as context length increases, even within their maximum window.

**Research Data**:
- **Effective context**: Models reliably process only 30-60% of their maximum window
- **Attention degradation**: Beyond ~50K tokens, attention mechanisms lose focus
- **"Lost in the middle"**: Information in the middle of long contexts is often ignored
- **Needle in haystack**: Simple retrieval tasks fail at high token counts

**Solution**: Compression focuses on keeping context within the "effective window" while maximizing information density.

## Compression Strategies

### Strategy 1: Extractive Summarization

**Extract key sentences/passages without rewriting**:

**Approach**: Score each sentence by importance, extract top N%.

**Implementation**:
```typescript
import { embed } from './embeddings';
import { cosineSimilarity } from './similarity';

class ExtractiveSummarizer {
  async summarize(text: string, compressionRatio: number = 0.3): Promise<string> {
    // 1. Split into sentences
    const sentences = this.splitIntoSentences(text);
    
    // 2. Embed each sentence
    const embeddings = await Promise.all(
      sentences.map(s => embed(s))
    );
    
    // 3. Compute document centroid (average embedding)
    const centroid = this.computeCentroid(embeddings);
    
    // 4. Score sentences by similarity to centroid
    const scores = embeddings.map(emb => 
      cosineSimilarity(emb, centroid)
    );
    
    // 5. Extract top sentences
    const targetCount = Math.ceil(sentences.length * compressionRatio);
    const topIndices = scores
      .map((score, idx) => ({ score, idx }))
      .sort((a, b) => b.score - a.score)
      .slice(0, targetCount)
      .map(x => x.idx)
      .sort((a, b) => a - b); // Preserve original order
    
    // 6. Reconstruct summary
    return topIndices.map(idx => sentences[idx]).join(' ');
  }
  
  private splitIntoSentences(text: string): string[] {
    return text.match(/[^.!?]+[.!?]+/g) || [text];
  }
  
  private computeCentroid(embeddings: number[][]): number[] {
    const dim = embeddings[0].length;
    const centroid = new Array(dim).fill(0);
    
    for (const emb of embeddings) {
      for (let i = 0; i < dim; i++) {
        centroid[i] += emb[i];
      }
    }
    
    return centroid.map(v => v / embeddings.length);
  }
}

// Usage
const summarizer = new ExtractiveSummarizer();

const longDocument = `[10,000 token document]`;
const compressed = await summarizer.summarize(longDocument, 0.2); // 80% compression

console.log(`Original: ${estimateTokens(longDocument)} tokens`);
console.log(`Compressed: ${estimateTokens(compressed)} tokens`);
// Original: 10,000 tokens
// Compressed: 2,000 tokens
```

**Pros**:
- ✅ Preserves original wording (factual accuracy)
- ✅ Fast (no LLM generation required)
- ✅ Predictable compression ratio

**Cons**:
- ❌ May include incomplete sentences
- ❌ Less fluent than abstractive summaries
- ❌ Can lose narrative flow

**Best For**: Technical documents, logs, code, legal texts where exact wording matters.

### Strategy 2: Abstractive Summarization

**Rewrite content in compressed form using LLM**:

**Approach**: Use smaller, fast model to summarize before passing to main agent.

**Implementation**:
```typescript
class AbstractiveSummarizer {
  async summarize(
    text: string, 
    targetLength: number = 500, // target token count
    model: string = 'gpt-4o-mini'
  ): Promise<string> {
    const prompt = `Summarize the following text in approximately ${targetLength} tokens. 
Preserve all key facts, decisions, and context. Be concise but complete.

TEXT:
${text}

SUMMARY:`;

    const response = await generateText({
      model,
      prompt,
      maxTokens: targetLength * 1.5, // Allow some flexibility
      temperature: 0.3 // Lower temp for factual accuracy
    });
    
    return response.text;
  }
  
  // Multi-level summarization for very long documents
  async hierarchicalSummarize(
    text: string,
    chunkSize: number = 4000,
    targetLength: number = 500
  ): Promise<string> {
    // 1. Split into chunks
    const chunks = this.chunkText(text, chunkSize);
    
    // 2. Summarize each chunk
    const chunkSummaries = await Promise.all(
      chunks.map(chunk => this.summarize(chunk, targetLength / chunks.length))
    );
    
    // 3. Combine and summarize again (recursive)
    const combined = chunkSummaries.join('\n\n');
    
    if (estimateTokens(combined) > targetLength * 2) {
      return this.hierarchicalSummarize(combined, chunkSize, targetLength);
    }
    
    return this.summarize(combined, targetLength);
  }
  
  private chunkText(text: string, maxTokens: number): string[] {
    // Simple chunking by paragraph
    const paragraphs = text.split('\n\n');
    const chunks: string[] = [];
    let currentChunk = '';
    
    for (const para of paragraphs) {
      if (estimateTokens(currentChunk + para) > maxTokens) {
        if (currentChunk) chunks.push(currentChunk);
        currentChunk = para;
      } else {
        currentChunk += (currentChunk ? '\n\n' : '') + para;
      }
    }
    
    if (currentChunk) chunks.push(currentChunk);
    return chunks;
  }
}

// Usage
const summarizer = new AbstractiveSummarizer();

// Simple summarization
const summary = await summarizer.summarize(longDoc, 500);

// Hierarchical for very long documents
const veryLongDoc = `[100,000 token document]`;
const hierarchicalSummary = await summarizer.hierarchicalSummarize(veryLongDoc, 4000, 1000);

console.log(`Original: 100,000 tokens → Compressed: 1,000 tokens (99% reduction)`);
```

**Pros**:
- ✅ Fluent, natural language output
- ✅ Can restructure for clarity
- ✅ Better narrative flow

**Cons**:
- ❌ Risk of hallucination
- ❌ Slower (requires LLM generation)
- ❌ Additional cost

**Best For**: Conversation summaries, articles, reports where fluency matters.

### Strategy 3: Selective Compression (Query-Aware)

**Compress based on relevance to current query**:

**Approach**: Only include information relevant to the user's question.

**Implementation**:
```typescript
class QueryAwareSummarizer {
  async compressForQuery(
    document: string,
    query: string,
    targetTokens: number = 1000
  ): Promise<string> {
    // 1. Embed query
    const queryEmbedding = await embed(query);
    
    // 2. Split document into chunks
    const chunks = this.chunkDocument(document, 200); // ~200 token chunks
    
    // 3. Embed each chunk
    const chunkEmbeddings = await Promise.all(
      chunks.map(chunk => embed(chunk.text))
    );
    
    // 4. Score chunks by relevance to query
    const scores = chunkEmbeddings.map(emb => 
      cosineSimilarity(queryEmbedding, emb)
    );
    
    // 5. Select top chunks until target token count
    const rankedChunks = chunks
      .map((chunk, idx) => ({ ...chunk, score: scores[idx] }))
      .sort((a, b) => b.score - a.score);
    
    let selectedChunks: typeof rankedChunks = [];
    let currentTokens = 0;
    
    for (const chunk of rankedChunks) {
      if (currentTokens + chunk.tokens > targetTokens) break;
      selectedChunks.push(chunk);
      currentTokens += chunk.tokens;
    }
    
    // 6. Re-order by original position
    selectedChunks.sort((a, b) => a.position - b.position);
    
    return selectedChunks.map(c => c.text).join('\n\n');
  }
  
  private chunkDocument(text: string, chunkTokens: number) {
    const paragraphs = text.split('\n\n');
    const chunks: Array<{ text: string; tokens: number; position: number }> = [];
    let currentChunk = '';
    let currentTokens = 0;
    let position = 0;
    
    for (const para of paragraphs) {
      const paraTokens = estimateTokens(para);
      
      if (currentTokens + paraTokens > chunkTokens) {
        if (currentChunk) {
          chunks.push({ text: currentChunk, tokens: currentTokens, position: position++ });
        }
        currentChunk = para;
        currentTokens = paraTokens;
      } else {
        currentChunk += (currentChunk ? '\n\n' : '') + para;
        currentTokens += paraTokens;
      }
    }
    
    if (currentChunk) {
      chunks.push({ text: currentChunk, tokens: currentTokens, position: position++ });
    }
    
    return chunks;
  }
}

// Usage
const compressor = new QueryAwareSummarizer();

const longDoc = `[Technical documentation: 20,000 tokens]`;
const userQuery = "How do I configure SSL certificates?";

const relevantContext = await compressor.compressForQuery(
  longDoc,
  userQuery,
  1000 // Only 1,000 tokens of most relevant content
);

const response = await generateText({
  prompt: `Context: ${relevantContext}\n\nQuestion: ${userQuery}\n\nAnswer:`,
  model: 'gpt-4o-mini'
});
```

**Pros**:
- ✅ Maximum relevance to current task
- ✅ No information loss for current query
- ✅ Highest compression ratios

**Cons**:
- ❌ May miss important context
- ❌ Requires query upfront
- ❌ Can't reuse across queries

**Best For**: RAG systems, documentation search, QA over large corpuses.

### Strategy 4: Conversation History Compression

**Compress multi-turn dialogues while preserving continuity**:

**Approach**: Summarize older turns, keep recent turns verbatim.

**Implementation**:
```typescript
interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: number;
}

class ConversationCompressor {
  async compress(
    messages: Message[],
    maxTokens: number = 4000,
    recentTurnsToKeep: number = 3
  ): Promise<Message[]> {
    // 1. Always keep system message
    const systemMessage = messages.find(m => m.role === 'system');
    const conversationMessages = messages.filter(m => m.role !== 'system');
    
    // 2. Keep recent N turns verbatim
    const recentMessages = conversationMessages.slice(-recentTurnsToKeep * 2); // user + assistant
    
    // 3. Compress older turns
    const olderMessages = conversationMessages.slice(0, -recentTurnsToKeep * 2);
    
    if (olderMessages.length === 0) {
      return messages; // All messages are recent
    }
    
    // 4. Create summary of older turns
    const olderConversation = olderMessages
      .map(m => `${m.role}: ${m.content}`)
      .join('\n\n');
    
    const summary = await generateText({
      model: 'gpt-4o-mini',
      prompt: `Summarize this conversation history. Preserve key facts, decisions, and context.

${olderConversation}

Summary:`,
      maxTokens: Math.min(1000, maxTokens - this.estimateMessages(recentMessages)),
      temperature: 0.3
    });
    
    // 5. Create compressed summary message
    const summaryMessage: Message = {
      role: 'system',
      content: `Previous conversation summary: ${summary.text}`,
      timestamp: olderMessages[olderMessages.length - 1].timestamp
    };
    
    // 6. Combine: system + summary + recent
    return [
      ...(systemMessage ? [systemMessage] : []),
      summaryMessage,
      ...recentMessages
    ];
  }
  
  private estimateMessages(messages: Message[]): number {
    return messages.reduce((sum, m) => sum + estimateTokens(m.content), 0);
  }
  
  // Progressive compression: compress when threshold reached
  async compressIfNeeded(
    messages: Message[],
    maxTokens: number = 4000
  ): Promise<Message[]> {
    const currentTokens = this.estimateMessages(messages);
    
    if (currentTokens <= maxTokens) {
      return messages; // No compression needed
    }
    
    console.log(`Compression needed: ${currentTokens} > ${maxTokens} tokens`);
    return this.compress(messages, maxTokens);
  }
}

// Usage
const compressor = new ConversationCompressor();

let conversation: Message[] = [
  { role: 'system', content: 'You are a helpful assistant.', timestamp: Date.now() },
  { role: 'user', content: 'Tell me about React hooks', timestamp: Date.now() },
  { role: 'assistant', content: '[Long explanation about hooks]', timestamp: Date.now() },
  // ... 20 more turns
];

// Compress when needed
conversation = await compressor.compressIfNeeded(conversation, 4000);

// Add new message
conversation.push({
  role: 'user',
  content: 'How do I use useEffect?',
  timestamp: Date.now()
});

console.log(`Compressed conversation: ${compressor.estimateMessages(conversation)} tokens`);
```

**Result**:
```
Original: 15 turns × 500 tokens = 7,500 tokens
Compressed: Summary (800 tokens) + 3 recent turns (1,500 tokens) = 2,300 tokens
Savings: 69% reduction
```

**Pros**:
- ✅ Preserves recent context fully
- ✅ Maintains conversation continuity
- ✅ Predictable compression

**Cons**:
- ❌ May lose nuanced earlier context
- ❌ Requires periodic re-compression
- ❌ Summary introduces latency

**Best For**: Multi-turn chatbots, customer support, extended dialogues.

### Strategy 5: Token Budget Allocation

**Allocate fixed token budgets to different context components**:

**Approach**: Divide context window into buckets, compress each independently.

**Implementation**:
```typescript
interface ContextBudget {
  systemPrompt: number;
  workingMemory: number;
  conversationHistory: number;
  externalKnowledge: number;
  examples: number;
  buffer: number; // For response
}

class BudgetedContextManager {
  private budget: ContextBudget = {
    systemPrompt: 1000,
    workingMemory: 500,
    conversationHistory: 1500,
    externalKnowledge: 1000,
    examples: 500,
    buffer: 1500 // Reserve for response
  };
  
  async buildContext(components: {
    systemPrompt: string;
    workingMemory: string;
    conversation: Message[];
    knowledge: string[];
    examples: string[];
  }): Promise<string> {
    // 1. System prompt (highest priority, don't compress)
    let systemPrompt = components.systemPrompt;
    if (estimateTokens(systemPrompt) > this.budget.systemPrompt) {
      console.warn('System prompt exceeds budget');
      systemPrompt = this.truncate(systemPrompt, this.budget.systemPrompt);
    }
    
    // 2. Working memory (high priority)
    let workingMemory = components.workingMemory;
    if (estimateTokens(workingMemory) > this.budget.workingMemory) {
      workingMemory = await this.compress(workingMemory, this.budget.workingMemory);
    }
    
    // 3. Conversation history (medium priority)
    const conversationCompressor = new ConversationCompressor();
    const compressedConversation = await conversationCompressor.compress(
      components.conversation,
      this.budget.conversationHistory
    );
    
    // 4. External knowledge (compress based on relevance)
    const queryAwareCompressor = new QueryAwareSummarizer();
    const lastUserMessage = components.conversation
      .filter(m => m.role === 'user')
      .slice(-1)[0]?.content || '';
    
    const relevantKnowledge = await queryAwareCompressor.compressForQuery(
      components.knowledge.join('\n\n'),
      lastUserMessage,
      this.budget.externalKnowledge
    );
    
    // 5. Examples (lowest priority, can skip if over budget)
    const examplesText = components.examples.join('\n\n');
    let includedExamples = examplesText;
    if (estimateTokens(examplesText) > this.budget.examples) {
      includedExamples = this.truncate(examplesText, this.budget.examples);
    }
    
    // 6. Build final context
    return `${systemPrompt}

WORKING MEMORY:
${workingMemory}

RECENT CONVERSATION:
${compressedConversation.map(m => `${m.role}: ${m.content}`).join('\n')}

RELEVANT KNOWLEDGE:
${relevantKnowledge}

EXAMPLES:
${includedExamples}`;
  }
  
  private truncate(text: string, maxTokens: number): string {
    // Simple truncation (better: use sentence boundaries)
    const words = text.split(' ');
    const targetWords = Math.floor(words.length * (maxTokens / estimateTokens(text)));
    return words.slice(0, targetWords).join(' ') + '...';
  }
  
  private async compress(text: string, targetTokens: number): Promise<string> {
    const summarizer = new AbstractiveSummarizer();
    return summarizer.summarize(text, targetTokens);
  }
}

// Usage
const contextManager = new BudgetedContextManager();

const finalContext = await contextManager.buildContext({
  systemPrompt: reactSystemPrompt,
  workingMemory: workingMemory.serialize(),
  conversation: conversationHistory,
  knowledge: retrievedDocuments,
  examples: fewShotExamples
});

console.log(`Total context: ${estimateTokens(finalContext)} tokens`);
// Guaranteed to fit within budget!
```

**Pros**:
- ✅ Predictable token usage
- ✅ Priority-based allocation
- ✅ Never exceeds budget

**Cons**:
- ❌ May over-compress important info
- ❌ Fixed budgets may not suit all cases
- ❌ Requires tuning

**Best For**: Production systems with strict cost/latency requirements.

## Production Best Practices

### 1. Compression Metrics

```typescript
class CompressionAnalytics {
  trackCompression(original: string, compressed: string, strategy: string) {
    const originalTokens = estimateTokens(original);
    const compressedTokens = estimateTokens(compressed);
    const ratio = compressedTokens / originalTokens;
    
    metrics.histogram('context.compression.ratio', ratio, {
      strategy
    });
    
    metrics.histogram('context.tokens.saved', originalTokens - compressedTokens, {
      strategy
    });
    
    const costSaved = (originalTokens - compressedTokens) * COST_PER_TOKEN;
    metrics.histogram('context.cost.saved', costSaved, {
      strategy
    });
  }
}
```

### 2. Quality Validation

```typescript
// Validate compression doesn't lose critical info
async function validateCompression(
  original: string,
  compressed: string,
  queries: string[]
): Promise<{ accuracy: number }> {
  let correctAnswers = 0;
  
  for (const query of queries) {
    const originalAnswer = await answerFromContext(original, query);
    const compressedAnswer = await answerFromContext(compressed, query);
    
    const similar = await semanticSimilarity(originalAnswer, compressedAnswer);
    if (similar > 0.9) correctAnswers++;
  }
  
  return { accuracy: correctAnswers / queries.length };
}
```

### 3. Adaptive Compression

```typescript
// Adjust compression based on task complexity
function selectCompressionStrategy(task: Task): CompressionStrategy {
  if (task.requiresFactualAccuracy) {
    return new ExtractiveSummarizer(); // Preserve exact wording
  } else if (task.requiresFluency) {
    return new AbstractiveSummarizer(); // Natural language
  } else {
    return new QueryAwareSummarizer(); // Maximum compression
  }
}
```

## Key Takeaways

**What is Context Compression**:
- Reduce token usage while preserving semantic meaning
- Essential for cost optimization and performance
- Multiple strategies for different use cases

**Why It Matters** (2024-2025):
- **Cost**: 60-90% token reduction = 60-90% cost savings
- **Performance**: Shorter context = faster inference
- **Context Rot**: Keep within effective window (30-60% of max)
- **Attention**: Compressed context improves focus

**Compression Strategies**:
1. **Extractive**: Extract key sentences (fast, factual)
2. **Abstractive**: LLM-generated summaries (fluent, slower)
3. **Query-Aware**: Relevance-based filtering (maximum compression)
4. **Conversation**: Summarize old turns, keep recent (continuity)
5. **Budget Allocation**: Fixed token budgets per component (predictable)

**Your Codebase**:
- Implement compression for working memory when it grows large
- Compress conversation history beyond recent N turns
- Query-aware compression for RAG retrieval

**Implementation**:
```typescript
// 1. Select strategy
const compressor = new ConversationCompressor();

// 2. Compress context
const compressed = await compressor.compress(messages, 4000);

// 3. Track metrics
analytics.trackCompression(original, compressed, 'conversation');

// 4. Use compressed context
const response = await generateText({ prompt: compressed });
```

## Navigation

- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 2.1.2 Importance Scoring](./2.1.2-importance-scoring.md)

---

*Part of Layer 2: Context Engineering - Optimizing token usage through compression*
