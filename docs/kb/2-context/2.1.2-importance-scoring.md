# 2.1.2 Token Optimization: Importance Scoring

## Overview

Importance scoring assigns relevance weights to different pieces of information, enabling intelligent prioritization of what to include in the LLM's context. Instead of treating all context equally, you rank information by its relevance to the current task—keeping high-value content, discarding low-value noise. This ensures the limited context window is filled with the most impactful information.

**Key Insight** (2024-2025): Not all context is equally valuable. Importance scoring ensures you spend tokens on what matters most.

**Current Date**: November 17, 2025

## Why Importance Scoring Matters

### The Context Relevance Problem

**Without Importance Scoring** (everything included equally):
```typescript
const context = [
  recentConversation,    // Highly relevant
  userProfile,           // Moderately relevant
  systemLogs,            // Low relevance
  historicalData,        // Varies by query
  exampleDemonstrations, // Moderate relevance
  documentationPages     // Varies by query
].join('\n');

// Result: 8,000 tokens, including 40% irrelevant information
```

**Problems**:
- ❌ Valuable info competes with noise for attention
- ❌ "Lost in the middle" effect (important info buried)
- ❌ Wasted tokens on irrelevant content
- ❌ Higher costs, slower responses
- ❌ Degraded accuracy from distraction

**With Importance Scoring**:
```typescript
const scoredContext = await scoreContext(allContext, currentQuery);
const topContext = scoredContext
  .filter(item => item.score > 0.7) // High relevance only
  .slice(0, 20) // Top 20 items
  .map(item => item.content)
  .join('\n');

// Result: 2,000 tokens, 95% relevant information
```

**Benefits**:
- ✅ Only high-value content included
- ✅ Critical info prioritized
- ✅ 60-75% token reduction typical
- ✅ Better accuracy (less distraction)
- ✅ Lower cost, faster responses

### Research Findings (2024-2025)

**Key Studies**:
1. **RankRAG (2024)**: Teaching LLMs to rank context importance during retrieval improved accuracy by 18% on complex QA tasks
2. **Context Distraction (LangChain 2025)**: Irrelevant information in context reduces answer accuracy by up to 30%
3. **Relevance Assessment (2025)**: LLM-generated relevance scores correlate 0.85+ with human judgments

## Importance Scoring Strategies

### Strategy 1: Semantic Similarity Scoring

**Score by similarity to current query**:

**Approach**: Embed query and context chunks, compute similarity scores.

**Implementation**:
```typescript
import { embed } from './embeddings';
import { cosineSimilarity } from './similarity';

interface ScoredContext {
  content: string;
  score: number;
  source: string;
}

class SemanticScorer {
  async scoreContext(
    query: string,
    contextItems: Array<{ content: string; source: string }>
  ): Promise<ScoredContext[]> {
    // 1. Embed query
    const queryEmbedding = await embed(query);
    
    // 2. Embed all context items
    const contextEmbeddings = await Promise.all(
      contextItems.map(item => embed(item.content))
    );
    
    // 3. Compute similarity scores
    const scored = contextItems.map((item, idx) => ({
      content: item.content,
      source: item.source,
      score: cosineSimilarity(queryEmbedding, contextEmbeddings[idx])
    }));
    
    // 4. Sort by score descending
    return scored.sort((a, b) => b.score - a.score);
  }
  
  async selectTopK(
    query: string,
    contextItems: Array<{ content: string; source: string }>,
    k: number = 10
  ): Promise<ScoredContext[]> {
    const scored = await this.scoreContext(query, contextItems);
    return scored.slice(0, k);
  }
  
  async selectByThreshold(
    query: string,
    contextItems: Array<{ content: string; source: string }>,
    threshold: number = 0.7
  ): Promise<ScoredContext[]> {
    const scored = await this.scoreContext(query, contextItems);
    return scored.filter(item => item.score >= threshold);
  }
}

// Usage
const scorer = new SemanticScorer();

const query = "How do I configure SSL certificates?";
const contextItems = [
  { content: "SSL configuration guide...", source: "docs" },
  { content: "Troubleshooting network issues...", source: "docs" },
  { content: "User authentication setup...", source: "docs" },
  { content: "Certificate generation commands...", source: "docs" }
];

// Get top 2 most relevant
const topContext = await scorer.selectTopK(query, contextItems, 2);
// Returns: SSL configuration guide (0.89), Certificate generation (0.82)

// Or get all above threshold
const relevantContext = await scorer.selectByThreshold(query, contextItems, 0.75);
```

**Pros**:
- ✅ Captures semantic relevance beyond keywords
- ✅ Works across languages/domains
- ✅ No training required

**Cons**:
- ❌ Embedding costs (cached in production)
- ❌ May miss keyword-specific matches
- ❌ Context-independent (doesn't consider conversation)

**Best For**: RAG systems, documentation search, knowledge base queries.

### Strategy 2: Recency + Relevance Scoring

**Combine temporal and semantic relevance**:

**Approach**: Weight recent information higher, especially in conversations.

**Implementation**:
```typescript
class RecencyRelevanceScorer {
  async score(
    query: string,
    contextItems: Array<{
      content: string;
      timestamp: number;
      source: string;
    }>,
    recencyWeight: number = 0.3 // 30% recency, 70% relevance
  ): Promise<ScoredContext[]> {
    // 1. Compute semantic relevance (0-1)
    const semanticScorer = new SemanticScorer();
    const semanticScores = await semanticScorer.scoreContext(query, contextItems);
    
    // 2. Compute recency scores (0-1)
    const now = Date.now();
    const maxAge = 7 * 24 * 60 * 60 * 1000; // 7 days
    
    const recencyScores = contextItems.map(item => {
      const age = now - item.timestamp;
      return Math.max(0, 1 - (age / maxAge)); // Linear decay over 7 days
    });
    
    // 3. Combine scores
    return semanticScores.map((item, idx) => ({
      ...item,
      semanticScore: item.score,
      recencyScore: recencyScores[idx],
      score: (item.score * (1 - recencyWeight)) + (recencyScores[idx] * recencyWeight)
    })).sort((a, b) => b.score - a.score);
  }
}

// Usage
const scorer = new RecencyRelevanceScorer();

const conversationHistory = [
  { 
    content: "User asked about React hooks yesterday",
    timestamp: Date.now() - 24 * 60 * 60 * 1000,
    source: "conversation"
  },
  {
    content: "User asked about deployment 2 minutes ago",
    timestamp: Date.now() - 2 * 60 * 1000,
    source: "conversation"
  },
  {
    content: "React hooks documentation (unchanged for months)",
    timestamp: Date.now() - 90 * 24 * 60 * 60 * 1000,
    source: "docs"
  }
];

const query = "How do I deploy my React app?";
const scored = await scorer.score(query, conversationHistory, 0.4); // 40% recency weight

// Result:
// 1. "deployment" (high semantic + very recent) = 0.92
// 2. "React hooks doc" (high semantic + old) = 0.65
// 3. "hooks yesterday" (medium semantic + recent) = 0.58
```

**Pros**:
- ✅ Prioritizes recent relevant information
- ✅ Good for conversational agents
- ✅ Balances history with current focus

**Cons**:
- ❌ Requires timestamp metadata
- ❌ Decay function needs tuning per use case
- ❌ May over-prioritize recency

**Best For**: Chatbots, customer support, multi-turn dialogues.

### Strategy 3: LLM-Based Relevance Scoring

**Use LLM to assess relevance directly**:

**Approach**: Ask fast model to score each context item's relevance.

**Implementation**:
```typescript
class LLMRelevanceScorer {
  async scoreItem(query: string, contextItem: string): Promise<number> {
    const prompt = `Given this query and context item, rate the relevance on a scale of 0-10.
Only respond with a number.

Query: ${query}

Context: ${contextItem}

Relevance (0-10):`;

    const response = await generateText({
      model: 'gpt-4o-mini',
      prompt,
      maxTokens: 5,
      temperature: 0
    });
    
    const score = parseInt(response.text.trim());
    return isNaN(score) ? 0 : score / 10; // Normalize to 0-1
  }
  
  async scoreBatch(
    query: string,
    contextItems: string[]
  ): Promise<number[]> {
    // Batch scoring for efficiency
    const prompt = `Rate the relevance of each context item to the query. 
Respond with scores (0-10) separated by commas, nothing else.

Query: ${query}

Context items:
${contextItems.map((item, idx) => `${idx + 1}. ${item.substring(0, 200)}...`).join('\n')}

Scores (comma-separated):`;

    const response = await generateText({
      model: 'gpt-4o-mini',
      prompt,
      maxTokens: 50,
      temperature: 0
    });
    
    const scores = response.text.split(',').map(s => {
      const score = parseInt(s.trim());
      return isNaN(score) ? 0 : score / 10;
    });
    
    return scores;
  }
  
  async scoreContext(
    query: string,
    contextItems: Array<{ content: string; source: string }>
  ): Promise<ScoredContext[]> {
    const contents = contextItems.map(item => item.content);
    const scores = await this.scoreBatch(query, contents);
    
    return contextItems.map((item, idx) => ({
      content: item.content,
      source: item.source,
      score: scores[idx] || 0
    })).sort((a, b) => b.score - a.score);
  }
}

// Usage
const scorer = new LLMRelevanceScorer();

const query = "How do I fix CORS errors?";
const contexts = [
  { content: "CORS (Cross-Origin Resource Sharing) configuration guide...", source: "docs" },
  { content: "React component styling best practices...", source: "docs" },
  { content: "Server-side error handling middleware...", source: "docs" }
];

const scored = await scorer.scoreContext(query, contexts);
// CORS guide: 0.95
// Error handling: 0.45
// Styling: 0.10
```

**Pros**:
- ✅ Deep semantic understanding
- ✅ Can reason about context relevance
- ✅ Handles nuanced queries

**Cons**:
- ❌ Slower (requires LLM calls)
- ❌ Costs per scoring operation
- ❌ May hallucinate scores

**Best For**: Complex queries, specialized domains, when accuracy is critical.

### Strategy 4: Multi-Factor Scoring

**Combine multiple signals for comprehensive scoring**:

**Approach**: Weight different relevance factors based on context.

**Implementation**:
```typescript
interface ScoringFactors {
  semanticSimilarity: number;    // 0-1
  recency: number;                // 0-1
  sourceAuthority: number;        // 0-1
  userEngagement: number;         // 0-1 (if available)
  tokenLength: number;            // Shorter = higher score
}

interface ScoringWeights {
  semantic: number;
  recency: number;
  authority: number;
  engagement: number;
  brevity: number;
}

class MultiFactorScorer {
  private defaultWeights: ScoringWeights = {
    semantic: 0.5,   // 50% semantic relevance
    recency: 0.2,    // 20% recency
    authority: 0.15, // 15% source authority
    engagement: 0.10, // 10% user engagement
    brevity: 0.05    // 5% prefer concise content
  };
  
  async computeFactors(
    query: string,
    contextItem: {
      content: string;
      timestamp: number;
      source: string;
      views?: number;
      upvotes?: number;
    }
  ): Promise<ScoringFactors> {
    // 1. Semantic similarity
    const queryEmb = await embed(query);
    const contentEmb = await embed(contextItem.content);
    const semanticSimilarity = cosineSimilarity(queryEmb, contentEmb);
    
    // 2. Recency (exponential decay)
    const ageHours = (Date.now() - contextItem.timestamp) / (1000 * 60 * 60);
    const recency = Math.exp(-ageHours / 168); // Half-life of 1 week
    
    // 3. Source authority
    const authorityMap: Record<string, number> = {
      'official_docs': 1.0,
      'verified_user': 0.8,
      'community': 0.6,
      'external': 0.4
    };
    const sourceAuthority = authorityMap[contextItem.source] || 0.5;
    
    // 4. User engagement (if available)
    const maxViews = 10000;
    const maxUpvotes = 1000;
    const userEngagement = contextItem.views && contextItem.upvotes
      ? (contextItem.views / maxViews * 0.3) + (contextItem.upvotes / maxUpvotes * 0.7)
      : 0.5; // Default if not available
    
    // 5. Token length (prefer concise, penalize very long)
    const tokens = estimateTokens(contextItem.content);
    const idealLength = 200;
    const tokenLength = Math.exp(-Math.abs(tokens - idealLength) / idealLength);
    
    return {
      semanticSimilarity,
      recency,
      sourceAuthority,
      userEngagement,
      tokenLength
    };
  }
  
  computeScore(factors: ScoringFactors, weights: ScoringWeights = this.defaultWeights): number {
    return (
      factors.semanticSimilarity * weights.semantic +
      factors.recency * weights.recency +
      factors.sourceAuthority * weights.authority +
      factors.userEngagement * weights.engagement +
      factors.tokenLength * weights.brevity
    );
  }
  
  async scoreContext(
    query: string,
    contextItems: Array<{
      content: string;
      timestamp: number;
      source: string;
      views?: number;
      upvotes?: number;
    }>,
    weights?: ScoringWeights
  ): Promise<Array<ScoredContext & { factors: ScoringFactors }>> {
    const scored = await Promise.all(
      contextItems.map(async (item) => {
        const factors = await this.computeFactors(query, item);
        const score = this.computeScore(factors, weights);
        
        return {
          content: item.content,
          source: item.source,
          score,
          factors
        };
      })
    );
    
    return scored.sort((a, b) => b.score - a.score);
  }
}

// Usage
const scorer = new MultiFactorScorer();

const query = "How to implement authentication?";
const contexts = [
  {
    content: "Official OAuth 2.0 guide (concise)",
    timestamp: Date.now() - 30 * 24 * 60 * 60 * 1000, // 30 days old
    source: 'official_docs',
    views: 5000,
    upvotes: 500
  },
  {
    content: "Community blog post about auth (very detailed)",
    timestamp: Date.now() - 2 * 24 * 60 * 60 * 1000, // 2 days old
    source: 'community',
    views: 100,
    upvotes: 10
  }
];

const scored = await scorer.scoreContext(query, contexts);
// Can inspect factors for explainability:
console.log(scored[0].factors);
// {
//   semanticSimilarity: 0.92,
//   recency: 0.78,
//   sourceAuthority: 1.0,
//   userEngagement: 0.75,
//   tokenLength: 0.85
// }
```

**Pros**:
- ✅ Comprehensive relevance assessment
- ✅ Explainable (can inspect factors)
- ✅ Tunable per use case
- ✅ Balances multiple signals

**Cons**:
- ❌ More complex to implement
- ❌ Requires metadata collection
- ❌ Weights need tuning

**Best For**: Production systems with rich metadata, when explainability matters.

### Strategy 5: Learned Importance Scoring (2025 Approach)

**Train model to predict importance based on user feedback**:

**Approach**: Collect implicit feedback (what context led to good responses), train scorer.

**Conceptual Implementation**:
```typescript
class LearnedImportanceScorer {
  private model: any; // Trained scoring model
  
  async scoreContext(
    query: string,
    contextItem: string,
    conversationHistory: string[]
  ): Promise<number> {
    // Features for scoring model
    const features = {
      queryEmbedding: await embed(query),
      contextEmbedding: await embed(contextItem),
      historySimilarity: await this.computeHistorySimilarity(contextItem, conversationHistory),
      contextLength: estimateTokens(contextItem),
      queryLength: estimateTokens(query)
    };
    
    // Predict importance score using trained model
    return await this.model.predict(features);
  }
  
  // Training: collect feedback on what context was useful
  async collectFeedback(
    query: string,
    usedContext: string[],
    responseQuality: number // User rating 0-1
  ) {
    // Store training example
    await this.trainingDataStore.add({
      query,
      contexts: usedContext,
      label: responseQuality // Higher quality = these contexts were important
    });
  }
  
  async retrain() {
    // Periodically retrain scoring model with accumulated feedback
    const trainingData = await this.trainingDataStore.getAll();
    await this.model.train(trainingData);
  }
  
  private async computeHistorySimilarity(
    context: string,
    history: string[]
  ): Promise<number> {
    const contextEmb = await embed(context);
    const historyEmbs = await Promise.all(history.map(h => embed(h)));
    
    const similarities = historyEmbs.map(hEmb => cosineSimilarity(contextEmb, hEmb));
    return Math.max(...similarities); // Most similar to any history item
  }
}

// Usage (conceptual - requires ML training infrastructure)
const scorer = new LearnedImportanceScorer();

// During inference
const score = await scorer.scoreContext(query, contextItem, conversationHistory);

// During feedback collection
await scorer.collectFeedback(query, usedContext, userRating);

// Periodic retraining
await scorer.retrain();
```

**Pros**:
- ✅ Learns from actual usage patterns
- ✅ Continuously improves
- ✅ Adapts to user preferences

**Cons**:
- ❌ Requires ML infrastructure
- ❌ Needs training data collection
- ❌ Complex to implement

**Best For**: Large-scale production systems with user feedback loops.

## Production Integration

### Priority-Based Context Building

```typescript
class PriorityContextBuilder {
  async buildContext(
    query: string,
    availableContext: {
      systemPrompt: string;
      workingMemory: string[];
      conversation: Message[];
      knowledge: string[];
      examples: string[];
    },
    maxTokens: number = 4000
  ): Promise<string> {
    // 1. Score all context items
    const scorer = new MultiFactorScorer();
    
    const scoredItems = [
      // High priority: working memory
      ...await scorer.scoreContext(
        query,
        availableContext.workingMemory.map(m => ({
          content: m,
          timestamp: Date.now(),
          source: 'working_memory'
        }))
      ),
      
      // Medium priority: knowledge
      ...await scorer.scoreContext(
        query,
        availableContext.knowledge.map(k => ({
          content: k,
          timestamp: Date.now(),
          source: 'knowledge_base'
        }))
      ),
      
      // Lower priority: examples
      ...await scorer.scoreContext(
        query,
        availableContext.examples.map(e => ({
          content: e,
          timestamp: Date.now(),
          source: 'examples'
        }))
      )
    ];
    
    // 2. Select items until token budget exhausted
    let selectedItems: typeof scoredItems = [];
    let currentTokens = estimateTokens(availableContext.systemPrompt);
    
    for (const item of scoredItems) {
      const itemTokens = estimateTokens(item.content);
      
      if (currentTokens + itemTokens > maxTokens * 0.9) break; // Leave 10% buffer
      
      selectedItems.push(item);
      currentTokens += itemTokens;
    }
    
    // 3. Build final context
    return `${availableContext.systemPrompt}

${selectedItems.map(item => item.content).join('\n\n')}`;
  }
}
```

### Monitoring & Analytics

```typescript
class ImportanceScoringAnalytics {
  trackScoring(query: string, scoredItems: ScoredContext[], selectedCount: number) {
    // Track score distribution
    const scores = scoredItems.map(i => i.score);
    metrics.histogram('importance.score.distribution', scores);
    
    // Track selection threshold
    const threshold = scoredItems[selectedCount - 1]?.score || 0;
    metrics.gauge('importance.selection.threshold', threshold);
    
    // Track token savings
    const totalTokens = scoredItems.reduce((sum, i) => sum + estimateTokens(i.content), 0);
    const selectedTokens = scoredItems.slice(0, selectedCount)
      .reduce((sum, i) => sum + estimateTokens(i.content), 0);
    const savings = 1 - (selectedTokens / totalTokens);
    
    metrics.gauge('importance.token.savings', savings);
  }
}
```

## Key Takeaways

**What is Importance Scoring**:
- Assign relevance weights to context items
- Prioritize high-value information
- Discard low-value noise

**Why It Matters** (2024-2025):
- **Token Efficiency**: 60-75% reduction typical
- **Accuracy**: Less distraction = better focus
- **Cost**: Fewer tokens = lower costs
- **Performance**: Shorter context = faster inference

**Scoring Strategies**:
1. **Semantic Similarity**: Embed + cosine similarity
2. **Recency + Relevance**: Weight recent + relevant
3. **LLM-Based**: Deep understanding, slower
4. **Multi-Factor**: Combine multiple signals
5. **Learned**: Train from user feedback

**Production Pattern**:
```typescript
// 1. Score all context
const scored = await scorer.scoreContext(query, allContext);

// 2. Select top items
const topContext = scored.filter(i => i.score > threshold);

// 3. Build context within budget
const finalContext = buildContext(topContext, maxTokens);

// 4. Track metrics
analytics.trackScoring(query, scored, topContext.length);
```

**Your Codebase**:
- Score retrieved RAG documents by relevance
- Prioritize working memory items
- Rank tool descriptions by query relevance

## Navigation

- [← Previous: 2.1.1 Compression Techniques](./2.1.1-compression.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 2.1.3 Lazy Loading (Fetch on Demand)](./2.1.3-lazy-loading.md)

---

*Part of Layer 2: Context Engineering - Intelligent context prioritization*
