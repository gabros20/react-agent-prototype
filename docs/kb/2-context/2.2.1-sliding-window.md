# 2.2.1 Context Management Patterns: Sliding Window

## Overview

Sliding window context management maintains a fixed-size buffer of recent tokens, automatically discarding older content as new tokens arrive. This pattern enables LLMs to handle conversations and documents longer than their training context while maintaining constant memory usage. Recent innovations like StreamingLLM's "attention sink" mechanism achieve up to 22.2× speedup by preserving strategic initial tokens alongside the sliding window.

**Key Insight** (2024-2025): Sliding windows aren't about limiting context—they're about maintaining optimal working memory while gracefully degrading older information.

**Current Date**: November 17, 2025

## Why Sliding Windows Matter

### The Infinite Context Problem

**Naive Approach** (keep growing context):
```typescript
let conversation: Message[] = [];

async function chat(userMessage: string) {
  // Add new message
  conversation.push({ role: 'user', content: userMessage });
  
  // Include ALL history
  const response = await generateText({
    messages: conversation,  // Grows unbounded!
    model: 'gpt-4o-mini'
  });
  
  conversation.push({ role: 'assistant', content: response.text });
  return response.text;
}

// After 50 turns:
// - Context: 50,000+ tokens
// - Cost per request: $0.75
// - Latency: 8+ seconds
// - Eventually: Context overflow error
```

**Problems**:
- ❌ Context grows unbounded (eventually overflows)
- ❌ Costs increase linearly with conversation length
- ❌ Latency degrades (more tokens = slower)
- ❌ "Lost in the middle" effect (model ignores mid-context info)

**Sliding Window Approach**:
```typescript
const WINDOW_SIZE = 4000; // tokens

async function chatWithWindow(userMessage: string) {
  conversation.push({ role: 'user', content: userMessage });
  
  // Keep only recent context within window
  const windowedContext = maintainWindow(conversation, WINDOW_SIZE);
  
  const response = await generateText({
    messages: windowedContext,  // Fixed size!
    model: 'gpt-4o-mini'
  });
  
  conversation.push({ role: 'assistant', content: response.text });
  return response.text;
}

// After 50 turns:
// - Context: 4,000 tokens (constant!)
// - Cost per request: $0.06 (constant!)
// - Latency: 1-2 seconds (constant!)
// - Never overflows
```

**Benefits**:
- ✅ Constant memory usage
- ✅ Predictable costs (no surprises)
- ✅ Consistent latency
- ✅ Infinite conversation length supported

## Research Foundation: StreamingLLM (2024)

**Key Paper**: "Efficient Streaming Language Models with Attention Sinks" (MIT, 2024)

**The Discovery**: StreamingLLM researchers found that transformer models allocate disproportionate attention to initial tokens regardless of their semantic content—a phenomenon called "attention sinks."

**The Insight**: By preserving just the first 4 tokens (attention sinks) alongside a sliding window of recent tokens, models maintain stability across infinite sequence lengths without retraining.

**Architecture**:
```
┌─────────────────────────────────────────────────────┐
│ KV Cache Structure                                   │
├─────────────────────────────────────────────────────┤
│ [Attention Sinks] [Sliding Window] [New Tokens]     │
│  (4 tokens kept)   (recent L tokens)  (incoming)    │
│                                                       │
│ Example with L=512:                                  │
│ [T0 T1 T2 T3] [T4996...T4507] → [T4508 T4509...]   │
│  Always kept    Slides forward    Being processed   │
└─────────────────────────────────────────────────────┘
```

**Performance Results** (2024 paper):
- **Speedup**: 22.2× faster than sliding window with recomputation
- **Memory**: Constant O(L) instead of O(T) where T = total sequence length
- **Quality**: Maintains perplexity comparable to unlimited cache
- **Scale**: Tested up to 4 million tokens without degradation

**Why It Works**:
1. Initial tokens serve as attention "dump points" for softmax normalization
2. All subsequent tokens can attend to initial tokens (positional advantage)
3. Preserving sinks stabilizes attention distribution
4. Sliding window captures recent context for coherence

## Sliding Window Implementation Patterns

### Pattern 1: Token-Based Window

**Track actual token count, not message count**:

**Implementation**:
```typescript
interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
  tokens: number;
}

class TokenBasedSlidingWindow {
  private maxTokens: number;
  private systemPrompt: Message;
  private messages: Message[] = [];
  
  constructor(systemPrompt: string, maxTokens: number = 4000) {
    this.systemPrompt = {
      role: 'system',
      content: systemPrompt,
      tokens: estimateTokens(systemPrompt)
    };
    this.maxTokens = maxTokens;
  }
  
  addMessage(role: 'user' | 'assistant', content: string): void {
    const tokens = estimateTokens(content);
    this.messages.push({ role, content, tokens });
    this.slideWindow();
  }
  
  private slideWindow(): void {
    // Calculate current usage
    let totalTokens = this.systemPrompt.tokens;
    for (const msg of this.messages) {
      totalTokens += msg.tokens;
    }
    
    // Remove oldest messages until within budget
    while (totalTokens > this.maxTokens && this.messages.length > 2) {
      const removed = this.messages.shift()!;
      totalTokens -= removed.tokens;
      
      metrics.increment('sliding_window.evicted_message', {
        role: removed.role,
        tokens: removed.tokens
      });
    }
  }
  
  getContext(): Message[] {
    return [this.systemPrompt, ...this.messages];
  }
  
  getTokenCount(): number {
    return this.systemPrompt.tokens + 
           this.messages.reduce((sum, m) => sum + m.tokens, 0);
  }
}

// Usage
const window = new TokenBasedSlidingWindow(systemPrompt, 4000);

window.addMessage('user', 'Hello!');
window.addMessage('assistant', 'Hi! How can I help?');
// ... many turns later ...
window.addMessage('user', 'What did we discuss at the start?');
// Oldest messages automatically evicted

console.log(`Context size: ${window.getTokenCount()} tokens`); // Always ≤ 4000
```

**Pros**:
- ✅ Precise token budget control
- ✅ Handles variable message lengths well
- ✅ Predictable costs

**Cons**:
- ❌ Requires token estimation (slight overhead)
- ❌ Abrupt information loss at boundaries

### Pattern 2: Attention Sink Window (StreamingLLM-Inspired)

**Preserve initial tokens + sliding window**:

**Implementation**:
```typescript
class AttentionSinkWindow {
  private sinkMessages: Message[] = []; // First N messages (kept forever)
  private slidingMessages: Message[] = [];
  private maxSlidingTokens: number;
  private sinkCount: number;
  
  constructor(
    systemPrompt: string,
    maxSlidingTokens: number = 3500,
    sinkCount: number = 4 // First 4 messages
  ) {
    this.maxSlidingTokens = maxSlidingTokens;
    this.sinkCount = sinkCount;
    
    // System prompt always in sink
    this.sinkMessages.push({
      role: 'system',
      content: systemPrompt,
      tokens: estimateTokens(systemPrompt)
    });
  }
  
  addMessage(role: 'user' | 'assistant', content: string): void {
    const message = {
      role,
      content,
      tokens: estimateTokens(content)
    };
    
    // First few messages go to sink (permanent)
    if (this.sinkMessages.length < this.sinkCount) {
      this.sinkMessages.push(message);
      metrics.increment('sliding_window.added_to_sink');
    } else {
      // Rest go to sliding window
      this.slidingMessages.push(message);
      this.slideWindow();
    }
  }
  
  private slideWindow(): void {
    let slidingTokens = this.slidingMessages.reduce((sum, m) => sum + m.tokens, 0);
    
    // Evict oldest sliding messages when over budget
    while (slidingTokens > this.maxSlidingTokens && this.slidingMessages.length > 2) {
      const removed = this.slidingMessages.shift()!;
      slidingTokens -= removed.tokens;
      
      metrics.increment('sliding_window.evicted_from_sliding', {
        tokens: removed.tokens
      });
    }
  }
  
  getContext(): Message[] {
    // Sink + sliding window
    return [...this.sinkMessages, ...this.slidingMessages];
  }
  
  getStats() {
    const sinkTokens = this.sinkMessages.reduce((sum, m) => sum + m.tokens, 0);
    const slidingTokens = this.slidingMessages.reduce((sum, m) => sum + m.tokens, 0);
    
    return {
      sinkMessages: this.sinkMessages.length,
      slidingMessages: this.slidingMessages.length,
      sinkTokens,
      slidingTokens,
      totalTokens: sinkTokens + slidingTokens
    };
  }
}

// Usage
const window = new AttentionSinkWindow(systemPrompt, 3500, 4);

// First 4 messages stay forever (attention sinks)
window.addMessage('user', 'I prefer concise answers');
window.addMessage('assistant', 'Understood, I\'ll be concise');
window.addMessage('user', 'My name is Alice');
window.addMessage('assistant', 'Nice to meet you, Alice');

// Later messages slide
for (let i = 0; i < 100; i++) {
  window.addMessage('user', `Question ${i}`);
  window.addMessage('assistant', `Answer ${i}`);
}

// Stats show:
// - Sink: 4 messages (preserved)
// - Sliding: ~10 recent messages
// - Old messages 5-90: evicted
console.log(window.getStats());
```

**Pros**:
- ✅ Preserves critical early context (preferences, identity)
- ✅ Inspired by successful StreamingLLM research
- ✅ Maintains model stability

**Cons**:
- ❌ Assumes first N messages are important (may not be true)
- ❌ More complex than basic sliding window

### Pattern 3: Hierarchical Window with Compression

**Recent messages at full detail, older messages compressed**:

**Implementation**:
```typescript
interface WindowTier {
  name: string;
  maxMessages: number;
  compressionRatio: number;
}

class HierarchicalWindow {
  private tiers: WindowTier[] = [
    { name: 'recent', maxMessages: 6, compressionRatio: 1.0 },      // Last 3 turns
    { name: 'medium', maxMessages: 10, compressionRatio: 0.3 },     // Turns 4-8
    { name: 'distant', maxMessages: 10, compressionRatio: 0.1 }     // Turns 9-18
  ];
  
  private messages: Array<{ message: Message; tier: string }> = [];
  
  async addMessage(role: 'user' | 'assistant', content: string): Promise<void> {
    const message = { role, content, tokens: estimateTokens(content) };
    this.messages.unshift({ message, tier: 'recent' }); // Add to front
    
    await this.reorganizeTiers();
  }
  
  private async reorganizeTiers(): Promise<void> {
    let currentIndex = 0;
    
    for (const tier of this.tiers) {
      // Messages in this tier's range
      const tierMessages = this.messages.slice(currentIndex, currentIndex + tier.maxMessages);
      
      // Compress if needed and update tier assignment
      for (const msg of tierMessages) {
        if (msg.tier !== tier.name) {
          // Message graduated to lower tier
          msg.tier = tier.name;
          
          if (tier.compressionRatio < 1.0) {
            // Compress message
            const compressed = await this.compressMessage(
              msg.message.content,
              tier.compressionRatio
            );
            
            msg.message.content = compressed;
            msg.message.tokens = estimateTokens(compressed);
            
            metrics.increment('sliding_window.compressed', {
              tier: tier.name,
              ratio: tier.compressionRatio
            });
          }
        }
      }
      
      currentIndex += tier.maxMessages;
    }
    
    // Remove messages beyond all tiers
    if (this.messages.length > currentIndex) {
      const evicted = this.messages.splice(currentIndex);
      metrics.increment('sliding_window.evicted', { count: evicted.length });
    }
  }
  
  private async compressMessage(content: string, ratio: number): Promise<string> {
    const targetTokens = Math.floor(estimateTokens(content) * ratio);
    
    const compressed = await generateText({
      model: 'gpt-4o-mini',
      prompt: `Compress this message to approximately ${targetTokens} tokens while preserving key information:\n\n${content}`,
      maxTokens: targetTokens * 1.2,
      temperature: 0.3
    });
    
    return compressed.text;
  }
  
  getContext(): Message[] {
    return this.messages.map(m => m.message);
  }
}

// Usage
const window = new HierarchicalWindow();

// As messages age, they gracefully degrade:
// Turn 1-3:   Full detail (100%)
// Turn 4-8:   Compressed to 30%
// Turn 9-18:  Compressed to 10%
// Turn 19+:   Evicted
```

**Pros**:
- ✅ Graceful degradation (no abrupt cutoff)
- ✅ Preserves old context in compressed form
- ✅ Better information retention

**Cons**:
- ❌ Compression costs (LLM calls)
- ❌ Complex implementation
- ❌ Compression may lose nuance

### Pattern 4: Selective Window (Importance-Based)

**Keep important messages regardless of age**:

**Implementation**:
```typescript
interface ImportantMessage extends Message {
  importanceScore: number;
  pinned: boolean;
}

class SelectiveSlidingWindow {
  private messages: ImportantMessage[] = [];
  private maxTokens: number;
  private minImportanceToKeep: number = 0.7;
  
  constructor(maxTokens: number = 4000) {
    this.maxTokens = maxTokens;
  }
  
  async addMessage(
    role: 'user' | 'assistant',
    content: string,
    pinned: boolean = false
  ): Promise<void> {
    const tokens = estimateTokens(content);
    const importanceScore = await this.scoreImportance(content);
    
    this.messages.push({
      role,
      content,
      tokens,
      importanceScore,
      pinned
    });
    
    this.slideWindow();
  }
  
  private async scoreImportance(content: string): Promise<number> {
    // Heuristic scoring (could use ML model)
    let score = 0.5; // Base score
    
    // Explicit user preferences get high scores
    if (content.includes('I prefer') || content.includes('always') || content.includes('never')) {
      score += 0.3;
    }
    
    // Questions tend to be important
    if (content.includes('?')) {
      score += 0.1;
    }
    
    // Short messages less important
    if (estimateTokens(content) < 10) {
      score -= 0.2;
    }
    
    return Math.max(0, Math.min(1, score));
  }
  
  private slideWindow(): void {
    // Calculate current usage
    let totalTokens = this.messages.reduce((sum, m) => sum + m.tokens, 0);
    
    if (totalTokens <= this.maxTokens) return;
    
    // Sort by importance (keeping order for same importance)
    const sortedMessages = [...this.messages].sort((a, b) => {
      if (a.pinned !== b.pinned) return a.pinned ? -1 : 1;
      if (Math.abs(a.importanceScore - b.importanceScore) > 0.1) {
        return b.importanceScore - a.importanceScore;
      }
      return this.messages.indexOf(a) - this.messages.indexOf(b); // Preserve order
    });
    
    // Keep messages until budget exhausted
    const kept: ImportantMessage[] = [];
    totalTokens = 0;
    
    for (const msg of sortedMessages) {
      if (totalTokens + msg.tokens <= this.maxTokens || msg.pinned) {
        kept.push(msg);
        totalTokens += msg.tokens;
      } else if (msg.importanceScore >= this.minImportanceToKeep) {
        // Important but doesn't fit - compress it
        msg.content = msg.content.substring(0, 100) + '...';
        msg.tokens = estimateTokens(msg.content);
        kept.push(msg);
        totalTokens += msg.tokens;
      }
    }
    
    // Restore chronological order
    this.messages = kept.sort((a, b) =>
      this.messages.indexOf(a) - this.messages.indexOf(b)
    );
  }
  
  getContext(): Message[] {
    return this.messages;
  }
}

// Usage
const window = new SelectiveSlidingWindow(4000);

// Pin critical messages
await window.addMessage('user', 'My name is Alice. Always address me formally.', true);

// Normal messages
await window.addMessage('user', 'What\'s the weather?');
await window.addMessage('assistant', 'I need your location.');

// Later: pinned message still present, unimportant ones evicted
```

**Pros**:
- ✅ Preserves semantically important content
- ✅ Adapts to conversation content dynamically
- ✅ Better context quality

**Cons**:
- ❌ Importance scoring complexity
- ❌ May disrupt narrative flow
- ❌ Harder to reason about behavior

## Production Deployment Patterns

### Pattern: Session-Based Windows

```typescript
class SessionWindowManager {
  private sessions = new Map<string, TokenBasedSlidingWindow>();
  
  getOrCreateWindow(sessionId: string): TokenBasedSlidingWindow {
    if (!this.sessions.has(sessionId)) {
      this.sessions.set(
        sessionId,
        new TokenBasedSlidingWindow(systemPrompt, 4000)
      );
      
      metrics.increment('session.created');
    }
    
    return this.sessions.get(sessionId)!;
  }
  
  async chat(sessionId: string, userMessage: string): Promise<string> {
    const window = this.getOrCreateWindow(sessionId);
    window.addMessage('user', userMessage);
    
    const response = await generateText({
      messages: window.getContext(),
      model: 'gpt-4o-mini'
    });
    
    window.addMessage('assistant', response.text);
    
    // Track metrics
    metrics.gauge('session.context_tokens', window.getTokenCount(), {
      sessionId
    });
    
    return response.text;
  }
  
  // Cleanup old sessions
  pruneInactiveSessions(inactiveThreshold: number = 30 * 60 * 1000): void {
    const now = Date.now();
    for (const [sessionId, window] of this.sessions.entries()) {
      if (now - window.lastActivity > inactiveThreshold) {
        this.sessions.delete(sessionId);
        metrics.increment('session.pruned');
      }
    }
  }
}
```

### Pattern: Hybrid Window + Semantic Retrieval

```typescript
class HybridContextManager {
  private window: TokenBasedSlidingWindow;
  private fullHistory: Message[] = [];
  
  async addMessage(role: 'user' | 'assistant', content: string): Promise<void> {
    const message = { role, content, tokens: estimateTokens(content) };
    
    // Add to full history
    this.fullHistory.push(message);
    
    // Add to sliding window
    this.window.addMessage(role, content);
  }
  
  async getRelevantContext(currentQuery: string): Promise<Message[]> {
    // Start with sliding window (recent context)
    const windowContext = this.window.getContext();
    
    // Retrieve semantically relevant old messages
    const queryEmb = await embed(currentQuery);
    const oldMessages = this.fullHistory.slice(0, -this.window.getContext().length);
    
    if (oldMessages.length === 0) {
      return windowContext;
    }
    
    // Score old messages by relevance
    const scored = await Promise.all(
      oldMessages.map(async (msg) => ({
        message: msg,
        score: cosineSimilarity(queryEmb, await embed(msg.content))
      }))
    );
    
    // Get top 3 relevant old messages
    const relevant = scored
      .sort((a, b) => b.score - a.score)
      .slice(0, 3)
      .filter(item => item.score > 0.7)
      .map(item => item.message);
    
    if (relevant.length > 0) {
      metrics.increment('context.hybrid.retrieved', { count: relevant.length });
      
      // Inject relevant old context
      return [
        windowContext[0], // System prompt
        ...relevant, // Old relevant messages
        ...windowContext.slice(1) // Recent window
      ];
    }
    
    return windowContext;
  }
}

// Usage
const hybrid = new HybridContextManager();

// User mentions preference in turn 1
await hybrid.addMessage('user', 'I prefer detailed explanations');
await hybrid.addMessage('assistant', 'Got it, I\'ll be detailed');

// ... 50 turns later, preference fell out of window ...

// User asks related question
const context = await hybrid.getRelevantContext('Explain how React hooks work');
// Context includes: recent window + retrieved "I prefer detailed" message!
```

## Monitoring & Optimization

```typescript
class WindowMetrics {
  trackEviction(message: Message): void {
    metrics.increment('sliding_window.eviction', {
      role: message.role,
      tokens: message.tokens,
      position: 'start' // which end of window
    });
  }
  
  trackWindowUtilization(window: TokenBasedSlidingWindow): void {
    const utilization = window.getTokenCount() / window.maxTokens;
    metrics.gauge('sliding_window.utilization', utilization);
    
    if (utilization > 0.95) {
      console.warn('Window near capacity, consider compression');
    }
  }
  
  async reportStats(): Promise<void> {
    console.log({
      averageWindowSize: await metrics.avg('session.context_tokens'),
      evictionRate: await metrics.rate('sliding_window.eviction'),
      avgUtilization: await metrics.avg('sliding_window.utilization')
    });
  }
}
```

## Key Takeaways

**What is Sliding Window**:
- Fixed-size buffer of recent tokens
- Oldest content evicted as new arrives
- Maintains constant memory/cost

**Research Foundation** (2024):
- **StreamingLLM**: 22.2× speedup via attention sinks
- **Attention sinks**: First 4 tokens stabilize attention
- **Performance**: Up to 4M tokens without degradation

**Implementation Patterns**:
1. **Token-based**: Track actual token count
2. **Attention sink**: Preserve initial + slide recent
3. **Hierarchical**: Compress older messages
4. **Selective**: Keep important regardless of age

**Production Patterns**:
- Session-based management
- Hybrid window + semantic retrieval
- Monitoring and optimization

**Trade-offs**:
- ✅ Constant cost/memory/latency
- ✅ Infinite conversation length
- ❌ Information loss (old context evicted)
- ❌ Need retrieval for old references

**Your Codebase Application**:
- Implement for long conversations
- Consider attention sink for critical early context
- Hybrid with semantic memory retrieval

## Navigation

- [← Previous: 2.1.4 Hybrid Content Fetching](./2.1.4-hybrid-fetching.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 2.2.2 Hierarchical Memory](./2.2.2-hierarchical-memory.md)

---

*Part of Layer 2: Context Engineering - Managing long conversations efficiently*
