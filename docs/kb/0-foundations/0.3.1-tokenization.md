# 0.3.1 Tokenization: BPE, WordPiece, and SentencePiece

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)  
**Next**: [0.3.2 Embedding Models](./0.3.2-embedding-models.md)

---

## Overview

**Tokenization** is the process of converting raw text into discrete units (tokens) that language models can process. It's the bridge between human-readable text and numerical representations that neural networks understand.

```
"Hello world!" → Tokenization → [15496, 995, 0]
                                    ↓
                            Input to LLM
```

**Why it matters**:
1. **Cost**: API pricing is per token ($2.50 per 1M tokens for GPT-4)
2. **Performance**: Efficient tokenization → smaller vocabularies → faster inference
3. **Context limits**: Token counts determine what fits in context windows
4. **Multilingual support**: How well models handle non-English languages

---

## The Token Problem

### Why Not Just Use Words?

**Naive approach**: Split text by spaces

```python
"Hello world!" → ["Hello", "world", "!"]
```

**Problems**:
1. **Massive vocabulary**: English has 170,000+ words
   - Larger vocabulary → Larger model → Slower inference
2. **Out-of-vocabulary (OOV) words**: New words, typos, proper names
   - "ChatGPT" wasn't in training data (word-level vocab)
   - Model can't handle unknown words
3. **Poor morphology handling**: "run", "running", "runner" treated as unrelated
4. **Language-specific**: Spaces don't work for Chinese, Japanese (no spaces!)

**Source**: [MachineLearningMastery: Tokenizers in Language Models](https://machinelearningmastery.com/tokenizers-in-language-models/)

### Why Not Use Characters?

**Approach**: Every character is a token

```python
"Hello" → ["H", "e", "l", "l", "o"]
```

**Problems**:
1. **Extremely long sequences**: 5-10x longer than word-based
   - Context windows fill up fast
   - More computation (attention is O(n²))
2. **Less semantic meaning**: Individual characters don't carry much information
3. **Harder to learn patterns**: Model must learn to group characters into words

---

## Subword Tokenization: The Sweet Spot

**Solution**: Break text into **subword units** - smaller than words, larger than characters.

**Benefits**:
- ✅ Reasonable vocabulary size (30k-100k tokens)
- ✅ Handles unknown words (break into known subwords)
- ✅ Captures morphology ("run" + "##ing" = "running")
- ✅ Works across languages
- ✅ Balances sequence length and semantic meaning

**Example**:
```
"unhappiness" → ["un", "##happiness"]
                  ↓         ↓
               prefix   root word

Model learns: "un" = negation, "happiness" = emotion
→ Understands meaning compositionally
```

**Source**: [Medium: Understanding Tokenization](https://medium.com/@lmpo/from-text-to-tokens-understanding-bpe-wordpiece-and-sentencepiece-b16f84a6ae6f)

---

## Three Major Tokenization Algorithms

### 1. Byte Pair Encoding (BPE)

**Used by**: GPT-2, GPT-3, GPT-4, Llama, Mistral, RoBERTa

**Core Idea**: Iteratively merge the most frequent pairs of symbols (bytes or characters).

#### How BPE Works

**Training Process**:

```
1. Start with character-level vocabulary: {a, b, c, ..., z, space, ...}

2. Count all adjacent pairs in corpus:
   Corpus: "low low low lower lowest"
   Pairs: {("l","o"): 6, ("o","w"): 5, ("w"," "): 3, ...}

3. Merge most frequent pair:
   ("l","o") → "lo"
   Vocabulary: {a, b, c, ..., lo, ...}
   Corpus: "lo w lo w lo w lo wer lo west"

4. Repeat until vocabulary reaches target size (e.g., 50,000 tokens)
   Next merge: ("lo","w") → "low"
   Then: ("low","er") → "lower"
```

**Result**: Vocabulary of subwords ordered by frequency

**Tokenization** (using trained vocabulary):

```python
Text: "unhappiness"

Step 1: Start with characters: ["u","n","h","a","p","p","i","n","e","s","s"]
Step 2: Apply learned merges:
  - Merge ("h","a") → "ha"
  - Merge ("ha","p") → "hap"
  - Merge ("hap","p") → "happ"
  - Merge ("happ","i") → "happi"
  - Merge ("happi","n") → "happin"
  - Merge ("happin","e") → "happine"
  - Merge ("happine","s") → "happines"
  - Merge ("happines","s") → "happiness"
  - No merge for "u", "n" (kept separate)

Final: ["un", "happiness"]
```

**Key Properties**:
- **Greedy**: Merges most frequent pairs (no backtracking)
- **Deterministic**: Same text → same tokens every time
- **Fully lossless**: Can reconstruct original text perfectly (preserves all spaces)

**Strengths**:
- ✅ Simple, fast, deterministic
- ✅ Handles unknown words well
- ✅ Small vocabulary (30k-50k tokens)

**Weaknesses**:
- ⚠️ Greedy (may not find optimal tokenization)
- ⚠️ Sensitive to text normalization (spaces, punctuation)

**Source**: [Hugging Face: Tokenizer Summary](https://huggingface.co/docs/transformers/en/tokenizer_summary)

#### BPE Variants

##### Byte-Level BPE (BBPE)

**Used by**: GPT-3, GPT-4 (via `tiktoken`)

**Innovation**: Operate on **bytes** instead of characters.

**Why?**:
- Unicode has 140,000+ characters (huge base vocabulary!)
- Bytes: Only 256 possible values (tiny base vocabulary)

**Example**:
```
Character-level BPE:
Base vocabulary: {a, b, c, ..., 中, 文, ...} → 140k+ symbols

Byte-level BPE:
Base vocabulary: {0x00, 0x01, ..., 0xFF} → 256 symbols
```

**Benefits**:
- ✅ Handles any language (even emojis, special characters)
- ✅ Smaller base vocabulary
- ✅ Never encounters OOV at byte level

**GPT-4 Tokenizer** (`cl100k_base`):
- Base: 256 bytes
- Vocabulary: 100,256 tokens (includes 100k learned merges)

**Source**: [Substack: Byte-Pair Encoding Algorithm](https://oneminutenlp.substack.com/p/byte-pair-encoding-algorithm)

---

### 2. WordPiece

**Used by**: BERT, DistilBERT, Electra

**Core Idea**: Similar to BPE, but chooses merges to **maximize likelihood** of training data (not just frequency).

#### How WordPiece Differs from BPE

**BPE**: Merge most frequent pair
```
Pair ("e","r"): 1000 occurrences → Merge!
```

**WordPiece**: Merge pair that maximizes probability of corpus
```
Evaluate: P(corpus with "er" token) vs P(corpus with "e" and "r" separately)
Choose merge that increases corpus probability most
```

**Mathematical Formulation**:
```
Score(pair) = P(pair in corpus) / (P(first symbol) × P(second symbol))

Higher score → More likely to be a meaningful unit → Merge
```

**Why better?**: Considers **context** and **semantic coherence**, not just frequency.

**Example**:
```
BPE might merge: ("th","e") → "the" (very frequent)
WordPiece might prefer: ("ing") → "ing" (semantically meaningful suffix)
```

#### WordPiece Tokenization

**Special notation**: `##` prefix indicates continuation (not word start)

```
Text: "unbelievable"

Tokens: ["un", "##believ", "##able"]
          ↑        ↑          ↑
       prefix    root      suffix

Benefits:
- Model learns "un" = negation
- "##able" = adjective suffix
- "##believ" = root related to "believe"
```

**BERT Vocabulary**: 30,000 tokens
- Common words: Single token ("the", "is", "was")
- Rare words: Multiple subwords ("unhappiness" → ["un", "##happiness"])
- Unknown words: Character-level fallback ("zzz" → ["z", "##z", "##z"])

**Strengths**:
- ✅ Semantically meaningful units
- ✅ Better handling of morphology (prefixes, suffixes)
- ✅ Learned via likelihood maximization

**Weaknesses**:
- ⚠️ Lossy: Doesn't preserve all spaces (spaces between tokens lost)
- ⚠️ Slower training (must compute likelihood scores)

**Source**: [CodeSignal: Comparing BPE, WordPiece, SentencePiece](https://codesignal.com/learn/courses/2-modern-tokenization-techniques-for-ai-and-llms/lessons/1-comparing-subword-tokenization-methods/blocks/1-bpe-wordpiece-and-sentencepiece/)

---

### 3. SentencePiece

**Used by**: T5, ALBERT, XLNet, Llama (some variants), mT5

**Core Idea**: Treat text as **raw byte/character sequence** (no preprocessing), learn tokenization directly from data.

#### Key Innovations

##### 1. Language-Agnostic (No Pre-tokenization)

**Problem with BPE/WordPiece**:
```python
# Requires pre-tokenization (language-specific)
English: "Hello world" → ["Hello", "world"] → Apply BPE
Chinese: "你好世界" → ??? (no spaces!)
```

**SentencePiece Solution**:
```python
# Treats input as raw sequence
English: "Hello world" → [H,e,l,l,o,▁,w,o,r,l,d] → Apply BPE/Unigram
Chinese: "你好世界" → [你,好,世,界] → Apply BPE/Unigram
                                    ↓
                        Works identically for all languages!
```

**Key**: Uses `▁` (U+2581 lower one-eighth block) to represent spaces as regular characters.

##### 2. Partially Lossless

**BPE**: Fully lossless (all spaces preserved)
```
"Hello  world" (2 spaces) → ["Hello", "▁▁", "world"]
Can reconstruct: "Hello  world"
```

**WordPiece**: Lossy (spaces between tokens lost)
```
"Hello  world" → ["Hello", "##world"]
Can only reconstruct: "Helloworld" (spaces lost!)
```

**SentencePiece**: Partially lossless (one space preserved, multiple spaces reduced to one)
```
"Hello  world" (2 spaces) → ["▁Hello", "▁world"]
Can reconstruct: "Hello world" (1 space, not 2)
```

**Trade-off**: Good enough for most applications, simpler vocabulary.

##### 3. Stochastic Sampling (Optional)

**Deterministic** (BPE, WordPiece):
```
"unbelievable" → ["un", "##believ", "##able"] (always)
```

**Stochastic** (SentencePiece with sampling):
```
"unbelievable" → ["un", "##believe", "##able"] (probability: 0.6)
"unbelievable" → ["unbe", "##liev", "##able"]  (probability: 0.3)
"unbelievable" → ["un", "##bel", "##iev", "##able"] (probability: 0.1)
```

**Benefits**:
- ✅ Data augmentation during training (regularization)
- ✅ More robust to typos and variations
- ⚠️ Non-deterministic (different tokenizations for same text)

**Use case**: Training (augmentation), not inference (want consistency)

##### 4. Multiple Algorithms

SentencePiece is a **framework** supporting:
- **BPE**: Same algorithm as GPT
- **Unigram LM**: Probabilistic model (likelihood-based)

**Unigram LM** (unique to SentencePiece):
```
1. Start with large vocabulary (all substrings)
2. Iteratively remove tokens that hurt likelihood least
3. Result: Vocabulary optimized for corpus probability
```

**Advantage**: Often produces better tokenization than BPE for morphologically rich languages.

**Source**: [Yu Zhu: BPE, WordPiece, SentencePiece Quicktake](https://yuzhu.run/tokenizers/)

---

## Comparison Table

| Feature | BPE | WordPiece | SentencePiece |
|---------|-----|-----------|---------------|
| **Used By** | GPT-2/3/4, Llama | BERT, DistilBERT | T5, ALBERT, mT5 |
| **Algorithm** | Greedy frequency merge | Likelihood merge | BPE or Unigram |
| **Base Units** | Characters or bytes | Characters | Characters/bytes |
| **Pre-tokenization** | Required (space split) | Required | NOT required ✅ |
| **Lossless** | Fully (all spaces) | Lossy (no spaces) | Partial (1 space) |
| **Deterministic** | Yes | Yes | Optional (sampling) |
| **Multilingual** | Good | Good | Excellent ✅ |
| **Training Speed** | Fast | Slower (likelihood) | Fast |
| **Typical Vocab** | 30k-50k | 30k | 32k-64k |
| **Special Tokens** | None | `##` continuation | `▁` space char |

**Source**: [AI Stack Exchange: Types of Tokenizers](https://ai.stackexchange.com/questions/47214/what-are-the-types-of-tokenizers)

---

## Real-World Examples

### Example 1: GPT-4 Tokenization (BPE)

**Tokenizer**: `cl100k_base` (OpenAI tiktoken)

```python
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")

# Example 1: Common English
text = "Hello, how are you?"
tokens = enc.encode(text)
# [9906, 11, 1268, 527, 499, 30]  (6 tokens)
# ["Hello", ",", " how", " are", " you", "?"]

# Example 2: Morphology
text = "unhappiness"
tokens = enc.encode(text)
# [359, 71, 67391]  (3 tokens)
# ["un", "h", "appiness"]

# Example 3: Code
text = "def hello_world():"
tokens = enc.encode(text)
# [755, 24748, 7424, 4019]  (4 tokens)
# ["def", " hello", "_world", "():"]

# Example 4: Multilingual
text = "你好世界"  (Chinese: Hello World)
tokens = enc.encode(text)
# [57668, 53901, 99489]  (3 tokens)
# Characters encoded as multi-byte UTF-8
```

**Token-to-Word Ratios**:
- English: 1 token ≈ 0.75 words (1 word ≈ 1.33 tokens)
- Code: 1 token ≈ 0.5 words (more symbols, punctuation)
- Chinese: 1 token ≈ 0.5 characters (each character is ~1-2 tokens)

**Source**: [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)

### Example 2: BERT Tokenization (WordPiece)

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Example 1: Common word
text = "running"
tokens = tokenizer.tokenize(text)
# ['running']  (1 token - common word)

# Example 2: Rare word
text = "unbelievable"
tokens = tokenizer.tokenize(text)
# ['un', '##bel', '##ie', '##va', '##ble']  (5 tokens)
#   ↑      ↑       ↑       ↑       ↑
# prefix  root   middle  middle  suffix

# Example 3: Unknown word
text = "ChatGPT"
tokens = tokenizer.tokenize(text)
# ['chat', '##gp', '##t']  (3 tokens)
# Breaks into known subwords

# Example 4: Convert to IDs
token_ids = tokenizer.encode(text, add_special_tokens=True)
# [101, 5442, 29278, 2102, 102]
#  ↑    ↑              ↑    ↑
# [CLS] tokens       [SEP]
```

**Vocabulary**: 30,522 tokens
- `[CLS]`: Classification token (start of sequence)
- `[SEP]`: Separator token (between sequences)
- `[UNK]`: Unknown token (fallback)
- `[PAD]`: Padding token
- `##`: Continuation marker

### Example 3: SentencePiece (T5)

```python
import sentencepiece as spm

# Load T5 model
sp = spm.SentencePieceProcessor(model_file='t5.model')

# Example 1: English (space becomes ▁)
text = "Hello world"
tokens = sp.encode_as_pieces(text)
# ['▁Hello', '▁world']  (2 tokens)
#    ↑        ↑
# Space char at start of each word

# Example 2: Multiple spaces
text = "Hello  world"  (2 spaces)
tokens = sp.encode_as_pieces(text)
# ['▁Hello', '▁world']  (still 2 tokens - spaces collapsed!)

# Example 3: No space at start
text = "world"
tokens = sp.encode_as_pieces(text)
# ['▁world']  (space added by default)

# Example 4: Chinese (no pre-tokenization needed!)
text = "你好世界"
tokens = sp.encode_as_pieces(text)
# ['▁你好', '世界']  (2 tokens)
# Works identically to English!
```

**Key Feature**: Language-agnostic - same code handles all languages.

**Source**: [Hugging Face: Tokenizer Summary](https://huggingface.co/docs/transformers/en/tokenizer_summary)

---

## Tokenization in Your Codebase

### OpenRouter Integration

Your agent uses **OpenRouter** as a gateway to multiple models, each with its own tokenizer:

```typescript
// server/agent/orchestrator.ts
const modelId = process.env.OPENROUTER_MODEL || 'google/gemini-2.5-flash'

const agent = new ToolLoopAgent({
  model: openrouter.languageModel(modelId),
  // Model-specific tokenizer used automatically
})
```

**Models and Their Tokenizers**:

| Model | Tokenizer | Vocabulary | Notes |
|-------|-----------|------------|-------|
| GPT-4, GPT-4o | cl100k_base (BPE) | 100,256 | Byte-level BPE |
| Claude 3.5 | Custom BPE | ~100k | Optimized for long context |
| Gemini 2.5 | SentencePiece | 256k | Multilingual, large vocab |
| Llama 3 | BPE (tiktoken-based) | 128k | Extended vocabulary |

**Implication**: Token counts vary by model for the same text!

```typescript
// Same text, different models → different token counts
text = "The quick brown fox jumps over the lazy dog."

// GPT-4 (cl100k_base): ~10 tokens
// Claude (custom BPE): ~11 tokens  
// Gemini (SentencePiece): ~9 tokens

// Why? Different vocabularies, merging strategies
```

### Token Counting for Cost Estimation

**Current approach** (implicit in AI SDK):
```typescript
const result = await agent.generate({
  messages: [...],
  experimental_context: context
})

// AI SDK tracks token usage automatically
console.log(result.usage.promptTokens)     // Input tokens
console.log(result.usage.completionTokens) // Output tokens
console.log(result.usage.totalTokens)      // Total
```

**Manual token counting** (for estimation before API call):
```typescript
import tiktoken from 'tiktoken'

// GPT-4 tokenizer
const enc = tiktoken.get_encoding('cl100k_base')

// Count tokens in system prompt
const systemPrompt = fs.readFileSync('server/prompts/react.xml', 'utf-8')
const systemTokens = enc.encode(systemPrompt).length
console.log(`System prompt: ${systemTokens} tokens`)  // ~800 tokens

// Count tokens in tool definitions
const toolsJson = JSON.stringify(ALL_TOOLS)
const toolTokens = enc.encode(toolsJson).length
console.log(`Tools: ${toolTokens} tokens`)  // ~1,500 tokens

// Estimate total cost before API call
const estimatedInputTokens = systemTokens + toolTokens + conversationTokens
const estimatedCost = (estimatedInputTokens / 1_000_000) * inputPricePerMillion
```

**Use case**: Budget alerting, cost optimization

### Vector Search Context

Your codebase uses **embeddings** for vector search (LanceDB):

```typescript
// server/services/vector-index.ts
async embedText(text: string): Promise<number[]> {
  // OpenRouter embeddings API
  const response = await fetch('https://openrouter.ai/api/v1/embeddings', {
    method: 'POST',
    body: JSON.stringify({
      model: 'openai/text-embedding-3-small',
      input: text  // ← Text is tokenized internally by embedding model
    })
  })
  
  const { data } = await response.json()
  return data[0].embedding  // 1536-dimensional vector
}
```

**Embedding model tokenization**:
- `text-embedding-3-small`: Uses **cl100k_base** (same as GPT-4)
- Input limit: **8,191 tokens** per text
- **Truncation**: Text longer than limit is automatically truncated

**Implication for your code**:
```typescript
// When indexing page content
const pageText = `${page.name} ${page.slug} ${sectionsText}`

// If pageText > 8,191 tokens → automatically truncated!
// Solution: Chunk long content before embedding
if (pageText.length > 30_000) {  // Rough estimate: 30k chars ≈ 7.5k tokens
  // Split into chunks and embed separately
}
```

**Related File**: [`server/services/vector-index.ts`](../../server/services/vector-index.ts)

---

## Best Practices

### 1. Minimize Token Usage (Cost Optimization)

**Strategy 1: Use shorter, structured formats**
```typescript
// ❌ Verbose (200 tokens)
const toolResult = `The page "About Us" was successfully created with the slug "about-us". It contains 5 sections: a hero section, a features section with 3 columns, a call-to-action section, a testimonials section with 4 reviews, and a footer section. The page is now live.`

// ✅ Structured (40 tokens - 80% savings!)
const toolResult = JSON.stringify({
  page: "About Us",
  slug: "about-us",
  sections: ["hero", "features", "cta", "testimonials", "footer"],
  status: "live"
})
```

**Strategy 2: Remove redundant whitespace**
```typescript
// ❌ Pretty JSON (more tokens)
JSON.stringify(data, null, 2)  // Indented, newlines

// ✅ Compact JSON (fewer tokens)
JSON.stringify(data)  // No whitespace
```

**Strategy 3: Use abbreviations in internal context**
```typescript
// For internal tool communication (not user-facing!)
{
  "pg": "About",      // page (not "page_name")
  "slg": "about",     // slug
  "secs": 5,          // section_count
  "upd": "2025-11-16" // updated_at
}

// Saves ~30% tokens in repeated tool results
```

### 2. Handle Token Limits Gracefully

```typescript
// Check token count before API call
import tiktoken from 'tiktoken'

function truncateToTokenLimit(text: string, maxTokens: number): string {
  const enc = tiktoken.get_encoding('cl100k_base')
  const tokens = enc.encode(text)
  
  if (tokens.length <= maxTokens) {
    return text
  }
  
  // Truncate to max tokens
  const truncatedTokens = tokens.slice(0, maxTokens)
  return enc.decode(truncatedTokens)
}

// Usage
const pageContent = getPageContent()
const truncated = truncateToTokenLimit(pageContent, 8_000)
await embedText(truncated)  // Safe: won't exceed embedding limit
```

### 3. Be Aware of Language Differences

**English** (1 word ≈ 1.33 tokens):
```typescript
text = "The quick brown fox"  // 4 words
tokens = enc.encode(text)     // ~5 tokens
```

**Chinese** (1 character ≈ 1-2 tokens):
```typescript
text = "快速的棕色狐狸"  // 7 characters
tokens = enc.encode(text)   // ~10-14 tokens (worse ratio!)
```

**Code** (1 line ≈ 5-10 tokens):
```typescript
code = "def hello_world():"  // 1 line
tokens = enc.encode(code)     // ~4 tokens
```

**Implication**: Budget more tokens for non-English content.

### 4. Test Token Counts in Development

```typescript
// Add token usage logging
import tiktoken from 'tiktoken'

const enc = tiktoken.get_encoding('cl100k_base')

console.log('Token counts:')
console.log('- System prompt:', enc.encode(systemPrompt).length)
console.log('- Tools:', enc.encode(JSON.stringify(tools)).length)
console.log('- Conversation:', enc.encode(conversationStr).length)
console.log('- Working memory:', enc.encode(workingMemory).length)

const total = /* sum of above */
console.log(`Total input: ${total} tokens`)
console.log(`Context usage: ${(total / 128_000 * 100).toFixed(1)}%`)

// Alert if approaching limit
if (total > 100_000) {
  console.warn('⚠️ Context usage >78% - consider compression')
}
```

---

## Key Takeaways

1. **Tokenization bridges text and models**: Text → Tokens → IDs → Embeddings
2. **Three main algorithms**: BPE (GPT), WordPiece (BERT), SentencePiece (T5)
3. **Subword is optimal**: Balances vocabulary size and sequence length
4. **Token counts = costs**: Optimize tokenization → reduce API costs
5. **Different models, different counts**: Same text → different token counts by model
6. **1 token ≈ 0.75 words** (English), but varies by language and content type
7. **Embedding limits**: text-embedding-3 max 8,191 tokens (truncate long text)
8. **Production tip**: Monitor token usage, compress prompts, use structured formats

---

## Further Reading

### Tools & Libraries
- [OpenAI Tokenizer (Web)](https://platform.openai.com/tokenizer) - Visualize GPT tokenization
- [tiktoken (Python)](https://github.com/openai/tiktoken) - Fast BPE tokenizer
- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers) - Fast, multilingual
- [SentencePiece](https://github.com/google/sentencepiece) - Google's tokenizer

### Research Papers
- [Neural Machine Translation of Rare Words with Subword Units (BPE)](https://arxiv.org/abs/1508.07909)
- [Japanese and Korean Voice Search (WordPiece)](https://research.google/pubs/pub37842/)
- [SentencePiece: A simple and language independent approach](https://arxiv.org/abs/1808.06226)

### Practical Guides
- [Hugging Face: Tokenizer Summary](https://huggingface.co/docs/transformers/en/tokenizer_summary)
- [One Minute NLP: BPE Algorithm](https://oneminutenlp.substack.com/p/byte-pair-encoding-algorithm)
- [CodeSignal: Modern Tokenization Techniques](https://codesignal.com/learn/courses/2-modern-tokenization-techniques-for-ai-and-llms/)

---

## Codebase Integration

**Files Using Tokenization**:
- [`server/services/vector-index.ts`](../../server/services/vector-index.ts) - Embedding API (tokenization implicit)
- [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Model usage tracking
- [`.env`](../../.env.example) - Model selection (determines tokenizer)

**Related Topics**:
- [0.1.3 Context Windows & Token Limits](./0.1.3-context-windows.md) - Token budgeting
- [0.3.2 Embedding Models](./0.3.2-embedding-models.md) - Tokenization for embeddings
- [11.4.1 Token Reduction](../11-production/11.4.1-token-reduction.md) - Production optimization

---

**Next Topic**: [0.3.2 Embedding Models & Vector Spaces](./0.3.2-embedding-models.md)  

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references including Hugging Face, Google Research, OpenAI
