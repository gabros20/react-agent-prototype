# 0.1.2 Training vs Inference

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)  
**Next**: [0.1.3 Context Windows & Token Limits](./0.1.3-context-windows.md)

---

## Overview

Understanding the distinction between **training** and **inference** is fundamental to working with LLMs effectively. This knowledge directly impacts cost modeling, deployment decisions, and understanding what you can (and can't) modify in production systems.

```
Training: Teaching the model (one time, expensive, requires massive resources)
    ↓
Inference: Using the trained model (every time, cheap, production workload)
```

---

## Training: Teaching the Model

### Definition

**Training** is the process of teaching an LLM by adjusting its billions of parameters (weights) based on massive datasets. The model learns patterns, grammar, facts, and reasoning through exposure to trillions of tokens.

**Analogy**: Training is like getting a university education - expensive, time-consuming, done once (or periodically), but provides foundational knowledge.

### Three Stages of Training

#### 1. Pretraining (Foundation)

**What**: Training model from scratch on massive, diverse datasets

**Data Scale**:

-   **GPT-3**: 300 billion tokens (~500GB of text)
-   **GPT-4**: Estimated 1-10 trillion tokens
-   **Llama 3**: 15 trillion tokens (!)

**Data Sources**:

-   Web crawl (Common Crawl): 70%
-   Books: 15%
-   Wikipedia: 5%
-   Code (GitHub): 5%
-   Academic papers: 3%
-   Other: 2%

**Objective**: Predict next token (unsupervised learning)

```
Input:  "The capital of France is"
Target: "Paris"

Model adjusts weights to maximize P(Paris | "The capital of France is")
```

**Cost**: $100,000 - $100,000,000+ depending on model size

**Time**: Weeks to months on thousands of GPUs

**Who does it**: Large organizations (OpenAI, Google, Meta, Anthropic)

**Source**: [Fireworks AI: LLM Fine-Tuning](https://fireworks.ai/blog/llm-fine-tuning)

#### 2. Fine-Tuning (Specialization)

**What**: Adapting pretrained model to specific tasks or domains using smaller, specialized datasets

**Types of Fine-Tuning**:

##### A. Supervised Fine-Tuning (SFT)

**Data**: Labeled input-output pairs curated by humans

**Example**:

```
Input:  "Translate to French: Hello, how are you?"
Output: "Bonjour, comment allez-vous?"

Input:  "Summarize this article: [long text]"
Output: "[concise summary]"
```

**Use Cases**:

-   Customer service (Q&A pairs)
-   Code generation (input: description → output: code)
-   Translation
-   Summarization

**Cost**: $1,000 - $100,000 (10-100x cheaper than pretraining)

**Data Requirements**: 1,000 - 100,000 examples (vs billions for pretraining)

**Training Time**: Hours to days

**Source**: [CleverX: SFT vs RLHF](https://cleverx.com/blog/supervised-fine-tuning-vs-rlhf-choosing-the-right-path-for-llm-training)

##### B. Parameter-Efficient Fine-Tuning (PEFT)

**Problem**: Full fine-tuning updates all billions of parameters → expensive

**Solution**: Techniques like **LoRA** (Low-Rank Adaptation) freeze most weights, train only small adapter layers

**Benefits**:

-   **Cost**: 10-100x cheaper than full fine-tuning
-   **Speed**: 2-5x faster
-   **Storage**: Adapter weights are tiny (10-100 MB vs 50+ GB for full model)

**Example** (LoRA):

```
Original Model: 7 billion parameters (14 GB)
LoRA Adapters: 8 million parameters (16 MB) ← 99.9% smaller!

Inference: Load base model + LoRA adapter (merged dynamically)
```

**Use Cases**:

-   Domain-specific adaptations (legal, medical, finance)
-   Multi-task models (one base model + multiple adapters)
-   Resource-constrained environments

**Source**: [Fireworks AI: LLM Fine-Tuning](https://fireworks.ai/blog/llm-fine-tuning)

#### 3. Alignment (Safety & Human Preferences)

##### Reinforcement Learning from Human Feedback (RLHF)

**What**: Training model to produce outputs aligned with human preferences (helpfulness, safety, honesty)

**Process**:

```
1. Generate multiple responses to same prompt
2. Human evaluators rank responses (best to worst)
3. Train reward model to predict human preferences
4. Use reward model to train LLM via reinforcement learning
```

**Example**:

```
Prompt: "How do I break into a car?"

Model (before RLHF):
"1. Use a slim jim tool
 2. Insert into door frame
 3. Manipulate lock mechanism"

Human Feedback: ❌ Unsafe (could encourage illegal activity)

Model (after RLHF):
"I can't provide instructions for breaking into cars, as this could be used for illegal purposes. If you've locked your keys in your car, I recommend:
- Calling a professional locksmith
- Contacting roadside assistance
- Calling the police (they can help in emergencies)"

Human Feedback: ✅ Safe, helpful, ethical
```

**Use Cases**:

-   ChatGPT (aligned via RLHF)
-   Claude (Constitutional AI, similar to RLHF)
-   Gemini (aligned with safety policies)

**Cost**: $50,000 - $1,000,000+ (requires extensive human evaluation)

**Time**: Weeks to months

**Trade-offs**:

-   ✅ Safer, more helpful responses
-   ✅ Better conversation flow
-   ⚠️ More expensive than SFT
-   ⚠️ Can reduce model's raw capabilities (over-cautious)

**Source**: [CleverX: SFT vs RLHF](https://cleverx.com/blog/supervised-fine-tuning-vs-rlhf-choosing-the-right-path-for-llm-training)

### Training Cost Breakdown

| Model         | Parameters | Pretraining Cost   | Hardware         | Training Time | Source                 |
| ------------- | ---------- | ------------------ | ---------------- | ------------- | ---------------------- |
| GPT-3         | 175B       | ~$4.6M             | 10,000 V100 GPUs | 1 month       | [Lambda Labs estimate] |
| GPT-4         | ~1.8T      | ~$100M (estimated) | Classified       | 3-6 months    | [Industry estimates]   |
| Llama 3 70B   | 70B        | ~$2M               | 16,000 H100 GPUs | 21 days       | [Meta blog]            |
| Claude 3 Opus | ~200B      | ~$10M (estimated)  | Classified       | 2-4 months    | [Industry estimates]   |

**Why so expensive?**

1. **GPU Costs**: $10,000-$30,000 per GPU × thousands of GPUs
2. **Energy**: Megawatts of electricity for months
3. **Storage**: Petabytes of training data
4. **Engineers**: Teams of ML engineers and researchers

**Source**: [arXiv: Understanding Performance & Cost of LLM Fine-Tuning](https://ui.adsabs.harvard.edu/abs/2024arXiv240804693X/abstract)

---

## Inference: Using the Model

### Definition

**Inference** is the process of using a trained model to generate outputs based on new inputs. This is the production workload - what happens every time a user sends a message to ChatGPT or your agent calls an LLM API.

**Analogy**: Inference is like applying your education to solve problems - fast, inexpensive (relative to training), done millions of times.

### Inference Process

```
1. User Input → "What is the capital of France?"
2. Tokenization → [825, 318, 262, 3139, 286, 4881, 30]
3. Forward Pass (one direction through all layers) → Compute probabilities
4. Sampling → Select "Paris" (highest probability)
5. Repeat → Generate " is" → "Paris" → "." → [STOP]
6. Output → "Paris is the capital of France."
```

**Key Difference from Training**:

-   Training: Adjust weights (backpropagation, multiple passes)
-   Inference: Use fixed weights (forward pass only, single direction)

### Inference Cost

**Pricing Models** (as of 2025):

| Model                     | Input Cost (per 1M tokens) | Output Cost (per 1M tokens) | Use Case                               |
| ------------------------- | -------------------------- | --------------------------- | -------------------------------------- |
| GPT-4o                    | $2.50                      | $10.00                      | General-purpose, high quality          |
| GPT-4o-mini               | $0.15                      | $0.60                       | Fast, cost-effective                   |
| Claude 3.5 Sonnet         | $3.00                      | $15.00                      | Long context, reasoning                |
| Gemini 2.5 Flash          | $0.10                      | $0.30                       | Ultra-fast, cheap                      |
| Llama 3 70B (self-hosted) | ~$0.50                     | ~$0.50                      | No API fees, fixed infrastructure cost |

**Example Cost Calculation**:

```
User sends 1,000-token message to GPT-4o
Agent has 5,000-token system prompt + tools + history
Total input: 6,000 tokens

Agent generates 500-token response

Cost:
- Input: 6,000 tokens × $2.50 / 1M = $0.015
- Output: 500 tokens × $10.00 / 1M = $0.005
- Total: $0.020 per request

For 100,000 requests/month: $2,000/month
```

**Source**: [OpenRouter Pricing](https://openrouter.ai/), [OpenAI Pricing](https://openai.com/pricing)

### Inference Optimization Techniques

#### 1. Quantization

**What**: Reduce precision of model weights (e.g., 32-bit → 8-bit → 4-bit)

**Trade-offs**:

-   ✅ 4-8x smaller model size
-   ✅ 2-4x faster inference
-   ✅ Lower memory requirements
-   ⚠️ Slight accuracy loss (typically <2%)

**Example**:

```
Llama 3 70B:
- Full precision (FP32): 280 GB
- 8-bit quantization (INT8): 70 GB  [75% smaller]
- 4-bit quantization (INT4): 35 GB  [87% smaller]
```

#### 2. KV-Cache

**What**: Cache intermediate key-value matrices during generation to avoid recomputation

**How it works**:

```
Without cache:
Token 1: Process full context
Token 2: Process full context + token 1  (redundant!)
Token 3: Process full context + tokens 1-2  (redundant!)

With cache:
Token 1: Process full context, cache KV
Token 2: Reuse cached KV, process only new token ✅
Token 3: Reuse cached KV, process only new token ✅
```

**Benefits**:

-   ✅ 3-10x faster generation
-   ✅ 60% cost reduction (less compute)
-   ⚠️ Higher memory usage (stores cache)

**Source**: [Manus.im: Context Engineering](https://www.manus.im/) (referenced in AGENTIC_PATTERNS_LIBRARY.md)

#### 3. Batching

**What**: Process multiple requests simultaneously to maximize GPU utilization

**Example**:

```
Sequential (1 request at a time):
Request 1: 100ms  (GPU 30% utilized)
Request 2: 100ms  (GPU 30% utilized)
Request 3: 100ms  (GPU 30% utilized)
Total: 300ms

Batched (3 requests simultaneously):
Requests 1-3: 120ms  (GPU 90% utilized) ✅
Total: 120ms  → 2.5x faster!
```

**Trade-offs**:

-   ✅ 2-10x higher throughput
-   ✅ Better GPU utilization
-   ⚠️ Slightly higher latency per request
-   ⚠️ Requires request buffering

#### 4. Model Distillation

**What**: Train smaller "student" model to mimic larger "teacher" model

**Example**:

```
Teacher: GPT-4 (1.8T parameters)
Student: GPT-4o-mini (~8B parameters)

Performance:
- Teacher: 90% accuracy, 2s latency, $10/1M tokens
- Student: 85% accuracy, 0.3s latency, $0.60/1M tokens ✅
```

**Use Cases**:

-   Edge deployment (mobile, IoT)
-   Cost-sensitive applications (high-volume)
-   Latency-critical systems (real-time chat)

---

## Key Differences: Training vs Inference

| Aspect          | Training                               | Inference                      |
| --------------- | -------------------------------------- | ------------------------------ |
| **Goal**        | Learn patterns from data               | Generate outputs from inputs   |
| **Frequency**   | Once (or periodic updates)             | Every API call (millions/day)  |
| **Cost**        | $1M - $100M+                           | $0.0001 - $0.10 per request    |
| **Time**        | Weeks to months                        | 0.1 - 5 seconds                |
| **Hardware**    | 1,000-10,000+ GPUs                     | 1 GPU per request (batched)    |
| **Who Does It** | Model providers (OpenAI, Google, Meta) | Application developers (you!)  |
| **Updates**     | Rare (months between versions)         | Constant (production workload) |
| **Compute**     | Forward + Backward passes (gradients)  | Forward pass only              |
| **Memory**      | Store gradients + optimizer state      | Store model weights only       |

---

## Practical Implications for Agent Development

### 1. You're an Inference User (Not a Trainer)

**Reality**: Unless you work at OpenAI/Google/Meta, you're using pretrained models via APIs.

**What you control**:

-   ✅ Model selection (GPT-4 vs Gemini vs Claude)
-   ✅ Prompt engineering (system prompts, few-shot examples)
-   ✅ Inference parameters (temperature, top-p)
-   ✅ Context management (what you send to the model)
-   ✅ Cost optimization (caching, prompt compression)

**What you don't control**:

-   ❌ Model architecture (number of layers, attention heads)
-   ❌ Training data (what the model was trained on)
-   ❌ Model weights (parameters are fixed)

### 2. Cost Optimization = Inference Optimization

**In production, 99.9% of costs are inference costs.**

**Optimization strategies** (from this codebase):

#### A. Token Reduction

```typescript
// ❌ Expensive: Send full page content every time (2,000 tokens)
const page = await cms_getPage({ slug: "about", includeContent: true });

// ✅ Cheap: Send metadata only (100 tokens)
const page = await cms_getPage({ slug: "about" }); // Default: includeContent=false

// Fetch content only when needed
if (userAsksAboutContent) {
	const section = await cms_getSectionContent({ pageSectionId: "hero-123" }); // 150 tokens
}

// Savings: 1,750 tokens (87% reduction) ✅
```

**Source**: [PROGRESS.md Sprint 15](../../PROGRESS.md#sprint-15-hybrid-content-fetching-token-optimization)

#### B. Context Compression

```typescript
// At 80% context capacity, compress completed subgoals
if (messages.length > 20) {
	// Compress first 15 messages into summary (10:1 ratio)
	const summary = await llm.summarize(messages.slice(0, 15));
	// Keep summary + recent 5 messages
	return { messages: [systemPrompt, summary, ...messages.slice(-5)] };
}

// Savings: 70% fewer tokens, same context ✅
```

**Source**: [AGENTIC_PATTERNS_LIBRARY.md Pattern #1](../../AGENTIC_PATTERNS_LIBRARY.md#1-hierarchical-memory-subgoal-based)

#### C. Caching (Future: Prompt Caching)

**OpenAI Prompt Caching** (beta as of 2025):

-   Cache system prompt + tool definitions
-   First request: Full cost
-   Subsequent requests: 90% discount on cached portion

```
Request 1:
- System prompt: 1,000 tokens ($0.0025)
- User message: 100 tokens ($0.00025)
Total: $0.00275

Request 2-100 (cached):
- System prompt: 1,000 tokens ($0.00025) ← 90% cheaper!
- User message: 100 tokens ($0.00025)
Total: $0.0005  → 81% savings!
```

### 3. When Fine-Tuning Makes Sense

**Consider fine-tuning when**:

1. **High volume** (>1M requests/month) → Fine-tuned model can be cheaper
2. **Specialized domain** (legal, medical) → Pretrained model lacks expertise
3. **Output format** → Need consistent JSON structure, specific style
4. **Privacy** → Can't send sensitive data to API (self-host fine-tuned model)

**Example**:

```
Scenario: Legal contract analysis (1M requests/month)

Option A: GPT-4 API
- Cost: $2.50 per 1M input tokens
- Quality: 85% accuracy on legal tasks
- Monthly cost: $2,500+

Option B: Fine-tuned Llama 3 70B (self-hosted)
- One-time fine-tuning: $5,000
- Infrastructure: $500/month (cloud GPU)
- Quality: 92% accuracy on legal tasks (specialized!)
- Monthly cost: $500  → 80% savings after month 1!
```

**When NOT to fine-tune**:

-   Low volume (<10k requests/month) → API cheaper
-   General-purpose tasks → Pretrained models already excellent
-   Rapid iteration → Fine-tuning takes days, prompt engineering takes minutes

---

## Real-World Cost Examples

### Example 1: This Codebase (Gemini 2.5 Flash)

**Configuration**:

```typescript
// .env
OPENROUTER_MODEL = google / gemini - 2.5 - flash;
// Pricing: $0.10 per 1M input tokens, $0.30 per 1M output tokens
```

**Typical Request**:

```
System prompt: 800 tokens
13 tool definitions: 1,500 tokens
Conversation history (10 messages): 3,000 tokens
Working memory: 200 tokens
User message: 50 tokens
───────────────────────────────
Total input: 5,550 tokens

Agent response: 500 tokens
Tool results: 1,000 tokens
───────────────────────────────
Total output: 1,500 tokens

Cost per request:
- Input: 5,550 × $0.10 / 1M = $0.00056
- Output: 1,500 × $0.30 / 1M = $0.00045
- Total: $0.00101  (~$0.001 or 0.1 cents)
```

**Monthly cost (10,000 requests)**:

```
10,000 requests × $0.001 = $10/month ✅
```

**Comparison with GPT-4**:

```
Same workload with GPT-4:
- Input: 5,550 × $2.50 / 1M = $0.01388
- Output: 1,500 × $10.00 / 1M = $0.015
- Total: $0.02888 per request

10,000 requests × $0.029 = $290/month ❌

Savings with Gemini: $280/month (97% cheaper!)
```

### Example 2: Customer Support Chatbot

**Scenario**: 100,000 conversations/month, average 10 messages each

**Model Selection**:

**Option A: GPT-4o-mini** (quality + cost)

```
Per conversation:
- 10 messages × 200 tokens = 2,000 tokens input
- 10 responses × 150 tokens = 1,500 tokens output

Cost:
- Input: 2,000 × $0.15 / 1M = $0.0003
- Output: 1,500 × $0.60 / 1M = $0.0009
- Total: $0.0012 per conversation

Monthly: 100,000 × $0.0012 = $120/month ✅
```

**Option B: GPT-4** (overkill)

```
Same workload:
- Input: 2,000 × $2.50 / 1M = $0.005
- Output: 1,500 × $10.00 / 1M = $0.015
- Total: $0.020 per conversation

Monthly: 100,000 × $0.020 = $2,000/month ❌
```

**Recommendation**: Use GPT-4o-mini → 94% cost savings with minimal quality loss.

---

## Key Takeaways

1. **Training**: Expensive ($1M-$100M), rare, done by model providers
2. **Inference**: Cheap ($0.0001-$0.10/request), constant, your workload
3. **Optimize inference**: Token reduction, compression, caching → 80-95% savings
4. **Fine-tuning**: Only for high-volume or specialized domains
5. **Model selection**: Balance quality, cost, speed for your use case
6. **You're an inference user**: Focus on prompt engineering, not training

---

## Codebase Examples

**Model Configuration**:

-   [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Model selection via OpenRouter
-   [`.env.example`](../../.env.example) - OPENROUTER_MODEL configuration

**Token Optimization**:

-   [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - Hybrid fetching (includeContent flag)
-   [`server/services/working-memory/`](../../server/services/working-memory/) - Entity extraction (context reduction)

**Related Documentation**:

-   [PROGRESS.md Sprint 15](../../PROGRESS.md#sprint-15-hybrid-content-fetching-token-optimization) - Token optimization strategies
-   [AGENTIC_PATTERNS_LIBRARY.md](../../AGENTIC_PATTERNS_LIBRARY.md) - Context management patterns

---

**Next Topic**: [0.1.3 Context Windows & Token Limits](./0.1.3-context-windows.md)  
**Related Topics**:

-   [0.1.5 Model Selection Guide](./0.1.5-model-selection.md)
-   [11.4.1 Token Reduction](../11-production/11.4.1-token-reduction.md)
-   [11.4.2 Model Selection](../11-production/11.4.2-model-selection.md)

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references + production cost analysis
