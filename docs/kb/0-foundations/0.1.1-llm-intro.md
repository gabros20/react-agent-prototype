# 0.1.1 What is a Large Language Model?

**Layer**: 0 - Foundations  
**Prerequisites**: None  
**Next**: [0.1.2 Training vs Inference](./0.1.2-training-vs-inference.md)

---

## Definition

A **Large Language Model (LLM)** is an advanced artificial intelligence system built on neural networks that can understand, generate, and manipulate human language at scale. LLMs are trained on massive datasets (billions of words from books, websites, code repositories) to learn statistical patterns, relationships, and structures in language.

### Core Characteristics

1. **Transformer Architecture**: All modern LLMs use the Transformer architecture introduced in the 2017 paper "Attention is All You Need" (Vaswani et al.)
2. **Massive Scale**: Billions to trillions of parameters (learned weights that store knowledge)
3. **Generative**: Can produce new text, not just classify or analyze existing text
4. **Contextual Understanding**: Process and generate text based on surrounding context

---

## How LLMs Work: The Transformer Architecture

### 1. Input Processing (Tokenization + Embedding)

```
User Input: "What is artificial intelligence?"
         ↓
Tokenization: ["What", "is", "artificial", "intelligence", "?"]
         ↓
Token IDs: [825, 318, 9552, 4430, 30]
         ↓
Embeddings: [[0.23, -0.45, ...], [0.12, 0.67, ...], ...]  (vectors)
```

**Tokenization**: Text is broken into smaller units (words or subwords) called tokens. For example, "AI" might be one token, while "artificial intelligence" might be split into 2-3 tokens.

**Embedding**: Each token is converted into a high-dimensional vector (e.g., 768 or 1536 dimensions) that represents its semantic meaning. Words with similar meanings have similar embeddings.

**Sources**: 
- [Google ML Crash Course: Transformers](https://developers.google.com/machine-learning/crash-course/llm/transformers)
- [Hugging Face: How Transformers Work](https://huggingface.co/learn/llm-course/en/chapter1/4)

### 2. Core Processing (Transformer Blocks)

The heart of an LLM consists of stacked Transformer blocks (12-96+ layers). Each block has two key components:

#### A. Self-Attention Mechanism

**What it does**: Allows the model to weigh the importance of different words relative to each other.

**Example**:
```
Input: "The cat sat on the mat because it was tired."

Self-attention helps the model understand:
- "it" refers to "cat" (not "mat")
- "tired" is related to "cat" (subject)
- "sat" is the action performed by "cat"
```

**How it works**:
1. For each token, compute Query (Q), Key (K), Value (V) vectors
2. Calculate attention scores: `score = Q · K^T / √d` (dot product scaled by dimension)
3. Apply softmax to get attention weights (sum to 1.0)
4. Weighted sum of Value vectors produces output

**Multi-Head Attention**: Run 8-32 parallel attention mechanisms to capture different types of relationships (syntax, semantics, co-reference, etc.)

**Sources**:
- [3Blue1Brown: Transformers Visual Explanation](https://www.youtube.com/watch?v=wjZofJX0v4M)
- [DataCamp: How Transformers Work](https://www.datacamp.com/tutorial/how-transformers-work)

#### B. Feed-Forward Network (MLP)

After attention, each token's representation passes through a **Multi-Layer Perceptron (MLP)**:

```
Token Embedding → Linear Layer → Non-Linearity (GELU/ReLU) → Linear Layer → Output
```

This refines each token's representation independently, adding capacity to learn complex patterns.

### 3. Output Generation (Language Modeling Head)

```
Final Embeddings → Linear Layer → Logits (vocab_size) → Softmax → Probabilities
```

**Language Modeling Head**: Converts final embeddings into probabilities over the entire vocabulary (50k-200k tokens).

**Example**:
```
Input: "The capital of France is"
Logits: [Paris: 0.85, London: 0.05, Berlin: 0.03, ...]
         ↓ (sampling or greedy selection)
Output Token: "Paris"
```

**Autoregressive Generation**: LLMs generate text one token at a time, feeding each new token back into the model to predict the next one.

---

## Key Concepts

### Parameters

**Definition**: Learned weights in the neural network (stored in matrices).

**Scale**:
- GPT-3: 175 billion parameters
- GPT-4: ~1.8 trillion parameters (estimated, not disclosed)
- Llama 3.1 70B: 70 billion parameters
- Gemini 1.5 Pro: ~1.5 trillion parameters (estimated)

**What parameters store**:
- Patterns in language (grammar, syntax)
- Factual knowledge (Paris is in France)
- Reasoning shortcuts (if A > B and B > C, then A > C)
- Style and tone patterns

**Source**: [Oracle: Large Language Models](https://www.oracle.com/artificial-intelligence/large-language-model/)

### Training Data

LLMs are trained on **web-scale datasets**:
- Common Crawl (web pages): ~800 billion tokens
- Books corpus: ~50 billion tokens
- Wikipedia: ~3 billion tokens
- GitHub code: ~100 billion tokens
- Reddit, arXiv papers, Stack Overflow, etc.

**Total training data**: Typically 1-10 trillion tokens (1 token ≈ 0.75 words).

**Unsupervised Learning**: Models learn by predicting the next word in sequences (no human labels required).

**Source**: [Elastic: Understanding LLMs](https://www.elastic.co/what-is/large-language-models)

### Inference

**Definition**: Using a trained model to generate new text.

**Process**:
1. User provides input (prompt)
2. Model processes tokens through Transformer layers
3. Model predicts next token probabilities
4. Sample next token (using temperature, top-p, etc.)
5. Repeat until stop condition (max length, stop token, etc.)

**Latency**: Generating text takes 0.1-2 seconds per token depending on model size and hardware.

---

## Types of Transformer Models

### 1. Encoder-Only (BERT-like)

**Architecture**: Input → Encoder → Contextualized Embeddings

**Use Case**: Classification, sentiment analysis, named entity recognition

**Examples**: BERT, RoBERTa, DeBERTa

**Limitation**: Cannot generate text (no autoregressive decoding)

### 2. Decoder-Only (GPT-like) ⭐

**Architecture**: Input → Decoder → Next Token Prediction

**Use Case**: Text generation, completion, chat, code generation

**Examples**: GPT-3, GPT-4, Llama, Claude, Gemini, Mistral

**Strength**: Simplicity, scalability, strong few-shot learning

**Why most popular**: Easier to scale, better at generation tasks, unified architecture.

**Source**: [Hugging Face: Transformer Types](https://huggingface.co/learn/llm-course/en/chapter1/4)

### 3. Encoder-Decoder (T5-like)

**Architecture**: Input → Encoder → Context → Decoder → Output

**Use Case**: Translation, summarization, question answering

**Examples**: T5, BART, mT5

**Strength**: Best for sequence-to-sequence tasks with different input/output formats.

---

## Capabilities & Limitations

### What LLMs Can Do Well ✅

1. **Text Generation**: Write essays, stories, emails, code
2. **Question Answering**: Provide factual answers based on knowledge
3. **Summarization**: Condense long documents into key points
4. **Translation**: Translate between languages (100+ languages)
5. **Code Generation**: Write functions, debug code, explain algorithms
6. **Few-Shot Learning**: Learn new tasks from 1-5 examples (no fine-tuning)
7. **Reasoning**: Perform multi-step reasoning (especially with chain-of-thought)

### What LLMs Struggle With ⚠️

1. **Hallucinations**: Generate plausible-sounding but factually incorrect information
2. **Math & Logic**: Struggle with precise calculations (improving with reasoning models)
3. **Up-to-Date Knowledge**: Training data has a cutoff date (e.g., GPT-4 trained on data until April 2023)
4. **Long-Range Coherence**: Can lose track of topics in very long conversations
5. **Common Sense**: Sometimes fail at tasks obvious to humans (e.g., physical reasoning)
6. **Bias**: Reflect biases present in training data

**Source**: [Google ML: LLM Limitations](https://developers.google.com/machine-learning/crash-course/llm/transformers)

---

## Timeline of LLM Evolution

| Year | Model | Parameters | Key Innovation |
|------|-------|------------|----------------|
| 2017 | Original Transformer | - | Attention mechanism (Vaswani et al.) |
| 2018 | GPT-1 | 117M | First pretrained generative model |
| 2018 | BERT | 340M | Bidirectional encoder (masked prediction) |
| 2019 | GPT-2 | 1.5B | Scaled up, text quality leap |
| 2020 | GPT-3 | 175B | Few-shot learning without fine-tuning |
| 2022 | InstructGPT / ChatGPT | 175B | RLHF (human feedback alignment) |
| 2023 | GPT-4 | ~1.8T | Multimodal (vision + text), longer context |
| 2023 | Llama 2 | 7B-70B | Open weights, competitive performance |
| 2024 | GPT-4o | ~1.8T | Faster, cheaper, multimodal native |
| 2024 | Claude 3 Opus | ~200B | 200k context window, superior reasoning |
| 2024 | Gemini 1.5 Pro | ~1.5T | 2 million token context window |
| 2024 | o1 (OpenAI) | Unknown | **Reasoning model** with chain-of-thought |

**Key Trends**:
- Exponential parameter growth (117M → 1.8T in 6 years)
- Context windows expanding (2k → 2M tokens)
- Shift toward reasoning and safety (RLHF, reasoning models)

**Source**: [Hugging Face: LLM Timeline](https://huggingface.co/learn/llm-course/en/chapter1/4)

---

## Practical Example: LLM in Your Codebase

### Codebase Integration (AI SDK v6 + OpenRouter)

In this project, we use **OpenRouter** as a gateway to multiple LLM providers:

```typescript
// server/agent/orchestrator.ts
import { openrouter } from '@openrouter/ai-sdk-provider'

const modelId = process.env.OPENROUTER_MODEL || 'google/gemini-2.5-flash'

const agent = new ToolLoopAgent({
  model: openrouter.languageModel(modelId),
  instructions: systemPrompt,  // From server/prompts/react.xml
  tools: ALL_TOOLS,
  experimental_context: context,
  // ... configuration
})
```

**Models available via OpenRouter**:
- `google/gemini-2.5-flash` (fast, cheap, 128k context)
- `anthropic/claude-3.5-sonnet` (reasoning, 200k context)
- `openai/gpt-4o` (multimodal, 128k context)
- `openai/o1` (reasoning model, chain-of-thought)

**Why this architecture?**:
- **Flexibility**: Switch models without code changes (just env var)
- **Cost optimization**: Use fast models for simple tasks, powerful models for complex
- **Future-proof**: New models available immediately via OpenRouter

**Related Files**:
- [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - LLM integration
- [`server/prompts/react.xml`](../../server/prompts/react.xml) - System instructions
- [`.env`](../../.env.example) - Model configuration

---

## Further Reading

### Academic Papers
- [Attention is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)

### Interactive Resources
- [Transformer Explainer (Visual Tool)](https://poloclub.github.io/transformer-explainer/) - Interactive visualization
- [3Blue1Brown: Transformers (Video)](https://www.youtube.com/watch?v=wjZofJX0v4M) - Best visual explanation
- [DeepLearning.AI: How Transformer LLMs Work (Course)](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)

### Documentation
- [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/en/chapter1/4)
- [Google ML Crash Course: LLMs](https://developers.google.com/machine-learning/crash-course/llm/transformers)
- [OpenAI Platform Docs](https://platform.openai.com/docs/)

---

## Key Takeaways

1. **LLMs are autoregressive**: They predict one token at a time, feeding output back as input
2. **Transformers use attention**: Self-attention allows models to understand relationships between words
3. **Scale matters**: Larger models (more parameters, more data) generally perform better
4. **Context is finite**: Models have token limits (2k-2M) constraining what they can process
5. **Training is expensive**: GPT-4 cost ~$100 million to train, but inference is cheap (~$0.01/1k tokens)
6. **They're not perfect**: Hallucinations, bias, and factual errors are common

---

**Next Topic**: [0.1.2 Training vs Inference](./0.1.2-training-vs-inference.md)  
**Related Topics**: 
- [0.1.3 Context Windows & Token Limits](./0.1.3-context-windows.md)
- [0.2.1 Standard Models](./0.2.1-standard-models.md)
- [3.2.1 ReAct Loop](../3-agents/3.2.1-react-loop.md) (How LLMs power agents)

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10 authoritative references cited
