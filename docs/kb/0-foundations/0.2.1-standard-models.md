# 0.2.1 Standard Models vs Thinking (Reasoning) Models

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)  
**Next**: [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)

---

## Overview

Starting in late 2024 and accelerating through 2025, a new class of language models emerged: **reasoning models** (also called **thinking models**). These models differ fundamentally from standard models in how they approach complex problems, introducing explicit reasoning chains before generating final answers.

**Key Innovation**: Instead of immediately responding, reasoning models **think step-by-step** internally before producing output.

**As of November 2025**: Reasoning models have become more sophisticated, with multiple providers (OpenAI, DeepSeek, Alibaba, Moonshot AI) offering both commercial and open-source options.

---

## Standard Models (GPT-5.1, Claude 4.5, Gemini 2.5)

### How They Work

**Architecture**: Transformer decoder (autoregressive generation)

**Process**:

```
User Prompt → Tokenization → Transformer Layers → Output Token → Repeat
                 ↓                    ↓                 ↓
              Embeddings          Attention        Probability
                                  + MLP            Distribution
```

**Generation**: One token at a time, immediately visible to the user.

**Example**:

```
User: "What is 347 × 829?"

GPT-5.1 (standard mode):
- Generates token by token: "347", "×", "829", "is", "approximately"...
- May calculate: "287,663" (sometimes correct, sometimes off by a bit)
- No visible "thinking" - goes straight to answer
```

### Characteristics

| Feature              | Standard Models                                   |
| -------------------- | ------------------------------------------------- |
| **Generation**       | Immediate, token-by-token                         |
| **Thinking**         | Implicit (happens in hidden layers)               |
| **Chain-of-Thought** | Only if prompted ("Let's think step by step")     |
| **Reasoning Depth**  | Limited by prompt engineering                     |
| **Best For**         | Text generation, chat, summarization, translation |
| **Latency**          | Fast (0.5-2s for short responses)                 |
| **Cost**             | $0.01-0.10 per 1k tokens                          |

**Examples** (November 2025):

-   GPT-5.1, GPT-4o, GPT-4o-mini
-   Claude 4.5 Opus, Claude 4.5 Sonnet, Claude 4.5 Haiku
-   Gemini 2.5 Pro, Gemini 2.5 Flash
-   Llama 4 405B, Llama 4 70B, Llama 4 8B
-   Mistral Large 3
-   MiniMax M2, Qwen3-235B, GLM-4.6

**Strengths** ✅:

-   Fast responses
-   Cost-effective
-   Versatile (good at many tasks)
-   Works well for conversational AI

**Weaknesses** ⚠️:

-   Struggles with complex math
-   Can skip logical steps
-   Limited multi-step reasoning
-   Hallucinations in complex scenarios

**Source**: [OpenAI Platform Docs: Standard Models](https://platform.openai.com/docs/)

---

## Reasoning Models (o1, o3, DeepSeek-R1, Qwen3-235B)

### How They Work

**Architecture**: Transformer decoder + **Reinforcement Learning (RL)** for chain-of-thought

**Process**:

```
User Prompt → Internal Reasoning Chain → Final Answer
                        ↓
                [Hidden from user]
                        ↓
            "Let me think about this..."
            Step 1: Parse the problem
            Step 2: Break into subproblems
            Step 3: Solve each subproblem
            Step 4: Verify solution
            Step 5: Combine results
                        ↓
                Final Answer
```

**Key Difference**: Models are **trained via RL** to generate extensive reasoning chains **before** producing the final answer.

**Training Method**:

1. Model generates reasoning chain + answer
2. Reward given based on correctness (verified ground truth)
3. Model learns: "Longer, more careful reasoning → Better answers"
4. Repeat millions of times → Model internalizes reasoning process

**Source**: [LessWrong: o1 Technical Primer](https://www.lesswrong.com/posts/byNYzsfFmb2TpYFPW/o1-a-technical-primer)

### Example: OpenAI o1

**Released**: December 5, 2024 (originally codenamed "Q\*" then "Strawberry")  
**Updated**: o1 and o3 models continue to lead in reasoning tasks as of November 2025

**How it differs**:

```
User: "What is 347 × 829?"

o1 (reasoning model):
[Internal reasoning - not shown to user by default]
- "This is a multiplication problem"
- "Let me break it down: 347 × 800 + 347 × 29"
- "347 × 800 = 277,600"
- "347 × 29 = 347 × 30 - 347 = 10,410 - 347 = 10,063"
- "Total: 277,600 + 10,063 = 287,663"
- "Let me verify: 287,663 ÷ 347 = 829 ✓"

[Final answer shown to user]
"287,663"
```

**Result**: **100% accuracy** on math problems vs ~85% for GPT-4 standard.

**Source**: [OpenAI o1 Announcement](https://openai.com/index/introducing-openai-o1-preview/)

### Chain-of-Thought (Built-in vs Prompted)

#### Standard Models: Prompted CoT

You must **explicitly ask** for step-by-step thinking:

```
Prompt: "Let's think step by step. What is 347 × 829?"

GPT-4:
"Let me break this down:
1. 347 × 800 = 277,600
2. 347 × 29 = 10,063
3. Total: 277,600 + 10,063 = 287,663"
```

**Problem**: CoT quality depends on prompt. Without it, model may skip steps.

#### Reasoning Models: Native CoT

No prompt engineering needed - **always thinks step-by-step**:

```
Prompt: "What is 347 × 829?"  (no CoT prompt!)

o1:
[Automatically generates internal reasoning chain]
→ Arrives at correct answer: 287,663
```

**Advantage**: Consistent reasoning, no prompt engineering required.

**Source**: [RAG with Reasoning LLMs: o1 vs GPT-4o](https://flowhunt.io/blog/rag-with-reasoning-llms-openai-o1-vs-openai-gpt4o)

### Benchmark Performance

| Benchmark                    | GPT-4o          | o1-preview      | o1              | Improvement |
| ---------------------------- | --------------- | --------------- | --------------- | ----------- |
| **AIME (Math Olympiad)**     | 13%             | 74%             | 83%             | **6.4x**    |
| **GPQA (Graduate Science)**  | 56%             | 78%             | 86%             | **1.5x**    |
| **Codeforces (Programming)** | 11th percentile | 89th percentile | 93rd percentile | **8x**      |
| **MMLU (General Knowledge)** | 88%             | 91%             | 92%             | **1.05x**   |

**Key Insight**: Reasoning models excel at **complex reasoning tasks** (math, science, code) but are only marginally better at **factual knowledge**.

**Source**: [FlowHunt: RAG with Reasoning LLMs](https://flowhunt.io/blog/rag-with-reasoning-llms-openai-o1-vs-openai-gpt4o)

---

## Key Differences

| Aspect              | Standard Models                          | Reasoning Models                       |
| ------------------- | ---------------------------------------- | -------------------------------------- |
| **Thinking**        | Implicit (hidden layers)                 | Explicit (reasoning tokens)            |
| **CoT**             | Must prompt ("Let's think step by step") | Built-in (always reasons)              |
| **Training**        | Supervised (predict next token)          | RL + Supervised (maximize correctness) |
| **Reasoning Depth** | Shallow (1-2 steps)                      | Deep (10-100+ steps)                   |
| **Latency**         | Fast (0.5-2s)                            | Slow (5-60s)                           |
| **Cost**            | $0.01-0.10 per 1k tokens                 | $0.15-3.00 per 1k tokens (10-30x more) |
| **Best For**        | Chat, text generation, summarization     | Math, science, coding, logic puzzles   |
| **Hallucinations**  | Moderate                                 | Lower (verifies reasoning)             |
| **Context Window**  | 128k-200k tokens                         | 128k-200k tokens (same)                |

---

## When to Use Each

### Use Standard Models When:

✅ **Speed matters** - Real-time chat, conversational AI  
✅ **Cost-sensitive** - High-volume applications  
✅ **Simple tasks** - Text generation, translation, summarization  
✅ **Versatile workload** - Mix of different task types

**Examples**:

-   Customer support chatbots
-   Content generation
-   Email drafting
-   Simple Q&A
-   Code completion (not complex algorithms)

### Use Reasoning Models When:

✅ **Accuracy is critical** - Math, science, legal analysis  
✅ **Complex reasoning** - Multi-step logic, strategy games  
✅ **Verification needed** - Must double-check answers  
✅ **Budget allows** - Can afford 10-30x higher cost

**Examples**:

-   Mathematical proofs
-   Scientific research assistance
-   Complex code generation (algorithms, debugging)
-   Legal contract analysis
-   Strategic planning (chess, business strategy)

**Source**: [OpenAI Platform: Reasoning Models Guide](https://platform.openai.com/docs/guides/reasoning)

---

## Reasoning Tokens (Exposed vs Hidden)

### OpenAI o1: Hidden Reasoning

By default, o1's reasoning chain is **not shown** to users:

```
User sees only:
Input: "Solve: 2x + 5 = 13"
Output: "x = 4"

Behind the scenes (hidden):
- "This is a linear equation"
- "Subtract 5 from both sides: 2x = 8"
- "Divide by 2: x = 4"
- "Verify: 2(4) + 5 = 13 ✓"
```

**Why hidden?**:

-   Cleaner UX
-   Faster perceived response
-   Reasoning tokens not counted toward output limits

**How to see reasoning** (in API):

```json
{
  "model": "o1",
  "messages": [...],
  "reasoning_effort": "high",
  "include_reasoning": true  // Show internal thoughts
}
```

**Source**: [OpenAI Reasoning Models Docs](https://platform.openai.com/docs/guides/reasoning)

### Reasoning Effort Levels

**API Parameter**: `reasoning_effort`

| Level      | Description                      | Latency | Cost     | Use Case                     |
| ---------- | -------------------------------- | ------- | -------- | ---------------------------- |
| **low**    | Quick reasoning (5-10 steps)     | 2-5s    | $0.15/1k | Simple math, basic logic     |
| **medium** | Moderate reasoning (20-50 steps) | 10-20s  | $0.50/1k | Code generation, proofs      |
| **high**   | Deep reasoning (100+ steps)      | 30-60s  | $3.00/1k | Research, complex algorithms |

**Default**: medium

**Example**:

```typescript
// Quick reasoning for simple task
const result = await openai.chat.completions.create({
	model: "o1",
	messages: [{ role: "user", content: "What is 5 + 3?" }],
	reasoning_effort: "low", // Fast, cheap
});

// Deep reasoning for complex task
const result = await openai.chat.completions.create({
	model: "o1",
	messages: [{ role: "user", content: "Prove Fermat's Last Theorem" }],
	reasoning_effort: "high", // Slow, expensive, thorough
});
```

---

## Technical Deep Dive: How Reasoning Emerges

### Reinforcement Learning (RL) Process

**Training Flow**:

```
1. Model generates reasoning chain → Answer
2. Check if answer is correct (using verifier/ground truth)
3. If correct → Reward +1
   If incorrect → Reward 0 or -1
4. Model adjusts weights to maximize reward
5. Repeat millions of times
```

**Emergent Behaviors** (not explicitly programmed):

-   **Backtracking**: "Wait, this approach won't work. Let me try another way."
-   **Verification**: "Let me check if this answer makes sense..."
-   **Alternative Paths**: "I'll try 3 different methods and compare results."
-   **Self-Correction**: "Actually, I made a mistake in step 2. Let me redo it."

**Key Research**: Sebastien Bubeck (Microsoft Research) showed these behaviors **emerge naturally** from RL training - they're not hardcoded rules!

**Source**: [AndoLogs: o1 and Reasoning](https://blog.ando.ai/posts/o1-and-reasoning/)

### Why Longer Thinking = Better Answers

**Observation**: Models trained with RL learn to "think longer" on harder problems.

**Mechanism**:

1. Model attempts quick answer → Gets it wrong → Low reward
2. Model attempts longer reasoning → Gets it right → High reward
3. Over time, model learns: "Hard problem → Think longer"

**Result**: o1 automatically adjusts reasoning depth based on problem difficulty.

**Source**: [LessWrong: o1 Technical Primer](https://www.lesswrong.com/posts/byNYzsfFmb2TpYFPW/o1-a-technical-primer)

---

## Model Comparison Table (November 2025)

| Model                 | Type      | Released | Context | Reasoning                    | Cost (1k tokens)                   | Best For                  |
| --------------------- | --------- | -------- | ------- | ---------------------------- | ---------------------------------- | ------------------------- |
| **GPT-5.1**           | Standard  | 2025-08  | 256k    | Prompted CoT + Thinking Mode | $0.10 (input), $0.20 (output)      | General tasks, multimodal |
| **GPT-4o**            | Standard  | 2024-05  | 128k    | Prompted CoT                 | $0.01 (input), $0.02 (output)      | Chat, general tasks       |
| **GPT-4o-mini**       | Standard  | 2024-07  | 128k    | Prompted CoT                 | $0.00015 (input), $0.0006 (output) | High-volume, simple tasks |
| **o1**                | Reasoning | 2024-12  | 128k    | Native CoT                   | $0.15 (input), $0.60 (output)      | Complex reasoning         |
| **o3**                | Reasoning | 2025-03  | 128k    | Advanced CoT                 | $0.20 (input), $0.80 (output)      | Expert-level reasoning    |
| **o1-mini**           | Reasoning | 2024-09  | 128k    | Native CoT (fast)            | $0.03 (input), $0.12 (output)      | Coding (80% cheaper)      |
| **Claude 4.5 Sonnet** | Standard  | 2025-09  | 200k    | Prompted CoT                 | $0.03 (input), $0.15 (output)      | Coding, agents            |
| **Claude 4.5 Haiku**  | Standard  | 2025-10  | 200k    | Prompted CoT                 | $0.01 (input), $0.05 (output)      | Fast, cost-effective      |
| **Gemini 2.5 Pro**    | Standard  | 2025-07  | 1M      | Prompted CoT + Deep Think    | $0.0015 (input), $0.007 (output)   | Long context              |
| **Gemini 2.5 Flash**  | Standard  | 2025-06  | 1M      | Prompted CoT                 | $0.0001 (input), $0.0004 (output)  | Fast, cheap               |
| **DeepSeek-R1**       | Reasoning | 2025-01  | 128k    | Native CoT                   | $0.55 (input), $2.19 (output)      | Cost-effective reasoning  |
| **Qwen3-235B**        | Reasoning | 2025-09  | 128k    | Native CoT (Thinking Mode)   | $1.20 (input), $6.00 (output)      | Open reasoning            |
| **MiniMax M2**        | Standard  | 2025-10  | 200k    | Prompted CoT                 | Free (open-source)                 | Agentic tasks, coding     |
| **Llama 4 405B**      | Standard  | 2025-08  | 256k    | Prompted CoT                 | Free (open-source)                 | Privacy, self-host        |

**Source**: [OpenRouter Pricing](https://openrouter.ai/), [OpenAI Pricing](https://openai.com/pricing)

---

## Practical Recommendations

### For General-Purpose Agents (November 2025)

**Use standard models (Claude 4.5, GPT-4o-mini, Gemini 2.5)**:

-   Most agentic tasks don't require deep reasoning
-   Tool calls, text generation, summarization work well with standard models
-   Cost-effective for high-volume workloads

**Recommended**: Claude 4.5 Haiku for best balance (speed + quality + tool calling)

**Example** (this codebase):

```typescript
// Recommended: Claude 4.5 Haiku (best for agents in 2025)
const modelId = "anthropic/claude-4.5-haiku";

// Alternative: Gemini 2.5 Flash (cheaper, faster)
// const modelId = 'google/gemini-2.5-flash'

// Alternative: GPT-4o-mini (most affordable)
// const modelId = 'openai/gpt-4o-mini'

const agent = new ToolLoopAgent({
	model: openrouter.languageModel(modelId),
	instructions: systemPrompt,
	tools: ALL_TOOLS,
	// ... config
});
```

### For Complex Reasoning Tasks (November 2025)

**Use reasoning models (o3, o1, DeepSeek-R1, Qwen3-235B)**:

-   Math calculations, proofs
-   Complex algorithms, debugging
-   Scientific reasoning, hypothesis testing
-   Multi-step planning with verification

**Best Options**:

-   **o3**: Expert-level reasoning (most capable)
-   **o1**: Strong general reasoning
-   **DeepSeek-R1**: Cost-effective reasoning ($0.55-$2.19)
-   **Qwen3-235B**: Open-source reasoning (best open option)

**Example**:

```typescript
// Switch to reasoning model for complex tasks
const modelId = process.env.USE_REASONING
	? "openai/o3" // Best reasoning (expensive)
	: process.env.USE_OPEN
	? "qwen/qwen3-235b" // Open-source reasoning
	: "anthropic/claude-4.5-haiku"; // Standard (fast, cheap)
```

### Hybrid Approach (Best of Both Worlds) - Recommended 2025

**Pattern**: Use standard model for coordination, reasoning model for hard subproblems.

```typescript
// Orchestrator: Claude 4.5 Haiku (best for agents)
const orchestrator = new ToolLoopAgent({
	model: openrouter.languageModel("anthropic/claude-4.5-haiku"),
	tools: { ...standardTools, callReasoningAgent },
});

// Reasoning sub-agent: o3 or Qwen3-235B (deep thinking)
const reasoningAgent = new ToolLoopAgent({
	model: openrouter.languageModel(process.env.USE_OPEN ? "qwen/qwen3-235b" : "openai/o3"),
	tools: { solveComplexProblem },
});

// Orchestrator delegates hard problems to reasoning agent
```

**Cost**: Only pay for reasoning on tasks that need it (~5-10% of requests).

**2025 Advantage**: MiniMax M2 and Qwen3-235B offer free/cheap open-source reasoning alternatives!

---

## Future: Reasoning at Scale (2025 and Beyond)

### Current Trends (November 2025)

1. **Faster reasoning models**: o1-mini, o3-mini variants emerging
2. **Open-source reasoning explosion**: DeepSeek-R1, Qwen3-235B, MiniMax M2, Kimi K2
3. **Hybrid models**: Standard models gaining "thinking modes" (GPT-5.1, Gemini 2.5 Pro)
4. **Reasoning specialization**: Domain-specific reasoning (Qwen3-Coder, math-focused models)
5. **Cost reduction**: Open-source alternatives reducing reasoning costs by 10-100×

### Research Directions

-   **Shorter reasoning chains**: Compress 100-step reasoning into 10 steps (same accuracy) ✅ _Achieved in 2025_
-   **Adaptive reasoning depth**: Automatically adjust effort based on problem difficulty ✅ _Partially achieved_
-   **Reasoning verification**: External verifiers check reasoning correctness _(Active research)_
-   **Multi-modal reasoning**: Vision + language reasoning (o3-vision, Gemini 2.5) ✅ _Available_
-   **Agentic reasoning**: Models optimized for tool calling + reasoning (MiniMax M2) ✅ _Emerging_

**Source**: [OpenAI Community: Reasoning Models](https://community.openai.com/t/new-reasoning-models-openai-o1-preview-and-o1-mini/893691)

---

## Key Takeaways

1. **Standard models**: Fast, cheap, versatile - best for most tasks
2. **Reasoning models**: Slow, expensive, specialized - best for complex reasoning
3. **Native CoT**: Reasoning models always think step-by-step (no prompting needed)
4. **RL training**: Enables emergent behaviors (backtracking, verification)
5. **10-30x cost difference**: Use reasoning models sparingly
6. **Hybrid approach**: Standard for coordination, reasoning for hard subproblems

---

## Codebase Integration (November 2025)

**Recommended setup** (flexible model selection):

```typescript
// .env - Updated for 2025
OPENROUTER_MODEL=anthropic/claude-4.5-haiku  // Best for agents (recommended)
# OPENROUTER_MODEL=google/gemini-2.5-flash  // Cheapest option
# OPENROUTER_MODEL=openai/gpt-4o-mini       // Most affordable commercial
# OPENROUTER_MODEL=openai/o3                // Reasoning (opt-in, expensive)
# OPENROUTER_MODEL=qwen/qwen3-235b          // Open reasoning (cost-effective)
```

**Files**:

-   [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Model configuration
-   [`.env.example`](../../.env.example) - Model selection

**2025 Recommendation**:

-   **Default**: Claude 4.5 Haiku (best balance for agentic systems)
-   **Budget**: GPT-4o-mini or Gemini 2.5 Flash
-   **Reasoning**: o3 (commercial) or Qwen3-235B (open-source)
-   **Open-source**: MiniMax M2 or Llama 4 405B

---

**Next Topic**: [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)  
**Related Topics**:

-   [1.1.3 Chain-of-Thought](../1-prompts/1.1.3-chain-of-thought.md)
-   [6.2.1 Reflexion Loop](../6-planning/6.2.1-reflexion-loop.md)
-   [3.2.1 ReAct Loop](../3-agents/3.2.1-react-loop.md)

---

**Last Updated**: 2025-11-17  
**Status**: ✅ Complete (Updated for November 2025)  
**Sources**: 15+ authoritative references including OpenAI, Anthropic, Google, Alibaba, research blogs, LiveBench
