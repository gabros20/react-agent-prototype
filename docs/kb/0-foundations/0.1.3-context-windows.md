# 0.1.3 Context Windows & Token Limits

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)  
**Next**: [0.1.4 Sampling Parameters](./0.1.4-sampling-parameters.md)

---

## Definition

A **context window** (also called **context length**) is the maximum amount of text (measured in tokens) that a language model can process in a single interaction. It functions as the model's "working memory" - everything the model can "see" and consider when generating a response.

```
Context Window = Input Tokens + Output Tokens
```

**Example**:
- GPT-4 with 128k context window can handle:
  - ~96,000 words of input text
  - Or ~300 pages of a book
  - Or ~50,000 lines of code

**Why it matters**: Context windows determine:
1. How much conversation history the model remembers
2. How large a document it can analyze
3. How complex a task it can handle in one go
4. How many tool results can be injected for agents

---

## Token Basics

### What is a Token?

A **token** is the atomic unit of text that LLMs process. Tokens can be:
- Whole words: `"hello"` = 1 token
- Subwords: `"unhappiness"` = 2 tokens (`"un"`, `"happiness"`)
- Characters: `"你好"` (Chinese) = 2 tokens
- Punctuation: `"."` = 1 token

### Token-to-Word Ratio

**English**: 1 token ≈ 0.75 words (or 1 word ≈ 1.33 tokens)

**Example**:
```
"The quick brown fox jumps over the lazy dog."
→ 10 words
→ ~13 tokens (including punctuation and spaces)
```

**Other Languages**:
- Spanish: 1 token ≈ 0.7 words
- German: 1 token ≈ 0.6 words (longer compound words)
- Chinese: 1 token ≈ 0.5 characters (each character is 1-2 tokens)
- Code: 1 token ≈ 0.5-1 word (depending on syntax)

**Source**: [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)

### Why Tokens Matter for Agents

In agentic systems, tokens accumulate from:
1. **System prompt**: 500-2,000 tokens (instructions, capabilities, rules)
2. **Conversation history**: 100-50,000 tokens (user messages + agent responses)
3. **Tool definitions**: 50-200 tokens per tool (descriptions, schemas)
4. **Tool results**: 100-5,000 tokens per result (database queries, API responses)
5. **Working memory**: 100-1,000 tokens (recently accessed entities)
6. **Output generation**: 500-4,000 tokens (agent's response)

**Real-world example** (from this codebase):
```
System prompt (react.xml): ~800 tokens
13 tools (metadata): ~1,500 tokens
Conversation (10 messages): ~3,000 tokens
Working memory (5 entities): ~200 tokens
Current user message: ~50 tokens
─────────────────────────────────────────
Total input: ~5,550 tokens

Agent generates response: ~500 tokens
─────────────────────────────────────────
Total context used: ~6,050 tokens
```

For a model with **128k context window**, this uses **4.7%** of capacity. ✅ Plenty of room.

But after **100 steps** in a long conversation, you could hit **80-100k tokens** → **Context management required**. ⚠️

---

## Context Window Sizes by Model (2025)

| Model | Context Window | Input | Output | Release |
|-------|----------------|-------|--------|---------|
| **GPT-3.5 Turbo** | 16k | 16k | 4k | 2022 |
| **GPT-4** | 128k | 128k | 4k | 2023 |
| **GPT-4 Turbo** | 128k | 128k | 4k | 2023 |
| **GPT-4o** | 128k | 128k | 4k | 2024 |
| **GPT-4o-mini** | 128k | 128k | 16k | 2024 |
| **Claude 2** | 100k | 100k | 4k | 2023 |
| **Claude 3 Opus** | 200k | 200k | 4k | 2024 |
| **Claude 3.5 Sonnet** | 200k | 200k | 8k | 2024 |
| **Claude Sonnet 4** | **1M** | 1M | 8k | 2024 |
| **Gemini 1.0 Pro** | 32k | 32k | 2k | 2023 |
| **Gemini 1.5 Pro** | **2M** | 2M | 8k | 2024 |
| **Gemini 2.5 Flash** | 128k | 128k | 8k | 2024 |
| **Llama 3** | 8k | 8k | 2k | 2024 |
| **Llama 3.1** | 128k | 128k | 4k | 2024 |
| **Llama 4 Maverick** | **1M** | 1M | 8k | 2025 |
| **Mistral Large** | 128k | 128k | 4k | 2024 |
| **DeepSeek-R1** | 128k | 128k | 8k | 2024 |

**Key Trends**:
- **2023**: 8k-128k windows (standard)
- **2024**: 128k-200k windows (mainstream)
- **2025**: 1M-2M windows (cutting-edge)

**Sources**:
- [Codingscape: LLMs with Largest Context Windows](https://codingscape.com/blog/llms-with-largest-context-windows)
- [DataStudios: Claude Context Windows](https://www.datastudios.org/post/claude-ai-context-window-token-limits-and-memory)
- [Medium: Context Length Extension](https://medium.com/@arghya05/expanding-the-horizons-of-language-models-a-deep-dive-into-context-length-extension-techniques-52d3f59d5fc3)

---

## Why Context Windows Have Limits

### 1. Computational Complexity (Quadratic Cost)

The **self-attention mechanism** has **O(n²)** complexity, where `n` is the sequence length.

**What this means**:
- Doubling context length → **4x computation cost**
- 10x context length → **100x computation cost**

**Example**:
```
8k context   → 64M operations (8k²)
128k context → 16.4B operations (128k²)  [256x more expensive!]
```

**Source**: [Augment Code: Llama 3 Context Window](https://www.augmentcode.com/guides/llama-3-context-window-explained-limits-and-opportunities)

### 2. Memory Requirements

Transformer models store **Key-Value (KV) caches** for each token to avoid recomputation.

**Memory per token**: ~2 KB (for a 70B parameter model)

**Total memory**:
```
128k tokens × 2 KB = 256 MB (per user session)
1M tokens × 2 KB = 2 GB (per user session)
```

For **1,000 concurrent users** with 1M context:
```
1,000 users × 2 GB = 2 TB of GPU memory required!
```

**Source**: [Qodo: Context Windows](https://www.qodo.ai/blog/context-windows/)

### 3. Accuracy Degradation ("Lost in the Middle")

**Research finding**: LLMs struggle to retrieve information from the **middle of long contexts**.

**Accuracy by position** (Liu et al. 2023):
- Beginning of context: **90% accuracy**
- End of context: **85% accuracy**
- Middle of context: **60% accuracy** ⚠️

**Implication for agents**: 
- Place critical information at the **start** (system prompt) or **end** (working memory)
- Don't bury important context in the middle of long conversations

**Source**: [Kolena: LLM Context Windows](https://www.kolena.com/guides/llm-context-windows-why-they-matter-and-5-solutions-for-context-limits)

### 4. Latency Increases

**Processing time grows superlinearly** with context length:

| Context Length | Latency (GPT-4) |
|----------------|-----------------|
| 1k tokens | 0.5s |
| 10k tokens | 2s |
| 50k tokens | 8s |
| 128k tokens | 20s+ |

**Why**: More tokens → More layers → More attention → More computation

---

## Implications for AI Agents

### Problem: Context Explosion

**Scenario**: Multi-step agent conversation

```
Step 1: User asks "List all pages"
→ Agent calls cms_listPages
→ Returns 20 pages (2,000 tokens)

Step 2: User asks "Show content for About page"
→ Agent calls cms_getPage(slug="about")
→ Returns page + 5 sections (3,500 tokens)

Step 3: User asks "What's the hero section?"
→ Agent references previous result
→ No new tools needed

Step 4-20: More questions and tool calls...
→ Context grows to 80,000 tokens
→ At 62% of 128k limit ⚠️
```

**Without management**: Agent will hit context limit → Crash or lose history.

### Solution 1: Hierarchical Memory (Subgoal Compression)

**Pattern**: Compress completed subgoals into summaries.

**Before compression**:
```
Messages: 50 (10,000 tokens)
- Step 1-10: Created page (2,000 tokens)
- Step 11-20: Added sections (3,000 tokens)
- Step 21-30: Updated content (2,500 tokens)
- Step 31-50: Debugging errors (2,500 tokens)
```

**After compression**:
```
Subgoal summaries:
- "Created 'About' page with slug validation" (50 tokens)
- "Added 5 sections: hero, feature, cta, footer, nav" (30 tokens)
- "Updated hero content: title, subtitle, CTA" (25 tokens)
- "Fixed slug constraint error, retried successfully" (30 tokens)

Working memory (last 5 messages): 1,500 tokens

Total: 1,635 tokens (down from 10,000 tokens)
→ 84% compression! ✅
```

**Implementation in this codebase**:
- Research: HiAgent (2024) - 2x success rate on long-horizon tasks
- Location: `server/agent/orchestrator.ts` (prepareStep callback)
- Trigger: At 80% context capacity or subgoal completion
- Compression: LLM summarizes completed work into 50-100 tokens

**Sources**:
- [AGENTIC_PATTERNS_LIBRARY.md](../../AGENTIC_PATTERNS_LIBRARY.md) - Pattern #1
- HiAgent (2024) research paper

### Solution 2: Working Memory (Entity-Based)

**Pattern**: Only inject recently accessed entities.

**Example** (from this codebase):
```
User: "Delete all sections from about page"
→ Agent calls cms_getPage(slug="about")
→ Working memory extracts: {"type": "page", "id": "abc", "name": "About"}

User: "Now how many sections are on this page?"
                                      ↑
                                 (ambiguous!)

Working memory injected:
[WORKING MEMORY]
pages:
  - "About" (abc)

→ Agent resolves "this page" → "About" (abc) ✅
```

**Token savings**:
- Without working memory: Agent must keep full conversation (10k tokens)
- With working memory: Only 5 most recent entities (~200 tokens)
- **Savings**: 98% reduction in context overhead

**Implementation**: See `server/services/working-memory/` (Sprint 15)

**Source**: [PROGRESS.md Sprint 15](../../PROGRESS.md#sprint-15-universal-working-memory-system)

### Solution 3: Hybrid Content Fetching

**Pattern**: Fetch lightweight metadata first, granular content only when needed.

**Before** (token-heavy):
```typescript
cms_getPage({slug: "about"})
→ Returns page + all 5 sections + all content
→ 2,000 tokens for every page query ⚠️
```

**After** (token-efficient):
```typescript
// Step 1: Lightweight query
cms_getPage({slug: "about"})  // Default: includeContent=false
→ Returns page metadata + sectionIds
→ 100 tokens ✅

// Step 2: Granular fetch (only if needed)
cms_getSectionContent({pageSectionId: "hero-123"})
→ Returns just hero section content
→ 150 tokens ✅

Total: 250 tokens (vs 2,000 tokens) → 88% savings!
```

**Implementation**: See `server/tools/all-tools.ts` (cms_getPage, cms_getSectionContent)

**Source**: [PROGRESS.md Sprint 15](../../PROGRESS.md#sprint-15-hybrid-content-fetching-token-optimization)

### Solution 4: Checkpointing & Message Trimming

**Pattern**: Save conversation to DB, trim history to last 20 messages.

```typescript
// In prepareStep callback (orchestrator.ts)
if (messages.length > 20) {
  // Keep system prompt + last 20 messages
  return {
    messages: [messages[0], ...messages.slice(-20)]
  };
}

// Auto-checkpoint every 3 steps
if (stepNumber % 3 === 0) {
  await sessionService.saveMessages(sessionId, messages);
}
```

**Benefits**:
- Context never exceeds 20 messages (~4k-8k tokens)
- Full history preserved in DB (can resume or replay)
- User can leave and return without losing progress

**Implementation**: `server/agent/orchestrator.ts` (prepareStep)

---

## Context Window Best Practices

### 1. Monitor Token Usage

```typescript
// Track tokens per request
const usage = {
  input_tokens: result.usage.promptTokens,
  output_tokens: result.usage.completionTokens,
  total_tokens: result.usage.totalTokens
};

// Alert if >70% of context window used
if (usage.total_tokens > 0.7 * contextWindowSize) {
  console.warn('Context window 70% full - consider compression');
}
```

### 2. Prioritize Critical Information

**Placement strategy**:
```
[System Prompt: Always at start]
  ↓
[Working Memory: Recently accessed entities]
  ↓
[Compressed Subgoals: Completed work summaries]
  ↓
[Recent Messages: Last 10-20 interactions]
  ↓
[Current User Message: Latest request]
```

**Why this order**:
- System prompt: Establishes agent identity (never pruned)
- Working memory: Resolves references (high priority)
- Subgoals: Provides context without detail
- Recent messages: Maintains conversation flow
- Current message: Agent's immediate focus

### 3. Use Structured Output

Instead of verbose descriptions, use **structured data**:

**❌ Verbose** (150 tokens):
```
"The page 'About Us' was created successfully with the slug 'about-us'. 
It has 5 sections: a hero section with a title and subtitle, 
a features section with 3 columns, a call-to-action section, 
a testimonials section with 4 testimonials, and a footer section."
```

**✅ Structured** (40 tokens):
```json
{
  "page": "About Us",
  "slug": "about-us",
  "sections": ["hero", "features", "cta", "testimonials", "footer"],
  "section_count": 5
}
```

**Savings**: 73% fewer tokens!

### 4. Lazy Loading

Don't fetch everything upfront. Fetch on-demand:

**❌ Eager Loading**:
```typescript
// Fetch all pages with full content
const pages = await cms_listPages({ includeContent: true });
// → 20 pages × 2k tokens = 40k tokens ⚠️
```

**✅ Lazy Loading**:
```typescript
// Fetch page metadata only
const pages = await cms_listPages();
// → 20 pages × 50 tokens = 1k tokens ✅

// Fetch content for specific page when user asks
const page = await cms_getPage({ slug: "about", includeContent: true });
// → 2k tokens (only when needed)
```

---

## Future: Long Context Models

### Gemini 1.5 Pro (2M Tokens)

**Capabilities**:
- Analyze entire codebases (500k+ lines of code)
- Process 1 hour of video
- Read 11 books simultaneously
- Handle 20,000-page documents

**Cost**: ~$7 per million input tokens (expensive!)

**Use case**: Deep research, legal document analysis, large-scale code refactoring

**Source**: [Codingscape: Largest Context Windows](https://codingscape.com/blog/llms-with-largest-context-windows)

### Claude Sonnet 4 (1M Tokens)

**Capabilities**:
- Legal discovery (thousands of contracts)
- Multi-document synthesis
- Long-form content generation

**Cost**: ~$3 per million input tokens

**Source**: [DataStudios: Claude Context](https://www.datastudios.org/post/claude-ai-context-window-token-limits-and-memory)

### Challenges with Ultra-Long Context

1. **Cost**: 10x-100x more expensive than standard models
2. **Latency**: 20-60 seconds for responses
3. **Accuracy**: "Lost in the middle" problem worsens
4. **Diminishing returns**: Beyond 32k tokens, accuracy plateaus

**Recommendation**: Use long context only when absolutely necessary. Most tasks work better with **smart context management** than brute-force long context.

---

## Key Takeaways

1. **Context window = working memory**: Everything the model can "see" at once
2. **Token limits matter**: 128k-200k is standard, 1M-2M is cutting-edge
3. **Tokens accumulate fast**: System prompt + tools + history + results = 10k-50k tokens
4. **Context management is essential**: Compression, pruning, checkpointing prevent crashes
5. **Placement matters**: Critical info at start/end, not middle
6. **Quality > Quantity**: Smart context injection beats brute-force long context

---

## Codebase Examples

**Files demonstrating context management**:
- [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Message trimming in prepareStep
- [`server/services/working-memory/`](../../server/services/working-memory/) - Entity extraction and injection
- [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - Hybrid fetching (includeContent flag)
- [`server/prompts/react.xml`](../../server/prompts/react.xml) - System prompt structure

**Related Documentation**:
- [AGENTIC_PATTERNS_LIBRARY.md](../../AGENTIC_PATTERNS_LIBRARY.md) - Patterns #1 (Hierarchical Memory), #2 (Sliding Window)
- [PROGRESS.md Sprint 15](../../PROGRESS.md#sprint-15-universal-working-memory-system) - Working memory implementation

---

**Next Topic**: [0.1.4 Sampling Parameters](./0.1.4-sampling-parameters.md)  
**Related Topics**: 
- [2.2.2 Hierarchical Memory](../2-context/2.2.2-hierarchical-memory.md)
- [2.3.4 Working Memory Pattern](../2-context/2.3.4-working-memory.md)
- [4.1.1 Working Memory Concept](../4-memory/4.1.1-working-memory-concept.md)

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references + codebase examples
