# 0.1.4 Sampling Parameters: Temperature, Top-P, Top-K

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)  
**Next**: [0.1.5 Model Selection Guide](./0.1.5-model-selection.md)

---

## Overview

When an LLM generates text, it doesn't simply output "the answer" - it produces a **probability distribution** over all possible next tokens. **Sampling parameters** control how the model selects from this distribution, dramatically affecting output quality, creativity, and randomness.

```
LLM Output: Probability distribution over vocabulary (50k-200k tokens)
            ↓
Sampling: Select next token using parameters (temperature, top-p, top-k)
            ↓
Token: "Paris" → Feed back to model → Repeat
```

Understanding these parameters is essential for:
- **Agents**: Controlling tool calling behavior (deterministic vs exploratory)
- **Creative tasks**: Balancing coherence and novelty
- **Cost optimization**: Fewer retries with better parameter tuning

---

## The Probability Distribution

### How LLMs Generate Probabilities

After processing input through all transformer layers, the model outputs **logits** (raw scores) for every token in its vocabulary:

```python
Input: "The capital of France is"

Logits (50,000 tokens):
Paris: 12.3
London: 8.1
Berlin: 7.9
Madrid: 7.5
Rome: 7.2
...
apple: -2.1  (unlikely!)
quantum: -5.3  (very unlikely!)
```

These logits are converted to probabilities using **softmax**:

```python
P(token) = exp(logit) / sum(exp(all_logits))

Result:
Paris: 0.72  (72% probability)
London: 0.11  (11%)
Berlin: 0.09  (9%)
Madrid: 0.05  (5%)
Rome: 0.03  (3%)
...
```

**Key Insight**: Most tokens have near-zero probability. Only a small subset is plausible.

**Source**: [phData: How to Tune LLM Parameters](https://www.phdata.io/blog/how-to-tune-llm-parameters-for-top-performance-understanding-temperature-top-k-and-top-p/)

---

## Sampling Methods

### 1. Greedy Decoding (Baseline)

**Method**: Always select the token with **highest probability**.

```
Input: "The capital of France is"

Probabilities:
Paris: 0.72  ← Always pick this!
London: 0.11
Berlin: 0.09

Output: "Paris"
```

**Characteristics**:
- ✅ **Deterministic**: Same input → same output (every time)
- ✅ **Fast**: No randomness, simple argmax
- ✅ **Coherent**: Picks most likely continuation
- ❌ **Repetitive**: Can get stuck in loops ("very very very...")
- ❌ **Boring**: No creativity or exploration

**Use Cases**:
- Factual Q&A (want single correct answer)
- Translation (deterministic preferred)
- Classification (need consistency)

**Example**:
```
Prompt: "List 5 colors:"

Greedy Output:
"1. Red
 2. Blue
 3. Green
 4. Yellow
 5. Orange"

(Same output every time)
```

**Source**: [Medium: Decoding Strategies](https://medium.com/version-1/from-greedy-to-genius-understanding-decoding-strategies-in-large-language-models-670c28c59a65)

---

### 2. Temperature Sampling

**Method**: Adjust the **sharpness** of the probability distribution before sampling.

**Formula**:
```python
# Original logits
logits = [12.3, 8.1, 7.9, ...]

# Apply temperature
adjusted_logits = logits / temperature

# Convert to probabilities
probabilities = softmax(adjusted_logits)
```

**Temperature Values**:
- `T = 0`: Greedy (deterministic)
- `T < 1`: Sharper distribution (more confident)
- `T = 1`: Original distribution (balanced)
- `T > 1`: Flatter distribution (more random)
- `T → ∞`: Uniform distribution (all tokens equally likely)

#### Example: Effect of Temperature

```
Original Probabilities (T=1):
Paris: 0.72
London: 0.11
Berlin: 0.09
Madrid: 0.05
Rome: 0.03

Low Temperature (T=0.3):
Paris: 0.95  ← Much more confident!
London: 0.03
Berlin: 0.01
Madrid: 0.005
Rome: 0.005

High Temperature (T=1.5):
Paris: 0.45  ← Less confident
London: 0.20
Berlin: 0.15
Madrid: 0.10
Rome: 0.10
```

**Visual Analogy**:
```
T=0.1: ████████████████ Paris (sharp peak)
T=1.0: ████████ Paris, ██ London, █ Berlin (balanced)
T=2.0: ███ Paris, ██ London, ██ Berlin, █ Madrid (flat)
```

#### Temperature Guidelines

| Temperature | Behavior | Use Case | Example |
|-------------|----------|----------|---------|
| **0.0-0.3** | Focused, deterministic | Factual Q&A, classification | "What is 2+2?" → "4" |
| **0.4-0.7** | Balanced, reliable | General chat, instructions | "Explain photosynthesis" |
| **0.8-1.2** | Creative, varied | Brainstorming, storytelling | "Write a poem about trees" |
| **1.3-2.0** | Experimental, random | Creative fiction, humor | "Write absurdist dialogue" |

**Practical Example**:
```python
# API call with temperature
response = openai.chat.completions.create(
  model="gpt-4",
  messages=[{"role": "user", "content": "Write a tagline for a coffee shop"}],
  temperature=0.9  # Creative!
)

# Possible outputs (varies each time):
# "Where every cup tells a story."
# "Brewing happiness, one sip at a time."
# "Your daily dose of liquid inspiration."
```

**Source**: [NVIDIA NIM: Sampling Control](https://docs.nvidia.com/nim/vision-language-models/latest/sampling-params.html)

---

### 3. Top-K Sampling

**Method**: Restrict sampling to the **top K most probable tokens**.

**Process**:
```
1. Sort tokens by probability
2. Keep only top K tokens
3. Set all other probabilities to 0
4. Renormalize probabilities
5. Sample from filtered distribution
```

**Example** (K=3):
```
Original:
Paris: 0.72
London: 0.11
Berlin: 0.09
Madrid: 0.05  ← Cut off
Rome: 0.03    ← Cut off

After filtering and renormalization:
Paris: 0.78   (0.72 / 0.92)
London: 0.12  (0.11 / 0.92)
Berlin: 0.10  (0.09 / 0.92)
```

**Benefits**:
- ✅ Prevents sampling unlikely tokens (reduces "hallucinations")
- ✅ Maintains diversity within top candidates
- ✅ Simple to tune (single integer parameter)

**Drawbacks**:
- ⚠️ Fixed K may be too restrictive or too permissive
- ⚠️ Doesn't adapt to model's confidence

**K Value Guidelines**:

| K Value | Behavior | Use Case |
|---------|----------|----------|
| **K=1** | Greedy (deterministic) | Factual tasks |
| **K=5-10** | Conservative, focused | Instructions, technical writing |
| **K=20-50** | Balanced | General conversation |
| **K=100+** | Permissive, creative | Creative writing |

**Example**:
```python
response = openai.chat.completions.create(
  model="gpt-4",
  messages=[{"role": "user", "content": "Name a fruit:"}],
  top_k=5  # Only consider top 5 most likely
)

# Possible outputs: apple, banana, orange, grape, mango
# Won't output: durian, dragonfruit (not in top 5)
```

**Source**: [LangCopilot: Decoding Strategies](https://langcopilot.com/posts/2025-07-02-decoding-strategies-for-large-language-models/)

---

### 4. Top-P Sampling (Nucleus Sampling)

**Method**: Sample from the **smallest set of tokens whose cumulative probability exceeds P**.

**Process**:
```
1. Sort tokens by probability (descending)
2. Calculate cumulative probability
3. Keep tokens until cumulative probability ≥ P
4. Set all other probabilities to 0
5. Renormalize and sample
```

**Example** (P=0.9):
```
Sorted probabilities:
Paris: 0.72   → Cumulative: 0.72
London: 0.11  → Cumulative: 0.83
Berlin: 0.09  → Cumulative: 0.92  ← Stop here! (≥ 0.9)
Madrid: 0.05  ← Cut off
Rome: 0.03    ← Cut off

Nucleus (kept tokens): {Paris, London, Berlin}
```

**Key Advantage**: **Adaptive** - nucleus size varies based on model's confidence.

**Comparison with Top-K**:
```
Scenario 1: Model is very confident
Probabilities: [0.95, 0.02, 0.01, 0.01, 0.01]

Top-K (K=5): Keeps all 5 tokens (unnecessary)
Top-P (P=0.9): Keeps only 1 token (adaptive!) ✅

Scenario 2: Model is uncertain
Probabilities: [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]

Top-K (K=5): Keeps 5 tokens (arbitrary cutoff)
Top-P (P=0.9): Keeps 5 tokens (naturally stops at 0.90 cumulative) ✅
```

**P Value Guidelines**:

| P Value | Behavior | Use Case |
|---------|----------|----------|
| **P=0.1-0.3** | Very focused | Factual, deterministic |
| **P=0.5-0.7** | Conservative | Instructions, Q&A |
| **P=0.8-0.95** | Balanced (default) | General chat |
| **P=0.95-1.0** | Creative, diverse | Storytelling, brainstorming |

**Recommended Default**: `P=0.9` (balances quality and diversity)

**Example**:
```python
response = openai.chat.completions.create(
  model="gpt-4",
  messages=[{"role": "user", "content": "Suggest a weekend activity:"}],
  top_p=0.9  # Most common setting
)

# Adaptive: More options if model uncertain, fewer if confident
```

**Source**: [Hugging Face: Decoding Strategies](https://huggingface.co/blog/mlabonne/decoding-strategies)

---

## Combining Parameters

### Common Combinations

#### 1. **Deterministic Output** (Factual, Consistent)

```python
temperature=0.1   # Very low randomness
top_p=0.3         # Narrow nucleus
top_k=5           # Few options

# Use case: Math problems, classification, translation
```

**Example**:
```
Prompt: "What is the capital of Japan?"
Output: "Tokyo" (same every time)
```

#### 2. **Balanced Output** (Default for most applications)

```python
temperature=0.7   # Moderate randomness
top_p=0.9         # Standard nucleus
top_k=40          # Reasonable diversity

# Use case: Chat, Q&A, instructions, agents
```

**Example**:
```
Prompt: "Explain how photosynthesis works"
Output: Coherent explanation with slight variations
```

#### 3. **Creative Output** (Storytelling, Brainstorming)

```python
temperature=1.2   # High randomness
top_p=0.95        # Wide nucleus
top_k=100         # Many options

# Use case: Creative writing, humor, poetry
```

**Example**:
```
Prompt: "Write a surrealist description of a coffee shop"
Output: Wildly creative, unique each time
```

#### 4. **Agent Tool Calling** (Reliable, Goal-Oriented)

```python
temperature=0.3-0.5   # Low-moderate randomness
top_p=0.7-0.9         # Conservative nucleus
top_k=20-40           # Focused options

# Use case: ReAct agents, function calling, task completion
```

**Why these settings?**
- Agent should reliably call correct tools
- Some exploration needed (alternative approaches)
- Avoid hallucinating non-existent tools

**Codebase Example**:
```typescript
// server/agent/orchestrator.ts
// Default settings for ReAct agent
const agent = new ToolLoopAgent({
  model: openrouter.languageModel(modelId),
  // Implicit: temperature=0.7, top_p=0.9 (balanced)
  // Can override for specific use cases
})
```

**Source**: [Sam McLeod: Comprehensive Guide to LLM Sampling](https://smcleod.net/2025/04/comprehensive-guide-to-llm-sampling-parameters/)

---

## Advanced Parameters

### 1. Presence Penalty

**What**: Penalize tokens that have **already appeared** in the generated text (encourages diversity).

**Value Range**: `-2.0` to `2.0`
- Negative: Encourage repetition
- `0.0`: No penalty (default)
- Positive: Discourage repetition

**Example**:
```python
temperature=0.8
presence_penalty=0.6  # Discourage repeating topics

Prompt: "List creative project ideas:"

Without penalty:
"1. Build a website
 2. Build an app
 3. Build a mobile app
 4. Build another website..."  (repetitive!)

With penalty:
"1. Build a website
 2. Write a short story
 3. Create a podcast
 4. Design a board game"  (diverse!)
```

**Use Cases**:
- Avoid repetitive lists
- Encourage topic diversity in long-form content
- Prevent models from "looping"

### 2. Frequency Penalty

**What**: Penalize tokens based on **how many times** they've appeared (stronger than presence penalty).

**Value Range**: `-2.0` to `2.0`

**Difference from Presence Penalty**:
- **Presence**: Binary (appeared or not)
- **Frequency**: Proportional (appeared 1x vs 10x)

**Example**:
```python
frequency_penalty=0.8  # Strong anti-repetition

Prompt: "Describe a forest"

Without penalty:
"The forest was very green. Very, very green. It was so very green..."

With penalty:
"The forest was lush and verdant, with towering oaks creating a dense canopy overhead."
```

### 3. Repetition Penalty

**What**: Similar to frequency penalty but specific to local repetition (last N tokens).

**Implementation** (model-specific):
```python
repetition_penalty=1.2  # Penalize recent repetitions
repetition_window=64    # Look back 64 tokens
```

**Use Cases**:
- Prevent stuttering ("the the the")
- Avoid immediate repetition
- Keep responses concise

**Source**: [NVIDIA NIM: Sampling Parameters](https://docs.nvidia.com/nim/vision-language-models/latest/sampling-params.html)

---

## Practical Guidelines

### For Agent Development

#### 1. **Tool Calling** (Your Codebase)

**Recommended Settings**:
```typescript
temperature: 0.3-0.5   // Focused but not overly rigid
top_p: 0.8-0.9         // Conservative nucleus
// No top_k (let nucleus sampling handle it)
```

**Why**:
- Agent must call correct tools reliably
- Some randomness allows alternative strategies
- Too deterministic (T=0) may miss creative solutions
- Too random (T>1) may hallucinate tools or arguments

**Example**:
```typescript
// Good: Agent explores multiple valid approaches
temperature=0.4
User: "Create an About page"
→ Agent might try: cms_createPage OR cms_listPages (check exists first)

// Bad: Too random, invalid tool calls
temperature=1.5
User: "Create an About page"
→ Agent might call: cms_deletePage (wrong!) OR cms_updatePage (doesn't exist yet!)
```

#### 2. **Chat Responses**

```typescript
temperature: 0.7-0.9   // Conversational, varied
top_p: 0.9-0.95        // Standard nucleus
```

**Why**: Users expect natural, slightly varied responses (not robotic repetition).

#### 3. **Factual Q&A**

```typescript
temperature: 0.1-0.3   // Deterministic
top_p: 0.3-0.5         // Narrow nucleus
```

**Why**: Factual answers should be consistent, not creative.

### Debugging with Sampling Parameters

**Problem**: Agent keeps failing task

**Try**: Lower temperature (more deterministic, fewer retries)
```typescript
// Before: temperature=0.7 (agent tries random approaches)
// After: temperature=0.3 (agent focuses on most reliable approach)
```

**Problem**: Agent stuck in repetitive loop

**Try**: Add presence penalty
```typescript
presence_penalty: 0.6  // Discourage repeating same action
```

**Problem**: Agent misses alternative solutions

**Try**: Increase temperature slightly
```typescript
// Before: temperature=0.2 (too rigid)
// After: temperature=0.5 (explores alternatives)
```

---

## API Examples

### OpenAI API

```python
import openai

response = openai.chat.completions.create(
  model="gpt-4",
  messages=[{"role": "user", "content": "Write a haiku about coding"}],
  temperature=0.9,        # Creative
  top_p=0.95,            # Wide nucleus
  frequency_penalty=0.3,  # Slight diversity
  presence_penalty=0.2,   # Encourage varied vocabulary
  max_tokens=50
)
```

### AI SDK v6 (Your Codebase)

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateText } from 'ai'

const result = await generateText({
  model: openrouter.languageModel('google/gemini-2.5-flash'),
  prompt: 'Explain quantum computing',
  temperature: 0.7,  // Balanced
  topP: 0.9,         // Standard nucleus
  maxTokens: 500
})
```

### Anthropic Claude

```python
import anthropic

client = anthropic.Anthropic(api_key="...")

response = client.messages.create(
  model="claude-3-5-sonnet-20241022",
  max_tokens=1024,
  temperature=0.5,  # Conservative
  top_p=0.8,
  messages=[{"role": "user", "content": "Analyze this code: ..."}]
)
```

---

## Common Pitfalls

### 1. Temperature = 0 is Not Always Best

**Myth**: "T=0 is most accurate"

**Reality**: T=0 is **most deterministic**, not necessarily most accurate.

**Example**:
```
Prompt: "Brainstorm 5 creative solutions to reduce plastic waste"

T=0.0:
1. Recycle more
2. Use reusable bags
3. Reduce consumption
4. Buy eco-friendly products
5. Support green initiatives
(Boring, generic, not creative!)

T=0.9:
1. Develop edible packaging made from seaweed
2. Create community plastic-to-fuel conversion hubs
3. Implement deposit-return systems with blockchain tracking
4. Design biodegradable alternatives using mushroom mycelium
5. Launch viral social media challenges rewarding reduction
(Creative, specific, actionable!)
```

**Lesson**: Match temperature to task. T=0 for facts, T=0.7-1.2 for creativity.

### 2. Overusing Both Top-K and Top-P

**Problem**: Setting both can be redundant or conflicting.

**Recommendation**: Choose **one**:
- **Top-P**: More common, adaptive (recommended)
- **Top-K**: Simpler, fixed cutoff

**Example**:
```python
# Redundant (pick one!)
top_k=50
top_p=0.9

# Better: Just use top_p
top_p=0.9
```

**Exception**: Some models apply top_k first, then top_p (check docs).

### 3. Ignoring Penalties for Long-Form Content

**Problem**: Long outputs become repetitive without penalties.

**Solution**: Always add presence/frequency penalties for >500 tokens.

```python
# Long-form content
max_tokens=2000
temperature=0.8
presence_penalty=0.5    # Encourage topic diversity
frequency_penalty=0.3   # Discourage word repetition
```

---

## Key Takeaways

1. **Temperature**: Controls randomness (0 = deterministic, 1+ = creative)
2. **Top-P (Nucleus)**: Adaptive sampling based on cumulative probability (default: 0.9)
3. **Top-K**: Fixed-size sampling pool (less common, less adaptive)
4. **Greedy**: Always picks highest probability (T=0, deterministic)
5. **Agent default**: T=0.3-0.5, top_p=0.8-0.9 (reliable tool calling)
6. **Chat default**: T=0.7-0.9, top_p=0.9-0.95 (natural conversation)
7. **Creative**: T=1.0-1.5, top_p=0.95 (storytelling, brainstorming)

---

## Codebase Integration

**Current Configuration** (implicit defaults):
```typescript
// server/agent/orchestrator.ts
const agent = new ToolLoopAgent({
  model: openrouter.languageModel(modelId),
  // Uses model defaults (typically T=0.7, top_p=0.9)
})
```

**Custom Settings** (if needed):
```typescript
import { generateText } from 'ai'

const result = await generateText({
  model: openrouter.languageModel(modelId),
  messages: [...],
  temperature: 0.4,  // More focused for tool calling
  topP: 0.85,        // Conservative nucleus
  maxTokens: 1000
})
```

**Files**:
- [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Agent configuration
- See AI SDK v6 docs for full parameter reference

---

**Next Topic**: [0.1.5 Model Selection Guide](./0.1.5-model-selection.md)  
**Related Topics**: 
- [0.2.1 Standard Models vs Thinking Models](./0.2.1-standard-models.md)
- [1.1.1 Single-Shot Prompting](../1-prompts/1.1.1-single-shot.md)

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references including NVIDIA, Hugging Face, OpenAI docs
