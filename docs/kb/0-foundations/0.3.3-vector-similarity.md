# 0.3.3 Vector Similarity Metrics

**Layer**: 0 - Foundations  
**Prerequisites**: [0.3.2 Embedding Models](./0.3.2-embedding-models.md)  
**Next**: [0.3.4 Dimensionality Trade-offs](./0.3.4-dimensionality.md)

---

## Overview

Once you have embeddings (vectors), you need to **measure similarity** between them. This is fundamental to:
- **Semantic search**: Find documents related to a query
- **Clustering**: Group similar items together
- **Recommendations**: Suggest similar content
- **Deduplication**: Identify near-identical items

**Example Use Case** (Your Codebase):
```typescript
// User searches: "contact form"
const queryEmbedding = await embedText("contact form")  // [0.3, 0.8, -0.2, ...]

// Compare with all indexed pages using cosine similarity
const results = await vectorIndex.search(queryEmbedding)
// Returns: "Get in Touch Section" (similarity: 0.89)
```

**Key Question**: How do you calculate that 0.89 similarity score?

**Four Common Metrics**:
1. **Cosine Similarity**: Angle between vectors (direction, not magnitude)
2. **Dot Product**: Direction + magnitude combined
3. **Euclidean Distance**: Straight-line distance in space
4. **Manhattan Distance**: Grid-like distance (sum of absolute differences)

**Your Codebase**: Uses **cosine similarity** in LanceDB (most common for embeddings).

**Source**: [Pinecone: Vector Similarity Explained](https://www.pinecone.io/learn/vector-similarity/)

---

## 1. Cosine Similarity

### What It Measures

**Cosine similarity** measures the **angle** between two vectors, ignoring their magnitudes.

**Formula**:
```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)

Where:
- A · B = dot product (sum of element-wise products)
- ||A|| = magnitude of A = sqrt(a₁² + a₂² + ... + aₙ²)
- ||B|| = magnitude of B
```

**Range**: -1 to 1
- **1.0**: Perfect similarity (same direction)
- **0.0**: Orthogonal (perpendicular, no similarity)
- **-1.0**: Opposite directions

### Visual Example

```
2D Space:

        B (1, 2)
       /
      /θ=20°
     /______ A (2, 1)

cosine_similarity(A, B) = cos(20°) ≈ 0.94 (very similar!)

        C (1, -2)
       /
      /θ=110°
     /______ A (2, 1)

cosine_similarity(A, C) = cos(110°) ≈ -0.34 (dissimilar)
```

### Why Ignore Magnitude?

**Problem**: Document length shouldn't affect similarity.

```python
A = [0.3, 0.8, -0.2]  # "The cat sleeps" (short document)
B = [0.6, 1.6, -0.4]  # "The cat sleeps and rests" (longer, but same topic!)

# B is just 2× A (same direction, different magnitude)
# They should be considered identical in meaning!

cosine_similarity(A, B) = 1.0  ✅ Correctly identifies as identical

euclidean_distance(A, B) = 0.92  ❌ Treats them as different
```

**Use Case**: Text embeddings, where document length varies.

### Implementation

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Two embeddings (1536 dimensions each)
embedding1 = np.array([0.3, 0.8, -0.2, ...])  # 1536 values
embedding2 = np.array([0.4, 0.7, -0.1, ...])

# Compute cosine similarity
similarity = cosine_similarity([embedding1], [embedding2])[0][0]
# Result: 0.91 (very similar!)

# Manual calculation
def cosine_sim(a, b):
    dot_product = np.dot(a, b)
    magnitude_a = np.linalg.norm(a)
    magnitude_b = np.linalg.norm(b)
    return dot_product / (magnitude_a * magnitude_b)
```

**TypeScript** (for your codebase):
```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have same length')
  }
  
  let dotProduct = 0
  let magnitudeA = 0
  let magnitudeB = 0
  
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    magnitudeA += a[i] * a[i]
    magnitudeB += b[i] * b[i]
  }
  
  magnitudeA = Math.sqrt(magnitudeA)
  magnitudeB = Math.sqrt(magnitudeB)
  
  return dotProduct / (magnitudeA * magnitudeB)
}

// Example
const query = [0.3, 0.8, -0.2, 0.5]
const doc = [0.4, 0.7, -0.1, 0.6]

console.log(cosineSimilarity(query, doc))  // 0.991 (very similar!)
```

### When to Use Cosine Similarity

✅ **Ideal for**:
- Text embeddings (document length varies)
- Semantic search (direction matters, not magnitude)
- High-dimensional sparse data (many zeros)
- Normalized embeddings (unit vectors)

❌ **Not ideal for**:
- When magnitude carries meaning (e.g., confidence scores)
- Spatial data where absolute distance matters
- Image embeddings where scale is important

**Your Codebase**:
```typescript
// LanceDB uses cosine similarity by default
await vectorIndex.table.search(queryEmbedding).limit(5)
// Returns results sorted by cosine similarity (highest first)
```

**Source**: [Weaviate: Distance Metrics in Vector Search](https://weaviate.io/blog/distance-metrics-in-vector-search)

---

## 2. Dot Product

### What It Measures

**Dot product** measures both **direction and magnitude** of vectors.

**Formula**:
```
dot_product(A, B) = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ
```

**Range**: -∞ to +∞
- **Large positive**: Similar direction, large magnitudes
- **~0**: Perpendicular (no similarity)
- **Large negative**: Opposite directions

### Key Difference from Cosine

```python
A = [0.3, 0.8, -0.2]  # Small vector
B = [3.0, 8.0, -2.0]  # Same direction, 10× larger

cosine_similarity(A, B) = 1.0  # Identical direction
dot_product(A, B) = 18.9       # 10× larger due to magnitude
```

**Insight**: Dot product = cosine similarity × magnitude of A × magnitude of B

### Normalized Vectors Special Case

**If vectors are normalized** (||A|| = 1, ||B|| = 1), then:
```
dot_product(A, B) = cosine_similarity(A, B)
```

**Proof**:
```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)
                        = (A · B) / (1 × 1)
                        = A · B (dot product)
```

**Implication**: Many embedding models normalize vectors, making dot product equivalent to cosine!

### Implementation

```python
import numpy as np

# Two embeddings
embedding1 = np.array([0.3, 0.8, -0.2, 0.5])
embedding2 = np.array([0.4, 0.7, -0.1, 0.6])

# Compute dot product
dot_prod = np.dot(embedding1, embedding2)
# Result: 0.93

# Manual calculation
def dot_product(a, b):
    return sum(a_i * b_i for a_i, b_i in zip(a, b))
```

**TypeScript**:
```typescript
function dotProduct(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have same length')
  }
  
  let result = 0
  for (let i = 0; i < a.length; i++) {
    result += a[i] * b[i]
  }
  
  return result
}

// Example
const a = [0.3, 0.8, -0.2, 0.5]
const b = [0.4, 0.7, -0.1, 0.6]

console.log(dotProduct(a, b))  // 0.93
```

### When to Use Dot Product

✅ **Ideal for**:
- Normalized embeddings (equivalent to cosine, faster)
- Neural network attention mechanisms
- Recommendation systems (magnitude = popularity/confidence)
- When magnitude carries meaning (e.g., TF-IDF weights)

❌ **Not ideal for**:
- Unnormalized embeddings with varying scales
- When magnitude is noise (e.g., document length)

**Performance Advantage**:
```python
# Cosine similarity (slower)
sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
# 3 operations: dot, 2 norms (with sqrt)

# Dot product (faster)
sim = np.dot(a, b)
# 1 operation: dot

# For normalized vectors: identical results, 3× faster!
```

**Source**: [Google: Measuring Similarity from Embeddings](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)

---

## 3. Euclidean Distance (L2 Distance)

### What It Measures

**Euclidean distance** measures the **straight-line distance** between two points in space.

**Formula**:
```
euclidean_distance(A, B) = sqrt((a₁-b₁)² + (a₂-b₂)² + ... + (aₙ-bₙ)²)
```

**Range**: 0 to ∞
- **0**: Identical vectors
- **Small values**: Similar vectors
- **Large values**: Dissimilar vectors

**Note**: Unlike similarity metrics, **smaller = more similar** (distance, not similarity).

### Visual Example

```
2D Space:

      B (4, 5)
       *
        \
         \  √((4-1)² + (5-2)²) = √18 ≈ 4.24
          \
           * A (1, 2)

euclidean_distance(A, B) = 4.24
```

### Normalized vs Unnormalized

**Problem**: Euclidean distance is sensitive to magnitude.

```python
A = [0.3, 0.8, -0.2]  # "cat sleeps"
B = [0.6, 1.6, -0.4]  # "cat sleeps and rests" (same topic, 2× longer)

euclidean_distance(A, B) = 0.92  ❌ Treats as different

# After L2 normalization (||v|| = 1)
A_norm = [0.33, 0.88, -0.22]
B_norm = [0.33, 0.88, -0.22]  # Same direction!

euclidean_distance(A_norm, B_norm) = 0.0  ✅ Correctly identical
```

**Key Insight**: For normalized vectors, Euclidean distance and cosine similarity are related:
```
euclidean_distance² = 2 × (1 - cosine_similarity)

When normalized:
- Cosine = 1.0 → Euclidean = 0 (identical)
- Cosine = 0.0 → Euclidean = √2 ≈ 1.41 (perpendicular)
- Cosine = -1.0 → Euclidean = 2 (opposite)
```

### Implementation

```python
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

# Two embeddings
embedding1 = np.array([0.3, 0.8, -0.2, 0.5])
embedding2 = np.array([0.4, 0.7, -0.1, 0.6])

# Compute Euclidean distance
distance = euclidean_distances([embedding1], [embedding2])[0][0]
# Result: 0.19 (small = similar)

# Manual calculation
def euclidean_distance(a, b):
    return np.sqrt(sum((a_i - b_i) ** 2 for a_i, b_i in zip(a, b)))
```

**TypeScript**:
```typescript
function euclideanDistance(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have same length')
  }
  
  let sumSquares = 0
  for (let i = 0; i < a.length; i++) {
    const diff = a[i] - b[i]
    sumSquares += diff * diff
  }
  
  return Math.sqrt(sumSquares)
}

// Convert distance to similarity score (0-1)
function distanceToSimilarity(distance: number, maxDistance: number): number {
  return 1 - (distance / maxDistance)
}

// Example
const a = [0.3, 0.8, -0.2, 0.5]
const b = [0.4, 0.7, -0.1, 0.6]

const dist = euclideanDistance(a, b)  // 0.19
const sim = distanceToSimilarity(dist, 2.0)  // 0.905 (similar!)
```

### When to Use Euclidean Distance

✅ **Ideal for**:
- Spatial data (geographic coordinates, 3D positions)
- Image embeddings (color spaces, feature maps)
- Clustering algorithms (K-means)
- When absolute position in space matters

❌ **Not ideal for**:
- Text embeddings with varying document lengths
- High-dimensional sparse data (curse of dimensionality)
- When direction matters more than magnitude

**Curse of Dimensionality**: In high dimensions (1536D), distances become less meaningful:
```python
# In 2D: distances vary widely
distances_2d = [0.5, 1.2, 3.8, 7.2]  # Clear separation

# In 1536D: distances cluster together
distances_1536d = [14.2, 15.1, 15.8, 16.3]  # Hard to distinguish!
```

**Source**: [Azure AI Search: Vector Relevance and Ranking](https://learn.microsoft.com/en-us/azure/search/vector-search-ranking)

---

## 4. Manhattan Distance (L1 Distance)

### What It Measures

**Manhattan distance** measures the sum of **absolute differences** along each dimension (like navigating city blocks).

**Formula**:
```
manhattan_distance(A, B) = |a₁-b₁| + |a₂-b₂| + ... + |aₙ-bₙ|
```

**Range**: 0 to ∞
- **0**: Identical vectors
- **Small values**: Similar vectors
- **Large values**: Dissimilar vectors

### Visual Example

```
2D City Grid:

      B (4, 5)
       *
       |
       |  (3 blocks right + 3 blocks up = 6 blocks total)
       |
       *----* A (1, 2)

manhattan_distance(A, B) = |4-1| + |5-2| = 3 + 3 = 6

vs

euclidean_distance(A, B) = √((4-1)² + (5-2)²) = √18 ≈ 4.24
```

**Intuition**: Manhattan distance is like navigating a city grid; Euclidean is "as the crow flies."

### Why Manhattan?

**Advantages over Euclidean**:
1. **Faster computation**: No square root needed
2. **Robust to outliers**: Linear contribution from each dimension
3. **Interpretable**: Sum of absolute differences
4. **Effective for sparse data**: Handles high-dimensional spaces better

```python
# Euclidean: sensitive to outliers
A = [1, 1, 1, 1]
B = [1, 1, 1, 10]  # One outlier dimension

euclidean_distance(A, B) = 9.0  # Dominated by outlier

manhattan_distance(A, B) = 9.0  # Linear contribution
```

### Implementation

```python
import numpy as np
from sklearn.metrics import manhattan_distances

# Two embeddings
embedding1 = np.array([0.3, 0.8, -0.2, 0.5])
embedding2 = np.array([0.4, 0.7, -0.1, 0.6])

# Compute Manhattan distance
distance = manhattan_distances([embedding1], [embedding2])[0][0]
# Result: 0.4

# Manual calculation
def manhattan_distance(a, b):
    return sum(abs(a_i - b_i) for a_i, b_i in zip(a, b))
```

**TypeScript**:
```typescript
function manhattanDistance(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have same length')
  }
  
  let distance = 0
  for (let i = 0; i < a.length; i++) {
    distance += Math.abs(a[i] - b[i])
  }
  
  return distance
}

// Example
const a = [0.3, 0.8, -0.2, 0.5]
const b = [0.4, 0.7, -0.1, 0.6]

console.log(manhattanDistance(a, b))  // 0.4
```

### When to Use Manhattan Distance

✅ **Ideal for**:
- High-dimensional sparse data (text features, one-hot encodings)
- When outliers should have linear impact
- Computational efficiency is critical
- Grid-like data structures

❌ **Not ideal for**:
- When diagonal movement is natural (not grid-based)
- Continuous spatial data (use Euclidean)
- When direction is more important than position

**Performance Comparison**:
```python
import timeit

# 1536-dimensional vectors
a = np.random.rand(1536)
b = np.random.rand(1536)

# Manhattan: 50 µs
timeit.timeit(lambda: np.sum(np.abs(a - b)), number=10000)

# Euclidean: 80 µs (60% slower due to sqrt)
timeit.timeit(lambda: np.sqrt(np.sum((a - b) ** 2)), number=10000)

# Cosine: 120 µs (140% slower due to norms)
timeit.timeit(lambda: np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)), number=10000)
```

**Source**: [Qdrant: Distance Metrics](https://qdrant.tech/course/essentials/day-1/distance-metrics/)

---

## Comparison Table

| Metric | Formula | Range | Normalized? | Magnitude Sensitive? | Best For |
|--------|---------|-------|-------------|----------------------|----------|
| **Cosine Similarity** | (A·B) / (‖A‖×‖B‖) | -1 to 1 | ✅ Yes | ❌ No | Text embeddings, semantic search |
| **Dot Product** | Σ(aᵢ×bᵢ) | -∞ to ∞ | ❌ No | ✅ Yes | Normalized embeddings, attention |
| **Euclidean Distance** | √Σ(aᵢ-bᵢ)² | 0 to ∞ | ❌ No | ✅ Yes | Spatial data, image embeddings |
| **Manhattan Distance** | Σ\|aᵢ-bᵢ\| | 0 to ∞ | ❌ No | ✅ Yes | Sparse data, outlier-robust |

### Which to Choose?

**Decision Tree**:
```
Are your embeddings normalized (unit vectors)?
├─ Yes
│  ├─ Use Dot Product (fastest, equivalent to cosine)
│  └─ Or Cosine Similarity (clearer interpretation)
│
└─ No
   ├─ Is direction more important than magnitude?
   │  └─ Yes → Use Cosine Similarity
   │
   └─ Is magnitude important?
      ├─ Spatial/continuous data → Euclidean Distance
      └─ Sparse/high-dim data → Manhattan Distance
```

**Your Codebase** (LanceDB):
```typescript
// LanceDB uses cosine similarity by default (best for text embeddings)
// This is the right choice for semantic search!

await vectorIndex.table
  .search(queryEmbedding)  // Cosine similarity under the hood
  .metric('cosine')         // Explicit metric (optional)
  .limit(10)
```

---

## Normalization: When and Why

### What is Vector Normalization?

**Normalization** scales a vector to **unit length** (magnitude = 1) while preserving direction.

**L2 Normalization** (most common):
```
normalized_vector = vector / ||vector||

Example:
vector = [3, 4]
||vector|| = √(3² + 4²) = √25 = 5
normalized = [3/5, 4/5] = [0.6, 0.8]

Check: √(0.6² + 0.8²) = √(0.36 + 0.64) = √1 = 1 ✅
```

### Why Normalize?

**1. Makes Cosine = Dot Product**

```python
# Unnormalized
a = [3, 4]
b = [6, 8]  # Same direction, 2× larger

cosine_similarity(a, b) = 1.0
dot_product(a, b) = 50  # Different!

# Normalized
a_norm = [0.6, 0.8]
b_norm = [0.6, 0.8]

cosine_similarity(a_norm, b_norm) = 1.0
dot_product(a_norm, b_norm) = 1.0  # Same! ✅
```

**Benefit**: Use faster dot product instead of cosine.

**2. Removes Magnitude Bias**

```python
# Without normalization: longer documents dominate
doc1 = "cat"           # embedding: [0.5, 0.3], ||v|| = 0.58
doc2 = "cat cat cat"   # embedding: [1.5, 0.9], ||v|| = 1.75

query = "feline"       # embedding: [0.6, 0.4]

# Dot product (unnormalized)
dot(doc1, query) = 0.42  # Score: 0.42
dot(doc2, query) = 1.26  # Score: 1.26 (3× higher!)

# Cosine similarity (normalized)
cosine(doc1, query) = 0.98  # Score: 0.98
cosine(doc2, query) = 0.98  # Score: 0.98 (same! ✅)
```

**Without normalization**: Longer documents rank higher regardless of relevance.

### Implementation

```python
import numpy as np

def l2_normalize(vector):
    """Normalize vector to unit length."""
    magnitude = np.linalg.norm(vector)
    if magnitude == 0:
        return vector  # Avoid division by zero
    return vector / magnitude

# Example
embedding = np.array([0.3, 0.8, -0.2, 0.5])
normalized = l2_normalize(embedding)

print(f"Original magnitude: {np.linalg.norm(embedding):.3f}")  # 0.988
print(f"Normalized magnitude: {np.linalg.norm(normalized):.3f}")  # 1.000

# Verify
assert abs(np.linalg.norm(normalized) - 1.0) < 1e-6  ✅
```

**TypeScript**:
```typescript
function l2Normalize(vector: number[]): number[] {
  const magnitude = Math.sqrt(
    vector.reduce((sum, val) => sum + val * val, 0)
  )
  
  if (magnitude === 0) {
    return vector  // Avoid division by zero
  }
  
  return vector.map(val => val / magnitude)
}

// Example
const embedding = [0.3, 0.8, -0.2, 0.5]
const normalized = l2Normalize(embedding)

console.log(normalized)  // [0.304, 0.810, -0.203, 0.506]
console.log(Math.sqrt(normalized.reduce((s, v) => s + v*v, 0)))  // 1.000 ✅
```

### When to Normalize

✅ **Normalize when**:
- Using dot product for similarity (want equivalence with cosine)
- Document/text embeddings (remove length bias)
- Combining embeddings from different sources (ensure same scale)
- High-dimensional data where magnitude is noise

❌ **Don't normalize when**:
- Magnitude carries meaning (confidence scores, TF-IDF weights)
- Using cosine similarity (already normalized internally)
- Anomaly detection (magnitude = anomaly strength)
- Embedding model already outputs normalized vectors

**Check if embeddings are normalized**:
```python
embedding = model.encode("Hello world")
magnitude = np.linalg.norm(embedding)

if abs(magnitude - 1.0) < 1e-6:
    print("✅ Embeddings are already normalized")
else:
    print(f"❌ Magnitude: {magnitude:.3f}, normalization needed")
```

**Source**: [Milvus: Vector Normalization](https://milvus.io/ai-quick-reference/what-is-the-relationship-between-vector-normalization-and-the-choice-of-metric)

---

## Your Codebase: LanceDB Integration

### Current Implementation

```typescript
// server/services/vector-index.ts

class VectorIndexService {
  async search(query: string, limit: number = 5) {
    // 1. Embed query
    const queryEmbedding = await this.embedText(query)
    
    // 2. Vector similarity search (cosine by default)
    const results = await this.table
      .search(queryEmbedding)
      .limit(limit)
      .execute()
    
    // 3. Results sorted by similarity (highest first)
    return results.map(r => ({
      id: r.id,
      name: r.name,
      score: r._distance  // Cosine similarity score (0-1)
    }))
  }
}
```

**Why Cosine Similarity?**

1. **Text embeddings**: Your vector index stores page/section names (text)
2. **Length-invariant**: "Hero Section" vs "Hero Section Component" treated fairly
3. **Industry standard**: OpenAI embeddings optimized for cosine similarity
4. **Works with LanceDB**: Default metric, highly optimized

### Alternative Metrics in LanceDB

```typescript
// Explicit metric selection
await this.table
  .search(queryEmbedding)
  .metric('cosine')      // Default (recommended for text)
  .limit(10)

// Or dot product (if embeddings are normalized)
await this.table
  .search(queryEmbedding)
  .metric('dot')         // Faster, equivalent for normalized vectors
  .limit(10)

// Or Euclidean distance (for spatial data)
await this.table
  .search(queryEmbedding)
  .metric('l2')          // Euclidean distance
  .limit(10)
```

**Recommendation**: Stick with **cosine** (current default) for your use case.

### Debugging Similarity Scores

```typescript
// server/tools/all-tools.ts

async vectorSearch({ query }: { query: string }) {
  const results = await vectorIndex.search(query, 10)
  
  // Log scores for debugging
  console.log('Vector search results:')
  for (const result of results) {
    console.log(`- ${result.name}: ${result.score.toFixed(3)}`)
  }
  
  // Filter low-confidence results
  const filtered = results.filter(r => r.score > 0.7)  // >0.7 = good match
  
  return filtered
}

// Example output:
// Vector search results:
// - Hero Section: 0.923 ✅ Excellent
// - Banner Component: 0.812 ✅ Good
// - Header Image: 0.734 ✅ Decent
// - Footer Links: 0.521 ⚠️ Weak (filtered out)
```

**Threshold Guidelines**:
- **> 0.9**: Excellent match (likely same topic)
- **0.7 - 0.9**: Good match (related topics)
- **0.5 - 0.7**: Weak match (tangentially related)
- **< 0.5**: Poor match (likely irrelevant)

---

## Best Practices

### 1. Match Training and Inference Metrics

**Critical**: Use the same metric your embedding model was trained with.

```python
# OpenAI text-embedding-3: trained with cosine similarity
# ✅ Use cosine for search
similarity = cosine_similarity(query_emb, doc_emb)

# ❌ Don't use Euclidean (suboptimal, different metric space)
distance = euclidean_distance(query_emb, doc_emb)
```

**How to check**: Read embedding model documentation (usually specifies metric).

### 2. Normalize Consistently

**Problem**: Mixing normalized and unnormalized vectors gives bad results.

```python
# ❌ BAD: Inconsistent normalization
query_emb = l2_normalize(embed("cat"))      # Normalized
doc_emb = embed("dog")                      # Unnormalized

similarity = dot_product(query_emb, doc_emb)  # Meaningless!

# ✅ GOOD: Consistent normalization
query_emb = l2_normalize(embed("cat"))
doc_emb = l2_normalize(embed("dog"))

similarity = dot_product(query_emb, doc_emb)  # Valid!
```

### 3. Test Multiple Metrics Empirically

**Strategy**: Compare metrics on your specific data.

```python
from sklearn.metrics import precision_score, recall_score

# Ground truth: known similar pairs
test_pairs = [
  ("contact form", "get in touch", True),   # Similar
  ("hero section", "banner", True),
  ("contact form", "pricing", False),       # Dissimilar
]

for metric in ['cosine', 'dot', 'euclidean', 'manhattan']:
    # Compute similarities
    predictions = []
    for q, d, label in test_pairs:
        q_emb = embed(q)
        d_emb = embed(d)
        
        if metric == 'cosine':
            sim = cosine_similarity(q_emb, d_emb)
        elif metric == 'dot':
            sim = dot_product(q_emb, d_emb)
        # ... etc
        
        predictions.append(sim > threshold)
    
    # Evaluate
    accuracy = sum(p == l for p, (_, _, l) in zip(predictions, test_pairs)) / len(test_pairs)
    print(f"{metric}: {accuracy:.2%}")

# Output:
# cosine: 95%     ✅ Best for text
# dot: 92%
# euclidean: 78%
# manhattan: 80%
```

### 4. Understand Trade-offs

| Priority | Recommended Metric | Reason |
|----------|-------------------|--------|
| **Accuracy** | Cosine Similarity | Handles varying magnitudes, proven for text |
| **Speed** | Dot Product (normalized) | No square root, 3× faster than cosine |
| **Robustness** | Manhattan Distance | Linear outlier impact, good for sparse data |
| **Spatial** | Euclidean Distance | Natural for continuous geometric data |

### 5. Monitor Similarity Distribution

```typescript
// Track similarity scores in production
async vectorSearch(query: string) {
  const results = await vectorIndex.search(query, 100)
  
  // Log distribution
  const scores = results.map(r => r.score)
  const avg = scores.reduce((a, b) => a + b, 0) / scores.length
  const max = Math.max(...scores)
  const min = Math.min(...scores)
  
  console.log(`Similarity stats: avg=${avg.toFixed(2)}, max=${max.toFixed(2)}, min=${min.toFixed(2)}`)
  
  // Alert if distribution is off
  if (avg < 0.3) {
    console.warn('⚠️ Low average similarity - check embeddings!')
  }
  
  return results.filter(r => r.score > 0.7)
}
```

---

## Key Takeaways

1. **Cosine Similarity**: Angle-based, ignores magnitude, best for text embeddings
2. **Dot Product**: Direction + magnitude, fast for normalized vectors (equivalent to cosine)
3. **Euclidean Distance**: Straight-line distance, best for spatial data
4. **Manhattan Distance**: Grid-like distance, robust to outliers, efficient
5. **Normalization**: Makes vectors unit length, enables dot product = cosine equivalence
6. **Your Codebase**: Uses cosine similarity in LanceDB (correct choice for semantic search)
7. **Match Training**: Use same metric as embedding model was trained with
8. **Test Empirically**: Compare metrics on your data to find best performer

**Quick Reference**:
- **Text/semantic search** → Cosine Similarity ✅ (your current setup)
- **Fast approximate search** → Dot Product (normalized vectors)
- **Image/spatial data** → Euclidean Distance
- **Sparse/high-dim data** → Manhattan Distance

---

## Further Reading

### Documentation
- [Pinecone: Vector Similarity Explained](https://www.pinecone.io/learn/vector-similarity/)
- [Weaviate: Distance Metrics in Vector Search](https://weaviate.io/blog/distance-metrics-in-vector-search)
- [Qdrant: Distance Metrics](https://qdrant.tech/course/essentials/day-1/distance-metrics/)

### Comparison Guides
- [RAG Walla: Choosing Between Cosine, Dot Product, and Euclidean](https://ragwalla.com/blog/choosing-between-cosine-similarity-dot-product-and-euclidean-distance-for-rag-applications)
- [Milvus: Distance Metric Comparison](https://milvus.io/ai-quick-reference/how-does-the-choice-of-distance-metric-euclidean-distance-vs-cosine-similarity-vs-dot-product-influence-the-results-of-a-vector-search)
- [Google ML: Measuring Similarity from Embeddings](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)

### Performance & Optimization
- [Azure AI Search: Vector Relevance and Ranking](https://learn.microsoft.com/en-us/azure/search/vector-search-ranking)
- [Dataquest: Measuring Similarity Between Embeddings](https://www.dataquest.io/blog/measuring-similarity-and-distance-between-embeddings)

---

## Codebase Integration

**Files Using Vector Similarity**:
- [`server/services/vector-index.ts`](../../server/services/vector-index.ts) - LanceDB cosine similarity search
- [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - search.vector tool implementation
- [`.env.example`](../../.env.example) - VECTOR_DB_METRIC configuration (optional)

**Related Topics**:
- [0.3.2 Embedding Models](./0.3.2-embedding-models.md) - How embeddings are created
- [0.3.4 Dimensionality Trade-offs](./0.3.4-dimensionality.md) - Impact of vector size on similarity
- [5.1.2 Vector Databases](../5-rag/5.1.2-vector-databases.md) - Indexing for fast similarity search

---

**Next Topic**: [0.3.4 Dimensionality Trade-offs](./0.3.4-dimensionality.md)

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references including Pinecone, Weaviate, Qdrant, Microsoft, Google
