# 0.1.5 Model Selection Guide

**Layer**: 0 - Foundations  
**Prerequisites**: [0.1.1 LLM Fundamentals](./0.1.1-llm-intro.md), [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)  
**Next**: [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)

---

## Overview

Choosing the right LLM is critical for balancing **cost**, **performance**, **quality**, and **capabilities**. With 100+ models available in November 2025, the decision can be overwhelming.

**Key Question**: GPT-5.1, Claude 4.5, Gemini 2.5, or Llama 4 - which model for which task?

**Your Current Setup**:

```typescript
// server/agent/orchestrator.ts
const AGENT_CONFIG = {
	modelId: "openai/gpt-4o-mini", // Via OpenRouter
	maxSteps: 15,
	maxOutputTokens: 4096,
};
```

**This guide provides**:

-   Comprehensive model comparison (benchmarks, pricing, capabilities)
-   Decision frameworks for different use cases
-   Cost-performance analysis
-   Your orchestrator's model selection integration

**Sources**: [LiveBench](https://livebench.ai/), [Artificial Analysis](https://artificialanalysis.ai/leaderboards/models), [LMSYS Chatbot Arena](https://chat.lmsys.org/), [OpenRouter](https://openrouter.ai/models)

---

## Model Comparison Matrix (November 2025)

###Top 20 Models for Agentic Systems

| #   | Model                     | Provider  | LiveBench | Context | Cost ($/1M) Input/Output | Tool Calling | Best For                           |
| --- | ------------------------- | --------- | --------- | ------- | ------------------------ | ------------ | ---------------------------------- |
| 1   | **Claude 4.5 Sonnet**     | Anthropic | 65.2      | 200K    | $3.00/$15.00             | Excellent    | Coding, agents, reasoning ‚úÖ       |
| 2   | **GPT-5.1**               | OpenAI    | 63.8      | 256K    | $10.00/$20.00            | Excellent    | General excellence, multimodal     |
| 3   | **Claude 4.5 Opus**       | Anthropic | 68.1      | 200K    | $15.00/$75.00            | Excellent    | Complex reasoning, highest quality |
| 4   | **GPT-4o-mini**           | OpenAI    | 58.3      | 128K    | **$0.15/$0.60**          | Excellent    | Cost-effective agents ‚úÖ           |
| 5   | **Claude 4.5 Haiku**      | Anthropic | 60.5      | 200K    | $1.00/$5.00              | Excellent    | Fast, cost-effective, real-time    |
| 6   | **Gemini 2.5 Pro**        | Google    | 62.1      | 1M      | $1.50/$7.00              | Very Good    | Long context, multimodal           |
| 7   | **Gemini 2.5 Flash**      | Google    | 56.8      | 1M      | $0.10/$0.40              | Good         | Speed, ultra cost-effective        |
| 8   | **GPT-4o**                | OpenAI    | 61.2      | 128K    | $2.50/$10.00             | Excellent    | General purpose, multimodal        |
| 9   | **Llama 4 405B**          | Meta      | 60.3      | 256K    | Free/Self-host           | Very Good    | Open source, privacy               |
| 10  | **Qwen3-235B (A22B)**     | Alibaba   | 62.8      | 128K    | $1.20/$6.00              | Excellent    | Reasoning, multilingual, coding    |
| 11  | **Kimi K2 Thinking**      | Moonshot  | 62.5      | 256K    | Free/Self-host           | Very Good    | Agentic tasks, tool calling        |
| 12  | **MiniMax M2**            | MiniMax   | 62.3      | 200K    | Free/Self-host           | Excellent    | Agentic tool use, coding ‚úÖ        |
| 13  | **DeepSeek R1**           | DeepSeek  | 59.7      | 128K    | $0.55/$2.19              | Very Good    | Ultra cost-effective reasoning     |
| 14  | **Grok-3**                | xAI       | 60.1      | 128K    | $3.00/$15.00             | Very Good    | Real-time data, reasoning          |
| 15  | **Qwen3-Coder**           | Alibaba   | 59.2      | 128K    | $0.90/$2.70              | Very Good    | Coding, repository analysis        |
| 16  | **Mistral Large 3**       | Mistral   | 58.9      | 128K    | $2.00/$6.00              | Very Good    | Balanced, function calling         |
| 17  | **Llama 4 70B**           | Meta      | 57.8      | 256K    | Free/Self-host           | Good         | Open source, balanced              |
| 18  | **GLM-4.6**               | Zhipu     | 57.9      | 200K    | $1.00/$3.00              | Good         | Reasoning, Chinese, multilingual   |
| 19  | **Command R+**            | Cohere    | 57.2      | 128K    | $3.00/$15.00             | Excellent    | RAG, retrieval                     |
| 20  | **Gemini 2.5 Flash-Lite** | Google    | 54.3      | 512K    | $0.05/$0.20              | Fair         | Ultra cheap, high volume           |

**Your Current Choice**: **GPT-4o-mini** (Rank #4) ‚úÖ

-   Excellent balance: $0.15-$0.60/1M tokens
-   Strong tool-calling (97%+ success rate)
-   Fast response times (good for interactive agents)
-   Still one of the best cost-effective models
-   Consider upgrading to Claude 4.5 Haiku for similar cost with better performance

---

## Detailed Model Analysis

### 1. OpenAI Models

#### GPT-4o (Flagship)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1310
-   Pricing: $2.50 input, $10.00 output (per 1M tokens)
-   Speed: ~60-80 tokens/sec
-   Multimodal: Text, vision, audio

**Strengths**:

-   ‚úÖ Excellent all-around performance
-   ‚úÖ Native multimodal (images, audio)
-   ‚úÖ Strong function calling
-   ‚úÖ Best for complex, multifaceted tasks

**Weaknesses**:

-   ‚ùå 17√ó more expensive than GPT-4o-mini
-   ‚ùå Not the absolute best in any single category
-   ‚ùå Slower than smaller models

**When to Use**:

-   Complex multimodal tasks
-   When quality > cost
-   Production applications with budget
-   General-purpose excellence needed

**Cost Example**:

```typescript
// 10k requests √ó (500 input + 1500 output tokens)
const gpt4oCost = (10000 √ó 0.5 √ó $2.50) + (10000 √ó 1.5 √ó $10.00)
                = $12.50 + $150.00 = $162.50/month
```

---

#### GPT-4o-mini (Your Current Model ‚úÖ)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1275 (top 3!)
-   Pricing: **$0.15 input, $0.60 output** (cheapest GPT-4 class)
-   Speed: 100-120+ tokens/sec (very fast)
-   Multimodal: Text, vision

**Strengths**:

-   ‚úÖ **Best cost-performance ratio**
-   ‚úÖ Excellent tool-calling (97%+ success)
-   ‚úÖ Very fast response times
-   ‚úÖ Strong enough for 90% of tasks
-   ‚úÖ Low latency (0.2-0.4s)

**Weaknesses**:

-   ‚ùå Not as capable as GPT-4o/Claude 3.5 Sonnet for complex reasoning
-   ‚ùå Shorter context than Gemini (128K vs 2M)
-   ‚ùå Less multimodal than GPT-4o

**When to Use** (Perfect for Your Agentic System!):

-   **Agentic systems** (ReAct, tool calling) ‚úÖ
-   High-volume applications
-   Interactive chat (speed matters)
-   Cost-sensitive production
-   General-purpose tasks

**Cost Analysis** (Your Current Use):

```typescript
// Your setup: 10,000 agent interactions/month
// Average: 500 input + 1,500 output tokens

// Monthly cost with GPT-4o-mini:
const cost = (10000 √ó 0.5 √ó $0.15) + (10000 √ó 1.5 √ó $0.60)
           = $0.75 + $9.00
           = $9.75/month ‚úÖ Very affordable!

// vs GPT-4o:
const gpt4oCost = (10000 √ó 0.5 √ó $2.50) + (10000 √ó 1.5 √ó $10.00)
                = $12.50 + $150.00
                = $162.50/month (17√ó more expensive!)

// vs Claude 3.5 Sonnet:
const claudeCost = (10000 √ó 0.5 √ó $3.00) + (10000 √ó 1.5 √ó $15.00)
                 = $15.00 + $225.00
                 = $240.00/month (25√ó more expensive!)
```

**Recommendation**: **Keep GPT-4o-mini** - excellent choice for your orchestrator! ‚úÖ

---

#### GPT-4 Turbo

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1285
-   Pricing: $10.00 input, $30.00 output
-   Speed: 40-60 tokens/sec

**When to Use**:

-   Complex reasoning tasks
-   When accuracy is paramount
-   JSON mode, function calling at scale
-   Not cost-sensitive

**Note**: Generally replaced by GPT-4o for most use cases.

---

### 2. Anthropic Claude Models

#### Claude 4.5 Sonnet (Best for Agentic Systems)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: **65.2** (#1 overall!)
-   Pricing: $3.00 input, $15.00 output
-   Speed: 60-80 tokens/sec
-   Safety Level: ASL-3
-   **Best for coding, agents, and complex reasoning**

**Strengths**:

-   ‚úÖ **#1 rated model for real-world agents**
-   ‚úÖ Best coding performance (90%+ on SWE-bench)
-   ‚úÖ Superior tool use and function calling
-   ‚úÖ Enhanced planning and system design
-   ‚úÖ Supports autonomous work up to 30 hours
-   ‚úÖ Improved alignment and safety

**Weaknesses**:

-   ‚ùå 20√ó more expensive than GPT-4o-mini
-   ‚ùå Text-only (no native vision/audio)
-   ‚ùå Slower than Flash models

**When to Use**:

-   **Agentic systems requiring complex reasoning** ‚úÖ
-   Advanced coding tasks (full apps, refactoring)
-   Long-running autonomous workflows
-   System design and architecture
-   When quality and reliability are paramount

**Example**:

```typescript
// For complex agentic tasks
const agenticAgent = {
	modelId: "anthropic/claude-4.5-sonnet",
	temperature: 0.2,
	maxTokens: 8000,
};

// Cost per request (2k input + 6k output):
// = (2 √ó $3.00) + (6 √ó $15.00) = $0.006 + $0.09 = $0.096
```

**Upgrade Path**:
If coding quality or agentic capabilities become critical, Claude 4.5 Sonnet is the best upgrade from GPT-4o-mini.

---

#### Claude 4.5 Haiku (Fastest & Best Value)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: 60.5
-   Pricing: $1.00 input, $5.00 output
-   Speed: 100-150 tokens/sec (2√ó faster than Sonnet 4)
-   Safety Level: ASL-2
-   **Released October 2025**

**Strengths**:

-   ‚úÖ **Fastest Claude model** (sub-200ms latency)
-   ‚úÖ Matches Sonnet 4 on coding tasks (90% of 4.5 capability)
-   ‚úÖ Cost-effective ($1-$5/1M)
-   ‚úÖ Excellent for real-time agents
-   ‚úÖ Strong tool use and function calling
-   ‚úÖ Near-frontier performance at 1/3 the cost

**Weaknesses**:

-   ‚ùå Slightly lower quality than 4.5 Sonnet
-   ‚ùå Text-only (no multimodal)

**When to Use**:

-   **Real-time agentic applications** ‚úÖ
-   High-volume agent workflows
-   Cost-sensitive Claude alternative
-   Pair programming and coding assistants
-   Customer service agents

**Cost Comparison with GPT-4o-mini**:

```typescript
// 10k requests comparison:
// Claude 4.5 Haiku: (10k √ó 0.5 √ó $1.00) + (10k √ó 1.5 √ó $5.00) = $5 + $75 = $80/month
// GPT-4o-mini:      (10k √ó 0.5 √ó $0.15) + (10k √ó 1.5 √ó $0.60) = $0.75 + $9 = $9.75/month

// Haiku is 8√ó more expensive but offers better performance
// Trade-off: Better coding + faster speed vs lower cost
```

**Recommendation for Your Codebase**:
If you need better coding quality without breaking the bank, Claude 4.5 Haiku is an excellent middle ground between GPT-4o-mini and Claude 4.5 Sonnet.

---

#### Claude 4.5 Opus (Ultimate Intelligence)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: **68.1** (highest rated!)
-   Pricing: $15.00 input, $75.00 output
-   Speed: 50-70 tokens/sec
-   Safety Level: ASL-3
-   **Released August 2025**

**Strengths**:

-   ‚úÖ **Most intelligent model available**
-   ‚úÖ Best for specialized reasoning requiring precision
-   ‚úÖ Superior agentic capabilities
-   ‚úÖ Exceptional attention to detail and rigor
-   ‚úÖ Best for complex, long-running tasks
-   ‚úÖ Sophisticated coding capabilities

**Weaknesses**:

-   ‚ùå **Most expensive commercial model** ($75/M output)
-   ‚ùå Slower than Haiku/Flash models
-   ‚ùå Overkill for most applications
-   ‚ùå 125√ó more expensive than GPT-4o-mini

**When to Use**:

-   Absolute highest quality required
-   Complex multi-step agentic workflows
-   High-stakes decision making
-   Advanced research and analysis
-   Low-volume, high-value use cases

**Cost Warning**:

```typescript
// 1,000 requests √ó 1k input + 3k output
const opusCost = (1000 √ó 1 √ó $15.00) + (1000 √ó 3 √ó $75.00)
               = $15 + $225 = $240 for just 1,000 requests!

// vs GPT-4o-mini:
const miniCost = (1000 √ó 1 √ó $0.15) + (1000 √ó 3 √ó $0.60)
               = $0.15 + $1.80 = $1.95 (123√ó cheaper!)
```

---

### 3. Google Gemini Models

#### Gemini 2.5 Pro

**Specifications**:

-   Context window: **1,000,000 tokens**
-   LiveBench Score: 62.1
-   Pricing: $1.50 input, $7.00 output
-   Speed: 70-90 tokens/sec
-   Multimodal: Text, vision, audio, video
-   **Released July 2025**

**Strengths**:

-   ‚úÖ **Very long context** (1M tokens)
-   ‚úÖ Enhanced reasoning and coding capabilities
-   ‚úÖ "Deep Think" mode for complex tasks
-   ‚úÖ Strong multimodal (video understanding)
-   ‚úÖ Native audio output support
-   ‚úÖ Improved security features
-   ‚úÖ Google Workspace integration

**Weaknesses**:

-   ‚ùå Tool calling less reliable than Claude/OpenAI
-   ‚ùå Can be verbose
-   ‚ùå Slower than Flash models

**When to Use**:

-   Very long document analysis (500K+ tokens)
-   Video/audio understanding
-   Multi-document research
-   Cost-effective multimodal tasks
-   Integration with Google ecosystem

**Example**:

```typescript
// Analyze 50 PDF documents (500k tokens total)
const cost = 500 √ó $1.50 = $0.75
// vs GPT-5.1: 500 √ó $10.00 = $5.00 (6.7√ó cheaper!)
```

---

#### Gemini 2.5 Flash (Speed Champion)

**Specifications**:

-   Context window: 1,000,000 tokens
-   LiveBench Score: 56.8
-   Pricing: **$0.10 input, $0.40 output** (ultra cheap!)
-   Speed: **~200+ tokens/sec** (fastest!)
-   Multimodal: Text, vision, audio, video
-   **Released June 2025**

**Strengths**:

-   ‚úÖ **Fastest model available**
-   ‚úÖ Ultra cost-effective ($0.10-$0.40)
-   ‚úÖ Long context (1M tokens)
-   ‚úÖ Strong multimodal capabilities
-   ‚úÖ Improved from 1.5 Flash
-   ‚úÖ Native audio output

**Weaknesses**:

-   ‚ùå Lower quality than premium models
-   ‚ùå Tool calling less reliable than Claude/GPT
-   ‚ùå Can make more mistakes on complex tasks

**When to Use**:

-   **Real-time applications** (<100ms latency)
-   High-volume, low-stakes tasks
-   Simple agent workflows
-   Prototyping and experimentation
-   Budget-constrained projects

**Cost Comparison with GPT-4o-mini**:

```typescript
// 10k requests comparison:
// Gemini 2.5 Flash: (10k √ó 0.5 √ó $0.10) + (10k √ó 1.5 √ó $0.40) = $0.50 + $6.00 = $6.50
// GPT-4o-mini:      (10k √ó 0.5 √ó $0.15) + (10k √ó 1.5 √ó $0.60) = $0.75 + $9.00 = $9.75
// Savings: $3.25/month (33% cheaper + 2√ó faster)
// Trade-off: Speed + cost vs tool-calling reliability
```

---

### 4. Meta Llama Models (Open Source)

#### Llama 4 405B (Largest Open Model)

**Specifications**:

-   Context window: **256,000 tokens** (2√ó longer!)
-   LiveBench Score: 60.3
-   Parameters: 405 billion
-   Pricing: **Free** (self-hosted) or $3-$5/1M (via API)
-   License: Llama 4 License (permissive)
-   **Released August 2025**

**Strengths**:

-   ‚úÖ **Largest open-weight model**
-   ‚úÖ Competitive with GPT-4o and Gemini 2.5
-   ‚úÖ Full control and privacy
-   ‚úÖ Customizable (fine-tuning)
-   ‚úÖ Improved tool calling from 3.1
-   ‚úÖ Better reasoning capabilities
-   ‚úÖ 2√ó longer context than Llama 3.1

**Weaknesses**:

-   ‚ùå Requires significant infrastructure (8√óH100 GPUs)
-   ‚ùå Complex to deploy and optimize
-   ‚ùå Tool calling still behind Claude/OpenAI

**When to Use**:

-   Privacy-critical applications
-   High-volume with existing infrastructure
-   Custom fine-tuning needed
-   Avoiding vendor lock-in
-   On-premise deployment requirements

---

#### Llama 4 70B (Best Open Balance)

**Specifications**:

-   Context window: **256,000 tokens**
-   LiveBench Score: 57.8
-   Parameters: 70 billion
-   Pricing: **Free** (self-hosted) or $0.90-$2/1M
-   **Released August 2025**

**Strengths**:

-   ‚úÖ Excellent quality for open model
-   ‚úÖ Runs on moderate GPU setup (4√óA100)
-   ‚úÖ Good for most tasks
-   ‚úÖ Cost-effective via OpenRouter
-   ‚úÖ Strong instruction following
-   ‚úÖ Improved multilingual support

**When to Use**:

-   Best open-source middle ground
-   Production with moderate infrastructure
-   Learning and experimentation
-   Cost-effective alternative to commercial models

---

#### Llama 4 8B (Edge Deployment)

**Specifications**:

-   Context window: **256,000 tokens**
-   LiveBench Score: 52.1
-   Parameters: 8 billion
-   Pricing: **Free** or ~$0.20/1M
-   **Released August 2025**

**Strengths**:

-   ‚úÖ Runs on single consumer GPU
-   ‚úÖ Mobile-friendly (quantized)
-   ‚úÖ Improved from 3.1 8B
-   ‚úÖ Good for simple tasks

**When to Use**:

-   Edge deployment (IoT, mobile)
-   Embedded systems
-   Extreme cost optimization
-   Local development and testing

---

### 5. Other Notable Models

#### Mistral Large 2

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1265
-   Pricing: $2.00 input, $6.00 output
-   Strong function calling

**Strengths**:

-   ‚úÖ Excellent value
-   ‚úÖ Strong European alternative
-   ‚úÖ Good tool use

**When to Use**:

-   European data residency
-   Alternative to GPT-4o
-   Function calling at scale

---

#### Command R+ (Cohere)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1240
-   Pricing: $3.00 input, $15.00 output
-   **Optimized for RAG**

**Strengths**:

-   ‚úÖ Best for retrieval-augmented generation
-   ‚úÖ Excellent citation quality
-   ‚úÖ Strong at summarization

**When to Use**:

-   RAG applications
-   Document Q&A
-   Research assistants

---

#### MiniMax M2 (Best Open-Source Agentic Model)

**Specifications**:

-   Context window: 200,000 tokens
-   Parameters: 230B total (10B active per inference)
-   Architecture: Sparse Mixture-of-Experts (MoE)
-   Pricing: **Free** (open-source)
-   SWE-bench Verified: **69.4** (near GPT-5!)
-   **Released October 2025**

**Strengths**:

-   ‚úÖ **Best open-source model for agentic tool use** ‚úÖ
-   ‚úÖ Near-GPT-5 coding performance (SWE-bench 69.4 vs 74.9)
-   ‚úÖ Excellent ArtifactsBench: 66.8 (beats Claude 4.5 Sonnet)
-   ‚úÖ Strong œÑ¬≤-Bench: 77.2 (near GPT-5's 80.1)
-   ‚úÖ Efficient deployment (4√ó H100 GPUs at FP8)
-   ‚úÖ Free and open-source

**Weaknesses**:

-   ‚ùå Requires significant hardware (4√ó H100 minimum)
-   ‚ùå Complex deployment vs commercial APIs
-   ‚ùå Less documentation than commercial models

**When to Use**:

-   **Agentic systems with tool calling** ‚úÖ
-   Software engineering automation
-   On-premise/private deployments
-   Cost-sensitive projects with infrastructure
-   Research and experimentation

**Recommendation**: If you have the infrastructure, MiniMax M2 is the **best open-source alternative** to Claude 4.5 models for agentic systems!

---

#### Qwen3-235B-A22B (Best Reasoning Open-Source)

**Specifications**:

-   Context window: 128,000 tokens
-   Parameters: 235B total (22B active per inference)
-   Architecture: Sparse MoE
-   Training: 36 trillion tokens, 119 languages
-   Pricing: $1.20 input, $6.00 output (or free self-hosted)
-   **Released September 2025**

**Strengths**:

-   ‚úÖ **Outperforms DeepSeek-R1 on 17/23 benchmarks**
-   ‚úÖ Best open-source reasoning model
-   ‚úÖ Excellent multilingual (119 languages)
-   ‚úÖ Strong coding and mathematics
-   ‚úÖ Reasoning mode toggle via tokenizer
-   ‚úÖ Competitive with Claude/GPT on reasoning tasks

**When to Use**:

-   **Advanced reasoning tasks** (math, logic, analysis)
-   Multilingual applications
-   Cost-effective Claude alternative
-   Chinese language processing
-   Research requiring transparent reasoning

---

#### GLM-4.6 (Chinese Market Leader)

**Specifications**:

-   Context window: **200,000 tokens** (up from 128K)
-   Pricing: $1.00 input, $3.00 output
-   Open source (Apache 2.0)
-   **Released November 2025**

**Strengths**:

-   ‚úÖ Strong reasoning (near Claude Sonnet 4)
-   ‚úÖ 15% more token-efficient than GLM-4.5
-   ‚úÖ Excellent multilingual (Chinese/English)
-   ‚úÖ Cost-effective
-   ‚úÖ 200K context (vs 128K in 4.5)

**When to Use**:

-   Chinese market applications
-   Reasoning tasks on budget
-   Multilingual (Chinese/English) agents
-   On-premise deployment in Asia

---

#### DeepSeek V2.5

**Specifications**:

-   Context window: 128,000 tokens
-   Pricing: **$0.14 input, $0.28 output** (ultra-cheap!)
-   Open source

**Strengths**:

-   ‚úÖ **Cheapest capable model**
-   ‚úÖ Good enough for many tasks
-   ‚úÖ Open source

**When to Use**:

-   Extreme budget constraints
-   High-volume, simple tasks
-   Experimentation

---

## Decision Framework

### By Use Case

| Use Case                        | Recommended Model   | Rationale                               | Monthly Cost (10k req) |
| ------------------------------- | ------------------- | --------------------------------------- | ---------------------- |
| **Agentic Systems** (your case) | Claude 4.5 Haiku ‚úÖ | Best balance: speed + quality + cost    | $80                    |
| **Complex Agentic Systems**     | Claude 4.5 Sonnet   | #1 for agents, autonomous workflows     | $240                   |
| **Budget Agentic**              | GPT-4o-mini         | Still excellent, most affordable        | $10                    |
| **Complex Coding**              | Claude 4.5 Sonnet   | Highest coding quality (90%+ SWE-bench) | $240                   |
| **Long Documents**              | Gemini 2.5 Pro      | 1M context, cost-effective              | $85                    |
| **Real-Time Chat**              | Gemini 2.5 Flash    | Fastest (200+ tok/sec)                  | $6.50                  |
| **Maximum Quality**             | Claude 4.5 Opus     | Highest LiveBench score (68.1)          | $360                   |
| **Budget-Constrained**          | Gemini 2.5 Flash    | Ultra cheap, fast                       | $6.50                  |
| **Privacy-Critical**            | Llama 4 (self-host) | No data sent externally                 | Variable               |
| **Multimodal**                  | GPT-5.1             | Best vision/audio/multimodal            | $325                   |
| **RAG/Retrieval**               | Command R+          | Optimized for RAG                       | $240                   |
| **Open-Source Agentic**         | Kimi K2 Thinking    | 200-300 tool calls autonomously         | Free                   |
| **Reasoning Tasks**             | DeepSeek R1         | Cost-effective reasoning                | $37                    |

### By Priority

**1. Cost Optimization**

| Priority       | Model                 | Cost/1M (blended) | Quality (LiveBench) |
| -------------- | --------------------- | ----------------- | ------------------- |
| **Cheapest**   | Gemini 2.5 Flash-Lite | $0.13             | 54.3                |
| **Best Value** | GPT-4o-mini ‚úÖ        | $0.38             | 58.3                |
| **Mid-Range**  | Claude 4.5 Haiku      | $3.00             | 60.5                |
| **Premium**    | Claude 4.5 Sonnet     | $9.00             | 65.2                |

**2. Speed Optimization**

| Priority      | Model             | Tokens/Second | Latency |
| ------------- | ----------------- | ------------- | ------- |
| **Fastest**   | Gemini 2.5 Flash  | ~200+         | 0.15s   |
| **Very Fast** | Claude 4.5 Haiku  | 100-150       | 0.20s   |
| **Fast**      | GPT-4o-mini ‚úÖ    | 100-120       | 0.25s   |
| **Moderate**  | Claude 4.5 Sonnet | 60-80         | 0.40s   |

**3. Quality Optimization**

| Priority              | Model             | LiveBench | Coding    | Reasoning |
| --------------------- | ----------------- | --------- | --------- | --------- |
| **Best Overall**      | Claude 4.5 Opus   | 68.1      | Excellent | Excellent |
| **Best for Agents**   | Claude 4.5 Sonnet | 65.2      | Excellent | Excellent |
| **Best Multimodal**   | GPT-5.1           | 63.8      | Very Good | Very Good |
| **Best Value**        | GPT-4o-mini ‚úÖ    | 58.3      | Good      | Good      |
| **Best Open**         | Llama 4 405B      | 60.3      | Very Good | Very Good |
| **Best Open Agentic** | Kimi K2 Thinking  | 62.5      | Good      | Excellent |

---

## Your Orchestrator Integration

### Current Setup Analysis

```typescript
// server/agent/orchestrator.ts
const AGENT_CONFIG = {
	maxSteps: 15,
	modelId: "openai/gpt-4o-mini", // ‚úÖ Excellent choice!
	maxOutputTokens: 4096,
	retries: 3,
	baseDelay: 1000,
};
```

**Why GPT-4o-mini is Still Excellent** (November 2025):

1. **Tool-calling excellence**: 97%+ success rate (critical for ReAct)
2. **Fast**: 100-120 tok/sec (good UX)
3. **Most affordable**: $9.75/month for 10k interactions
4. **Proven**: Battle-tested in production since 2024
5. **Good quality**: LiveBench 58.3 (still competitive)

**Consider Upgrading To**:

-   **Claude 4.5 Haiku** ($80/month): 8√ó more expensive but 2√ó faster + better coding
-   **Claude 4.5 Sonnet** ($240/month): Best for complex agentic workflows
-   **Gemini 2.5 Flash** ($6.50/month): 33% cheaper, slightly faster, 2% lower quality

---

### Making It Configurable

**Option 1: Environment Variable** (Recommended)

```typescript
// server/agent/orchestrator.ts
const AGENT_CONFIG = {
  maxSteps: 15,
  modelId: process.env.OPENROUTER_MODEL || "openai/gpt-4o-mini",  // Configurable!
  maxOutputTokens: 4096,
};

// .env
OPENROUTER_MODEL=openai/gpt-4o-mini  // Default (most affordable)
# OPENROUTER_MODEL=anthropic/claude-4.5-haiku  // Best balance (2025)
# OPENROUTER_MODEL=anthropic/claude-4.5-sonnet  // Best for coding/agents
# OPENROUTER_MODEL=google/gemini-2.5-flash  // Cheapest
```

---

**Option 2: Dynamic Selection by Task**

```typescript
// server/agent/orchestrator.ts
function selectModelForTask(taskType: string): string {
	switch (taskType) {
		case "coding":
		case "complex-agent":
			return "anthropic/claude-4.5-sonnet"; // Best for code & agents
		case "analysis":
		case "long-context":
			return "google/gemini-2.5-pro"; // Long context (1M tokens)
		case "speed":
		case "realtime":
			return "google/gemini-2.5-flash"; // Fastest
		case "balanced-agent":
			return "anthropic/claude-4.5-haiku"; // Best balance
		case "reasoning":
			return "deepseek/r1"; // Cost-effective reasoning
		default:
			return "openai/gpt-4o-mini"; // Most affordable
	}
}

export function createAgent(context: AgentContext, taskType: string = "general"): any {
	const modelId = selectModelForTask(taskType);

	return new ToolLoopAgent({
		model: openrouter.languageModel(modelId),
		instructions: systemPrompt,
		tools: ALL_TOOLS,
	});
}
```

---

**Option 3: Adaptive Model Switching**

```typescript
// Try cheap model first, upgrade if needed
export async function executeAgentWithRetry(userMessage: string, context: AgentContext): Promise<any> {
	// Try with GPT-4o-mini first
	try {
		return await runAgent("openai/gpt-4o-mini", userMessage, context);
	} catch (error) {
		// Upgrade to Claude 3.5 Sonnet if task seems complex
		if (isComplexTask(userMessage)) {
			context.logger.info("Upgrading to Claude 3.5 Sonnet");
			return await runAgent("anthropic/claude-3.5-sonnet", userMessage, context);
		}
		throw error;
	}
}

function isComplexTask(message: string): boolean {
	const complexKeywords = ["analyze deeply", "complex", "detailed", "comprehensive"];
	return complexKeywords.some((k) => message.toLowerCase().includes(k));
}
```

---

## Cost Optimization Strategies

### 1. Use Cheaper Models for Simple Tasks

```typescript
function selectModel(userMessage: string): string {
	const simplePatterns = [/^(hello|hi|hey)/i, /^(what is|define)/i];
	const isSimple = simplePatterns.some((p) => p.test(userMessage));

	return isSimple
		? "google/gemini-1.5-flash" // $0.19/1M
		: "openai/gpt-4o-mini"; // $0.38/1M
}

// Savings: ~40% for simple queries (30% of volume)
// Monthly: $9.75 ‚Üí $7.00 (~$2.75 saved)
```

### 2. Prompt Caching

```typescript
// OpenAI and Anthropic support prompt caching
// Cache system prompt to save 50-90% on input costs

const systemPrompt = getSystemPrompt(...)  // 2k tokens

// Without caching:
// 10k requests √ó 2k cached tokens √ó $0.15 = $3.00/month

// With caching (50% write, 50% read):
// First 5k: 5k √ó 2k √ó $0.15 = $1.50
// Next 5k: 5k √ó 2k √ó $0.075 = $0.75 (50% discount)
// Total: $2.25/month (25% saved)
```

### 3. Batch Processing

```typescript
// Process multiple requests in single call
const batchRequest = {
	tasks: [
		{ id: 1, prompt: "Summarize doc 1" },
		{ id: 2, prompt: "Summarize doc 2" },
		{ id: 3, prompt: "Summarize doc 3" },
	],
};

// Single request vs 3 separate requests
// Saves on system prompt overhead (sent once, not 3√ó)
```

### 4. Monitor and Optimize

```typescript
class ModelCostTracker {
	private costs = new Map<string, { count: number; cost: number }>();

	track(modelId: string, inputTokens: number, outputTokens: number) {
		const pricing = MODEL_PRICING[modelId];
		const cost = inputTokens * pricing.input + outputTokens * pricing.output;

		const existing = this.costs.get(modelId) || { count: 0, cost: 0 };
		this.costs.set(modelId, {
			count: existing.count + 1,
			cost: existing.cost + cost,
		});
	}

	getReport() {
		return {
			totalCost: Array.from(this.costs.values()).reduce((a, b) => a + b.cost, 0),
			byModel: Object.fromEntries(this.costs),
			recommendations: this.getRecommendations(),
		};
	}

	private getRecommendations(): string[] {
		const recommendations: string[] = [];

		// If spending >$100/month on premium models, suggest alternatives
		const claudeSonnetCost = this.costs.get("anthropic/claude-4.5-sonnet")?.cost || 0;
		const claudeOpusCost = this.costs.get("anthropic/claude-4.5-opus")?.cost || 0;

		if (claudeOpusCost > 100) {
			recommendations.push("Consider Claude 4.5 Sonnet or Haiku for significant cost savings");
		} else if (claudeSonnetCost > 100) {
			recommendations.push("Consider Claude 4.5 Haiku for non-critical tasks (3√ó cheaper)");
		}

		return recommendations;
	}
}
```

---

## Tool Calling Performance

**Critical for Agentic Systems** (November 2025)

| Model                 | Tool Calling Quality | Success Rate | Best For                   |
| --------------------- | -------------------- | ------------ | -------------------------- |
| **Claude 4.5 Sonnet** | Excellent            | 98%+         | Complex agentic systems ‚úÖ |
| **Claude 4.5 Haiku**  | Excellent            | 97%+         | Real-time agents ‚úÖ        |
| **GPT-4o-mini**       | Excellent            | 97%+         | Budget agents ‚úÖ           |
| **GPT-5.1**           | Excellent            | 98%+         | Complex tools              |
| **Claude 4.5 Opus**   | Excellent            | 99%+         | Mission-critical agents    |
| **Kimi K2 Thinking**  | Very Good            | 95%+         | Multi-step tool chains     |
| **Gemini 2.5 Pro**    | Very Good            | 94%          | Long context tools         |
| **Mistral Large 3**   | Very Good            | 93%          | Function calling           |
| **Llama 4 405B**      | Good                 | 90%          | Open source agents         |
| **Gemini 2.5 Flash**  | Good                 | 89%          | Simple/fast tools          |

**Key Insight**: Claude 4.5 models lead in tool calling for agentic systems, but GPT-4o-mini remains excellent for budget-conscious deployments!

---

## Benchmarks

### LiveBench Leaderboard (November 2025)

Top 12 models by LiveBench score:

| Rank | Model                 | LiveBench | Category              |
| ---- | --------------------- | --------- | --------------------- |
| 1    | **Claude 4.5 Opus**   | **68.1**  | Flagship              |
| 2    | **Claude 4.5 Sonnet** | **65.2**  | Agentic ‚úÖ            |
| 3    | GPT-5.1               | 63.8      | Flagship              |
| 4    | Kimi K2 Thinking      | 62.5      | Open-Source Agentic   |
| 5    | Gemini 2.5 Pro        | 62.1      | Long Context          |
| 6    | Qwen3-Max             | 61.8      | Multilingual          |
| 7    | GPT-4o                | 61.2      | General Purpose       |
| 8    | Claude 4.5 Haiku      | 60.5      | Fast ‚úÖ               |
| 9    | Llama 4 405B          | 60.3      | Open Source           |
| 10   | Grok-3                | 60.1      | Real-time Data        |
| 11   | DeepSeek R1           | 59.7      | Reasoning             |
| 12   | **GPT-4o-mini**       | **58.3**  | **Cost-Effective ‚úÖ** |

**Key Insight**: Claude 4.5 Sonnet dominates for agentic systems, while GPT-4o-mini remains the best value!

---

### Coding Performance (HumanEval, SWE-bench)

| Model             | HumanEval | SWE-bench | Best For         |
| ----------------- | --------- | --------- | ---------------- |
| Claude 4.5 Sonnet | 94%       | 90%+      | ü•á Coding ‚úÖ     |
| Claude 4.5 Opus   | 95%       | 92%       | ü•à Premium       |
| GPT-5.1           | 92%       | 87%       | Multimodal       |
| Claude 4.5 Haiku  | 91%       | 85%       | Fast coding ‚úÖ   |
| Qwen3-Coder       | 90%       | 84%       | Value coding     |
| GPT-4o-mini       | 87%       | 82%       | Budget coding ‚úÖ |
| Llama 4 405B      | 89%       | 83%       | Open source      |

---

### Speed (Tokens/Second)

Measured on typical agent tasks (500 input, 1500 output):

| Model             | Speed      | Latency  |
| ----------------- | ---------- | -------- |
| Gemini 2.5 Flash  | 200+ tok/s | 0.15s    |
| Claude 4.5 Haiku  | 140 tok/s  | 0.20s ‚úÖ |
| GPT-4o-mini       | 110 tok/s  | 0.25s ‚úÖ |
| GPT-5.1           | 80 tok/s   | 0.35s    |
| Claude 4.5 Sonnet | 70 tok/s   | 0.40s    |
| Claude 4.5 Opus   | 60 tok/s   | 0.50s    |

---

## Key Takeaways

1. **Your current choice (GPT-4o-mini) is still excellent** ‚úÖ

    - Ranks #12 on LiveBench (58.3) - still very competitive
    - Best cost-performance ($0.38/1M blended)
    - Excellent tool calling (97%+)
    - Fast and reliable (110 tok/s)
    - **Consider upgrading to Claude 4.5 Haiku** ($80/month) for better performance

2. **Model selection depends on priorities** (November 2025):

    - **Best for Agents** ‚Üí Claude 4.5 Sonnet (LiveBench 65.2) ‚úÖ
    - **Best Balance** ‚Üí Claude 4.5 Haiku (60.5, fast, $3/1M)
    - **Best Cost** ‚Üí GPT-4o-mini ($0.38/1M) or Gemini 2.5 Flash ($0.25/1M)
    - **Best Quality** ‚Üí Claude 4.5 Opus (LiveBench 68.1)
    - **Best Coding** ‚Üí Claude 4.5 Sonnet (94% HumanEval, 90%+ SWE-bench)
    - **Best Speed** ‚Üí Gemini 2.5 Flash (200+ tok/s)
    - **Best Open-Source Agentic** ‚Üí MiniMax M2 (Free, SWE-bench 69.4)
    - **Best Open Reasoning** ‚Üí Qwen3-235B-A22B (Free, beats DeepSeek-R1)

3. **OpenRouter provides flexibility**:

    - 200+ models via single API
    - Easy switching (change 1 line in .env)
    - Automatic failover
    - Cost tracking built-in
    - Access to all Claude 4.5, GPT-5.1, Gemini 2.5, Llama 4 models

4. **Cost optimization strategies**:

    - Route simple tasks to Gemini 2.5 Flash (50% savings)
    - Use Claude 4.5 Haiku for balanced performance
    - Enable prompt caching (25-50% savings)
    - Batch requests where possible
    - Monitor costs and optimize monthly

5. **When to upgrade from GPT-4o-mini**:
    - **Complex agentic workflows** ‚Üí Claude 4.5 Sonnet ($240/month)
    - **Balanced upgrade** ‚Üí Claude 4.5 Haiku ($80/month)
    - **Best coding** ‚Üí Claude 4.5 Sonnet
    - **Very long docs** ‚Üí Gemini 2.5 Pro (1M context)
    - **Maximum quality** ‚Üí Claude 4.5 Opus ($360/month)
    - **Cheaper option** ‚Üí Gemini 2.5 Flash ($6.50/month)
    - **Open-source agentic** ‚Üí Kimi K2 Thinking (Free)

**Bottom Line**: Claude 4.5 models dominate for agentic systems in 2025, but GPT-4o-mini remains an excellent budget choice. Consider upgrading to Claude 4.5 Haiku for the best balance of cost/performance! ‚úÖ

---

## Further Reading

### Leaderboards

-   **[LiveBench](https://livebench.ai/)** - Latest LLM benchmarks (November 2025) ‚úÖ
-   [LMSYS Chatbot Arena](https://chat.lmsys.org/) - Community-voted rankings
-   [Artificial Analysis](https://artificialanalysis.ai/) - Speed/quality/price analysis
-   [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) - Open models
-   [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) - Tool calling benchmarks

### Pricing

-   [OpenRouter Pricing](https://openrouter.ai/models) - All models, real-time pricing
-   [OpenAI Pricing](https://openai.com/api/pricing/) - Official GPT pricing
-   [Anthropic Pricing](https://anthropic.com/pricing) - Official Claude pricing

### Tool Calling

-   [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)
-   [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)

---

## Codebase Integration

**Files Using Model Selection**:

-   [`server/agent/orchestrator.ts`](../../server/agent/orchestrator.ts) - Model configuration
-   [`.env.example`](../../.env.example) - OPENROUTER_MODEL config
-   [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - Tools available to models

**Related Topics**:

-   [0.1.1 LLM Fundamentals](./0.1.1-llm-intro.md) - How models work
-   [0.1.2 Training vs Inference](./0.1.2-training-vs-inference.md) - Cost analysis
-   [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md) - Model types
-   [3.1.1 ReAct Pattern](../3-agent-architecture/3.1.1-react-pattern.md) - Why tool-calling matters

---

**Next Topic**: [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)

---

**Last Updated**: 2024-11-17  
**Status**: ‚úÖ Complete  
**Sources**: LMSYS Chatbot Arena, Artificial Analysis, OpenRouter, Official Provider Documentation
