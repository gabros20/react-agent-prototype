# 4.1.3 Sliding Window (Recent N Entities)

**Status**: ✅ Complete  
**Last Updated**: 2025-11-18  
**Grounded In**: Azure OpenAI patterns, LangChain, Strands Agents SDK, production systems (2024-2025)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [The Problem: Infinite Memory Growth](#the-problem-infinite-memory-growth)
3. [What is a Sliding Window?](#what-is-a-sliding-window)
4. [How Sliding Windows Work](#how-sliding-windows-work)
5. [Window Sizing Strategies](#window-sizing-strategies)
6. [Eviction Policies](#eviction-policies)
7. [Implementation in JavaScript/TypeScript](#implementation-in-javascripttypescript)
8. [Production Patterns](#production-patterns)
9. [Trade-offs and Considerations](#trade-offs-and-considerations)
10. [References](#references)

---

## Executive Summary

A **sliding window** is a fixed-size buffer that keeps only the **most recent N items**, automatically evicting older items when the buffer is full. In AI agents, sliding windows prevent memory growth by maintaining a bounded set of recent entities or conversation turns.

### Key Metrics (Industry Standards, 2024-2025)
- **Azure OpenAI**: Recommends 3-5 turn window for multi-turn agents
- **LangChain**: Default 2000 token limit (~10 turns)
- **Strands Agents SDK**: Default 20 message window
- **Production Best Practice**: 5-10 turn window balances context vs cost

### Why It Matters
- **Bounded Memory**: Fixed memory footprint (O(N) space, not O(messages))
- **Predictable Performance**: Constant-time access and eviction
- **Cost Control**: Token usage stays constant regardless of conversation length
- **Simple Implementation**: FIFO queue (first-in-first-out)

---

## The Problem: Infinite Memory Growth

### Scenario: Long Conversation Without Limits

```typescript
// ❌ BAD: Unbounded memory growth
const messages: Message[] = [];

// Turn 1
messages.push({ role: 'user', content: 'Show page 1' });
messages.push({ role: 'assistant', content: '...' });

// Turn 2
messages.push({ role: 'user', content: 'Show page 2' });
messages.push({ role: 'assistant', content: '...' });

// ... 100 turns later
// messages.length = 200 (100 turns × 2 messages)
// Memory usage: 200KB+
// Token usage: 50,000+ tokens per LLM call
```

**Problems**:
- ❌ **Memory explosion**: 200+ messages after 100 turns
- ❌ **Token overflow**: Exceeds 128K context window
- ❌ **High cost**: $0.15/1M tokens × 50K tokens = $0.0075 per request
- ❌ **Slow performance**: LLM processes 50K tokens (5-10s latency)

### Solution: Sliding Window

```typescript
// ✅ GOOD: Bounded memory with sliding window
class SlidingWindow<T> {
  private items: T[] = [];
  private readonly maxSize = 10;  // Keep last 10 items

  add(item: T): void {
    this.items.push(item);
    
    // Evict oldest if window full
    if (this.items.length > this.maxSize) {
      this.items.shift();  // Remove first item (oldest)
    }
  }

  getAll(): T[] {
    return this.items;
  }

  size(): number {
    return this.items.length;
  }
}

const messages = new SlidingWindow<Message>();

// Turn 1
messages.add({ role: 'user', content: 'Show page 1' });
messages.add({ role: 'assistant', content: '...' });

// ... 100 turns later
// messages.size() = 10 (last 5 turns)
// Memory usage: 5KB (constant)
// Token usage: 2,000 tokens (constant)
```

**Result**:
- ✅ **Constant memory**: Always 10 items regardless of conversation length
- ✅ **No token overflow**: Stays within limits
- ✅ **Low cost**: $0.0003 per request (2K tokens vs 50K)
- ✅ **Fast performance**: LLM processes 2K tokens (0.5s latency)

---

## What is a Sliding Window?

### Definition

A **sliding window** is a data structure that maintains a **fixed-size buffer** of the most recent items, automatically **evicting** (removing) older items when the buffer reaches capacity.

### Visual Representation

```
Initial State (empty, maxSize = 5):
┌───────────────────────────┐
│                           │  ← Empty window
└───────────────────────────┘

After adding 3 items:
┌───────────────────────────┐
│ [A] [B] [C]               │  ← 3/5 capacity
└───────────────────────────┘

After adding 5 items (window full):
┌───────────────────────────┐
│ [A] [B] [C] [D] [E]       │  ← 5/5 capacity (full)
└───────────────────────────┘
  ↑                     ↑
  oldest              newest

After adding 6th item (F) → evict oldest (A):
┌───────────────────────────┐
│ [B] [C] [D] [E] [F]       │  ← Still 5/5, but slid forward
└───────────────────────────┘
  ↑                     ↑
  oldest              newest

After adding 7th item (G) → evict B:
┌───────────────────────────┐
│ [C] [D] [E] [F] [G]       │  ← Window slides forward
└───────────────────────────┘
  ↑                     ↑
  oldest              newest
```

### Key Characteristics

1. **Fixed Size**: Window never exceeds `maxSize`
2. **FIFO Eviction**: Oldest item removed first (first-in-first-out)
3. **Automatic Management**: No manual cleanup needed
4. **Constant Time**: O(1) add, O(1) evict
5. **Recency Bias**: Only keeps recent items

---

## How Sliding Windows Work

### Core Algorithm (FIFO Queue)

```typescript
class SlidingWindow<T> {
  private items: T[] = [];
  private readonly maxSize: number;

  constructor(maxSize: number = 10) {
    this.maxSize = maxSize;
  }

  // Add item and evict if needed
  add(item: T): void {
    this.items.push(item);  // Add to end (newest)
    
    if (this.items.length > this.maxSize) {
      this.items.shift();   // Remove from start (oldest)
    }
  }

  // Get all items in window
  getAll(): T[] {
    return [...this.items];  // Return copy
  }

  // Get N most recent items
  getRecent(n: number): T[] {
    return this.items.slice(-n);
  }

  // Get oldest item
  getOldest(): T | undefined {
    return this.items[0];
  }

  // Get newest item
  getNewest(): T | undefined {
    return this.items[this.items.length - 1];
  }

  // Check if window is full
  isFull(): boolean {
    return this.items.length >= this.maxSize;
  }

  // Clear window
  clear(): void {
    this.items = [];
  }

  // Current size
  size(): number {
    return this.items.length;
  }
}
```

### Example: Message Sliding Window

```typescript
interface Message {
  role: 'user' | 'assistant';
  content: string;
}

const messageWindow = new SlidingWindow<Message>(6);  // Keep last 6 messages (3 turns)

// Turn 1
messageWindow.add({ role: 'user', content: 'Show page 1' });
messageWindow.add({ role: 'assistant', content: 'Here is page 1...' });
// Window: [User:1, Assistant:1]

// Turn 2
messageWindow.add({ role: 'user', content: 'Show page 2' });
messageWindow.add({ role: 'assistant', content: 'Here is page 2...' });
// Window: [User:1, Assistant:1, User:2, Assistant:2]

// Turn 3
messageWindow.add({ role: 'user', content: 'Show page 3' });
messageWindow.add({ role: 'assistant', content: 'Here is page 3...' });
// Window: [User:1, Assistant:1, User:2, Assistant:2, User:3, Assistant:3]

// Turn 4 (window full → evict oldest 2)
messageWindow.add({ role: 'user', content: 'Show page 4' });
messageWindow.add({ role: 'assistant', content: 'Here is page 4...' });
// Window: [User:2, Assistant:2, User:3, Assistant:3, User:4, Assistant:4]
//          ↑ Turn 1 evicted (oldest)
```

---

## Window Sizing Strategies

### 1. Fixed Turn Count (Most Common)

**Keep last N conversation turns** (1 turn = user + assistant messages).

```typescript
// Azure OpenAI recommendation: 3-5 turns
const messageWindow = new SlidingWindow<Message>(10);  // 5 turns × 2 messages

// LangChain default: ~10 turns
const messageWindow = new SlidingWindow<Message>(20);  // 10 turns × 2 messages
```

**Pros**:
- ✅ Predictable: Always N turns in context
- ✅ Simple: Easy to reason about
- ✅ Works well for most use cases

**Cons**:
- ❌ Ignores message length (short vs long messages treated same)
- ❌ May drop important context if turns are info-dense

### 2. Token-Based (Dynamic Size)

**Keep messages until token limit reached**.

```typescript
import { encoding_for_model } from 'tiktoken';

class TokenBasedWindow {
  private messages: Message[] = [];
  private readonly maxTokens = 2000;
  private encoder = encoding_for_model('gpt-4o-mini');

  add(message: Message): void {
    this.messages.push(message);
    
    // Calculate total tokens
    while (this.getTotalTokens() > this.maxTokens) {
      this.messages.shift();  // Evict oldest until under limit
    }
  }

  private getTotalTokens(): number {
    return this.messages.reduce((total, msg) => {
      const tokens = this.encoder.encode(msg.content).length;
      return total + tokens;
    }, 0);
  }
}
```

**Pros**:
- ✅ Precise token control (never exceed LLM limit)
- ✅ Adapts to message length
- ✅ Cost-efficient

**Cons**:
- ❌ More complex (requires tokenization)
- ❌ Slower (tokenize every message)
- ❌ May evict many short messages or few long messages

### 3. Adaptive Sizing (Advanced)

**Adjust window size based on conversation complexity**.

```typescript
interface AdaptiveWindowConfig {
  minSize: number;  // Minimum 5 turns
  maxSize: number;  // Maximum 20 turns
  targetTokens: number;  // Target 2000 tokens
}

class AdaptiveWindow {
  private config: AdaptiveWindowConfig;
  private messages: Message[] = [];

  add(message: Message): void {
    this.messages.push(message);
    
    const tokens = this.getTotalTokens();
    
    // Expand if under target and room to grow
    if (tokens < this.config.targetTokens && 
        this.messages.length < this.config.maxSize * 2) {
      return;  // Keep growing
    }
    
    // Shrink if over target
    while (tokens > this.config.targetTokens && 
           this.messages.length > this.config.minSize * 2) {
      this.messages.shift();
    }
  }
}
```

**Pros**:
- ✅ Balances turn count and token limit
- ✅ Adapts to conversation style (dense vs sparse)

**Cons**:
- ❌ Complex to implement and tune
- ❌ Less predictable behavior

### 4. Recency Weighted (Relevance-Based)

**Weight recent items higher** when making eviction decisions.

```typescript
interface WeightedMessage extends Message {
  timestamp: number;
  relevanceScore: number;
}

class RecencyWeightedWindow {
  private messages: WeightedMessage[] = [];
  private readonly maxSize = 10;

  add(message: Message): void {
    this.messages.push({
      ...message,
      timestamp: Date.now(),
      relevanceScore: 1.0  // Initial score
    });

    if (this.messages.length > this.maxSize) {
      this.evictLowestScore();
    }
  }

  private evictLowestScore(): void {
    // Update scores based on recency
    this.messages.forEach(msg => {
      const ageInMinutes = (Date.now() - msg.timestamp) / 60000;
      msg.relevanceScore = Math.exp(-0.1 * ageInMinutes);  // Exponential decay
    });

    // Sort by score and evict lowest
    this.messages.sort((a, b) => a.relevanceScore - b.relevanceScore);
    this.messages.shift();
  }
}
```

**Pros**:
- ✅ Keeps important messages longer
- ✅ Better than pure FIFO for complex tasks

**Cons**:
- ❌ More complex scoring logic
- ❌ Harder to debug

---

## Eviction Policies

### 1. FIFO (First-In-First-Out) — Most Common

**Always evict oldest item**.

```typescript
class FIFOWindow<T> {
  private items: T[] = [];
  private maxSize: number;

  add(item: T): void {
    this.items.push(item);
    if (this.items.length > this.maxSize) {
      this.items.shift();  // Remove oldest
    }
  }
}
```

**Pros**: Simple, predictable, fast  
**Cons**: May drop important old items

### 2. LRU (Least Recently Used)

**Evict item that hasn't been accessed longest**.

```typescript
interface LRUItem<T> {
  value: T;
  lastAccessed: number;
}

class LRUWindow<T> {
  private items: LRUItem<T>[] = [];
  private maxSize: number;

  add(item: T): void {
    this.items.push({
      value: item,
      lastAccessed: Date.now()
    });

    if (this.items.length > this.maxSize) {
      // Evict least recently accessed
      this.items.sort((a, b) => a.lastAccessed - b.lastAccessed);
      this.items.shift();
    }
  }

  get(index: number): T | undefined {
    const item = this.items[index];
    if (item) {
      item.lastAccessed = Date.now();  // Update access time
      return item.value;
    }
    return undefined;
  }
}
```

**Pros**: Keeps frequently accessed items  
**Cons**: More complex, slower

### 3. Priority-Based

**Assign priorities**, evict lowest priority first.

```typescript
interface PriorityItem<T> {
  value: T;
  priority: number;  // Higher = more important
}

class PriorityWindow<T> {
  private items: PriorityItem<T>[] = [];
  private maxSize: number;

  add(item: T, priority: number = 1): void {
    this.items.push({ value: item, priority });

    if (this.items.length > this.maxSize) {
      // Evict lowest priority
      this.items.sort((a, b) => a.priority - b.priority);
      this.items.shift();
    }
  }
}
```

**Pros**: Keeps important items longer  
**Cons**: Requires manual priority assignment

### 4. Hybrid: FIFO + Sticky Items

**Protect important items** from eviction.

```typescript
interface StickyItem<T> {
  value: T;
  isSticky: boolean;  // Don't evict if true
}

class StickyWindow<T> {
  private items: StickyItem<T>[] = [];
  private maxSize: number;

  add(item: T, sticky: boolean = false): void {
    this.items.push({ value: item, isSticky: sticky });

    if (this.items.length > this.maxSize) {
      // Find first non-sticky item to evict
      const index = this.items.findIndex(i => !i.isSticky);
      if (index !== -1) {
        this.items.splice(index, 1);
      }
    }
  }
}
```

**Pros**: Preserves critical context  
**Cons**: May refuse eviction if all items sticky

---

## Implementation in JavaScript/TypeScript

### Basic Sliding Window Class

```typescript
// sliding-window.ts
export interface SlidingWindowOptions {
  maxSize?: number;  // Default: 10
  onEvict?: (item: T) => void;  // Callback when item evicted
}

export class SlidingWindow<T> {
  private items: T[] = [];
  private readonly maxSize: number;
  private readonly onEvict?: (item: T) => void;

  constructor(options: SlidingWindowOptions = {}) {
    this.maxSize = options.maxSize ?? 10;
    this.onEvict = options.onEvict;
  }

  // Add item to window
  add(item: T): void {
    this.items.push(item);

    // Evict oldest if over capacity
    while (this.items.length > this.maxSize) {
      const evicted = this.items.shift();
      if (evicted && this.onEvict) {
        this.onEvict(evicted);
      }
    }
  }

  // Get all items
  getAll(): T[] {
    return [...this.items];
  }

  // Get N most recent items
  getRecent(n: number): T[] {
    return this.items.slice(-n);
  }

  // Get N oldest items
  getOldest(n: number): T[] {
    return this.items.slice(0, n);
  }

  // Find item by predicate
  find(predicate: (item: T) => boolean): T | undefined {
    return this.items.find(predicate);
  }

  // Filter items
  filter(predicate: (item: T) => boolean): T[] {
    return this.items.filter(predicate);
  }

  // Check if window contains item
  has(predicate: (item: T) => boolean): boolean {
    return this.items.some(predicate);
  }

  // Get current size
  size(): number {
    return this.items.length;
  }

  // Check if full
  isFull(): boolean {
    return this.items.length >= this.maxSize;
  }

  // Check if empty
  isEmpty(): boolean {
    return this.items.length === 0;
  }

  // Clear all items
  clear(): void {
    this.items = [];
  }
}
```

### Message Window for Conversations

```typescript
import { SlidingWindow } from './sliding-window';

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp?: number;
}

export class MessageWindow extends SlidingWindow<Message> {
  constructor(maxTurns: number = 5) {
    super({
      maxSize: maxTurns * 2,  // 2 messages per turn (user + assistant)
      onEvict: (msg) => {
        console.log(`Evicted ${msg.role} message: "${msg.content.slice(0, 50)}..."`);
      }
    });
  }

  // Add user message
  addUser(content: string): void {
    this.add({
      role: 'user',
      content,
      timestamp: Date.now()
    });
  }

  // Add assistant message
  addAssistant(content: string): void {
    this.add({
      role: 'assistant',
      content,
      timestamp: Date.now()
    });
  }

  // Get messages in OpenAI format
  getMessages(): Message[] {
    return this.getAll();
  }

  // Get last N turns (excluding system messages)
  getLastTurns(n: number): Message[] {
    return this.getRecent(n * 2)
      .filter(msg => msg.role !== 'system');
  }
}
```

### Entity Window for Working Memory

```typescript
import { SlidingWindow } from './sliding-window';
import type { Entity } from './types';

export class EntityWindow extends SlidingWindow<Entity> {
  constructor(maxEntities: number = 20) {
    super({ maxSize: maxEntities });
  }

  // Get entity by type and ID
  get(type: string, id: number | string): Entity | undefined {
    return this.find(e => e.type === type && e.id === id);
  }

  // Get all entities of a type
  getByType(type: string): Entity[] {
    return this.filter(e => e.type === type);
  }

  // Check if entity exists
  has(type: string, id: number | string): boolean {
    return this.has(e => e.type === type && e.id === id);
  }

  // Get most recent entity of a type
  getRecentByType(type: string): Entity | undefined {
    const entities = this.getByType(type);
    return entities[entities.length - 1];
  }
}
```

---

## Production Patterns

### Pattern 1: Azure OpenAI Best Practices

**Hybrid: Summarized Context + Sliding Window**

```typescript
interface ConversationContext {
  summary: string;           // Long-term summary
  recentMessages: Message[];  // Last 3-5 turns
}

class AzureAgentMemory {
  private summary: string = '';
  private messageWindow: MessageWindow;

  constructor() {
    this.messageWindow = new MessageWindow(5);  // 5 turns = Azure recommendation
  }

  async addTurn(user: string, assistant: string): Promise<void> {
    this.messageWindow.addUser(user);
    this.messageWindow.addAssistant(assistant);

    // Update summary every 10 turns
    if (this.messageWindow.size() % 20 === 0) {
      await this.updateSummary();
    }
  }

  getContext(): ConversationContext {
    return {
      summary: this.summary,
      recentMessages: this.messageWindow.getMessages()
    };
  }

  private async updateSummary(): Promise<void> {
    const messages = this.messageWindow.getMessages();
    const prompt = `Summarize key points from this conversation:\n${
      messages.map(m => `${m.role}: ${m.content}`).join('\n')
    }`;

    // Call LLM to generate summary
    const response = await llm.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }]
    });

    this.summary = response.choices[0].message.content || '';
  }
}
```

**Token Budgeting**:
- Summary: 500-1000 tokens (persistent)
- Recent 5 turns: 1000-2000 tokens (sliding window)
- Tool results: 2000-4000 tokens (current task)
- **Total**: ~5000-8000 tokens (safe for 16K window)

### Pattern 2: LangChain.js ConversationBufferMemory

```typescript
import { ConversationBufferMemory } from 'langchain/memory';
import { ChatOpenAI } from '@langchain/openai';
import { AgentExecutor, createReactAgent } from 'langchain/agents';

// Sliding window with token limit
const memory = new ConversationBufferMemory({
  memoryKey: 'chat_history',
  returnMessages: true,
  maxTokenLimit: 2000  // Dynamic sizing based on tokens
});

const llm = new ChatOpenAI({ 
  modelName: 'gpt-4o-mini' 
});

const agent = await createReactAgent({
  llm,
  tools: [/* your tools */],
  prompt: /* your prompt */
});

const executor = AgentExecutor.fromAgentAndTools({
  agent,
  tools: [/* your tools */],
  memory  // Automatically manages sliding window
});

// Use agent (memory auto-managed)
const result = await executor.invoke({ input: 'Show me page 42' });
```

### Pattern 3: Strands Agents SDK SlidingWindowConversationManager

```typescript
import { Agent } from 'strands';
import { SlidingWindowConversationManager } from 'strands/agent/conversation_manager';

// Configure sliding window
const conversationManager = new SlidingWindowConversationManager({
  windowSize: 20,  // Max 20 messages
  shouldTruncateResults: true  // Truncate large tool results
});

const agent = new Agent({
  conversationManager
});

// Agent automatically manages window
await agent.invoke('Show me page 42');
await agent.invoke('Update this page');
// Only last 20 messages kept in context
```

### Pattern 4: Custom Sliding Window with Entity Extraction

```typescript
import { WorkingMemory } from './working-memory';
import { EntityWindow } from './entity-window';
import type { Entity } from './types';

class AgentMemory {
  private messageWindow: MessageWindow;
  private entityWindow: EntityWindow;

  constructor() {
    this.messageWindow = new MessageWindow(5);  // 5 turns
    this.entityWindow = new EntityWindow(20);   // 20 entities
  }

  addTurn(
    userMessage: string,
    assistantResponse: string,
    entities: Entity[]
  ): void {
    // Add to message window (automatic eviction)
    this.messageWindow.addUser(userMessage);
    this.messageWindow.addAssistant(assistantResponse);

    // Add entities to entity window (automatic eviction)
    for (const entity of entities) {
      this.entityWindow.add(entity);
    }
  }

  getContext(): string {
    const messages = this.messageWindow.getMessages();
    const entities = this.entityWindow.getAll();

    return `
Conversation:
${messages.map(m => `${m.role}: ${m.content}`).join('\n')}

Working Memory (Entities):
${entities.map(e => `- ${e.type} ${e.id}: ${e.label}`).join('\n')}
    `.trim();
  }
}
```

---

## Trade-offs and Considerations

### Pros ✅
- **Bounded Memory**: O(N) space, never grows unbounded
- **Predictable**: Always know max memory usage
- **Simple**: Easy to implement (FIFO queue)
- **Fast**: O(1) add, O(1) evict
- **Cost Control**: Token usage stays constant

### Cons ❌
- **Context Loss**: May drop important old information
- **No Semantic Awareness**: Evicts by age, not importance
- **Fixed Size**: Can't adapt to conversation complexity
- **Recency Bias**: Only keeps recent items

### Best Practices

1. **Choose appropriate window size**
   ```typescript
   // Rule of thumb:
   // - Short conversations: 3-5 turns (6-10 messages)
   // - Long conversations: 10-20 turns (20-40 messages)
   // - Cost-sensitive: 3-5 turns
   // - Quality-sensitive: 10-20 turns
   
   const window = new MessageWindow(5);  // 5 turns (Azure recommendation)
   ```

2. **Combine with summarization for long conversations**
   ```typescript
   // After 10 turns, summarize and compress
   if (turns >= 10) {
     const summary = await summarizeConversation(messages);
     window.clear();
     window.addSystem(summary);
   }
   ```

3. **Use token-based sizing for variable-length messages**
   ```typescript
   // If messages vary greatly in length, use token-based window
   const window = new TokenBasedWindow({ maxTokens: 2000 });
   ```

4. **Monitor eviction rate in production**
   ```typescript
   let evictions = 0;
   const window = new SlidingWindow({
     maxSize: 10,
     onEvict: () => evictions++
   });

   // Log metrics
   console.log({
     evictionRate: evictions / totalAdds,
     avgWindowSize: window.size()
   });
   ```

5. **Protect important context with sticky items**
   ```typescript
   // System prompt should never be evicted
   window.add(systemPrompt, { sticky: true });
   ```

---

## References

### Research Papers & Best Practices

1. **Azure OpenAI: Conversation Context Management** (Microsoft, 2025)  
   [https://learn.microsoft.com/en-us/answers/questions/2259997](https://learn.microsoft.com/en-us/answers/questions/2259997)  
   - Recommends 3-5 turn sliding window
   - Hybrid strategy: Summary + recent turns
   - Token budgeting: 8K-16K safe range

2. **Optimizing Context Windows in AI Agents** (Sparkco AI, 2025)  
   [https://sparkco.ai/blog/optimizing-context-windows-in-ai-agents](https://sparkco.ai/blog/optimizing-context-windows-in-ai-agents)  
   - Sliding window vs summarization trade-offs
   - Adaptive window sizing strategies
   - LangChain implementation patterns

3. **Mastering AI Conversation Context Limits** (Sparkco AI, 2025)  
   [https://sparkco.ai/blog/mastering-ai-conversation-context-limits-2025-deep-dive](https://sparkco.ai/blog/mastering-ai-conversation-context-limits-2025-deep-dive)  
   - Multi-turn conversation handling
   - Memory management frameworks
   - Production metrics and benchmarks

### Production Frameworks

4. **Strands Agents SDK: Conversation Management** (2024-2025)  
   [https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/conversation-management/](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/conversation-management/)  
   - `SlidingWindowConversationManager` implementation
   - Default 20 message window
   - Configurable truncation for large tool results

5. **Google ADK: Context Management Strategies** (2025)  
   [https://github.com/google/adk-python/discussions/826](https://github.com/google/adk-python/discussions/826)  
   - Sliding window vs summarization
   - Processor-based context filtering
   - LangChain integration examples

6. **LangChain.js Memory Management** (2024-2025)  
   [https://js.langchain.com/docs/modules/memory/](https://js.langchain.com/docs/modules/memory/)  
   - `ConversationBufferMemory` (token-based)
   - `ConversationSummaryMemory` (summarization)
   - Window sizing best practices

### Implementation Resources

7. **Agentic Design: Sliding Window Management Pattern** (2024)  
   [https://agentic-design.ai/patterns/context-management/sliding-window-management](https://agentic-design.ai/patterns/context-management/sliding-window-management)  
   - Adaptive window sizing
   - Recency weighting formulas
   - Relevance-based retention

8. **Context Window Management UI Patterns** (2024)  
   [https://agentic-design.ai/patterns/ui-ux/context-window-management](https://agentic-design.ai/patterns/ui-ux/context-window-management)  
   - Visual indicators for window capacity
   - Token usage meters
   - Compression controls

### Community Discussions

9. **OpenAI Community: Managing Context in Conversation Bots** (2025)  
   [https://community.openai.com/t/managing-context-in-a-conversation-bot](https://community.openai.com/t/managing-context-in-a-conversation-bot)  
   - Threshold-based summarization
   - Secondary LLM for refinement
   - Production challenges and solutions

10. **Factory AI: The Context Window Problem** (2025)  
    [https://factory.ai/news/context-window-problem](https://factory.ai/news/context-window-problem)  
    - Scaling agents beyond token limits
    - Enterprise monorepo challenges
    - Context management strategies

---

**Next Topic**: [4.1.4 Reference Resolution ("this page", "that entry")](./4.1.4-reference-resolution.md)

**Related Topics**:
- [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md)
- [4.1.2 Entity Extraction](./4.1.2-entity-extraction.md)
- [2.2.1 Sliding Window (Context Management)](../2-context/2.2.1-sliding-window.md)
