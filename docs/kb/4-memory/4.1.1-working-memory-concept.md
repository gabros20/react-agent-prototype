# 4.1.1 Working Memory Concept (RAM Analogy)

**Status**: ✅ Complete  
**Last Updated**: 2025-11-18  
**Grounded In**: Mem0 (2025), Azure OpenAI patterns, LangChain, production implementations

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [The Problem: Context Overload](#the-problem-context-overload)
3. [The RAM Analogy](#the-ram-analogy)
4. [What is Working Memory?](#what-is-working-memory)
5. [How Working Memory Works](#how-working-memory-works)
6. [Architecture Patterns](#architecture-patterns)
7. [Implementation in JavaScript/TypeScript](#implementation-in-javascripttypescript)
8. [Production Examples](#production-examples)
9. [When to Use Working Memory](#when-to-use-working-memory)
10. [Trade-offs and Considerations](#trade-offs-and-considerations)
11. [References](#references)

---

## Executive Summary

**Working memory** is a short-term cognitive system that temporarily stores and manipulates information needed for immediate tasks. In AI agents, it acts like **RAM** in computers — holding recently extracted entities, facts, and context without persisting everything to long-term storage.

### Key Metrics (Mem0, 2025)
- **26% accuracy boost** over OpenAI Memory baseline
- **91% lower p95 latency** (1.44s vs 17.12s) compared to full context
- **90% fewer tokens** (~1.8K vs 26K tokens per conversation)
- Scalable to **extended multi-session dialogues**

### Why It Matters
- **Efficient Context Management**: Only keep what's relevant for the current task
- **Fast Access**: Recent entities are immediately available without database queries
- **Adaptive Capacity**: Automatically evicts old information when space is limited
- **Cost Optimization**: Reduces token usage by 90% vs. full conversation history

---

## The Problem: Context Overload

### Traditional Approach: Send Everything
```typescript
// ❌ BAD: Sending entire conversation history every time
const messages = [
  { role: 'user', content: 'Tell me about page 42' },
  { role: 'assistant', content: 'Page 42 is titled...' },
  { role: 'user', content: 'What about page 57?' },
  { role: 'assistant', content: 'Page 57 discusses...' },
  // ... 50 more exchanges
  { role: 'user', content: 'Now show me page 12' }
];

// Agent must process ALL 50+ messages to answer simple question
const response = await llm.chat({ messages });
```

**Problems**:
- **Token Explosion**: 26K+ tokens for long conversations
- **High Latency**: 17.12s p95 response time (Mem0 benchmark)
- **Context Dilution**: Important info buried in irrelevant messages
- **Cost**: Processing 26K tokens per request is expensive

---

## The RAM Analogy

Think of working memory as **RAM** in your computer:

### Computer RAM vs AI Working Memory

| Computer RAM | AI Working Memory |
|--------------|-------------------|
| **Volatile Storage** | **Temporary Context** |
| Fast read/write access | Fast entity lookup |
| Limited capacity (16GB, 32GB) | Limited token capacity (8K, 16K) |
| Stores currently running programs | Stores recent conversation entities |
| Evicts old data when full | Evicts old entities when full |
| **Does NOT** persist on shutdown | **Does NOT** persist after session |

### Example: Browser Tabs (RAM) vs Bookmarks (Long-term Memory)

```typescript
// Browser Tabs = Working Memory (fast, limited, temporary)
const openTabs = [
  'docs.google.com/document/xyz',  // Currently editing
  'stackoverflow.com/question/123', // Just referenced
  'github.com/repo/issues/456'     // Recently viewed
];
// Close browser → tabs disappear (volatile)

// Bookmarks = Long-term Memory (slow, unlimited, persistent)
const bookmarks = [
  'Important research papers',
  'Favorite recipes',
  'Project documentation'
];
// Close browser → bookmarks remain (persistent)
```

### Why Not Just Use Database for Everything?

**Query latency comparison** (typical production values):
- **Working Memory**: 0.2ms (in-memory object access)
- **Vector Database**: 50-200ms (semantic search + retrieval)
- **SQL Database**: 20-100ms (indexed lookup)

For real-time agents, working memory is **100-1000× faster** than database queries.

---

## What is Working Memory?

### Definition

**Working memory** is an in-memory buffer that stores **recently extracted entities** and **conversation context** needed for immediate agent tasks.

### Key Characteristics

1. **Short-Term**: Only retains recent information (last 5-10 turns)
2. **Task-Relevant**: Stores entities the agent is currently working with
3. **Volatile**: Does NOT persist across sessions (lost on restart)
4. **Fast Access**: In-memory lookup without database queries
5. **Limited Capacity**: Fixed size to prevent token overflow

### What Goes in Working Memory?

| Store in Working Memory | Don't Store |
|-------------------------|-------------|
| ✅ Recently mentioned page IDs | ❌ Full page content |
| ✅ Current user's name | ❌ Historical user preferences |
| ✅ Active task context | ❌ Completed task history |
| ✅ Last 5 tool results | ❌ All tool execution logs |
| ✅ Pronouns to resolve ("this", "that") | ❌ Unrelated past conversations |

---

## How Working Memory Works

### The Three-Phase Cycle

```
┌─────────────────┐
│  1. EXTRACT     │  Extract entities from tool results
│  Entities       │  "Page 42 contains navigation menu"
└────────┬────────┘  → Extract: { pageId: 42, type: 'navigation' }
         │
         ▼
┌─────────────────┐
│  2. UPDATE      │  Merge with existing working memory
│  Memory         │  Check if pageId:42 already exists
└────────┬────────┘  → Update if exists, Add if new
         │
         ▼
┌─────────────────┐
│  3. EVICT       │  Remove oldest entities if capacity exceeded
│  Old Items      │  Keep last N entities (sliding window)
└─────────────────┘
```

### Example: CMS Agent Workflow

```typescript
// Turn 1: User asks about page 42
User: "Show me page 42"
Agent: [Calls cms_getPage(42)]
Tool Result: { id: 42, title: 'Home', type: 'page' }

// Extract entity
workingMemory.add({ 
  type: 'page', 
  id: 42, 
  title: 'Home',
  lastMentioned: Date.now() 
});

// Working Memory State: [{ page: 42, title: 'Home' }]

// Turn 2: User references "this page"
User: "What's the content of this page?"
Agent: [Resolves "this page" → pageId: 42 from working memory]
      [Calls cms_getPage(42, { includeContent: true })]

// Working Memory State: [{ page: 42, title: 'Home', hasContent: true }]

// Turn 5: User switches context
User: "Now show me page 57"
Agent: [Calls cms_getPage(57)]
Tool Result: { id: 57, title: 'About' }

// Add new entity
workingMemory.add({ type: 'page', id: 57, title: 'About' });

// Working Memory State: [
//   { page: 42, title: 'Home' },   // Still in memory (recent)
//   { page: 57, title: 'About' }   // Newly added
// ]

// Turn 10: User asks "Show me page 99"
// Working memory may have evicted page 42 by now (sliding window = 5 turns)
```

---

## Architecture Patterns

### 1. Sliding Window (Most Common)

**Keep last N conversation turns** — simple and predictable.

```typescript
interface WorkingMemoryConfig {
  maxTurns: number;        // Default: 5 turns
  maxEntities: number;     // Default: 20 entities
  evictionPolicy: 'fifo' | 'lru' | 'relevance';
}

class SlidingWindowMemory {
  private entities: Entity[] = [];
  private readonly maxSize = 20;

  add(entity: Entity): void {
    this.entities.push({ ...entity, timestamp: Date.now() });
    
    // Evict oldest if capacity exceeded
    if (this.entities.length > this.maxSize) {
      this.entities.shift(); // Remove oldest (FIFO)
    }
  }

  get(type: string, id: number): Entity | undefined {
    return this.entities.find(e => e.type === type && e.id === id);
  }

  getRecent(limit: number = 5): Entity[] {
    return this.entities.slice(-limit);
  }
}
```

**Pros**:
- ✅ Simple to implement
- ✅ Predictable memory usage
- ✅ Good for short conversations (5-10 turns)

**Cons**:
- ❌ May drop important entities if conversation is long
- ❌ No semantic relevance (just recency)

### 2. Relevance-Based Eviction (Advanced)

**Keep semantically important entities** — score by relevance, not just age.

```typescript
interface ScoredEntity extends Entity {
  relevanceScore: number;  // 0-1 (higher = more important)
  accessCount: number;     // How many times referenced
  lastAccessed: number;    // Timestamp
}

class RelevanceBasedMemory {
  private entities: ScoredEntity[] = [];
  private readonly maxSize = 20;

  add(entity: Entity): void {
    const existing = this.entities.find(e => 
      e.type === entity.type && e.id === entity.id
    );

    if (existing) {
      // Update existing entity
      existing.accessCount++;
      existing.lastAccessed = Date.now();
      existing.relevanceScore = this.calculateRelevance(existing);
    } else {
      // Add new entity with initial score
      this.entities.push({
        ...entity,
        relevanceScore: 0.5,
        accessCount: 1,
        lastAccessed: Date.now()
      });
    }

    // Evict lowest-scoring entity if capacity exceeded
    if (this.entities.length > this.maxSize) {
      this.evictLowestScore();
    }
  }

  private calculateRelevance(entity: ScoredEntity): number {
    const recency = Math.exp(-0.1 * (Date.now() - entity.lastAccessed) / 60000); // Decay over minutes
    const frequency = Math.min(entity.accessCount / 10, 1.0); // Cap at 10 accesses
    return 0.6 * recency + 0.4 * frequency;
  }

  private evictLowestScore(): void {
    this.entities.sort((a, b) => a.relevanceScore - b.relevanceScore);
    this.entities.shift(); // Remove lowest-scored entity
  }
}
```

**Pros**:
- ✅ Keeps important entities longer
- ✅ Better for complex multi-step tasks
- ✅ Adapts to conversation flow

**Cons**:
- ❌ More complex to implement
- ❌ Requires scoring heuristics
- ❌ Harder to debug

### 3. Hybrid: Mem0 Architecture (Production-Grade)

**Two-phase pipeline**: Extract → Update → Retrieve.

```typescript
import { OpenAI } from 'openai';

interface Memory {
  id: string;
  content: string;
  entities: string[];
  timestamp: number;
  operation: 'ADD' | 'UPDATE' | 'DELETE' | 'NOOP';
}

class Mem0WorkingMemory {
  private memories: Memory[] = [];
  private llm: OpenAI;

  constructor(llm: OpenAI) {
    this.llm = llm;
  }

  // Phase 1: Extract candidate memories from new message pair
  async extractMemories(
    userMessage: string, 
    assistantResponse: string,
    conversationSummary: string
  ): Promise<Memory[]> {
    const prompt = `
Extract key facts from this exchange:
User: ${userMessage}
Assistant: ${assistantResponse}

Context: ${conversationSummary}

Return JSON array of facts to remember:
[{ "content": "User prefers dark mode", "entities": ["user", "preference"] }]
    `.trim();

    const response = await this.llm.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' }
    });

    return JSON.parse(response.choices[0].message.content || '[]');
  }

  // Phase 2: Update existing memories with new facts
  async updateMemories(candidateFacts: Memory[]): Promise<void> {
    for (const candidate of candidateFacts) {
      // Find similar existing memories using semantic similarity
      const similar = this.memories.filter(m => 
        this.semanticSimilarity(m.content, candidate.content) > 0.8
      );

      if (similar.length === 0) {
        // ADD: No similar memory exists
        candidate.operation = 'ADD';
        this.memories.push(candidate);
      } else {
        // Determine operation: UPDATE, DELETE, or NOOP
        const operation = await this.determineOperation(candidate, similar[0]);
        
        if (operation === 'UPDATE') {
          similar[0].content = candidate.content;
          similar[0].timestamp = Date.now();
        } else if (operation === 'DELETE') {
          this.memories = this.memories.filter(m => m.id !== similar[0].id);
        }
        // NOOP: Do nothing
      }
    }
  }

  private async determineOperation(
    newFact: Memory, 
    existingFact: Memory
  ): Promise<'UPDATE' | 'DELETE' | 'NOOP'> {
    const prompt = `
Compare these two facts:
Existing: ${existingFact.content}
New: ${newFact.content}

Determine operation:
- UPDATE: New fact adds information
- DELETE: New fact contradicts existing
- NOOP: Facts are equivalent

Return: { "operation": "UPDATE" | "DELETE" | "NOOP" }
    `.trim();

    const response = await this.llm.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' }
    });

    const result = JSON.parse(response.choices[0].message.content || '{}');
    return result.operation as 'UPDATE' | 'DELETE' | 'NOOP';
  }

  // Phase 3: Retrieve relevant memories for query
  search(query: string, limit: number = 5): Memory[] {
    // Sort by semantic similarity + recency
    return this.memories
      .map(m => ({
        memory: m,
        score: this.semanticSimilarity(query, m.content) * 0.7 +
               this.recencyScore(m.timestamp) * 0.3
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, limit)
      .map(item => item.memory);
  }

  private semanticSimilarity(text1: string, text2: string): number {
    // Simplified: In production, use embeddings + cosine similarity
    const words1 = new Set(text1.toLowerCase().split(/\s+/));
    const words2 = new Set(text2.toLowerCase().split(/\s+/));
    const intersection = new Set([...words1].filter(w => words2.has(w)));
    return intersection.size / Math.max(words1.size, words2.size);
  }

  private recencyScore(timestamp: number): number {
    const ageInMinutes = (Date.now() - timestamp) / 60000;
    return Math.exp(-0.05 * ageInMinutes); // Exponential decay
  }
}
```

**Key Features**:
- **Extraction**: LLM extracts facts from message pairs
- **Update Logic**: Determines ADD/UPDATE/DELETE/NOOP operations
- **Semantic Search**: Retrieves relevant memories by similarity
- **Automatic Deduplication**: Merges redundant facts

**Production Results** (Mem0, 2025):
- **66.9% accuracy** (vs 52.9% OpenAI Memory baseline)
- **0.20s median latency** (vs 9.87s full context)
- **1.8K tokens** per conversation (vs 26K full context)

---

## Implementation in JavaScript/TypeScript

### Basic Working Memory Class

```typescript
// types.ts
export interface Entity {
  type: string;          // 'page' | 'entry' | 'user' | 'task'
  id: number | string;
  label?: string;        // Human-readable name
  attributes?: Record<string, unknown>;
  timestamp: number;     // When added to memory
}

export interface WorkingMemoryOptions {
  maxSize?: number;      // Max entities to store (default: 20)
  ttl?: number;          // Time-to-live in ms (default: 5 minutes)
}

// working-memory.ts
export class WorkingMemory {
  private entities: Map<string, Entity> = new Map();
  private readonly maxSize: number;
  private readonly ttl: number;

  constructor(options: WorkingMemoryOptions = {}) {
    this.maxSize = options.maxSize ?? 20;
    this.ttl = options.ttl ?? 5 * 60 * 1000; // 5 minutes
  }

  // Add or update entity
  add(entity: Omit<Entity, 'timestamp'>): void {
    const key = this.getKey(entity.type, entity.id);
    
    this.entities.set(key, {
      ...entity,
      timestamp: Date.now()
    });

    // Evict old entries if capacity exceeded
    if (this.entities.size > this.maxSize) {
      this.evictOldest();
    }
  }

  // Get entity by type and ID
  get(type: string, id: number | string): Entity | undefined {
    const key = this.getKey(type, id);
    const entity = this.entities.get(key);

    // Check if entity expired
    if (entity && Date.now() - entity.timestamp > this.ttl) {
      this.entities.delete(key);
      return undefined;
    }

    return entity;
  }

  // Get all entities of a specific type
  getByType(type: string): Entity[] {
    return Array.from(this.entities.values())
      .filter(e => e.type === type && Date.now() - e.timestamp <= this.ttl);
  }

  // Get N most recent entities
  getRecent(limit: number = 5): Entity[] {
    return Array.from(this.entities.values())
      .sort((a, b) => b.timestamp - a.timestamp)
      .slice(0, limit);
  }

  // Clear all entities
  clear(): void {
    this.entities.clear();
  }

  // Get memory size
  size(): number {
    return this.entities.size;
  }

  // Private helpers
  private getKey(type: string, id: number | string): string {
    return `${type}:${id}`;
  }

  private evictOldest(): void {
    let oldestKey: string | null = null;
    let oldestTime = Infinity;

    for (const [key, entity] of this.entities) {
      if (entity.timestamp < oldestTime) {
        oldestTime = entity.timestamp;
        oldestKey = key;
      }
    }

    if (oldestKey) {
      this.entities.delete(oldestKey);
    }
  }
}
```

### Usage in Agent Context

```typescript
// agent-context.ts
import { WorkingMemory, type Entity } from './working-memory';

export interface AgentContext {
  workingMemory: WorkingMemory;
  userId: string;
  sessionId: string;
}

// Tool execution with automatic entity extraction
export async function cms_getPage(
  pageId: number,
  options: { includeContent?: boolean },
  context: AgentContext
): Promise<string> {
  // Fetch page data
  const page = await db.pages.findUnique({ where: { id: pageId } });
  
  if (!page) {
    return `Page ${pageId} not found`;
  }

  // Automatically add page to working memory
  context.workingMemory.add({
    type: 'page',
    id: pageId,
    label: page.title,
    attributes: {
      slug: page.slug,
      status: page.status,
      type: page.type
    }
  });

  return JSON.stringify(page, null, 2);
}

// Reference resolution using working memory
export function resolveReference(
  pronoun: string, 
  context: AgentContext
): Entity | undefined {
  // "this page" → Most recent page entity
  if (pronoun === 'this' || pronoun === 'that') {
    const recentPages = context.workingMemory.getByType('page');
    return recentPages[0]; // Most recent
  }

  return undefined;
}
```

---

## Production Examples

### Example 1: Azure OpenAI Best Practices

**Hybrid Strategy**: Summarized Context + Sliding Window

```typescript
interface ConversationMemory {
  summary: string;           // Running summary of key points
  recentTurns: Message[];    // Last 3-5 turns for tone/continuity
}

class AzureAgentMemory {
  private summary: string = '';
  private turns: Message[] = [];
  private readonly windowSize = 5;

  async addTurn(user: string, assistant: string): Promise<void> {
    this.turns.push(
      { role: 'user', content: user },
      { role: 'assistant', content: assistant }
    );

    // Keep only last N turns
    if (this.turns.length > this.windowSize * 2) {
      this.turns = this.turns.slice(-this.windowSize * 2);
    }

    // Update summary every 5 turns
    if (this.turns.length % 10 === 0) {
      await this.updateSummary();
    }
  }

  async updateSummary(): Promise<void> {
    const prompt = `
Summarize the key discussion points from this conversation:
${this.turns.map(t => `${t.role}: ${t.content}`).join('\n')}

Focus on:
- User goals and preferences
- Important decisions made
- Relevant context for future turns
    `.trim();

    const response = await llm.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }]
    });

    this.summary = response.choices[0].message.content || '';
  }

  getContext(): string {
    return `
Summary: ${this.summary}

Recent conversation:
${this.turns.map(t => `${t.role}: ${t.content}`).join('\n')}
    `.trim();
  }
}
```

**Token Budgeting** (Microsoft recommended):
- **Summary**: ~500-1000 tokens (session-level context)
- **Recent Turns**: ~1000-2000 tokens (last 3-5 exchanges)
- **Tool Results**: ~2000-4000 tokens (current task data)
- **System Prompt**: ~1000 tokens
- **Total**: ~5000-8000 tokens (safe for 16K context window)

### Example 2: LangChain.js ConversationBufferMemory

```typescript
import { ConversationBufferMemory } from 'langchain/memory';
import { ChatOpenAI } from '@langchain/openai';
import { AgentExecutor, createReactAgent } from 'langchain/agents';

// Initialize memory with sliding window
const memory = new ConversationBufferMemory({
  memoryKey: 'chat_history',
  returnMessages: true,
  inputKey: 'input',
  outputKey: 'output',
  // Keep only last 10 messages (5 turns)
  maxTokenLimit: 2000
});

// Create agent with memory
const llm = new ChatOpenAI({ 
  modelName: 'gpt-4o-mini',
  temperature: 0 
});

const executor = AgentExecutor.fromAgentAndTools({
  agent: await createReactAgent({ llm, tools: [...] }),
  tools: [...],
  memory
});

// Use agent (memory automatically managed)
const result1 = await executor.invoke({ 
  input: 'Show me page 42' 
});
// Memory now contains: [User: "Show me page 42", AI: "..."]

const result2 = await executor.invoke({ 
  input: 'What about that page?' 
});
// Memory resolves "that page" using chat_history
```

### Example 3: Vercel AI SDK with Working Memory

```typescript
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const workingMemory = new WorkingMemory({ maxSize: 20 });

const tools = {
  getPage: tool({
    description: 'Get page by ID',
    parameters: z.object({ 
      pageId: z.number() 
    }),
    execute: async ({ pageId }, { session }) => {
      const page = await db.pages.findUnique({ 
        where: { id: pageId } 
      });

      // Add to working memory
      workingMemory.add({
        type: 'page',
        id: pageId,
        label: page.title,
        attributes: { slug: page.slug }
      });

      return JSON.stringify(page);
    }
  }),

  resolveReference: tool({
    description: 'Resolve pronouns like "this page", "that entry"',
    parameters: z.object({ 
      pronoun: z.string() 
    }),
    execute: async ({ pronoun }) => {
      const recent = workingMemory.getRecent(1);
      if (recent.length === 0) {
        return 'No recent entities in memory';
      }
      return JSON.stringify(recent[0]);
    }
  })
};

// Agent automatically uses working memory via tools
const result = await generateText({
  model: openai('gpt-4o-mini'),
  tools,
  maxSteps: 5,
  prompt: 'Show me page 42, then update that page title'
});
```

---

## When to Use Working Memory

### Use When:
- ✅ **Multi-turn conversations** (5+ exchanges)
- ✅ **Reference resolution** needed ("this page", "that entry")
- ✅ **Real-time agents** requiring sub-second response
- ✅ **Cost optimization** priority (reduce token usage)
- ✅ **Session-scoped** tasks (no cross-session persistence needed)

### Don't Use When:
- ❌ **Single-shot queries** (no conversation context)
- ❌ **Long-term persistence** required (use vector database)
- ❌ **Cross-session memory** needed (user preferences, history)
- ❌ **Complex semantic search** (working memory is keyword-based)
- ❌ **Compliance requirements** (audit trails, legal docs)

### Decision Matrix

| Scenario | Working Memory | Long-term Memory | Both |
|----------|----------------|------------------|------|
| "Show me page 42, then update that page" | ✅ | ❌ | ❌ |
| "What did I ask about last week?" | ❌ | ✅ | ❌ |
| "Find all pages about AI" | ❌ | ✅ | ❌ |
| "User prefers dark mode" (session) | ✅ | ❌ | ❌ |
| "User prefers dark mode" (permanent) | ❌ | ✅ | ❌ |
| Multi-step task with history search | ❌ | ❌ | ✅ |

---

## Trade-offs and Considerations

### Pros ✅
- **Fast**: In-memory access (0.2ms vs 50-200ms database)
- **Simple**: Easy to implement and debug
- **Cost-effective**: 90% token reduction vs full context
- **Low latency**: 91% faster than full-context approaches
- **Automatic eviction**: No manual cleanup needed

### Cons ❌
- **Volatile**: Lost on crash/restart (no persistence)
- **Limited capacity**: Can't store large datasets
- **No semantic search**: Relies on exact matches or simple similarity
- **Session-scoped**: Can't retrieve cross-session information
- **Eviction risk**: Important entities may be dropped if conversation is long

### Best Practices

1. **Set appropriate capacity limits**
   ```typescript
   // Rule of thumb: 1-2 entities per conversation turn
   const memory = new WorkingMemory({ 
     maxSize: 20,  // 10 turns × 2 entities/turn
     ttl: 5 * 60 * 1000 // 5 minutes
   });
   ```

2. **Combine with summarization for long conversations**
   ```typescript
   // After 10 turns, summarize and compress
   if (turns.length >= 10) {
     const summary = await summarizeConversation(turns);
     memory.clear();
     memory.add({ 
       type: 'summary', 
       id: 'session', 
       label: summary 
     });
   }
   ```

3. **Use relevance scoring for complex tasks**
   ```typescript
   // Weight entities by access frequency + recency
   const score = 0.6 * recency + 0.4 * frequency;
   ```

4. **Monitor memory usage in production**
   ```typescript
   // Log memory stats
   console.log({
     memorySize: memory.size(),
     evictionRate: evictions / totalAdds,
     avgLatency: avgAccessTime
   });
   ```

5. **Fallback to long-term memory when needed**
   ```typescript
   // Check working memory first, then vector DB
   let entity = workingMemory.get('page', pageId);
   if (!entity) {
     entity = await vectorDB.search(`page ${pageId}`);
   }
   ```

---

## References

### Research Papers

1. **Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory** (2025)  
   [https://arxiv.org/abs/2504.19413](https://arxiv.org/abs/2504.19413)  
   - 26% accuracy boost, 91% lower latency, 90% token savings
   - Two-phase pipeline: Extraction → Update
   - Graph-based variant (Mem0g) for relational memory

2. **Azure OpenAI: Conversation Context Management Best Practices** (2025)  
   [https://learn.microsoft.com/azure/ai/openai/](https://learn.microsoft.com/azure/ai/openai/)  
   - Summarized context + sliding window strategy
   - 3-5 turn window recommended
   - Token budgeting: 8K-16K safe range

3. **Context Management Strategies in AI Agents** (Google ADK, 2025)  
   [https://github.com/google/adk-python/discussions/826](https://github.com/google/adk-python/discussions/826)  
   - Sliding window vs summarization
   - Processor-based context filtering
   - LangChain integration patterns

### Production Frameworks

4. **LangChain.js Memory Management** (2024-2025)  
   [https://js.langchain.com/docs/modules/memory/](https://js.langchain.com/docs/modules/memory/)  
   - ConversationBufferMemory
   - ConversationSummaryMemory
   - VectorStoreRetrieverMemory

5. **Vercel AI SDK Tool Context** (2024-2025)  
   [https://sdk.vercel.ai/docs/ai-sdk-core/tools](https://sdk.vercel.ai/docs/ai-sdk-core/tools)  
   - experimental_context parameter
   - Tool-level state management
   - Session-scoped memory

### Implementation Examples

6. **Mem0 GitHub Repository** (2025)  
   [https://github.com/mem0ai/mem0](https://github.com/mem0ai/mem0)  
   - Python implementation with 3-line API
   - Multi-level memory (User, Session, Agent)
   - Vector + graph memory support

7. **Advanced Working Memory in LLM Agents** (Sparkco AI, 2025)  
   [https://sparkco.ai/blog/advanced-working-memory-in-llm-agents](https://sparkco.ai/blog/advanced-working-memory-in-llm-agents)  
   - Active memory management patterns
   - Cognitive workspace concepts
   - MCP protocol integration

8. **Sliding Window Management Pattern** (Agentic Design, 2024)  
   [https://agentic-design.ai/patterns/sliding-window](https://agentic-design.ai/patterns/sliding-window)  
   - Adaptive window sizing
   - Recency weighting formulas
   - Relevance-based retention

### Community Resources

9. **LangChain Discord** — Memory Patterns Channel  
   [https://discord.gg/langchain](https://discord.gg/langchain)

10. **OpenAI Developer Forum** — Context Management  
    [https://community.openai.com/c/api](https://community.openai.com/c/api)

---

**Next Topic**: [4.1.2 Entity Extraction from Tool Results](./4.1.2-entity-extraction.md)

**Related Topics**:
- [2.2.2 Hierarchical Memory](../2-context/2.2.2-hierarchical-memory.md)
- [2.3.4 Working Memory Pattern](../2-context/2.3.4-working-memory.md)
- [4.3.1 Vector Databases](./4.3.1-vector-databases.md)
