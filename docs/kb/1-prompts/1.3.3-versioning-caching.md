# 1.3.3 Prompt Templates: Versioning & Caching

## Overview

Prompt versioning and caching are essential production practices for managing LLM applications at scale. **Versioning** tracks prompt changes over time, enabling rollbacks, A/B testing, and audit trails—treating prompts like code. **Caching** stores frequently used prompts and their responses, reducing costs by 60-90% and latency by 80% in production systems.

**Key Insight** (2024-2025): Every production LLM application needs versioning (for reliability) and caching (for cost optimization).

**Current Date**: November 17, 2025

## Part 1: Prompt Versioning

### Why Prompt Versioning Matters

**The Problem**: Prompts are code, but often treated as throwaway text.

**Without Versioning**:
```typescript
// Version 1 (no tracking)
const prompt = "You are an AI assistant.";

// Later... (what changed? when? why?)
const prompt = "You are an autonomous AI assistant using ReAct.";

// Even later... (how do we rollback?)
const prompt = "You are an autonomous AI assistant. Think step-by-step.";
```

**Problems**:
- ❌ No history of what changed
- ❌ Can't identify when issues started
- ❌ No way to rollback bad prompts
- ❌ No audit trail for compliance
- ❌ Can't A/B test variants
- ❌ Multiple engineers cause conflicts

**With Versioning** (tracked changes):
```typescript
// Version 1.0.0 (2025-01-15)
// Initial release
const prompt_v1 = "You are an AI assistant.";

// Version 1.1.0 (2025-02-10)
// Added: ReAct framework specification
// Reason: Improve reasoning transparency
const prompt_v1_1 = "You are an autonomous AI assistant using ReAct.";

// Version 1.2.0 (2025-03-05)
// Added: Chain-of-thought prompt
// A/B test: v1.1.0 vs v1.2.0
// Metrics: 15% accuracy improvement
const prompt_v1_2 = "You are an autonomous AI assistant. Think step-by-step.";

// Rollback to v1.1.0 (2025-03-08)
// Reason: v1.2.0 caused increased latency
const prompt_current = prompt_v1_1;
```

**Benefits**:
- ✅ Full change history
- ✅ Easy rollbacks
- ✅ A/B testing capabilities
- ✅ Audit trail for compliance
- ✅ Team collaboration
- ✅ Performance tracking per version

### Semantic Versioning for Prompts

**Format**: `MAJOR.MINOR.PATCH`

**Version Semantics**:
- **MAJOR** (1.0.0 → 2.0.0): Breaking changes, completely new approach
  - Example: Switching from standard to ReAct pattern
  - Example: Changing from XML to JSON output format
  
- **MINOR** (1.0.0 → 1.1.0): New features, backward-compatible additions
  - Example: Adding new examples section
  - Example: Adding conditional logic for new user type
  
- **PATCH** (1.0.0 → 1.0.1): Bug fixes, clarifications, minor tweaks
  - Example: Fixing typo in instructions
  - Example: Clarifying ambiguous rule

**Example Evolution**:
```
v1.0.0 - Initial CMS agent prompt
v1.0.1 - Fixed typo in tool description
v1.1.0 - Added working memory section
v1.2.0 - Added few-shot examples
v1.2.1 - Clarified deletion confirmation rule
v2.0.0 - Switched to modular architecture (breaking)
v2.1.0 - Added production/dev environment conditionals
v2.1.1 - Fixed formatting in examples
```

### Prompt Versioning Platforms (2024-2025)

#### 1. Langfuse (Open Source Leader)

**Features**:
- Git-style version control
- Custom labels (staging, production, experiment)
- Prompt diff viewer
- One-click rollbacks
- SDK integration (Python, JS/TS)
- Performance tracking per version

**Usage**:
```typescript
// Langfuse TypeScript SDK
import { Langfuse } from 'langfuse';

const langfuse = new Langfuse({
  publicKey: process.env.LANGFUSE_PUBLIC_KEY,
  secretKey: process.env.LANGFUSE_SECRET_KEY
});

// Create new prompt version
await langfuse.createPrompt({
  name: 'react-agent',
  prompt: reactPromptTemplate,
  labels: ['staging'],
  config: {
    model: 'gpt-4o-mini',
    temperature: 0.7
  }
});

// Fetch prompt by label
const prompt = await langfuse.getPrompt('react-agent', { label: 'production' });

// Use prompt
const rendered = prompt.compile({ 
  workingMemory: context.memory,
  tools: context.tools 
});

// Track usage
const generation = langfuse.generation({
  name: 'react-execution',
  prompt: prompt,
  model: 'gpt-4o-mini'
});
```

**Labeling Strategy**:
```typescript
// Development flow
await langfuse.createPrompt({ name: 'react', labels: ['dev'] }); // v1.3.0-dev
await langfuse.createPrompt({ name: 'react', labels: ['staging'] }); // v1.3.0-staging
await langfuse.createPrompt({ name: 'react', labels: ['production'] }); // v1.3.0

// A/B testing
await langfuse.createPrompt({ name: 'react', labels: ['experiment-cot'] }); // v1.3.1-cot
await langfuse.createPrompt({ name: 'react', labels: ['experiment-react'] }); // v1.3.1-react

// Rollback
await langfuse.updatePrompt({
  name: 'react',
  version: 12, // Previous stable version
  labels: ['production'] // Reassign production label
});
```

#### 2. PromptLayer

**Features**:
- Visual prompt editor
- Version comparison
- Automatic versioning on change
- Prompt registry
- Collaborative editing
- Performance metrics

**Usage**:
```python
# PromptLayer Python SDK
import promptlayer

promptlayer.api_key = os.environ.get("PROMPTLAYER_API_KEY")

# Create/update prompt
promptlayer.prompts.publish(
    prompt_name="react-agent",
    prompt_template={
        "system": react_system_prompt,
        "user": "{{user_input}}"
    },
    tags=["production", "v2.1.0"],
    commit_message="Added production mode conditionals"
)

# Retrieve prompt
prompt = promptlayer.prompts.get(
    prompt_name="react-agent",
    version=None,  # Latest
    label="production"
)

# Track usage with metadata
openai_with_pl = promptlayer.openai.OpenAI()
response = openai_with_pl.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": prompt['system']},
        {"role": "user", "content": user_input}
    ],
    pl_tags=["react-execution"],
    return_pl_id=True
)
```

#### 3. Future AGI

**Features** (2025 platform):
- Advanced optimization tools
- Automated prompt refinement
- Hierarchical template storage
- Real-time performance monitoring
- Synthetic data generation for testing
- Production-ready deployment

**Use Case**: Enterprise teams needing automated optimization + versioning

#### 4. LaunchDarkly (Feature Flag Integration)

**Features**:
- Prompt versioning via feature flags
- Gradual rollouts (5% → 25% → 50% → 100%)
- Instant rollbacks without deployment
- A/B testing with statistical significance
- User targeting (premium users get v2, others get v1)

**Usage**:
```typescript
import LaunchDarkly from 'launchdarkly-node-server-sdk';

const ldClient = LaunchDarkly.init(process.env.LAUNCHDARKLY_SDK_KEY);

// Get prompt version for user
const promptVersion = ldClient.variation(
  'react-prompt-version',
  { key: user.id, custom: { tier: user.tier } },
  'v2.0.0' // Default
);

// Load appropriate prompt
const prompt = await promptRegistry.get('react', promptVersion);

// Gradual rollout
// LaunchDarkly dashboard:
// - v2.1.0: 5% of users (canary)
// - Monitor error rates, latency
// - If stable: 25% → 50% → 100%
// - If issues: instant rollback to v2.0.0
```

### File-Based Versioning (Git)

**Simple approach for small teams**:

```
prompts/
├── react-agent/
│   ├── v1.0.0.hbs         # Initial version
│   ├── v1.1.0.hbs         # Added examples
│   ├── v2.0.0.hbs         # Modular architecture
│   ├── v2.1.0.hbs         # Current production
│   └── changelog.md       # Version history
├── cms-agent/
│   ├── v1.0.0.hbs
│   └── changelog.md
└── registry.json          # Version metadata
```

**registry.json**:
```json
{
  "react-agent": {
    "current": "v2.1.0",
    "production": "v2.1.0",
    "staging": "v2.2.0-beta",
    "versions": {
      "v2.1.0": {
        "file": "react-agent/v2.1.0.hbs",
        "created": "2025-03-15T10:30:00Z",
        "author": "team@company.com",
        "changelog": "Added production/dev conditionals",
        "metrics": {
          "accuracy": 0.92,
          "latency_p95": 1200
        }
      }
    }
  }
}
```

**Loader**:
```typescript
class PromptRegistry {
  private registry: any;
  private cache = new Map<string, HandlebarsTemplateDelegate>();
  
  constructor(registryPath: string) {
    this.registry = JSON.parse(fs.readFileSync(registryPath, 'utf-8'));
  }
  
  getPrompt(name: string, version?: string, label?: string): string {
    const promptConfig = this.registry[name];
    if (!promptConfig) throw new Error(`Prompt not found: ${name}`);
    
    // Resolve version
    const targetVersion = version 
      || (label && promptConfig[label])
      || promptConfig.current;
    
    const versionInfo = promptConfig.versions[targetVersion];
    if (!versionInfo) throw new Error(`Version not found: ${targetVersion}`);
    
    // Load and compile template
    const cacheKey = `${name}:${targetVersion}`;
    if (!this.cache.has(cacheKey)) {
      const templateSource = fs.readFileSync(versionInfo.file, 'utf-8');
      this.cache.set(cacheKey, Handlebars.compile(templateSource));
    }
    
    return cacheKey;
  }
  
  render(name: string, context: any, options?: { version?: string; label?: string }): string {
    const cacheKey = this.getPrompt(name, options?.version, options?.label);
    const template = this.cache.get(cacheKey)!;
    return template(context);
  }
  
  // Rollback to previous version
  rollback(name: string, label: string = 'production'): void {
    const promptConfig = this.registry[name];
    const currentVersion = promptConfig[label];
    
    // Find previous version
    const versions = Object.keys(promptConfig.versions).sort().reverse();
    const currentIndex = versions.indexOf(currentVersion);
    const previousVersion = versions[currentIndex + 1];
    
    if (!previousVersion) throw new Error('No previous version');
    
    // Update label
    promptConfig[label] = previousVersion;
    fs.writeFileSync(this.registryPath, JSON.stringify(this.registry, null, 2));
    
    console.log(`Rolled back ${name} ${label} from ${currentVersion} to ${previousVersion}`);
  }
}

// Usage
const registry = new PromptRegistry('./prompts/registry.json');

// Use production version
const prompt = registry.render('react-agent', context, { label: 'production' });

// Test staging version
const stagingPrompt = registry.render('react-agent', context, { label: 'staging' });

// Rollback if needed
registry.rollback('react-agent', 'production');
```

## Part 2: Prompt Caching

### Why Prompt Caching Matters

**The Cost Problem**:
- LLM APIs charge per token (input + output)
- Long system prompts consume many input tokens
- Repeating same prompt context is expensive

**Without Caching**:
```typescript
// Every request pays for full prompt
Request 1: 2,000 token system prompt + 100 token user input = 2,100 tokens ($0.021)
Request 2: 2,000 token system prompt + 150 token user input = 2,150 tokens ($0.0215)
Request 3: 2,000 token system prompt + 80 token user input = 2,080 tokens ($0.0208)

Total: 6,330 tokens ($0.0633)
```

**With Caching** (cache 2,000 token system prompt):
```typescript
Request 1: 2,000 cached tokens ($0.001) + 100 new tokens ($0.001) = $0.002
Request 2: 2,000 cached tokens ($0.001) + 150 new tokens ($0.0015) = $0.0025
Request 3: 2,000 cached tokens ($0.001) + 80 new tokens ($0.0008) = $0.0018

Total: $0.0063 (90% savings vs non-cached)
```

**Real-World Impact** (2024-2025 data):
- **Cost reduction**: 60-90% for applications with long system prompts
- **Latency reduction**: 80% for cached content (no reprocessing)
- **Throughput improvement**: 1.4-5.9× more requests/second

### Prompt Caching Strategies

#### Strategy 1: Prefix Caching (Most Common)

**Cache static prompt prefix, vary suffix**:

```typescript
// Cached portion (2,000 tokens - stays same)
const SYSTEM_PROMPT_PREFIX = `
You are an autonomous AI assistant using the ReAct pattern.

**CORE LOOP:** Think → Act → Observe → Repeat

**CRITICAL RULES:**
1. THINK before acting
2. EXECUTE immediately
...
[All static instructions]

**AVAILABLE TOOLS:** 50 tools
[Full tool descriptions]
`;

// Dynamic portion (changes per request)
const dynamicSuffix = `
**WORKING MEMORY:**
${context.workingMemory}

**CURRENT SESSION:**
- Session ID: ${sessionId}
- User: ${user.name}
`;

// Combined prompt
const fullPrompt = SYSTEM_PROMPT_PREFIX + dynamicSuffix;

// LLM API caches prefix automatically (if supported)
const response = await generateText({
  prompt: fullPrompt,
  cache: { prefixLength: SYSTEM_PROMPT_PREFIX.length }
});
```

**Best For**:
- Long static system prompts
- Frequently used tool descriptions
- Standard examples/demonstrations

#### Strategy 2: Semantic Caching

**Cache responses for semantically similar prompts**:

```typescript
import { embed } from './embeddings';
import { cosineSimilarity } from './similarity';

class SemanticPromptCache {
  private cache: Array<{
    embedding: number[];
    prompt: string;
    response: string;
    timestamp: number;
  }> = [];
  
  async get(prompt: string, threshold: number = 0.95): Promise<string | null> {
    // Embed new prompt
    const promptEmbedding = await embed(prompt);
    
    // Find most similar cached prompt
    for (const entry of this.cache) {
      const similarity = cosineSimilarity(promptEmbedding, entry.embedding);
      
      if (similarity >= threshold) {
        console.log(`Cache hit! Similarity: ${similarity}`);
        return entry.response;
      }
    }
    
    return null; // Cache miss
  }
  
  async set(prompt: string, response: string): Promise<void> {
    const embedding = await embed(prompt);
    
    this.cache.push({
      embedding,
      prompt,
      response,
      timestamp: Date.now()
    });
    
    // Evict old entries (LRU)
    if (this.cache.length > 1000) {
      this.cache.sort((a, b) => b.timestamp - a.timestamp);
      this.cache = this.cache.slice(0, 1000);
    }
  }
}

// Usage
const semanticCache = new SemanticPromptCache();

async function generateWithSemanticCache(prompt: string) {
  // Check cache
  const cached = await semanticCache.get(prompt, 0.95);
  if (cached) return cached;
  
  // Generate
  const response = await generateText({ prompt });
  
  // Cache for future similar prompts
  await semanticCache.set(prompt, response);
  
  return response;
}

// These prompts would hit cache:
// "What is React?" → generates response, caches
// "Can you explain React?" → cache hit (similarity ~0.96)
// "Tell me about React" → cache hit (similarity ~0.97)
```

**Best For**:
- Chatbots with repetitive questions
- FAQ systems
- Documentation assistants

#### Strategy 3: Anthropic Prompt Caching (Native)

**Claude with native prompt caching** (2024-2025):

```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
});

// Define cached system prompt
const cachedSystemPrompt = {
  type: "text",
  text: `You are an autonomous AI assistant...
  
  [2,000 tokens of static instructions, tools, examples]`,
  cache_control: { type: "ephemeral" } // Cache this block
};

// Request 1: Full processing
const response1 = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  max_tokens: 1024,
  system: [cachedSystemPrompt],
  messages: [
    { role: "user", content: "Create a new page titled 'Getting Started'" }
  ]
});

// Requests 2-N: Use cached system prompt (90% cost reduction)
const response2 = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  max_tokens: 1024,
  system: [cachedSystemPrompt], // Cached!
  messages: [
    { role: "user", content: "Update homepage content" }
  ]
});

// Usage statistics
console.log(response1.usage);
// {
//   input_tokens: 2150,
//   cache_creation_tokens: 2000,  // Created cache
//   cache_read_tokens: 0,
//   output_tokens: 350
// }

console.log(response2.usage);
// {
//   input_tokens: 150,
//   cache_creation_tokens: 0,
//   cache_read_tokens: 2000,       // Used cache (90% cheaper!)
//   output_tokens: 280
// }
```

**Performance** (Anthropic 2025 data):
- **Chat with book** (100K tokens): 11.5s → 2.4s (79% latency reduction, 90% cost reduction)
- **Many-shot prompting** (10K tokens): 1.6s → 1.1s (31% latency reduction, 86% cost reduction)
- **Multi-turn conversations**: ~10s → ~2.5s (75% latency reduction, 53% cost reduction)

**Cache Duration**: 5 minutes (resets on each use)

#### Strategy 4: Redis Caching

**Distributed cache for scaled applications**:

```typescript
import Redis from 'ioredis';
import crypto from 'crypto';

class PromptCache {
  private redis: Redis;
  
  constructor() {
    this.redis = new Redis(process.env.REDIS_URL);
  }
  
  private hash(prompt: string): string {
    return crypto.createHash('sha256').update(prompt).digest('hex');
  }
  
  async get(prompt: string): Promise<string | null> {
    const key = `prompt:${this.hash(prompt)}`;
    return await this.redis.get(key);
  }
  
  async set(prompt: string, response: string, ttl: number = 3600): Promise<void> {
    const key = `prompt:${this.hash(prompt)}`;
    await this.redis.setex(key, ttl, response);
  }
  
  async invalidate(pattern: string): Promise<void> {
    const keys = await this.redis.keys(`prompt:${pattern}*`);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }
}

// Usage
const cache = new PromptCache();

async function generateWithCache(prompt: string) {
  // Check cache
  const cached = await cache.get(prompt);
  if (cached) {
    console.log('Cache hit!');
    return cached;
  }
  
  // Generate
  const response = await generateText({ prompt });
  
  // Cache for 1 hour
  await cache.set(prompt, response, 3600);
  
  return response;
}

// Invalidate when prompt version changes
await cache.invalidate('react-agent-v2.1.0');
```

#### Strategy 5: IC-Cache (2025 Research)

**In-Context Caching for multi-request optimization**:

**Key Concept**: Use historical request-response pairs as in-context examples to improve quality and reduce costs.

**Performance** (2025 research):
- **Throughput**: 1.4-5.9× improvement
- **Latency**: 28-71% reduction
- **Cost**: Offload 70%+ requests to smaller models
- **Quality**: Match or exceed larger model quality

**Pattern**:
```typescript
// Build context from historical successful responses
const historicalContext = await icCache.getSimilarResponses(userQuery, limit: 3);

const prompt = `
You are an AI assistant. Learn from these previous examples:

${historicalContext.map(ex => `
User: ${ex.query}
Assistant: ${ex.response}
`).join('\n')}

Now answer this query:
User: ${userQuery}
Assistant:`;

// Smaller model can now match larger model quality
const response = await generateText({
  model: 'gpt-4o-mini', // Instead of gpt-4o
  prompt
});
```

### Caching Best Practices

**1. Cache Static Content Only**

```typescript
// ❌ Don't cache dynamic user-specific content
const badCacheKey = `prompt:${systemPrompt}:${userId}:${timestamp}`;

// ✅ Cache static portion, inject dynamic
const cachedPrefix = await cache.get('react-system-prompt-v2.1.0');
const fullPrompt = cachedPrefix + `\n\nUSER: ${userQuery}`;
```

**2. Invalidate on Version Change**

```typescript
// When deploying new prompt version
async function deployPromptVersion(name: string, version: string) {
  // Clear old cache
  await cache.invalidatePattern(`${name}:*`);
  
  // Deploy new version
  await promptRegistry.setProduction(name, version);
  
  console.log(`Deployed ${name} ${version}, cache invalidated`);
}
```

**3. Monitor Cache Hit Rate**

```typescript
class MonitoredCache {
  private hits = 0;
  private misses = 0;
  
  async get(key: string): Promise<string | null> {
    const value = await this.redis.get(key);
    
    if (value) {
      this.hits++;
      metrics.increment('cache.hit');
    } else {
      this.misses++;
      metrics.increment('cache.miss');
    }
    
    return value;
  }
  
  getHitRate(): number {
    const total = this.hits + this.misses;
    return total > 0 ? this.hits / total : 0;
  }
}

// Alert if hit rate drops
setInterval(() => {
  const hitRate = cache.getHitRate();
  if (hitRate < 0.5) { // Below 50%
    console.warn(`Low cache hit rate: ${hitRate}`);
  }
}, 60000);
```

**4. Set Appropriate TTL**

```typescript
// Short TTL for frequently changing content
await cache.set(promptKey, response, 300); // 5 minutes

// Long TTL for stable system prompts
await cache.set(systemPromptKey, prompt, 86400); // 24 hours

// Infinite TTL for immutable content (with version in key)
await cache.set(`system-prompt-v2.1.0`, prompt, -1); // Never expires
```

## Combined: Versioning + Caching

**Production pattern**:

```typescript
class VersionedCachedPromptEngine {
  constructor(
    private registry: PromptRegistry,
    private cache: PromptCache
  ) {}
  
  async render(
    name: string, 
    context: any, 
    options: { version?: string; label?: string } = {}
  ): string {
    // Resolve version
    const version = this.registry.resolveVersion(name, options);
    
    // Build cache key (includes version)
    const contextHash = this.hashContext(context);
    const cacheKey = `prompt:${name}:${version}:${contextHash}`;
    
    // Check cache
    const cached = await this.cache.get(cacheKey);
    if (cached) {
      metrics.increment('prompt.cache.hit', { name, version });
      return cached;
    }
    
    // Render prompt
    const rendered = this.registry.render(name, context, { version });
    
    // Cache result (24 hour TTL)
    await this.cache.set(cacheKey, rendered, 86400);
    
    metrics.increment('prompt.cache.miss', { name, version });
    return rendered;
  }
  
  // Invalidate cache when version changes
  async deployVersion(name: string, version: string, label: string): Promise<void> {
    // Update registry
    await this.registry.setLabel(name, version, label);
    
    // Invalidate all cached prompts for this name
    await this.cache.invalidatePattern(`prompt:${name}:*`);
    
    console.log(`Deployed ${name} ${version} as ${label}, cache invalidated`);
  }
  
  private hashContext(context: any): string {
    // Hash only cache-relevant fields
    const cacheableFields = {
      environment: context.environment,
      userTier: context.user?.tier,
      modelFamily: context.model?.family
    };
    
    return crypto
      .createHash('sha256')
      .update(JSON.stringify(cacheableFields))
      .digest('hex')
      .substring(0, 16);
  }
}

// Usage
const engine = new VersionedCachedPromptEngine(registry, cache);

// Render production prompt (with caching)
const prompt = await engine.render('react-agent', context, { label: 'production' });

// Deploy new version (invalidates cache)
await engine.deployVersion('react-agent', 'v2.2.0', 'production');
```

## Key Takeaways

**Prompt Versioning**:
- Track all prompt changes like code (Git, semantic versioning)
- Enable rollbacks, A/B testing, audit trails
- Platforms: Langfuse (open source), PromptLayer, Future AGI, LaunchDarkly
- Essential for production reliability

**Prompt Caching**:
- Reduce costs by 60-90%, latency by 80%
- Cache static prompt prefixes, not dynamic content
- Native support: Anthropic Claude (2024-2025)
- Strategies: Prefix caching, semantic caching, Redis, IC-Cache

**Combined Benefits**:
- Versioning ensures reliability
- Caching ensures cost efficiency
- Together: Production-ready LLM applications

**Implementation**:
```typescript
// 1. Version control prompts
const registry = new PromptRegistry('./prompts');

// 2. Add caching layer
const cache = new PromptCache(redis);

// 3. Combined engine
const engine = new VersionedCachedPromptEngine(registry, cache);

// 4. Use in production
const prompt = await engine.render('react', context, { label: 'production' });
```

**Your Codebase Enhancement**:
```typescript
// Add versioning to your prompts
// prompts/react/v2.1.0.hbs (current)
// prompts/react/v2.2.0.hbs (staging)

// Add caching for cost optimization
const cachedPrompt = await engine.render('react', context);

// Deploy with confidence
await engine.deployVersion('react', 'v2.2.0', 'production');
// (Automatically invalidates cache)
```

## Navigation

- [← Previous: 1.3.2 Conditional Sections & Control Flow](./1.3.2-conditional-sections.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: Layer 2 - Context Engineering](../2-context/README.md)

---

*Part of Layer 1: Prompt Engineering - Production prompt management*
