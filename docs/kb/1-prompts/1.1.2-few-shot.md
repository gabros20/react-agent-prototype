# 1.1.2 Few-Shot Learning: Teaching Through Examples

## Overview

Few-shot learning is a prompt engineering technique where you provide a small number of input-output examples (2-10) to guide an LLM's behavior on a new task. Instead of fine-tuning (expensive, slow) or zero-shot (hoping the model understands), few-shot learning teaches the model "by demonstration" what you want.

**Core Concept**: Show, don't tell. Examples are more powerful than instructions.

**Spectrum**:
```
Zero-Shot       One-Shot        Few-Shot        Fine-Tuning
(no examples) → (1 example) → (2-10 examples) → (1000s of examples)
   Fast             ↓              ↓              Slow
   Cheap            ↓              ↓              Expensive
   Less accurate    ↓              ↓              Most accurate
```

## How Few-Shot Learning Works

### In-Context Learning

LLMs learn patterns from examples provided in the same prompt (context), without parameter updates.

**Process**:
1. You provide examples in the prompt
2. Model identifies patterns from examples
3. Model applies learned pattern to new input
4. No training/fine-tuning required

**Example**:
```
Translate English to French:

English: Hello, how are you?
French: Bonjour, comment allez-vous?

English: I love programming.
French: J'aime la programmation.

English: Where is the library?
French: [model generates: Où est la bibliothèque?]
```

The model learns:
- Task type (translation)
- Source language (English)
- Target language (French)
- Format (question→answer pairs)

### When to Use Few-Shot

**✅ Use Few-Shot When**:
- Task has specific format/style requirements
- Zero-shot results are inconsistent
- You need reliable, repeatable outputs
- Task is non-obvious (custom classification, niche domain)
- You want to control tone, structure, or conventions

**❌ Skip Few-Shot When**:
- Task is simple and common (summarization, basic Q&A)
- Zero-shot already works well
- Examples are hard to create
- Token budget is very limited
- Speed is critical (examples add latency)

## Example Selection: The Key to Success

### Quality Over Quantity

**Research Finding** (Wei et al., 2022): 
- 5-10 good examples > 20 mediocre examples
- Even 1 well-chosen example can improve accuracy 20-40%

### Selection Criteria

#### 1. Diversity

Examples should cover different patterns in your task.

**Bad** (redundant):
```
Input: cat → Output: mammal
Input: dog → Output: mammal
Input: horse → Output: mammal
Input: cow → Output: mammal
```

**Good** (diverse):
```
Input: cat → Output: mammal
Input: eagle → Output: bird
Input: salmon → Output: fish
Input: python → Output: reptile
```

Covers different categories, helps model generalize.

#### 2. Representativeness

Examples should reflect real inputs model will see.

**Scenario**: Classifying customer support tickets

**Bad** (not representative):
```
Input: "Help me" → Category: General
Input: "Question" → Category: General
```

**Good** (realistic):
```
Input: "Can't log in after password reset" → Category: Authentication
Input: "Charged twice for same order" → Category: Billing
Input: "Feature request: dark mode" → Category: Product Feedback
```

#### 3. Difficulty Range

Include easy, medium, and hard examples.

**Example** (Math Word Problems):
```
Easy:
Q: John has 5 apples. He gets 3 more. How many does he have?
A: 5 + 3 = 8 apples

Medium:
Q: Sarah buys 4 packs of pencils. Each pack has 6 pencils. How many total?
A: 4 × 6 = 24 pencils

Hard:
Q: A store sells notebooks at $3 each. If you buy 5+, you get 20% off. 
   How much do 6 notebooks cost?
A: 6 × $3 = $18. With 20% off: $18 × 0.8 = $14.40
```

Models learn from easy → hard progression.

#### 4. Edge Cases

Include examples of tricky situations.

**Example** (Email Classification):
```
Input: "Thanks for the refund!"
Output: Positive (not refund request, it's gratitude)

Input: "I still haven't received my refund?"
Output: Refund Request (question mark indicates uncertainty)

Input: "When can I expect my refund to process?"
Output: Refund Inquiry (not request, asking about timeline)
```

Edge cases teach nuance.

#### 5. Correct Formatting

Examples must match your exact desired output format.

**Bad** (inconsistent):
```
Input: "Great product!"
Output: positive

Input: "Worst purchase ever"
Output: This is negative sentiment

Input: "It's okay"
Output: NEUTRAL
```

**Good** (consistent):
```
Input: "Great product!"
Output: Positive

Input: "Worst purchase ever"
Output: Negative

Input: "It's okay"
Output: Neutral
```

## Example Ordering: Does Sequence Matter?

### Research Findings

**Short Answer**: Yes, but less than expected with modern LLMs.

**Classic Finding** (Brown et al., 2020 - GPT-3):
- Random order: 60% accuracy
- Optimal order: 75% accuracy
- Order mattered significantly

**Modern Finding** (2024 studies - GPT-4, Claude):
- Random order: 82% accuracy
- Optimal order: 85% accuracy
- Order matters, but models are more robust

### Ordering Strategies

#### 1. Simple → Complex (Progressive)

**Best for**: Teaching new concepts, educational tasks

```
Convert temperatures from Fahrenheit to Celsius:

F: 32°F → C: 0°C (freezing point - easy)
F: 212°F → C: 100°C (boiling point - easy)
F: 98.6°F → C: 37°C (body temp - medium)
F: -40°F → C: -40°C (rare coincidence - complex)
F: 68°F → C: 20°C (room temp - practical)

Now convert: 85°F
```

#### 2. Typical → Edge Cases

**Best for**: Classification, handling exceptions

```
Classify refund requests:

"I'd like a refund for order #1234" → Refund Request (typical)
"Can I return this item?" → Return Request (typical)
"This product is broken, need refund" → Refund Request (typical)
"You charged me but I cancelled!" → Billing Dispute (edge case)
"Refund??" → Unclear (edge case - ambiguous)

Classify: "Haven't received order, want my money back"
```

#### 3. Chronological

**Best for**: Time-series, historical analysis

```
Stock price movements:

2024-01-15: +5% (strong growth)
2024-02-10: +2% (modest growth)
2024-03-05: -3% (decline)
2024-03-20: -1% (slight recovery)

Predict trend for: 2024-04-01
```

#### 4. Random (Surprisingly Effective)

**Modern LLMs** (GPT-4, Claude 3+) are robust to random ordering.

**When to use**: 
- When no clear progression exists
- Examples are equally complex
- You want model to extract pattern regardless of order

#### 5. Similarity-Based (Advanced)

**Technique**: Order examples by semantic similarity to new input

**Process**:
1. Embed all examples and new input (e.g., OpenAI embeddings)
2. Calculate cosine similarity
3. Select top-K most similar examples
4. Use those in prompt

**Example** (Using RAG for Few-Shot):
```typescript
// Your codebase pattern: LanceDB + embeddings
const exampleEmbeddings = await vectorIndex.search({
  query: userInput,
  limit: 5
});

const prompt = `
${exampleEmbeddings.map(ex => `Input: ${ex.input}\nOutput: ${ex.output}`).join('\n\n')}

Now process: ${userInput}
`;
```

## Prompt Structure

### Standard Format

```
[Task Description - Optional]

[Example 1]
[Example 2]
[Example 3]
...
[Example N]

[New Input]
```

### With Explicit Labels

```
Task: Classify sentiment of movie reviews

Example 1:
Input: "Absolute masterpiece! Best film of the year."
Output: Positive

Example 2:
Input: "Boring, predictable plot. Wasted 2 hours."
Output: Negative

Example 3:
Input: "Some good moments, but overall disappointing."
Output: Mixed

Now classify:
Input: "Incredible performances but slow pacing."
Output:
```

### With Reasoning (Hybrid with CoT)

```
Solve math word problems:

Q: If 3 apples cost $6, how much do 5 apples cost?
Reasoning: $6 ÷ 3 apples = $2 per apple. 5 × $2 = $10
A: $10

Q: A train travels 120 miles in 2 hours. How far in 5 hours?
Reasoning: 120 ÷ 2 = 60 mph. 60 × 5 = 300 miles
A: 300 miles

Q: [new problem]
Reasoning:
A:
```

## Practical Examples

### Example 1: Custom Classification

**Task**: Classify bug reports by severity

```
Classify bug severity:

"App crashes when I tap Settings" → Critical
"Button text is slightly cut off on iPhone SE" → Low
"Can't export data to CSV, get error message" → High
"Feature request: add dark mode" → Not a Bug
"Payment fails with all credit cards" → Critical
"Typo in help text: 'recieve' should be 'receive'" → Low

Classify: "Users can see other people's private data"
```

**Model Output**: `Critical`

**Why it works**:
- 6 diverse examples covering severity range
- Shows edge case ("Feature request: add dark mode" → Not a Bug)
- Clear pattern: security/functionality = Critical, UI = Low

### Example 2: Structured Data Extraction

**Task**: Extract product info from descriptions

```
Extract product details:

Input: "MacBook Pro 16-inch, M3 chip, 36GB RAM, Space Gray - $2999"
Output: {"name": "MacBook Pro 16-inch", "chip": "M3", "ram": "36GB", "color": "Space Gray", "price": 2999}

Input: "iPhone 15 Pro in Natural Titanium with 256GB storage for $1099"
Output: {"name": "iPhone 15 Pro", "chip": null, "ram": null, "color": "Natural Titanium", "storage": "256GB", "price": 1099}

Input: "iPad Air 11\" with M2 chip and 128GB, Blue color - sale price $549"
Output: {"name": "iPad Air 11\"", "chip": "M2", "ram": null, "color": "Blue", "storage": "128GB", "price": 549}

Extract from: "Sony WH-1000XM5 Noise-Cancelling Headphones, Black, $399.99"
```

**Model Output**:
```json
{"name": "Sony WH-1000XM5", "chip": null, "ram": null, "color": "Black", "price": 399.99}
```

**Why it works**:
- Consistent JSON format
- Shows how to handle missing fields (null)
- Diverse product types
- Handles variations in description style

### Example 3: Tone Matching

**Task**: Write product descriptions in brand voice

```
Write product descriptions in our brand voice:

Product: Wireless Earbuds
Description: "Tired of tangled wires? Our earbuds set you free. Crystal-clear sound, all-day battery, and a fit so comfy you'll forget you're wearing them. Music, your way."

Product: Smart Watch
Description: "Your wrist just got smarter. Track your runs, answer texts, monitor your heart—all without reaching for your phone. Life moves fast. Keep up."

Product: Portable Charger
Description: "Dead battery? Not on our watch. Slip our portable charger in your bag and power up anywhere. 3 full charges, pocket-sized. Stay connected, always."

Now write for:
Product: Bluetooth Speaker
Description:
```

**Model Output**: 
```
"Turn up the volume on life. Our Bluetooth speaker brings the party wherever you go. 
Waterproof, wireless, and packs a punch. Beach, backyard, or road trip—sound that moves with you."
```

**Why it works**:
- 3 examples establish clear brand voice (short sentences, conversational, action-focused)
- Consistent structure (problem → solution → benefit)
- Model learns tone, not just content

### Example 4: Code Generation

**Task**: Generate API request functions

```
Generate API functions from endpoint descriptions:

Endpoint: GET /users/:id
Function:
async function getUser(id) {
  const response = await fetch(`/api/users/${id}`);
  if (!response.ok) throw new Error('Failed to fetch user');
  return response.json();
}

Endpoint: POST /products
Function:
async function createProduct(data) {
  const response = await fetch('/api/products', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(data)
  });
  if (!response.ok) throw new Error('Failed to create product');
  return response.json();
}

Now generate for:
Endpoint: DELETE /orders/:id
Function:
```

**Model Output**:
```javascript
async function deleteOrder(id) {
  const response = await fetch(`/api/orders/${id}`, {
    method: 'DELETE'
  });
  if (!response.ok) throw new Error('Failed to delete order');
  return response.json();
}
```

**Why it works**:
- Shows pattern for GET and POST
- Model infers DELETE follows similar structure
- Consistent error handling pattern
- Clear naming convention

## Advanced Techniques

### 1. Dynamic Few-Shot with RAG

**Technique**: Retrieve relevant examples from vector database

**Your Codebase Integration**:
```typescript
// Use LanceDB to store example library
import { vectorIndex } from '@/server/services/vector-index';

async function buildFewShotPrompt(task: string, numExamples = 5) {
  // Search for similar examples
  const similarExamples = await vectorIndex.search({
    query: task,
    limit: numExamples
  });
  
  // Build prompt with retrieved examples
  const examplesText = similarExamples
    .map(ex => `Input: ${ex.input}\nOutput: ${ex.output}`)
    .join('\n\n');
  
  return `${examplesText}\n\nNow process:\nInput: ${task}\nOutput:`;
}
```

**Benefit**: Always use most relevant examples for each unique input.

### 2. Self-Generated Examples

**Technique**: Ask model to generate its own examples first

```
Task: Classify customer complaints by department

First, generate 5 example customer complaints and classify them by department 
(Billing, Technical, Shipping, Product, Other).

[Model generates examples]

Good! Now classify this real complaint:
"My credit card was charged 3 times for one order"
```

**Benefit**: Model "warms up" by thinking through the task before real work.

### 3. Contrastive Examples

**Technique**: Show what NOT to do

```
Summarize articles (good vs bad examples):

Article: [long text about climate change]

❌ Bad Summary: "The article talks about climate stuff and mentions some statistics."

✅ Good Summary: "Global temperatures have risen 1.1°C since pre-industrial times, 
primarily due to fossil fuel emissions. Scientists warn of irreversible damage beyond 
1.5°C warming, urging immediate policy action."

Now summarize this article:
[new article]
```

**Benefit**: Clarifies nuances by showing anti-patterns.

### 4. Vote-K (Ensemble)

**Technique**: Generate multiple outputs, select best via voting

```
Generate 5 different sentiment classifications for:
"The product is okay, but customer service was terrible"

Vote 1: Mixed
Vote 2: Negative
Vote 3: Mixed
Vote 4: Negative
Vote 5: Mixed

Final (majority): Mixed
```

**Benefit**: More robust results for ambiguous cases.

## Troubleshooting

### Problem: Model Ignores Examples

**Symptoms**: Output doesn't match example pattern

**Causes**:
1. Examples are too diverse (no clear pattern)
2. New input is very different from examples
3. Instructions conflict with examples

**Solutions**:
```
❌ Bad: "Classify sentiment. Be creative!"
   [examples show strict format]
   Instruction conflicts with examples

✅ Good: "Classify sentiment following these examples exactly:"
   [examples]
```

### Problem: Inconsistent Format

**Symptoms**: Output format varies between runs

**Solution**: Add explicit format instruction after examples

```
[Examples]

Important: Always output in this exact format:
Sentiment: [Positive/Negative/Neutral]
Confidence: [0-100]%
```

### Problem: Too Many Tokens

**Symptoms**: Context length exceeded, high costs

**Solutions**:
1. **Reduce examples**: 3-5 is often enough
2. **Shorter examples**: Trim unnecessary context
3. **Dynamic selection**: Use RAG to pick most relevant
4. **Fine-tuning**: For 100+ examples, fine-tune instead

**Example**:
```
❌ Long (200 tokens per example × 10 = 2000 tokens)
✅ Short (50 tokens per example × 5 = 250 tokens)
87.5% token reduction, similar accuracy
```

### Problem: Model Copies Examples Verbatim

**Symptoms**: Output is exact copy of an example

**Solution**: Make examples more diverse, or add instruction

```
Examples:
[examples]

Important: Use these examples as a guide, but generate original responses 
for the new input. Do not copy examples directly.

New input: [task]
```

## Few-Shot + Your Agent

**Your ReAct Prompt** already uses few-shot implicitly:

```xml
**EXAMPLE SESSION:**

User: "Add a hero section to the about page with dummy content"

Thought: I need to find the about page first using fuzzy search.
Action: cms_findResource
Action Input: {"query": "about", "resourceType": "page"}
Observation: Found page-abc123...

[Full example showing 5-step process]
```

**Why it works**:
- Shows complete Think → Act → Observe cycle
- Demonstrates tool usage patterns
- Illustrates error handling (deletion confirmation)
- Teaches working memory lookup

**Enhancement Opportunity**:
```xml
<!-- Add second example for variety -->
**EXAMPLE SESSION 2:**

User: "What's the hero button link on the homepage?"

Thought: Use granular fetching to minimize tokens.
[Show efficient pattern]
```

## Key Takeaways

**Core Concepts**:
- Few-shot = teaching by example (2-10 demonstrations)
- More effective than instructions alone
- No training/fine-tuning required
- Works via in-context learning

**Example Selection**:
1. **Quality > Quantity**: 5 good examples > 20 mediocre
2. **Diversity**: Cover different patterns in your task
3. **Representative**: Match real-world inputs
4. **Range**: Easy → Medium → Hard progression
5. **Edge cases**: Teach nuance and exceptions

**Ordering**:
- Modern LLMs (GPT-4, Claude) are robust to order
- Progressive (simple → complex) works well
- Similarity-based (RAG) is most advanced
- Random is acceptable default

**When to Use**:
- ✅ Format/style requirements
- ✅ Inconsistent zero-shot results
- ✅ Custom tasks
- ❌ Simple, common tasks
- ❌ Very limited token budget

**Integration with Your Codebase**:
- Use LanceDB for dynamic example selection
- Store example library with embeddings
- Retrieve top-K similar examples per task
- Implement in agent prompts for consistent tool usage

## Practical Exercise

Transform this zero-shot prompt into effective few-shot:

**Zero-Shot**:
```
Extract city and date from travel queries.
```

**Your Few-Shot** (Create 5 examples covering edge cases):
```
Extract city and date from travel queries:

[Your examples here]

Extract from: "Flying to Tokyo next Wednesday"
```

Consider:
- Different date formats
- Multiple cities
- Ambiguous dates
- Missing information

## Navigation

- [← Previous: 1.1.1 Instruction Design](./1.1.1-instruction-design.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 1.1.3 Chain-of-Thought Prompting](./1.1.3-chain-of-thought.md)

---

*Part of Layer 1: Prompt Engineering - Teaching models by example*
