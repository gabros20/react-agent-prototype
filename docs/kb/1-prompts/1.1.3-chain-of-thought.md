# 1.1.3 Chain-of-Thought (CoT) Prompting

## Overview

Chain-of-Thought (CoT) prompting is a breakthrough technique that dramatically improves LLM performance on complex reasoning tasks by encouraging models to "think step-by-step" before answering. Instead of jumping to conclusions, CoT guides models through intermediate reasoning steps, similar to showing your work in math class.

**Key Paper**: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., 2022)

**Impact**: 50-400% improvement on reasoning benchmarks (math, logic, common sense)

## The Problem CoT Solves

### Direct Prompting Fails on Complex Tasks

**Example Problem**:
```
Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, 
how many apples do they have?
```

**Standard Prompting**:
```
Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, 
how many apples do they have?

A: 9 apples
```

❌ **Wrong!** Model jumped to answer without reasoning.

**Chain-of-Thought Prompting**:
```
Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, 
how many apples do they have?

A: The cafeteria started with 23 apples. They used 20 to make lunch, so they had 
23 - 20 = 3 apples left. Then they bought 6 more apples, so they have 3 + 6 = 9 apples.
```

✅ **Correct!** Model shows work, catches calculation steps.

### Why Step-by-Step Matters

**Without CoT**:
- Model pattern-matches to superficially similar problems
- Misses intermediate dependencies
- Can't correct errors mid-reasoning
- "Black box" - can't debug wrong answers

**With CoT**:
- Forces decomposition into logical steps
- Each step builds on previous (chain)
- Errors visible and fixable
- Transparent reasoning

## How CoT Works

### Mechanism: Computational Intermediate Steps

**Theory** (from research):
1. LLMs have limited "working memory" in a single forward pass
2. Complex problems exceed this capacity
3. CoT externalizes working memory as text
4. Each reasoning step becomes input for next step
5. This creates a "chain" of thought

**Analogy**: Like using scratch paper for long division instead of doing it all in your head.

### Two Variants

#### 1. Few-Shot CoT (Original)

Provide examples with reasoning steps.

```
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can 
has 3 balls. How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans of 3 balls each is 2 × 3 = 6 balls. 
5 + 6 = 11. The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 
6 more, how many apples do they have?

A: [model generates step-by-step reasoning]
```

#### 2. Zero-Shot CoT (Kojima et al., 2022)

Just add: "Let's think step by step"

```
Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 
6 more, how many apples do they have?

Let's think step by step:
```

**Surprisingly effective!** This simple addition improves performance 20-50% on many tasks.

## When to Use CoT

### ✅ Best For

**1. Multi-Step Reasoning**
```
Q: If a train travels at 60 mph for 2.5 hours, how far does it go?

Bad (direct): "150 miles" ❌ (wrong calculation)
CoT: "Distance = speed × time. 60 × 2.5 = 150 miles" ✅
```

**2. Math Word Problems**
```
Q: Sarah has $50. She buys 3 books at $12 each and a pen for $4. 
How much money does she have left?

CoT:
- Started with: $50
- 3 books at $12 each: 3 × $12 = $36
- Pen: $4
- Total spent: $36 + $4 = $40
- Money left: $50 - $40 = $10
```

**3. Logical Deduction**
```
Q: All birds can fly. Penguins are birds. Can penguins fly?

CoT:
- Premise 1: All birds can fly
- Premise 2: Penguins are birds
- Logical conclusion: Penguins should fly
- Reality check: But wait—penguins are flightless birds
- Answer: The initial premise is flawed. Not all birds can fly.
```

**4. Common Sense Reasoning**
```
Q: If you drop a glass bottle on concrete, will it break?

CoT:
- Glass is fragile and brittle
- Concrete is very hard
- When fragile object hits hard surface, it experiences sudden force
- Glass can't absorb this impact
- Result: Yes, it will likely shatter
```

**5. Symbolic Manipulation**
```
Q: Solve for x: 3x + 7 = 22

CoT:
- Start: 3x + 7 = 22
- Subtract 7 from both sides: 3x = 15
- Divide both sides by 3: x = 5
- Verify: 3(5) + 7 = 15 + 7 = 22 ✓
```

**6. Code Debugging**
```
Q: Why does this code return undefined?

function getUser(id) {
  const user = database.find(id);
  return user.name;
}

CoT:
- Step 1: database.find(id) searches for user
- Step 2: If user not found, find() returns undefined
- Step 3: Accessing undefined.name throws error
- Issue: No null check before accessing .name
- Fix: Add if (!user) return null check
```

### ❌ Skip CoT For

**1. Simple Lookups**
```
Q: What is the capital of France?
A: Paris

(No need for: "France is a country in Europe. Its capital is Paris. Therefore Paris.")
```

**2. Single-Step Problems**
```
Q: What is 5 + 3?
A: 8

(No benefit from: "5 + 3 means adding 5 and 3. 5...6...7...8. The answer is 8.")
```

**3. Creative Tasks**
```
Q: Write a poem about sunset.

(CoT doesn't help creative generation)
```

**4. When Speed > Accuracy**
```
High-throughput tasks where 80% accuracy is acceptable and 
step-by-step reasoning would 3x the cost/latency
```

## Prompt Patterns

### Pattern 1: Few-Shot with Reasoning Examples

```
Solve these math word problems:

Q: There are 15 trees in the grove. Grove workers will plant trees today. 
After they're done, there will be 21 trees. How many did they plant?

A: There were originally 15 trees. After planting, there are 21 trees. 
So they must have planted 21 - 15 = 6 trees. The answer is 6.

Q: If there are 3 cars in the parking lot and 2 more cars arrive, 
how many cars are there?

A: Already had 3 cars. 2 more arrive. 3 + 2 = 5 cars total. The answer is 5.

Q: Leah had 32 chocolates. Her sister had 42. If they ate 35, how many 
do they have left?

A:
```

**Key Elements**:
- 2-3 examples with complete reasoning chains
- Each step is explicit
- Final answer clearly stated
- Variety in problem complexity

### Pattern 2: Zero-Shot with Magic Phrase

```
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, 
and half are beach balls. How many golf balls are there?

Let's think step by step:
```

**Result**:
```
1. The juggler has 16 balls total
2. Half are golf balls
3. Half of 16 is 16 ÷ 2 = 8
4. Therefore, there are 8 golf balls
```

**Magic Phrases That Work**:
- "Let's think step by step"
- "Let's work this out step by step"
- "Let's break this down"
- "Let's solve this systematically"
- "First, let's analyze..."

### Pattern 3: Structured Reasoning Template

```
Problem: [state problem]

Given Information:
- [fact 1]
- [fact 2]
- [fact 3]

Step-by-Step Solution:
1. [step 1 with calculation]
2. [step 2 with calculation]
3. [step 3 with calculation]

Verification:
[check answer makes sense]

Final Answer: [answer]
```

**Example**:
```
Problem: A store sells notebooks at $3 each. Buy 5 or more and get 20% off. 
How much do 6 notebooks cost?

Given Information:
- Regular price: $3 per notebook
- Quantity: 6 notebooks
- Discount: 20% off for 5+ notebooks

Step-by-Step Solution:
1. Calculate base price: 6 × $3 = $18
2. Apply discount: 20% of $18 = $18 × 0.20 = $3.60
3. Final price: $18 - $3.60 = $14.40

Verification:
6 notebooks > 5, so discount applies ✓
20% off means paying 80%: $18 × 0.80 = $14.40 ✓

Final Answer: $14.40
```

### Pattern 4: Self-Ask (Advanced)

Model asks itself clarifying questions.

```
Q: Who was president of the U.S. when the first iPhone was released?

Self-Ask CoT:
- First question: When was the first iPhone released?
  Answer: June 2007
- Second question: Who was U.S. president in June 2007?
  Answer: George W. Bush (served 2001-2009)
- Final answer: George W. Bush
```

### Pattern 5: Least-to-Most Prompting

Break complex problems into simpler subproblems.

```
Q: How many keyboard keys would you press to type the HTML for a 
basic webpage with header, paragraph, and footer?

Least-to-Most Decomposition:
1. First, what's the minimal HTML structure?
   ```html
   <html><body></body></html>
   ```
   
2. Add header:
   ```html
   <header>Title</header>
   ```
   
3. Add paragraph:
   ```html
   <p>Content</p>
   ```
   
4. Add footer:
   ```html
   <footer>Footer</footer>
   ```

5. Count characters in complete structure:
   [counting logic]

6. Each character = one keypress (simplified)

Final answer: [number]
```

## Your Codebase Integration

### ReAct = Built-in CoT

Your agent prompt (`server/prompts/react.xml`) already implements CoT:

```xml
**CORE LOOP:**
Think → Act → Observe → Repeat until completion

Think step-by-step:
1. Analyze the question and identify what information/actions you need
2. Execute ONE tool at a time with the appropriate input
3. Observe the result and integrate it into your reasoning
4. Continue until you have enough information or the task is complete
5. When done, provide a final answer
```

**This IS Chain-of-Thought!**

**Example from your prompt**:
```
User: "Add a hero section to the about page with dummy content"

Thought: I need to find the about page first using fuzzy search.
Action: cms_findResource
Action Input: {"query": "about", "resourceType": "page"}
Observation: Found page-abc123 (slug: about-us, name: "About Us")

Thought: Now I need to find the hero section definition.
Action: cms.findResource
Action Input: {"query": "hero", "resourceType": "section"}
Observation: Found section-def456 (key: hero, name: "Hero Section")

[continues...]
```

**Why it works**:
- **Think**: Explicit reasoning before each action
- **Act**: Single tool execution
- **Observe**: Integration of result into reasoning
- **Repeat**: Chains multiple thoughts together

This is **ReAct = Reasoning + Acting**, a specialized form of CoT for agents.

### Enhancing Your Prompt with CoT

**Current** (already good):
```xml
**CRITICAL RULES:**
1. **THINK before acting** - Explain your reasoning for each step
```

**Enhancement** (more explicit):
```xml
**CRITICAL RULES:**
1. **THINK before acting** - Explain your reasoning for each step:
   - What information do I have?
   - What am I trying to achieve?
   - What's the logical next action?
   - What could go wrong?
```

**Add Verification Step**:
```xml
5. When done, verify your solution:
   - Did I complete all subtasks?
   - Are there any errors or inconsistencies?
   - Does the final result make sense?
   - Should I test or validate anything?
```

## Advanced CoT Techniques

### 1. Self-Consistency (Voting)

Generate multiple reasoning paths, pick most common answer.

```
Q: John has 5 apples. He gives 2 to Mary and buys 7 more. How many does he have?

Path 1:
Start: 5 apples
Give away: 5 - 2 = 3 apples
Buy: 3 + 7 = 10 apples ✓

Path 2:
Initial: 5
After giving: 5 - 2 = 3
After buying: 3 + 7 = 10 ✓

Path 3:
5 - 2 = 3
3 + 7 = 10 ✓

Consensus: 10 apples (3/3 paths agree)
```

**Implementation**:
```typescript
async function selfConsistentCoT(problem: string, paths = 5) {
  const results = await Promise.all(
    Array(paths).fill(null).map(() => 
      generateText({
        model: openai("gpt-4o-mini"),
        prompt: `${problem}\n\nLet's think step by step:`
      })
    )
  );
  
  // Extract final answers and vote
  const answers = results.map(r => extractAnswer(r.text));
  return mostCommon(answers);
}
```

**When to use**: Critical decisions, ambiguous problems, when accuracy > cost

### 2. Automatic CoT (Auto-CoT)

Model generates its own diverse reasoning examples.

```
Task: Classify customer complaints

Step 1: Generate diverse example complaints (model creates these)
[model generates 5-10 examples]

Step 2: For each example, generate reasoning (model creates CoT)
Example: "I can't log in"
Reasoning: User mentions authentication issue → Category: Technical Support

Step 3: Use generated examples as few-shot for real task
```

**Benefit**: No manual example creation needed.

### 3. Chain-of-Thought with Knowledge Retrieval (CoT + RAG)

Combine CoT with your vector search.

```typescript
async function cotWithRAG(question: string) {
  // Step 1: Retrieve relevant context
  const context = await vectorIndex.search({
    query: question,
    limit: 3
  });
  
  // Step 2: Build CoT prompt with context
  const prompt = `
Context from knowledge base:
${context.map(c => c.content).join('\n\n')}

Question: ${question}

Let's think step by step using the context above:
`;
  
  return generateText({ model: openai("gpt-4o-mini"), prompt });
}
```

**Your use case**: Agent could use this for complex tool decisions

### 4. Program-of-Thoughts (PoT)

Generate code to solve problems instead of natural language reasoning.

```
Q: Calculate compound interest: $1000 at 5% annual for 3 years

Instead of:
Year 1: $1000 × 1.05 = $1050
Year 2: $1050 × 1.05 = $1102.50
...

Generate code:
```python
principal = 1000
rate = 0.05
years = 3
final_amount = principal * (1 + rate) ** years
print(f"${final_amount:.2f}")  # $1157.63
```
```

**Benefit**: More accurate for complex math, verifiable execution

## Troubleshooting

### Problem: Model Skips Reasoning Steps

**Symptom**: Model jumps to answer despite "think step by step"

**Causes**:
1. Model too small (CoT emerges at ~10B+ parameters)
2. Task too simple (model pattern-matches directly)
3. Zero-shot instruction not strong enough

**Solutions**:
```
❌ Weak: "Let's think step by step"

✅ Strong: "Before answering, write out your complete reasoning process. 
Show all calculations and intermediate steps. Do not skip any steps."

✅ Few-shot: Provide 2-3 examples with full reasoning
```

### Problem: Reasoning is Correct but Answer Wrong

**Symptom**: Steps make sense but final answer is off

**Cause**: Error in final extraction or summarization

**Solution**: Explicit final answer format
```
After showing your reasoning, write your final answer on a new line:

FINAL ANSWER: [answer here]
```

### Problem: Too Verbose

**Symptom**: Excessive reasoning, high token costs

**Solution**: Guided CoT with constraints
```
Solve this problem in exactly 3 steps:
1. [first step]
2. [second step]
3. [final calculation]

Answer: [result]
```

### Problem: Reasoning Loops or Repeats

**Symptom**: Model generates circular logic

**Solution**: Structure with numbered steps
```
Follow this exact structure:

Step 1: Identify what we know
[facts]

Step 2: Identify what we need to find
[goal]

Step 3: Calculate or deduce
[process]

Step 4: Verify
[check]

Final Answer:
[result]
```

## Combining CoT with Other Techniques

### CoT + Few-Shot

```
Example 1:
[problem]
Reasoning: [step-by-step]
Answer: [result]

Example 2:
[problem]
Reasoning: [step-by-step]
Answer: [result]

New problem:
[problem]
Reasoning:
```

### CoT + Structured Outputs

```typescript
const schema = z.object({
  reasoning_steps: z.array(z.string()),
  intermediate_calculations: z.array(z.number()),
  final_answer: z.number(),
  confidence: z.number().min(0).max(1)
});

const result = await generateObject({
  model: openai("gpt-4o-mini", { structuredOutputs: true }),
  schema,
  prompt: `Solve this problem with step-by-step reasoning: ${problem}`
});
```

### CoT + Role-Playing

```
You are a math tutor explaining to a student. Solve this problem:
- Show each step clearly
- Explain WHY you're doing each step
- Use simple language
- Check your work at the end

Problem: [math problem]
```

## Benchmarks & Impact

**Original Paper Results** (Wei et al., 2022):

| Benchmark | Without CoT | With CoT | Improvement |
|-----------|-------------|----------|-------------|
| **GSM8K** (Math) | 17% | 57% | +235% |
| **SVAMP** (Math) | 69% | 79% | +14% |
| **StrategyQA** (Logic) | 54% | 67% | +24% |
| **CommonsenseQA** | 68% | 75% | +10% |

**Key Finding**: Larger models benefit more (emergence at 10B+ parameters)

**Zero-Shot CoT** (Kojima et al., 2022):

| Model | Task | Without | With "Let's think step by step" | Improvement |
|-------|------|---------|--------------------------------|-------------|
| **PaLM 540B** | MultiArith | 41% | 66% | +61% |
| **GPT-3 175B** | GSM8K | 18% | 41% | +128% |

**Modern Models** (2024):
- o1/o3 have native CoT (thinking tokens)
- GPT-4o, Claude 3.5 naturally do some CoT
- Still beneficial to explicitly request step-by-step

## Key Takeaways

**What is CoT**:
- Technique to improve reasoning by showing intermediate steps
- Forces model to "think out loud"
- Dramatically improves accuracy on complex tasks
- Two variants: few-shot (with examples) and zero-shot ("Let's think step by step")

**When to Use**:
- ✅ Multi-step math problems
- ✅ Logical deduction
- ✅ Common sense reasoning
- ✅ Code debugging
- ✅ Complex analysis
- ❌ Simple lookups
- ❌ Creative tasks
- ❌ Single-step problems

**How to Implement**:
1. **Zero-Shot**: Add "Let's think step by step" to any problem
2. **Few-Shot**: Provide 2-3 examples with full reasoning
3. **Structured**: Use templates for consistent formatting
4. **Verification**: Add step to check answer makes sense

**Your Codebase**:
- ReAct pattern is CoT for agents
- Think → Act → Observe is reasoning chain
- Already excellent implementation
- Could add explicit verification step

**Advanced Techniques**:
- Self-consistency: Generate multiple paths, vote
- Auto-CoT: Model generates own examples
- CoT + RAG: Combine with knowledge retrieval
- PoT: Generate code instead of text reasoning

**Impact**:
- 10-400% improvement on reasoning tasks
- Minimal cost (just more output tokens)
- Works with any LLM (better with larger models)
- Transparent, debuggable reasoning

## Practical Exercise

Transform these direct prompts into CoT prompts:

**Exercise 1** (Math):
```
Direct: "If a shirt costs $40 after a 20% discount, what was the original price?"

Your CoT version:
```

**Exercise 2** (Logic):
```
Direct: "If all programmers drink coffee and Alice is a programmer, does Alice drink coffee?"

Your CoT version:
```

**Exercise 3** (Your Agent):
```
Direct: "Delete the hero section from the About page"

Your CoT version (modify ReAct prompt):
```

## Navigation

- [← Previous: 1.1.2 Few-Shot Learning](./1.1.2-few-shot.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 1.1.4 Zero-Shot CoT](./1.1.4-zero-shot-cot.md)

---

*Part of Layer 1: Prompt Engineering - Eliciting step-by-step reasoning*
