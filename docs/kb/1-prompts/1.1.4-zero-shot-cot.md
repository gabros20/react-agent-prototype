# 1.1.4 Zero-Shot Chain-of-Thought (Zero-Shot CoT)

## Overview

Zero-Shot Chain-of-Thought is a remarkably simple yet powerful technique: just adding "Let's think step by step" to any prompt can improve reasoning accuracy by 20-50% on complex tasks—without any examples. It's the most cost-effective upgrade to standard prompting.

**Key Paper**: "Large Language Models are Zero-Shot Reasoners" (Kojima et al., 2022)

**The Breakthrough**: Single phrase unlocks reasoning abilities already present in LLMs.

## The Discovery

### Original Experiment

**Standard Zero-Shot** (baseline):
```
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. 
   Each can has 3 tennis balls. How many tennis balls does he have now?
A:
```
**GPT-3 Result**: "11" ❌ (wrong, or random guess)

**Zero-Shot CoT** (adding magic phrase):
```
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. 
   Each can has 3 tennis balls. How many tennis balls does he have now?
A: Let's think step by step.
```

**GPT-3 Result**:
```
Roger started with 5 tennis balls.
2 cans of tennis balls means 2 × 3 = 6 tennis balls.
So in total he has 5 + 6 = 11 tennis balls.
The answer is 11.
```
✅ Correct with reasoning!

### Why It Works

**Theory** (from research):
1. LLMs are trained on internet text containing step-by-step solutions
2. "Let's think step by step" activates these learned patterns
3. Model enters "reasoning mode" instead of "answer mode"
4. Each step becomes prompt for next step (chain)
5. Errors can self-correct mid-reasoning

**Analogy**: Like telling someone "talk me through it" versus "give me the answer now"—the process changes.

## The Magic Phrases

### Most Effective Triggers

**Kojima et al. (2022) tested dozens of phrases. Top performers**:

1. **"Let's think step by step"** (original, most effective)
2. **"Let's work this out step by step"** (similar performance)
3. **"Let's break this down"** (good for complex problems)
4. **"Let's solve this problem by splitting it into steps"** (explicit)
5. **"First, let's consider..."** (guided start)

### What Doesn't Work Well

❌ **Too vague**:
- "Think about it" (no structure)
- "Be careful" (no action)
- "Do your best" (motivational but useless)

❌ **Too rigid**:
- "Show your work" (school-like, less natural)
- "Explain your answer" (might trigger explanation rather than reasoning)

❌ **Counterproductive**:
- "Answer quickly" (discourages reasoning)
- "Just give the final answer" (skips steps)

## Two-Stage Process

Zero-Shot CoT actually works in two stages:

### Stage 1: Reasoning Extraction

**Prompt**:
```
Q: [question]
A: Let's think step by step.
```

**Model Output**:
```
[step 1]
[step 2]
[step 3]
...
```

### Stage 2: Answer Extraction

**Prompt** (automatic follow-up):
```
Q: [question]
A: [reasoning from stage 1]

Therefore, the answer is:
```

**Model Output**:
```
[final answer in desired format]
```

**Your Implementation**:
```typescript
async function zeroShotCoT(question: string) {
  // Stage 1: Get reasoning
  const reasoning = await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `Q: ${question}\nA: Let's think step by step.`
  });
  
  // Stage 2: Extract answer
  const answer = await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `Q: ${question}\nA: ${reasoning.text}\n\nTherefore, the answer is:`
  });
  
  return { reasoning: reasoning.text, answer: answer.text };
}
```

**Single-Stage Alternative** (modern models handle this):
```typescript
async function zeroShotCoTSimple(question: string) {
  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `${question}\n\nLet's think step by step, then provide the final answer.`
  });
}
```

## When to Use Zero-Shot CoT

### ✅ Perfect For

**1. Quick Reasoning Enhancement**
- No time to create few-shot examples
- Need immediate improvement over direct prompting
- Testing if CoT helps before investing in examples

**2. Diverse Question Types**
- Questions vary too much for fixed examples
- Can't predict what users will ask
- General-purpose reasoning assistance

**3. Token-Constrained Scenarios**
- Context window nearly full
- Can't fit few-shot examples
- 4 tokens ("Let's think step by step") vs 200+ for examples

**4. Arithmetic & Math**
```
Q: If a train travels 120 km in 2 hours, how far in 5 hours at same speed?
A: Let's think step by step.

Output:
Speed = 120 km ÷ 2 hours = 60 km/h
Distance in 5 hours = 60 km/h × 5 hours = 300 km
The answer is 300 km.
```

**5. Common Sense Reasoning**
```
Q: If you put a metal spoon in a hot drink, will the spoon get hot?
A: Let's think step by step.

Output:
1. Hot drink has high temperature
2. Metal conducts heat well
3. Heat transfers from drink to spoon through conduction
4. Spoon temperature increases
Yes, the spoon will get hot.
```

**6. Logical Puzzles**
```
Q: All roses are flowers. Some flowers fade quickly. Do all roses fade quickly?
A: Let's think step by step.

Output:
Premise 1: All roses are flowers (roses ⊂ flowers)
Premise 2: Some flowers fade quickly (not all)
Conclusion: We can't determine if all roses fade quickly
The roses that are flowers might or might not be in the subset that fades quickly.
Answer: No, we cannot conclude that all roses fade quickly.
```

### ❌ Skip For

**1. Simple Lookups**
```
Q: What is the capital of France?
A: Let's think step by step.  ← Unnecessary overhead

Better: Direct answer "Paris"
```

**2. Creative Tasks**
```
Write a poem about sunset.
Let's think step by step.  ← Doesn't help creativity
```

**3. When You Have Good Examples**
```
If you have 3-5 perfect few-shot examples, use those instead.
Few-Shot CoT > Zero-Shot CoT for same task type.
```

**4. When Speed is Critical**
```
Zero-Shot CoT adds latency:
- More output tokens (reasoning steps)
- Potentially second model call (answer extraction)

Direct prompting: 50 tokens, 0.5s
Zero-Shot CoT: 200 tokens, 2.0s
```

## Advanced Variants

### 1. Plan-and-Solve (PS) Prompting

**Problem**: "Let's think step by step" can be too vague.

**Solution**: More specific instructions.

**Standard PS**:
```
Q: [question]
A: Let's first understand the problem and devise a plan to solve it. 
   Then, let's carry out the plan to solve the problem step by step.
```

**Enhanced PS+** (even more specific):
```
Q: [question]
A: Let's devise a plan and solve the problem step by step:
   1. Extract relevant information
   2. Devise a calculation plan
   3. Execute the plan with accurate calculations
   4. Verify the result
```

**Results** (Arithmetic Reasoning - 2024):
| Method | Accuracy |
|--------|----------|
| Standard Zero-Shot | 70.4% |
| Zero-Shot CoT | 73.2% |
| Plan-and-Solve (PS) | 75.1% |
| Plan-and-Solve+ (PS+) | 76.7% |

**Your Implementation**:
```typescript
async function planAndSolve(problem: string) {
  const prompt = `${problem}

Let's devise a plan and solve this step by step:
1. Extract the relevant information from the problem
2. Devise a calculation plan
3. Execute the plan with careful calculations
4. Verify the result makes sense

Plan and solution:`;

  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt
  });
}
```

### 2. Instance-Adaptive Zero-Shot CoT

**Concept**: Select best trigger phrase per question automatically.

**Process**:
1. Maintain library of trigger phrases
2. For each question, try multiple triggers
3. Evaluate quality using confidence scores
4. Select reasoning with highest score

**Trigger Library**:
```typescript
const triggerPhrases = [
  "Let's think step by step.",
  "Let's break this down into steps.",
  "Let's solve this systematically.",
  "First, let's analyze the problem.",
  "Let's work through this carefully.",
];

async function adaptiveZeroShotCoT(question: string) {
  // Generate reasoning with all triggers
  const results = await Promise.all(
    triggerPhrases.map(trigger =>
      generateText({
        model: openai("gpt-4o-mini"),
        prompt: `Q: ${question}\nA: ${trigger}`
      })
    )
  );
  
  // Score each reasoning path (example: by length, confidence, etc.)
  const scored = results.map((r, i) => ({
    trigger: triggerPhrases[i],
    reasoning: r.text,
    score: scoreReasoning(r.text)  // Your scoring logic
  }));
  
  // Return best
  return scored.sort((a, b) => b.score - a.score)[0];
}
```

### 3. Chain of Draft (CoD)

**Problem**: Zero-Shot CoT can be too verbose (high token cost).

**Solution**: Concise intermediate thoughts.

**Standard CoT**:
```
Q: Solve 3x + 7 = 22
A: Let's think step by step.

First, we have the equation 3x + 7 = 22.
To isolate the term with x, we need to subtract 7 from both sides.
This gives us: 3x + 7 - 7 = 22 - 7
Simplifying: 3x = 15
Now, to solve for x, we divide both sides by 3: 3x ÷ 3 = 15 ÷ 3
This gives us: x = 5
[... 150 tokens]
```

**Chain of Draft** (concise):
```
Q: Solve 3x + 7 = 22
A: Let's solve concisely.

3x + 7 = 22
3x = 15  (subtract 7)
x = 5    (divide by 3)
[... 15 tokens, 90% reduction!]
```

**Prompt**:
```
Solve this problem with concise reasoning. 
Write only essential steps without explanation.

Q: [problem]
A: Let's solve concisely.
```

**Benefit**: 7.6% tokens of standard CoT, same accuracy.

### 4. Tabular CoT

**Problem**: Unstructured reasoning can be hard to follow.

**Solution**: Force table format.

```
Q: Compare features of GPT-4o-mini vs Claude Sonnet

A: Let's create a comparison table:

| Feature         | GPT-4o-mini | Claude Sonnet |
|-----------------|-------------|---------------|
| Cost            | $0.15/$0.60 | $3.00/$15.00  |
| Context Window  | 128k        | 200k          |
| Speed           | Very Fast   | Medium        |
| Best For        | Agents      | Coding        |

Conclusion: GPT-4o-mini for cost-sensitive agents, Claude for complex coding.
```

**Prompt Pattern**:
```
Answer this question using a structured table format.

Q: [question]
A: Let's organize this in a table:
```

### 5. Uncertainty-Guided Selection (ZEUS)

**Concept**: Use model confidence to select best reasoning path.

**Process**:
1. Generate multiple reasoning paths with Zero-Shot CoT
2. Estimate confidence/uncertainty for each
3. Select path with highest confidence
4. Or combine high-confidence paths

**Implementation**:
```typescript
async function uncertaintyGuidedCoT(question: string, paths = 5) {
  // Generate multiple reasoning paths
  const results = await Promise.all(
    Array(paths).fill(null).map(() =>
      generateText({
        model: openai("gpt-4o-mini"),
        prompt: `Q: ${question}\nA: Let's think step by step.`,
        temperature: 0.7  // Some randomness for diversity
      })
    )
  );
  
  // Estimate confidence (could use logprobs if available)
  const scored = results.map(r => ({
    reasoning: r.text,
    confidence: estimateConfidence(r.text)  // Your metric
  }));
  
  // Return highest confidence path
  return scored.sort((a, b) => b.confidence - a.confidence)[0];
}
```

## Combining with Other Techniques

### Zero-Shot CoT + Structured Output

```typescript
const schema = z.object({
  reasoning_steps: z.array(z.string()).describe("Step-by-step reasoning"),
  final_answer: z.string().describe("Concise final answer"),
  confidence: z.number().min(0).max(1).describe("Confidence score")
});

const result = await generateObject({
  model: openai("gpt-4o-mini", { structuredOutputs: true }),
  schema,
  prompt: `${question}\n\nLet's think step by step and provide structured output.`
});
```

### Zero-Shot CoT + Role-Playing

```
You are a math tutor. Explain your reasoning as you would to a student.

Q: [math problem]
A: Let's work through this together, step by step.
```

### Zero-Shot CoT + Your Agent

**Current** (your ReAct prompt):
```xml
Think step-by-step:
1. Analyze the question and identify what information/actions you need
2. Execute ONE tool at a time with the appropriate input
3. Observe the result and integrate it into your reasoning
```

**Already Zero-Shot CoT!** Your "Think" step is the reasoning trigger.

**Enhancement** (more explicit):
```xml
**REASONING PROTOCOL:**
Before each action, explicitly state:
- "Let's think through what we need to do:"
- [Your reasoning steps]
- "Therefore, I will: [action]"

Example:
Thought: Let's think through what we need to do:
1. User wants hero section on about page
2. First, I need to find the about page ID
3. Then find the hero section definition
4. Finally, add section to page
Therefore, I will: search for about page
```

## Benchmarks & Results

### Original Paper (Kojima et al., 2022)

| Task | Zero-Shot | Zero-Shot CoT | Improvement |
|------|-----------|---------------|-------------|
| **MultiArith** | 17.7% | 78.7% | +345% |
| **GSM8K** | 10.4% | 40.7% | +291% |
| **SVAMP** | 63.7% | 71.8% | +13% |
| **StrategyQA** | 53.2% | 66.1% | +24% |
| **AQuA** | 24.4% | 38.0% | +56% |

### Modern Models (2024)

**GPT-4o**:
| Task | Direct | Zero-Shot CoT | Delta |
|------|--------|---------------|-------|
| Math Word Problems | 78% | 89% | +11% |
| Logic Puzzles | 71% | 83% | +12% |
| Common Sense | 85% | 90% | +5% |

**Note**: Modern models naturally do some CoT, but explicit trigger still helps.

### Cost-Benefit Analysis

**Scenario**: 10k queries on math problems

**Direct Prompting**:
- Cost: 10k × 50 tokens × $0.60/1M = $0.30
- Accuracy: 70%
- Cost per correct answer: $0.30 ÷ 7000 = $0.000043

**Zero-Shot CoT**:
- Cost: 10k × 200 tokens × $0.60/1M = $1.20
- Accuracy: 85%
- Cost per correct answer: $1.20 ÷ 8500 = $0.000141

**Analysis**:
- 4x higher total cost
- 3.3x higher cost per correct answer
- But: 15% absolute accuracy gain (21% relative)
- **Worth it for**: High-stakes decisions, exams, critical calculations
- **Not worth it for**: High-volume, low-stakes queries

## Troubleshooting

### Problem: Model Still Jumps to Answer

**Symptom**: Adds phrase but skips reasoning

```
Q: [question]
A: Let's think step by step. The answer is 42.  ❌
```

**Solutions**:

1. **More explicit**:
```
Q: [question]
A: Let's think step by step. Before giving the final answer, show all 
   intermediate steps:
```

2. **Force structure**:
```
Q: [question]
A: Let's solve this in exactly 3 steps:
   Step 1: [what to do]
   Step 2: [what to do]
   Step 3: [final calculation]
```

3. **Upgrade model**: Smaller models (< 10B parameters) may not have emergent reasoning.

### Problem: Reasoning is Circular

**Symptom**: Model repeats same logic without progress

**Solution**: Structured template
```
Q: [question]
A: Let's solve this systematically:

Given information:
-

What we need to find:
-

Solution process:
1.
2.
3.

Final answer:
```

### Problem: Too Verbose

**Symptom**: Excessive explanation, high cost

**Solutions**:

1. **Chain of Draft**:
```
Solve this concisely. Show only essential calculation steps.
Let's think step by step.
```

2. **Token limit**:
```typescript
await generateText({
  model: openai("gpt-4o-mini"),
  prompt: `${question}\n\nLet's think step by step.`,
  maxTokens: 150  // Force brevity
});
```

### Problem: Wrong Answer Despite Good Reasoning

**Symptom**: Logic correct but final answer wrong

**Solution**: Two-stage extraction
```
Stage 1: Get reasoning
Stage 2: "Based on the reasoning above, the final answer is: [extract]"
```

## Key Takeaways

**What Is Zero-Shot CoT**:
- Adding "Let's think step by step" to any prompt
- No examples needed (zero-shot)
- Activates reasoning mode in LLMs
- 20-50% improvement on complex reasoning tasks

**Magic Phrases** (ranked):
1. "Let's think step by step" (best)
2. "Let's work this out step by step"
3. "Let's break this down"
4. "Let's solve this systematically"

**When to Use**:
- ✅ Quick reasoning boost without examples
- ✅ Diverse question types
- ✅ Token-constrained scenarios
- ✅ Math, logic, common sense tasks
- ❌ Simple lookups
- ❌ Creative tasks
- ❌ When you have good few-shot examples

**Advanced Variants**:
- **Plan-and-Solve**: More structured instructions (+3-4% accuracy)
- **Chain of Draft**: Concise reasoning (90% fewer tokens)
- **Instance-Adaptive**: Auto-select best trigger per question
- **Uncertainty-Guided**: Use confidence scores to pick best path

**Cost-Benefit**:
- 4x token increase (50 → 200 tokens)
- 15-20% accuracy gain on reasoning tasks
- Worth it for high-stakes, accuracy-critical applications
- Not worth it for high-volume, low-stakes queries

**Your Agent**:
- ReAct already implements CoT (Think → Act → Observe)
- "Thought:" prefix is reasoning trigger
- Could make more explicit with "Let's think through..."
- Consider Plan-and-Solve variant for complex multi-step goals

**Integration**:
```typescript
// Simple wrapper for any reasoning task
async function withZeroShotCoT(question: string) {
  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `${question}\n\nLet's think step by step.`
  });
}

// Use in your agent for complex planning
if (task.complexity > 0.7) {
  thought = await withZeroShotCoT(`How should I approach: ${task.goal}`);
}
```

## Practical Exercise

Transform these direct prompts to Zero-Shot CoT:

**Exercise 1**:
```
Direct: "If a $80 item is 25% off, what's the final price?"

Your Zero-Shot CoT:
```

**Exercise 2**:
```
Direct: "Should I use GPT-4o-mini or Claude Sonnet for a coding assistant?"

Your Zero-Shot CoT:
```

**Exercise 3** (Your Agent):
```
Direct: User asks "Create a blog post about our product"

Add Zero-Shot CoT reasoning to your agent's planning phase:
```

## Navigation

- [← Previous: 1.1.3 Chain-of-Thought Prompting](./1.1.3-chain-of-thought.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 1.1.5 Self-Consistency](./1.1.5-self-consistency.md)

---

*Part of Layer 1: Prompt Engineering - The simplest reasoning upgrade*
