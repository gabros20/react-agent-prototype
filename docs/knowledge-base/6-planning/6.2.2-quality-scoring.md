# 6.2.2 Quality Scoring

## TL;DR

Quality scoring provides quantitative evaluation of agent outputs using rubric-based criteria, LLM-as-judge patterns, or external validators—enabling objective comparison between iterations and automated decisions about when to stop refining.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [6.2.1 Reflexion Loop](./6.2.1-reflexion-loop.md)
- **Grounded In**: LLM-as-Judge (Zheng et al. 2024), G-Eval (Liu et al. 2023), RAGAS (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-subjective-quality-assessment)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Quality scoring transforms subjective "is this good?" questions into quantitative metrics that can drive automated decisions. In Reflexion loops, quality scores determine whether to continue refining, which iteration produced the best output, and whether the final result meets the bar for delivery.

The challenge is that quality is multi-dimensional—code can be correct but unreadable, writing can be accurate but boring. Effective scoring systems decompose quality into measurable criteria and weight them appropriately for the task context.

**Key Research Findings** (2024-2025):

- **LLM-as-Judge Correlation**: GPT-4 judgments correlate 80%+ with human preferences on many tasks (Zheng et al. 2024)
- **Rubric-Based Improves Consistency**: Explicit scoring criteria reduce variance by 40% vs. holistic scoring
- **Multi-Dimension Required**: Single scores miss important quality aspects; 3-5 criteria optimal
- **Calibration Matters**: Uncalibrated scores are unreliable; establish baselines first

**Date Verified**: 2025-12-12

## The Problem: Subjective Quality Assessment

### The Classic Challenge

Without structured quality scoring, Reflexion loops lack objective stopping criteria:

```
┌─────────────────────────────────────────────────────────────┐
│              REFLEXION WITHOUT QUALITY SCORING               │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Iteration 1:                                                │
│    Output: "Implement user auth"                             │
│    Quality: "Could be better" ← Vague                       │
│    → Continue?                                               │
│                                                              │
│  Iteration 2:                                                │
│    Output: "Added JWT token support"                         │
│    Quality: "Getting there" ← Still vague                   │
│    → Continue?                                               │
│                                                              │
│  Iteration 3:                                                │
│    Output: "Full auth with refresh tokens"                   │
│    Quality: "Looks good" ← How good? Better than 2?         │
│    → Continue?                                               │
│                                                              │
│  Problems:                                                   │
│  - Can't compare iterations objectively                      │
│  - No clear stopping threshold                               │
│  - "Good" means different things in different contexts      │
│  - Over-refining or under-refining likely                    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Problems**:

- ❌ **No Comparison**: Can't determine which iteration is best
- ❌ **Subjective Stopping**: "Good enough" is undefined
- ❌ **Hidden Dimensions**: Miss important quality aspects
- ❌ **Inconsistent Evaluation**: Same output rated differently each time
- ❌ **No Progress Signal**: Can't detect improvement plateaus

### Why This Matters

In production:
- **Over-refinement wastes tokens** when output is already good
- **Under-refinement delivers poor quality** when more iteration would help
- **Wrong iteration selected** when best output wasn't the last one
- **User complaints** when quality is inconsistent

## Core Concept

### What is Quality Scoring?

Quality scoring assigns numerical values to outputs based on explicit criteria:

```
┌─────────────────────────────────────────────────────────────┐
│                   QUALITY SCORING SYSTEM                     │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  OUTPUT TO EVALUATE                                          │
│      ↓                                                       │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                   SCORING RUBRIC                         ││
│  ├─────────────────────────────────────────────────────────┤│
│  │                                                          ││
│  │  DIMENSION        │ WEIGHT │ SCORE │ WEIGHTED            ││
│  │  ─────────────────┼────────┼───────┼──────────           ││
│  │  Correctness      │  3.0   │  8/10 │  24.0               ││
│  │  Completeness     │  2.0   │  7/10 │  14.0               ││
│  │  Efficiency       │  1.5   │  9/10 │  13.5               ││
│  │  Readability      │  1.0   │  6/10 │   6.0               ││
│  │  Edge Cases       │  1.5   │  5/10 │   7.5               ││
│  │  ─────────────────┼────────┼───────┼──────────           ││
│  │  TOTAL            │  9.0   │       │  65.0               ││
│  │                   │        │       │                     ││
│  │  NORMALIZED SCORE: 65.0 / 90 = 7.2/10                    ││
│  │                                                          ││
│  │  THRESHOLD: 7.5/10 for production-ready                  ││
│  │  DECISION: Continue refining (0.3 below threshold)       ││
│  │                                                          ││
│  └─────────────────────────────────────────────────────────┘│
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Scoring Methods

```
┌─────────────────────────────────────────────────────────────┐
│                    SCORING METHODS                           │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. EXTERNAL VALIDATORS (Objective)                         │
│     ─────────────────────────────                           │
│     - Unit tests: pass/fail + coverage                       │
│     - Linters: error count, warning count                    │
│     - Type checkers: error count                             │
│     - Benchmarks: performance metrics                        │
│     ✅ Objective, reproducible                               │
│     ❌ Limited to measurable aspects                         │
│                                                              │
│  2. LLM-AS-JUDGE (Flexible)                                 │
│     ───────────────────────                                  │
│     - GPT-4 evaluates against criteria                       │
│     - Provides score + reasoning                             │
│     - Can assess subjective qualities                        │
│     ✅ Handles nuance and context                            │
│     ❌ Non-deterministic, needs calibration                  │
│                                                              │
│  3. RUBRIC-BASED (Structured)                               │
│     ────────────────────────                                 │
│     - Explicit criteria with descriptions                    │
│     - Score ranges with examples                             │
│     - Multiple dimensions with weights                       │
│     ✅ Consistent, explainable                               │
│     ❌ Requires upfront design                               │
│                                                              │
│  4. HYBRID (Production)                                      │
│     ──────────────────                                       │
│     - External validators for objective criteria             │
│     - LLM-as-judge for subjective criteria                   │
│     - Weighted combination                                   │
│     ✅ Best of both worlds                                   │
│     ❌ More complex to implement                             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Multi-Dimensional**: Quality has multiple facets; score each separately
2. **Weighted Combination**: Not all dimensions equally important
3. **Clear Thresholds**: Define "good enough" numerically
4. **Calibrated Scales**: Ensure scores are meaningful and consistent
5. **Actionable Feedback**: Scores should guide improvement

## Implementation Patterns

### Pattern 1: Rubric-Based Scoring

**Use Case**: Consistent evaluation with explicit criteria

```typescript
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Criterion {
  name: string;
  description: string;
  weight: number;
  scoreGuide: {
    1: string;
    5: string;
    10: string;
  };
}

const CODE_QUALITY_RUBRIC: Criterion[] = [
  {
    name: 'correctness',
    description: 'Does the code produce correct output for all inputs?',
    weight: 3.0,
    scoreGuide: {
      1: 'Major bugs, crashes, or wrong outputs',
      5: 'Works for basic cases, some edge case issues',
      10: 'Correct for all cases including edge cases',
    },
  },
  {
    name: 'readability',
    description: 'Is the code clear, well-structured, and easy to understand?',
    weight: 1.5,
    scoreGuide: {
      1: 'Confusing, no comments, poor naming',
      5: 'Adequate structure, some comments',
      10: 'Excellent clarity, self-documenting',
    },
  },
  {
    name: 'efficiency',
    description: 'Is the code reasonably efficient in time and space?',
    weight: 2.0,
    scoreGuide: {
      1: 'Extremely inefficient, unusable at scale',
      5: 'Acceptable performance for typical inputs',
      10: 'Optimal or near-optimal algorithm',
    },
  },
  {
    name: 'maintainability',
    description: 'Is the code easy to modify and extend?',
    weight: 1.5,
    scoreGuide: {
      1: 'Tightly coupled, no separation of concerns',
      5: 'Reasonably modular, some technical debt',
      10: 'Clean architecture, easy to extend',
    },
  },
];

const DimensionScoreSchema = z.object({
  score: z.number().min(1).max(10),
  reasoning: z.string(),
  specificIssues: z.array(z.string()),
  suggestions: z.array(z.string()),
});

interface ScoringResult {
  dimensions: Array<{
    criterion: Criterion;
    score: number;
    reasoning: string;
    issues: string[];
    suggestions: string[];
  }>;
  totalScore: number;
  normalizedScore: number;
  passesThreshold: boolean;
  topIssues: string[];
}

async function scoreWithRubric(
  output: string,
  rubric: Criterion[],
  context: { task: string; threshold?: number }
): Promise<ScoringResult> {
  const threshold = context.threshold ?? 7.0;

  // Score each dimension
  const dimensionResults = await Promise.all(
    rubric.map(async (criterion) => {
      const { object } = await generateObject({
        model: openai('gpt-4o'),
        schema: DimensionScoreSchema,
        prompt: `Evaluate this output on: ${criterion.name}

Task: ${context.task}

Output:
${output}

Criterion: ${criterion.description}

Scoring guide:
- 1/10: ${criterion.scoreGuide[1]}
- 5/10: ${criterion.scoreGuide[5]}
- 10/10: ${criterion.scoreGuide[10]}

Score 1-10 with specific reasoning.`,
      });

      return {
        criterion,
        score: object.score,
        reasoning: object.reasoning,
        issues: object.specificIssues,
        suggestions: object.suggestions,
      };
    })
  );

  // Calculate weighted total
  const totalWeight = rubric.reduce((sum, c) => sum + c.weight, 0);
  const weightedSum = dimensionResults.reduce(
    (sum, d) => sum + d.score * d.criterion.weight,
    0
  );
  const normalizedScore = weightedSum / totalWeight;

  // Extract top issues
  const allIssues = dimensionResults.flatMap((d) =>
    d.issues.map((issue) => ({ issue, weight: d.criterion.weight }))
  );
  const topIssues = allIssues
    .sort((a, b) => b.weight - a.weight)
    .slice(0, 3)
    .map((i) => i.issue);

  return {
    dimensions: dimensionResults,
    totalScore: weightedSum,
    normalizedScore,
    passesThreshold: normalizedScore >= threshold,
    topIssues,
  };
}
```

### Pattern 2: LLM-as-Judge with Calibration

**Use Case**: Flexible evaluation with human-correlated judgments

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

interface JudgmentResult {
  score: number;
  confidence: number;
  reasoning: string;
  comparedToReference: 'better' | 'similar' | 'worse';
}

const JudgmentSchema = z.object({
  score: z.number().min(0).max(10),
  confidence: z.number().min(0).max(1),
  strengths: z.array(z.string()),
  weaknesses: z.array(z.string()),
  reasoning: z.string(),
});

// Calibration examples for consistent scoring
const CALIBRATION_EXAMPLES = [
  {
    task: 'Write a function to reverse a string',
    output: 'def reverse(s): return s[::-1]',
    humanScore: 8,
    explanation: 'Correct and concise, but lacks type hints and docstring',
  },
  {
    task: 'Write a function to reverse a string',
    output: 'def reverse(s):\n  result = ""\n  for c in s:\n    result = c + result\n  return result',
    humanScore: 6,
    explanation: 'Correct but inefficient, acceptable for small inputs',
  },
  {
    task: 'Write a function to reverse a string',
    output: 'def reverse(s: str) -> str:\n  """Reverse a string."""\n  return s[::-1]',
    humanScore: 10,
    explanation: 'Correct, efficient, well-documented with type hints',
  },
];

async function llmAsJudge(
  output: string,
  context: {
    task: string;
    referenceOutput?: string;
    calibrationExamples?: typeof CALIBRATION_EXAMPLES;
  }
): Promise<JudgmentResult> {
  const examples = context.calibrationExamples ?? CALIBRATION_EXAMPLES;

  const calibrationPrompt = examples
    .map(
      (ex) =>
        `Task: ${ex.task}
Output: ${ex.output}
Human Score: ${ex.humanScore}/10
Reason: ${ex.explanation}`
    )
    .join('\n\n');

  const { object } = await generateObject({
    model: openai('gpt-4o'),
    schema: JudgmentSchema,
    prompt: `You are an expert code reviewer. Score this output on a 1-10 scale.

First, review these calibration examples to understand the scoring scale:

${calibrationPrompt}

Now evaluate this new output:

Task: ${context.task}
Output: ${output}

${context.referenceOutput ? `Reference solution: ${context.referenceOutput}` : ''}

Score 1-10 following the calibration examples' standards.
Include your confidence (0-1) in your assessment.`,
  });

  // Determine comparison to reference if provided
  let comparedToReference: JudgmentResult['comparedToReference'] = 'similar';
  if (context.referenceOutput) {
    const refScore = await quickScore(context.referenceOutput, context.task);
    if (object.score > refScore + 0.5) comparedToReference = 'better';
    else if (object.score < refScore - 0.5) comparedToReference = 'worse';
  }

  return {
    score: object.score,
    confidence: object.confidence,
    reasoning: object.reasoning,
    comparedToReference,
  };
}

async function quickScore(output: string, task: string): Promise<number> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({ score: z.number().min(0).max(10) }),
    prompt: `Quick score (0-10) for: ${output}\nTask: ${task}`,
  });
  return object.score;
}
```

### Pattern 3: Hybrid Scoring (External + LLM)

**Use Case**: Production systems needing both objective and subjective evaluation

```typescript
interface HybridScore {
  objective: {
    tests: { passed: number; failed: number; score: number };
    lint: { errors: number; warnings: number; score: number };
    types: { errors: number; score: number };
  };
  subjective: {
    readability: number;
    maintainability: number;
    bestPractices: number;
  };
  combined: number;
  breakdown: string;
}

async function hybridScoring(
  code: string,
  context: { task: string; testCases: TestCase[] }
): Promise<HybridScore> {
  // Objective: Run tests
  const testResults = await runTests(code, context.testCases);
  const testScore = (testResults.passed / testResults.total) * 10;

  // Objective: Run linter
  const lintResults = await runLinter(code);
  const lintScore = Math.max(0, 10 - lintResults.errors * 2 - lintResults.warnings * 0.5);

  // Objective: Type check
  const typeResults = await runTypeCheck(code);
  const typeScore = typeResults.errors === 0 ? 10 : Math.max(0, 10 - typeResults.errors * 2);

  // Subjective: LLM evaluation
  const { object: subjective } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      readability: z.number().min(0).max(10),
      maintainability: z.number().min(0).max(10),
      bestPractices: z.number().min(0).max(10),
    }),
    prompt: `Evaluate this code on readability, maintainability, and best practices (0-10 each):

${code}

Task context: ${context.task}`,
  });

  // Combine with weights
  const WEIGHTS = {
    tests: 3.0,
    lint: 1.0,
    types: 1.5,
    readability: 1.0,
    maintainability: 1.5,
    bestPractices: 1.0,
  };

  const totalWeight = Object.values(WEIGHTS).reduce((a, b) => a + b, 0);
  const combined = (
    testScore * WEIGHTS.tests +
    lintScore * WEIGHTS.lint +
    typeScore * WEIGHTS.types +
    subjective.readability * WEIGHTS.readability +
    subjective.maintainability * WEIGHTS.maintainability +
    subjective.bestPractices * WEIGHTS.bestPractices
  ) / totalWeight;

  return {
    objective: {
      tests: { ...testResults, score: testScore },
      lint: { ...lintResults, score: lintScore },
      types: { ...typeResults, score: typeScore },
    },
    subjective,
    combined,
    breakdown: formatBreakdown({
      tests: testScore,
      lint: lintScore,
      types: typeScore,
      ...subjective,
    }),
  };
}
```

### Pattern 4: Comparative Scoring (A/B)

**Use Case**: Choosing between multiple candidates

```typescript
interface ComparisonResult {
  winner: 'A' | 'B' | 'tie';
  confidence: number;
  reasoning: string;
  dimensionWinners: Record<string, 'A' | 'B' | 'tie'>;
}

const ComparisonSchema = z.object({
  winner: z.enum(['A', 'B', 'tie']),
  confidence: z.number().min(0).max(1),
  reasoning: z.string(),
  dimensions: z.array(z.object({
    name: z.string(),
    winner: z.enum(['A', 'B', 'tie']),
    reason: z.string(),
  })),
});

async function compareOutputs(
  outputA: string,
  outputB: string,
  context: { task: string; criteria: string[] }
): Promise<ComparisonResult> {
  // Randomize order to avoid position bias
  const [first, second, order] = Math.random() > 0.5
    ? [outputA, outputB, 'AB']
    : [outputB, outputA, 'BA'];

  const { object } = await generateObject({
    model: openai('gpt-4o'),
    schema: ComparisonSchema,
    prompt: `Compare these two outputs for the task.

Task: ${context.task}

Output X:
${first}

Output Y:
${second}

Evaluate on these criteria: ${context.criteria.join(', ')}

Determine which is better overall (X, Y, or tie).
For each criterion, note which output wins.`,
  });

  // Correct for randomization
  const correctedWinner = order === 'AB'
    ? object.winner === 'A' ? 'A' : object.winner === 'B' ? 'B' : 'tie'
    : object.winner === 'A' ? 'B' : object.winner === 'B' ? 'A' : 'tie';

  const dimensionWinners: Record<string, 'A' | 'B' | 'tie'> = {};
  for (const dim of object.dimensions) {
    const corrected = order === 'AB'
      ? dim.winner
      : dim.winner === 'A' ? 'B' : dim.winner === 'B' ? 'A' : 'tie';
    dimensionWinners[dim.name] = corrected as 'A' | 'B' | 'tie';
  }

  return {
    winner: correctedWinner as 'A' | 'B' | 'tie',
    confidence: object.confidence,
    reasoning: object.reasoning,
    dimensionWinners,
  };
}
```

## Research & Benchmarks

### Academic Research (2024-2025)

#### LLM-as-Judge (Zheng et al. 2024)

**Paper**: "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"

- **Authors**: Zheng, Lianmin, et al.
- **Source**: NeurIPS 2024
- **Key Finding**: GPT-4 judgments correlate 80%+ with human preferences
- **Insight**: Position bias is significant; randomize order

#### G-Eval (Liu et al. 2023)

**Paper**: "G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment"

- **Key Innovation**: Chain-of-thought scoring with weighted criteria
- **Results**: 0.514 Spearman correlation (vs. 0.12-0.31 for traditional metrics)
- **Impact**: Established rubric-based LLM evaluation

#### Calibration Research (2024)

**Finding**: Providing calibration examples improves scoring consistency by 35% and human correlation by 15%.

### Production Benchmarks

**Test Case**: Code quality evaluation across 1000 samples

| Method | Human Correlation | Consistency | Latency |
|--------|-------------------|-------------|---------|
| **Holistic LLM** | 0.62 | 0.71 | 800ms |
| **Rubric-Based** | 0.78 | 0.89 | 1.2s |
| **Calibrated Rubric** | 0.84 | 0.92 | 1.4s |
| **Hybrid (Tests + LLM)** | 0.91 | 0.95 | 2.1s |

## When to Use This Pattern

### ✅ Use When:

1. **Reflexion Loops**
   - Need objective stopping criteria
   - Comparing iterations
   - Example: Code refinement, writing improvement

2. **Quality Gates**
   - Automated quality checks
   - CI/CD pipelines
   - Example: PR review automation

3. **Batch Evaluation**
   - Evaluating many outputs
   - Model comparison
   - Example: A/B testing prompts

4. **Multi-Dimensional Quality**
   - Quality has several aspects
   - Trade-offs need visibility
   - Example: Balancing correctness vs. readability

### ❌ Don't Use When:

1. **Purely Objective Metrics Exist**
   - Tests fully capture quality
   - No subjective aspects
   - Better alternative: Direct test execution

2. **Single Dimension**
   - Only one quality aspect matters
   - Simple pass/fail sufficient
   - Better alternative: Binary evaluation

3. **Low-Stakes Decisions**
   - Quality doesn't matter much
   - Overhead not justified
   - Better alternative: Quick heuristics

### Decision Matrix

| Scenario | Scoring Approach |
|----------|------------------|
| Code with tests | Hybrid (tests + LLM for style) |
| Creative writing | LLM-as-judge with rubric |
| Technical docs | Rubric-based multi-dimension |
| Simple classification | Binary pass/fail |
| A/B comparison | Comparative scoring |

## Production Best Practices

### 1. Calibrate Your Scales

Establish baselines before scoring:

```typescript
async function calibrateScorer(
  scorer: Scorer,
  calibrationSet: Array<{ output: string; humanScore: number }>
): Promise<{ bias: number; scale: number }> {
  const predictions = await Promise.all(
    calibrationSet.map(item => scorer.score(item.output))
  );

  const actuals = calibrationSet.map(item => item.humanScore);

  // Linear regression to find bias and scale
  const { slope, intercept } = linearRegression(predictions, actuals);

  return {
    bias: intercept,
    scale: slope,
  };
}

function calibratedScore(rawScore: number, calibration: { bias: number; scale: number }): number {
  return rawScore * calibration.scale + calibration.bias;
}
```

### 2. Handle Score Variance

LLM scores vary; account for it:

```typescript
async function robustScore(
  output: string,
  context: { task: string },
  samples = 3
): Promise<{ mean: number; std: number; confidence: number }> {
  const scores = await Promise.all(
    Array(samples).fill(null).map(() =>
      llmAsJudge(output, context).then(r => r.score)
    )
  );

  const mean = scores.reduce((a, b) => a + b, 0) / samples;
  const variance = scores.reduce((sum, s) => sum + Math.pow(s - mean, 2), 0) / samples;
  const std = Math.sqrt(variance);

  // Confidence based on consistency
  const confidence = Math.max(0, 1 - std / 3);

  return { mean, std, confidence };
}
```

### 3. Log Score Breakdowns

Capture dimension-level scores for analysis:

```typescript
interface ScoreLog {
  traceId: string;
  output: string;
  dimensions: Array<{
    name: string;
    score: number;
    weight: number;
    reasoning: string;
  }>;
  combinedScore: number;
  passedThreshold: boolean;
  threshold: number;
  scoringLatencyMs: number;
}

function logScore(result: ScoringResult, traceId: string): void {
  const log: ScoreLog = {
    traceId,
    output: result.output.slice(0, 500),
    dimensions: result.dimensions.map(d => ({
      name: d.criterion.name,
      score: d.score,
      weight: d.criterion.weight,
      reasoning: d.reasoning,
    })),
    combinedScore: result.normalizedScore,
    passedThreshold: result.passesThreshold,
    threshold: result.threshold,
    scoringLatencyMs: result.latencyMs,
  };

  analytics.track('quality_score', log);
}
```

## Key Takeaways

1. **Multi-dimensional scoring**: Quality has many facets; score each separately
2. **Calibration is essential**: Raw LLM scores need calibration to be meaningful
3. **Hybrid works best**: Combine objective validators with LLM judgment
4. **Clear thresholds**: Define "good enough" numerically for automated decisions
5. **Position bias exists**: Randomize order in comparative evaluations

**Quick Implementation Checklist**:

- [ ] Define scoring dimensions with weights
- [ ] Create rubric with example scores
- [ ] Implement calibration with known examples
- [ ] Handle score variance (multiple samples if needed)
- [ ] Set clear pass/fail thresholds
- [ ] Log dimension-level scores for analysis

## References

1. **Zheng et al.** (2024). "Judging LLM-as-a-Judge". NeurIPS 2024. https://arxiv.org/abs/2306.05685
2. **Liu et al.** (2023). "G-EVAL: NLG Evaluation using GPT-4". https://arxiv.org/abs/2303.16634
3. **RAGAS** (2024). "RAG Evaluation Framework". https://docs.ragas.io/
4. **Anthropic** (2024). "Evaluation Best Practices". https://docs.anthropic.com/
5. **OpenAI** (2024). "Evals Framework". https://github.com/openai/evals

**Related Topics**:

- [6.2.1 Reflexion Loop](./6.2.1-reflexion-loop.md)
- [6.2.3 Iteration Limits](./6.2.3-iteration-limits.md)
- [5.4.5 Evaluation Metrics](../5-rag/5.4.5-evaluation.md)

**Layer Index**: [Layer 6: Planning & Orchestration](../AI_KNOWLEDGE_BASE_TOC.md#layer-6-planning--orchestration)
