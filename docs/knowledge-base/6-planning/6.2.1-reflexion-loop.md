# 6.2.1 Generate → Critique → Refine Loop (Reflexion)

## TL;DR

Reflexion is a verbal reinforcement learning pattern where agents generate output, self-critique against explicit criteria, store reflections in episodic memory, and refine in subsequent attempts—achieving +22% on AlfWorld and +20% on HotPotQA without weight updates.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [3.1.1 ReAct Pattern](../3-agents/3.1.1-react-pattern.md), [4.2.1 Short-Term Memory](../4-memory/4.2.1-short-term.md)
- **Grounded In**: Reflexion (Shinn et al. 2023), Self-Refine (Madaan et al. 2023), Constitutional AI (2023)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-agents-dont-learn-from-mistakes)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Observability & Debugging](#observability--debugging)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Reflexion enables LLM agents to learn from trial-and-error by converting feedback (success/failure signals, environment responses, self-evaluation) into verbal reflections stored in memory. Unlike traditional reinforcement learning that updates model weights, Reflexion uses natural language to encode lessons learned, allowing the agent to improve within a conversation without fine-tuning.

The pattern follows a simple loop: **Generate** an output, **Critique** it against criteria, and **Refine** based on the critique. The key innovation is persisting reflections across attempts, so the agent doesn't repeat the same mistakes.

**Key Research Findings** (2024-2025):

- **+22% AlfWorld**: Reflexion improves task completion on interactive environments (Shinn et al. 2023)
- **+20% HotPotQA**: Multi-hop reasoning accuracy significantly improves with reflection
- **+11% HumanEval**: Code generation pass@1 improves from 80.1% to 91.0%
- **2-3 Iterations Optimal**: Most gains occur in first 2-3 refinement cycles

**Date Verified**: 2025-12-12

## The Problem: Agents Don't Learn from Mistakes

### The Classic Challenge

Without reflection, agents repeat the same errors across attempts:

```
┌─────────────────────────────────────────────────────────────┐
│                AGENT WITHOUT REFLECTION                      │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Task: "Write a function to find palindromes in a list"     │
│                                                              │
│  Attempt 1:                                                  │
│    def find_palindromes(lst):                                │
│        return [x for x in lst if x == x[::-1]]               │
│    ❌ Error: Can't slice integers                            │
│                                                              │
│  Attempt 2: (no memory of previous error)                   │
│    def find_palindromes(lst):                                │
│        return [x for x in lst if x == x[::-1]]               │
│    ❌ Same error: Can't slice integers                       │
│                                                              │
│  Attempt 3: (still no learning)                             │
│    def find_palindromes(lst):                                │
│        return [item for item in lst if item == item[::-1]]   │
│    ❌ Same error: Can't slice integers                       │
│                                                              │
│  Agent is stuck in a loop, repeating the same mistake       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Problems**:

- ❌ **No Error Memory**: Agent forgets what went wrong in previous attempts
- ❌ **Repeated Mistakes**: Same errors occur attempt after attempt
- ❌ **Wasted Tokens**: Generating identical failing outputs
- ❌ **No Improvement Signal**: Agent doesn't know what to fix
- ❌ **Poor User Experience**: Users see the same failures repeatedly

### Why This Matters

In production:
- **30-40% of agent failures** are repeated errors that could be avoided with reflection
- **Token costs multiply** when agents retry without learning
- **User trust erodes** when agents make the same mistake multiple times
- **Complex tasks fail** because agents can't learn from intermediate feedback

## Core Concept

### What is Reflexion?

Reflexion is a three-component system:

```
┌─────────────────────────────────────────────────────────────┐
│                    REFLEXION ARCHITECTURE                    │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                    ACTOR                                 ││
│  │  Generates actions/outputs based on task + reflections  ││
│  └──────────────────────┬──────────────────────────────────┘│
│                         ↓                                    │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                  EVALUATOR                               ││
│  │  Scores output quality (binary, scalar, or LLM-based)   ││
│  └──────────────────────┬──────────────────────────────────┘│
│                         ↓                                    │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                SELF-REFLECTION                           ││
│  │  Generates verbal feedback on what went wrong/right     ││
│  │  Stores reflection in episodic memory                    ││
│  └──────────────────────┬──────────────────────────────────┘│
│                         ↓                                    │
│  ┌─────────────────────────────────────────────────────────┐│
│  │              EPISODIC MEMORY                             ││
│  │  Stores reflections for future attempts                  ││
│  │  "Previous attempt failed because..."                    ││
│  └─────────────────────────────────────────────────────────┘│
│                                                              │
│  Loop continues until:                                       │
│    - Evaluator returns success                               │
│    - Max iterations reached                                  │
│    - No improvement detected                                 │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### The Generate → Critique → Refine Loop

```
┌─────────────────────────────────────────────────────────────┐
│              GENERATE → CRITIQUE → REFINE LOOP              │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌────────────────────────────────────────────────────┐     │
│  │  ITERATION 1                                        │     │
│  │  ─────────────                                      │     │
│  │  Generate: def find_palindromes(lst):               │     │
│  │              return [x for x in lst if x == x[::-1]]│     │
│  │                                                      │     │
│  │  Critique: "Failed - can't slice integers.          │     │
│  │            Need to convert to string first."        │     │
│  │                                                      │     │
│  │  Reflection stored: "Must convert elements to       │     │
│  │    strings before checking palindrome property"     │     │
│  └────────────────────────────────────────────────────┘     │
│                         ↓                                    │
│  ┌────────────────────────────────────────────────────┐     │
│  │  ITERATION 2                                        │     │
│  │  ─────────────                                      │     │
│  │  Context: Previous reflection injected              │     │
│  │                                                      │     │
│  │  Generate: def find_palindromes(lst):               │     │
│  │              return [x for x in lst                 │     │
│  │                      if str(x) == str(x)[::-1]]     │     │
│  │                                                      │     │
│  │  Critique: "Passes tests! But could be clearer."    │     │
│  │                                                      │     │
│  │  Reflection: "Solution works. Consider adding       │     │
│  │    docstring and type hints for clarity."           │     │
│  └────────────────────────────────────────────────────┘     │
│                         ↓                                    │
│                     SUCCESS ✓                               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Components

**1. Actor (Generator)**
- Produces actions or outputs
- Receives task description + previous reflections
- Can be any LLM or agent system

**2. Evaluator**
- Assesses output quality
- Can be binary (pass/fail), scalar (0-10), or descriptive
- Often uses test execution, external validators, or LLM judges

**3. Self-Reflection Generator**
- Analyzes failures to produce learning signal
- Generates natural language reflections
- Key insight: Verbal RL without weight updates

**4. Episodic Memory**
- Stores reflections across attempts
- Injected into actor's context
- Enables cross-attempt learning

### Key Principles

1. **Verbal Reinforcement**: Learning happens through language, not gradient updates
2. **Explicit Reflection**: Agent must articulate what went wrong
3. **Memory Persistence**: Reflections survive across attempts
4. **Bounded Iteration**: Limit refinement cycles to prevent loops

## Implementation Patterns

### Pattern 1: Basic Reflexion Loop

**Use Case**: Single-task refinement with binary success/fail

```typescript
import { generateText, generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface ReflexionState {
  task: string;
  attempts: Array<{
    output: string;
    evaluation: { success: boolean; feedback: string };
    reflection: string;
  }>;
  currentAttempt: number;
  maxAttempts: number;
}

const ReflectionSchema = z.object({
  whatWentWrong: z.string(),
  rootCause: z.string(),
  lesson: z.string(),
  nextAttemptStrategy: z.string(),
});

async function reflexionLoop(
  task: string,
  evaluate: (output: string) => Promise<{ success: boolean; feedback: string }>,
  maxAttempts = 3
): Promise<{ success: boolean; output: string; attempts: number }> {
  const state: ReflexionState = {
    task,
    attempts: [],
    currentAttempt: 0,
    maxAttempts,
  };

  while (state.currentAttempt < maxAttempts) {
    state.currentAttempt++;

    // Build context with previous reflections
    const reflections = state.attempts
      .map((a, i) => `Attempt ${i + 1} reflection: ${a.reflection}`)
      .join('\n');

    // Generate output
    const { text: output } = await generateText({
      model: openai('gpt-4o'),
      prompt: `Task: ${task}

${reflections ? `Previous reflections (learn from these):\n${reflections}\n` : ''}

Generate a solution that addresses any issues from previous attempts.`,
    });

    // Evaluate output
    const evaluation = await evaluate(output);

    if (evaluation.success) {
      return { success: true, output, attempts: state.currentAttempt };
    }

    // Generate reflection
    const { object: reflection } = await generateObject({
      model: openai('gpt-4o'),
      schema: ReflectionSchema,
      prompt: `Your attempt failed. Generate a reflection to improve.

Task: ${task}
Your output: ${output}
Feedback: ${evaluation.feedback}

Analyze what went wrong and how to fix it.`,
    });

    // Store attempt
    state.attempts.push({
      output,
      evaluation,
      reflection: `${reflection.whatWentWrong} Root cause: ${reflection.rootCause}. Lesson: ${reflection.lesson}. Next strategy: ${reflection.nextAttemptStrategy}`,
    });
  }

  // Return best attempt after exhausting retries
  return {
    success: false,
    output: state.attempts.at(-1)?.output ?? '',
    attempts: maxAttempts,
  };
}

// Usage example
const result = await reflexionLoop(
  'Write a function to find palindromes in a list of mixed types',
  async (code) => {
    try {
      // Execute code and run tests
      const testResults = await runTests(code, [
        { input: [121, 'aba', 123], expected: [121, 'aba'] },
        { input: ['hello', 'level'], expected: ['level'] },
      ]);
      return { success: testResults.allPassed, feedback: testResults.message };
    } catch (error) {
      return { success: false, feedback: `Error: ${error}` };
    }
  }
);
```

### Pattern 2: Self-Refine (No External Feedback)

**Use Case**: Quality improvement without external validators

```typescript
const CritiqueSchema = z.object({
  issues: z.array(z.object({
    category: z.enum(['clarity', 'accuracy', 'completeness', 'style']),
    description: z.string(),
    severity: z.enum(['minor', 'major', 'critical']),
    suggestion: z.string(),
  })),
  overallScore: z.number().min(1).max(10),
  shouldRefine: z.boolean(),
});

async function selfRefine(
  task: string,
  initialOutput: string,
  maxIterations = 3
): Promise<{ output: string; iterations: number; score: number }> {
  let currentOutput = initialOutput;
  let iteration = 0;

  while (iteration < maxIterations) {
    iteration++;

    // Self-critique
    const { object: critique } = await generateObject({
      model: openai('gpt-4o'),
      schema: CritiqueSchema,
      prompt: `Critique this output for the given task.

Task: ${task}
Output: ${currentOutput}

Identify specific issues and rate overall quality (1-10).
Set shouldRefine=false if quality is satisfactory (8+).`,
    });

    if (!critique.shouldRefine || critique.overallScore >= 8) {
      return { output: currentOutput, iterations: iteration, score: critique.overallScore };
    }

    // Refine based on critique
    const issuesSummary = critique.issues
      .map(i => `- [${i.severity}] ${i.category}: ${i.description} → ${i.suggestion}`)
      .join('\n');

    const { text: refined } = await generateText({
      model: openai('gpt-4o'),
      prompt: `Refine this output based on the critique.

Task: ${task}
Current output: ${currentOutput}

Issues to address:
${issuesSummary}

Generate an improved version that fixes these issues.`,
    });

    currentOutput = refined;
  }

  // Final evaluation
  const { object: finalCritique } = await generateObject({
    model: openai('gpt-4o'),
    schema: CritiqueSchema.pick({ overallScore: true }),
    prompt: `Rate this final output (1-10): ${currentOutput}`,
  });

  return { output: currentOutput, iterations: maxIterations, score: finalCritique.overallScore };
}
```

### Pattern 3: Reflexion with Episodic Memory

**Use Case**: Learning across multiple tasks

```typescript
interface EpisodicMemory {
  taskType: string;
  reflections: Array<{
    context: string;
    lesson: string;
    timestamp: number;
  }>;
}

class ReflexionAgent {
  private memory: Map<string, EpisodicMemory> = new Map();

  async execute(
    task: string,
    taskType: string,
    evaluate: Evaluator
  ): Promise<ExecutionResult> {
    // Retrieve relevant past reflections
    const episodic = this.memory.get(taskType);
    const relevantLessons = episodic?.reflections
      .slice(-5) // Last 5 lessons
      .map(r => r.lesson)
      .join('\n') ?? '';

    let attempt = 0;
    const maxAttempts = 3;

    while (attempt < maxAttempts) {
      attempt++;

      // Generate with lessons
      const output = await this.generate(task, relevantLessons);

      // Evaluate
      const evaluation = await evaluate(output);

      if (evaluation.success) {
        // Store positive reflection
        this.storeReflection(taskType, {
          context: task,
          lesson: `Success: ${evaluation.feedback}`,
          timestamp: Date.now(),
        });
        return { success: true, output, attempts: attempt };
      }

      // Generate and store reflection
      const reflection = await this.reflect(task, output, evaluation.feedback);
      this.storeReflection(taskType, {
        context: task,
        lesson: reflection,
        timestamp: Date.now(),
      });

      // Add to relevant lessons for next attempt
      relevantLessons = [relevantLessons, reflection].filter(Boolean).join('\n');
    }

    return { success: false, output: '', attempts: maxAttempts };
  }

  private async generate(task: string, lessons: string): Promise<string> {
    const { text } = await generateText({
      model: openai('gpt-4o'),
      prompt: `Task: ${task}

${lessons ? `Lessons from past experience:\n${lessons}\n` : ''}

Generate a solution.`,
    });
    return text;
  }

  private async reflect(task: string, output: string, feedback: string): Promise<string> {
    const { text } = await generateText({
      model: openai('gpt-4o-mini'), // Cheaper model for reflection
      prompt: `Analyze this failure and extract a reusable lesson.

Task: ${task}
Output: ${output}
Feedback: ${feedback}

Write a concise lesson (1-2 sentences) that will help in future similar tasks.`,
    });
    return text;
  }

  private storeReflection(taskType: string, reflection: EpisodicMemory['reflections'][0]) {
    if (!this.memory.has(taskType)) {
      this.memory.set(taskType, { taskType, reflections: [] });
    }
    this.memory.get(taskType)!.reflections.push(reflection);

    // Prune old reflections
    const mem = this.memory.get(taskType)!;
    if (mem.reflections.length > 20) {
      mem.reflections = mem.reflections.slice(-20);
    }
  }
}
```

### Pattern 4: Critic-Guided Reflexion

**Use Case**: Domain-specific quality criteria

```typescript
interface CriticConfig {
  criteria: Array<{
    name: string;
    description: string;
    weight: number;
  }>;
  passingScore: number;
}

const CODE_CRITIC: CriticConfig = {
  criteria: [
    { name: 'correctness', description: 'Code produces correct output', weight: 3 },
    { name: 'efficiency', description: 'Reasonable time/space complexity', weight: 2 },
    { name: 'readability', description: 'Clear, well-structured code', weight: 1 },
    { name: 'edge_cases', description: 'Handles edge cases properly', weight: 2 },
  ],
  passingScore: 7,
};

async function criticGuidedReflexion(
  task: string,
  criticConfig: CriticConfig,
  maxIterations = 3
): Promise<{ output: string; finalScore: number; iterations: number }> {
  let currentOutput = await generateInitialOutput(task);

  for (let i = 0; i < maxIterations; i++) {
    // Evaluate against all criteria
    const criteriaScores = await Promise.all(
      criticConfig.criteria.map(async (criterion) => {
        const { object } = await generateObject({
          model: openai('gpt-4o'),
          schema: z.object({
            score: z.number().min(0).max(10),
            reasoning: z.string(),
            suggestions: z.array(z.string()),
          }),
          prompt: `Evaluate this output on: ${criterion.name}
Description: ${criterion.description}

Task: ${task}
Output: ${currentOutput}

Score 0-10 and provide specific suggestions.`,
        });
        return { ...criterion, ...object };
      })
    );

    // Calculate weighted score
    const totalWeight = criteriaScores.reduce((sum, c) => sum + c.weight, 0);
    const weightedScore = criteriaScores.reduce(
      (sum, c) => sum + c.score * c.weight,
      0
    ) / totalWeight;

    if (weightedScore >= criticConfig.passingScore) {
      return { output: currentOutput, finalScore: weightedScore, iterations: i + 1 };
    }

    // Generate targeted reflection
    const lowScoreCriteria = criteriaScores
      .filter(c => c.score < 7)
      .sort((a, b) => a.score - b.score);

    const reflection = lowScoreCriteria
      .map(c => `${c.name} (${c.score}/10): ${c.suggestions.join('; ')}`)
      .join('\n');

    // Refine with specific guidance
    const { text } = await generateText({
      model: openai('gpt-4o'),
      prompt: `Improve this output based on specific feedback.

Task: ${task}
Current output: ${currentOutput}

Areas needing improvement:
${reflection}

Generate an improved version focusing on these issues.`,
    });

    currentOutput = text;
  }

  return { output: currentOutput, finalScore: 0, iterations: maxIterations };
}
```

## Framework Integration

### AI SDK v6 Reflexion Agent

```typescript
import { ToolLoopAgent, tool, stepCountIs } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const reflexionAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  instructions: `You are a reflexive agent that learns from mistakes.

After each action:
1. Evaluate if it achieved the intended goal
2. If not, use reflectOnFailure to analyze what went wrong
3. Use the reflection to guide your next attempt

Never repeat the same failed approach. Always incorporate lessons learned.`,

  tools: {
    executeAction: tool({
      description: 'Execute an action and get result',
      inputSchema: z.object({
        action: z.string(),
        parameters: z.record(z.unknown()),
      }),
      execute: async ({ action, parameters }) => {
        // Execute and return result with success/failure
        const result = await performAction(action, parameters);
        return result;
      },
    }),

    reflectOnFailure: tool({
      description: 'Reflect on why an action failed',
      inputSchema: z.object({
        action: z.string(),
        error: z.string(),
        context: z.string(),
      }),
      execute: async ({ action, error, context }) => {
        const { object } = await generateObject({
          model: openai('gpt-4o-mini'),
          schema: ReflectionSchema,
          prompt: `Analyze this failure:
Action: ${action}
Error: ${error}
Context: ${context}

What went wrong and how should the next attempt differ?`,
        });

        return {
          lesson: object.lesson,
          nextStrategy: object.nextAttemptStrategy,
        };
      },
    }),

    storeLesson: tool({
      description: 'Store a lesson learned for future reference',
      inputSchema: z.object({
        taskType: z.string(),
        lesson: z.string(),
      }),
      execute: async ({ taskType, lesson }) => {
        // Store in memory system
        await memoryStore.add(taskType, lesson);
        return { stored: true };
      },
    }),

    recallLessons: tool({
      description: 'Recall lessons from past similar tasks',
      inputSchema: z.object({
        taskType: z.string(),
      }),
      execute: async ({ taskType }) => {
        const lessons = await memoryStore.get(taskType);
        return { lessons };
      },
    }),
  },

  stopWhen: stepCountIs(15), // Limit total steps
});
```

## Research & Benchmarks

### Academic Research (2024-2025)

#### Reflexion (Shinn et al. 2023)

**Paper**: "Reflexion: Language Agents with Verbal Reinforcement Learning"

- **Authors**: Shinn, Noah, et al.
- **Source**: NeurIPS 2023
- **Key Innovation**: Verbal reinforcement without weight updates
- **Results**:
  - **+22% AlfWorld**: Interactive environment tasks
  - **+20% HotPotQA**: Multi-hop reasoning
  - **+11% HumanEval**: Code generation (80.1% → 91.0%)

#### Self-Refine (Madaan et al. 2023)

**Paper**: "Self-Refine: Iterative Refinement with Self-Feedback"

- **Authors**: Madaan, Aman, et al.
- **Source**: NeurIPS 2023
- **Key Innovation**: No external feedback needed
- **Results**:
  - +20% on code optimization
  - +13% on dialogue response
  - Average 2.3 iterations to converge

#### Self-Reflection Analysis (2024)

**Research Finding**: Meta-analysis of self-reflection in LLMs shows statistically significant improvement (p < 0.001) across all reflection types, with critique-based reflection showing strongest gains (Huang et al. 2024).

### Production Benchmarks

**Test Case**: Code generation with unit tests

| Method | Pass@1 | Avg Attempts | Token Cost |
|--------|--------|--------------|------------|
| **Single shot** | 65% | 1.0 | 1.0x |
| **Simple retry** | 71% | 2.3 | 2.3x |
| **Reflexion (2 iter)** | 84% | 1.8 | 2.5x |
| **Reflexion (3 iter)** | 89% | 2.1 | 3.2x |

**Key Insight**: Reflexion achieves higher success with fewer total attempts than blind retry.

## When to Use This Pattern

### ✅ Use When:

1. **Clear Success Criteria**
   - Tests can verify correctness
   - External validators available
   - Example: Code generation, math problems

2. **Iterative Improvement Valuable**
   - Quality improves with refinement
   - Feedback is actionable
   - Example: Writing, code review

3. **Similar Tasks Repeat**
   - Lessons transfer across tasks
   - Episodic memory provides value
   - Example: Customer support, content generation

4. **Cost of Failure High**
   - Getting it right matters more than speed
   - Retries are acceptable
   - Example: Critical system changes

### ❌ Don't Use When:

1. **No Clear Evaluation**
   - Success is subjective
   - Can't determine if output improved
   - Better alternative: Human-in-the-loop

2. **Real-Time Requirements**
   - Latency constraints are tight
   - Single-shot needed
   - Better alternative: Better prompting, fine-tuning

3. **High Token Costs**
   - Budget is constrained
   - Each iteration is expensive
   - Better alternative: Single-shot with CoT

### Decision Matrix

| Scenario | Use Reflexion? | Alternative |
|----------|----------------|-------------|
| Code with tests | ✅ Yes | - |
| Creative writing | ⚠️ Maybe | Human feedback |
| Real-time chat | ❌ No | Single-shot |
| Similar repeat tasks | ✅ Yes (with memory) | - |
| One-off query | ❌ No | CoT prompting |

## Production Best Practices

### 1. Bounded Iterations

Always limit refinement cycles:

```typescript
const REFLEXION_LIMITS = {
  maxIterations: 3,         // Rarely improves after 3
  minImprovement: 0.5,      // Stop if score improves <0.5 per iteration
  timeoutMs: 30000,         // Maximum time for full loop
};

function shouldContinue(
  iterations: number,
  scores: number[]
): boolean {
  if (iterations >= REFLEXION_LIMITS.maxIterations) return false;

  // Check for improvement plateau
  if (scores.length >= 2) {
    const improvement = scores.at(-1)! - scores.at(-2)!;
    if (improvement < REFLEXION_LIMITS.minImprovement) return false;
  }

  return true;
}
```

### 2. Efficient Reflection

Use smaller models for reflection generation:

```typescript
// Use GPT-4 for generation, GPT-4o-mini for reflection
async function efficientReflexion(task: string): Promise<string> {
  const output = await generateText({
    model: openai('gpt-4o'),  // Powerful for generation
    prompt: task,
  });

  const reflection = await generateText({
    model: openai('gpt-4o-mini'),  // Cheaper for reflection
    prompt: `Critique briefly: ${output}`,
  });

  return reflection;
}
```

### 3. Memory Pruning

Keep episodic memory manageable:

```typescript
function pruneMemory(
  reflections: Reflection[],
  maxSize: number = 20
): Reflection[] {
  if (reflections.length <= maxSize) return reflections;

  // Keep most recent and highest-quality reflections
  const sorted = [...reflections].sort((a, b) => {
    // Prefer recent + high quality
    const recencyScore = (a.timestamp - b.timestamp) / 86400000; // Days
    const qualityScore = a.helpfulness ?? 0;
    return (b.helpfulness ?? 0) + recencyScore - (a.helpfulness ?? 0) - recencyScore;
  });

  return sorted.slice(0, maxSize);
}
```

## Observability & Debugging

### Logging Strategy

```typescript
interface ReflexionLog {
  traceId: string;
  task: string;
  iterations: Array<{
    attempt: number;
    output: string;
    evaluation: { success: boolean; score?: number; feedback: string };
    reflection: string;
    tokenUsage: number;
    durationMs: number;
  }>;
  finalOutcome: 'success' | 'failure' | 'timeout';
  totalTokens: number;
  totalDurationMs: number;
}
```

### Common Failure Modes

1. **Reflection Loop**: Agent keeps generating similar reflections
   - **Detection**: Compare reflection embeddings
   - **Mitigation**: Force different approach after 2 similar reflections

2. **Over-Refinement**: Output gets worse with more iterations
   - **Detection**: Track score progression
   - **Mitigation**: Keep best output, not latest

3. **Generic Reflections**: "I should try harder" - not actionable
   - **Detection**: Check reflection specificity
   - **Mitigation**: Require concrete next steps in reflection schema

## Key Takeaways

1. **Verbal RL works**: Agents can learn through language without weight updates
2. **2-3 iterations optimal**: Most improvement happens early
3. **Specific reflections matter**: "Try harder" doesn't help; "convert to string first" does
4. **Memory enables transfer**: Lessons from one task help similar future tasks
5. **Bound your loops**: Always limit iterations to prevent runaway costs

**Quick Implementation Checklist**:

- [ ] Implement generate → evaluate → reflect loop
- [ ] Define clear success criteria for evaluation
- [ ] Store reflections in accessible memory
- [ ] Inject past reflections into generation context
- [ ] Set iteration limits (2-3 max)
- [ ] Track improvement to detect plateaus
- [ ] Use cheaper model for reflection generation

## References

1. **Shinn et al.** (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning". NeurIPS 2023. https://arxiv.org/abs/2303.11366
2. **Madaan et al.** (2023). "Self-Refine: Iterative Refinement with Self-Feedback". NeurIPS 2023. https://arxiv.org/abs/2303.17651
3. **Huang et al.** (2024). "Large Language Models Cannot Self-Correct Reasoning Yet". ICLR 2024. https://arxiv.org/abs/2310.01798
4. **Anthropic** (2023). "Constitutional AI". https://arxiv.org/abs/2212.08073
5. **LangChain** (2024). "Reflexion Agents". https://python.langchain.com/docs/langgraph

**Related Topics**:

- [6.2.2 Quality Scoring](./6.2.2-quality-scoring.md)
- [6.2.3 Iteration Limits](./6.2.3-iteration-limits.md)
- [4.2.1 Short-Term Memory](../4-memory/4.2.1-short-term.md)

**Layer Index**: [Layer 6: Planning & Orchestration](../AI_KNOWLEDGE_BASE_TOC.md#layer-6-planning--orchestration)
