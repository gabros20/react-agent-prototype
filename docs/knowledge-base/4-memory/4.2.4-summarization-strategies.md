# 4.2.4 - Summarization Strategies

## TL;DR

**Summarization converts detailed action-observation pairs into concise representations, achieving 10:1 compression ratios while preserving critical information—outcome-focused summaries capture what was accomplished rather than how, enabling hierarchical memory systems to maintain context without overwhelming the LLM.**

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-11
- **Prerequisites**: [4.2.3 Subgoal Detection](./4.2.3-subgoal-detection.md), [4.2.2 Compression Triggers](./4.2.2-compression-triggers.md)
- **Grounded In**: HiAgent (ACL 2025), Mem0 (2025), LangChain Context Engineering (2025)

## Table of Contents

- [Overview](#overview)
- [The Problem: Context Overload](#the-problem-context-overload)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Summarization is the process of converting detailed action histories into concise representations that preserve critical information while dramatically reducing token count. Effective summarization transforms verbose logs like "loosen_nut1 → Nut 1 loosened, loosen_nut2 → Nut 2 loosened..." into compact summaries like "All wheel nuts loosened."

The key insight is that LLMs perform better with outcome-focused context rather than detailed process logs. A summary of what was accomplished is more useful for planning next steps than a blow-by-blow account of how it was done.

**Key Research Findings (2024-2025)**:

- **HiAgent (ACL 2025)**: 10:1 compression with <15% information loss
- **Token Reduction**: 35% fewer context tokens with proper summarization
- **Quality Impact**: Better summaries → higher task success rates (42% vs 21%)
- **Cost Savings**: 90% reduction in token costs for long conversations

## The Problem: Context Overload

### The Classic Challenge

Without summarization, every action consumes precious context tokens:

```typescript
// WITHOUT summarization: 120+ tokens
const detailedHistory = [
  { action: 'open_boot', observation: 'Boot opened successfully' },
  { action: 'locate_jack', observation: 'Jack located in boot compartment' },
  { action: 'retrieve_jack', observation: 'Jack retrieved and placed on ground' },
  { action: 'locate_wrench', observation: 'Wrench found in tool kit' },
  { action: 'retrieve_wrench', observation: 'Wrench retrieved' }
];
// ~24 tokens per action × 5 actions = 120 tokens
```

```typescript
// WITH summarization: ~12 tokens
const summary = "Retrieved jack and wrench from boot";
// 10× compression achieved
```

**Problems**:

- ❌ **Token Waste**: Detailed logs consume 10× more tokens than needed
- ❌ **Context Dilution**: Important information buried in verbose history
- ❌ **Cognitive Overload**: LLM processes irrelevant details
- ❌ **Cost Spiral**: Linear growth in processing costs

### Why This Matters

| Approach | Tokens per Subgoal | Task Success | Context Quality |
|----------|-------------------|--------------|-----------------|
| Full detail | 500+ | 21% | Poor (diluted) |
| Basic summarization | 150 | 32% | Moderate |
| Outcome-focused | 50 | 42% | Excellent |

The research is clear: summarization isn't just about saving tokens—it actually **improves** agent performance by providing cleaner, more actionable context.

## Core Concept

### What is Summarization?

Summarization converts a sequence of action-observation pairs into a concise representation that captures the essential outcome. Good summaries answer "What was accomplished?" not "What steps were taken?"

### Visual Representation

```
Actions (Detailed):                    Summary (Compressed):
┌─────────────────────────────────┐   ┌───────────────────────────┐
│ loosen_nut1 → Nut 1 loosened    │   │                           │
│ loosen_nut2 → Nut 2 loosened    │──▶│ "All wheel nuts loosened" │
│ loosen_nut3 → Nut 3 loosened    │   │                           │
│ loosen_nut4 → Nut 4 loosened    │   │   (10:1 compression)      │
└─────────────────────────────────┘   └───────────────────────────┘
        ~100 tokens                          ~10 tokens
```

### Summarization Approaches

```
┌─────────────────────────────────────────────────────────────┐
│                 SUMMARIZATION SPECTRUM                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Extractive                                   Abstractive    │
│  (Rule-based)                                 (LLM-based)    │
│                                                              │
│  ├──────────────────────────────────────────────────────┤   │
│  │                                                      │   │
│  │  Fast, free         Balanced         Accurate, costly│   │
│  │  Low compression    Moderate         High compression│   │
│  │  Literal            Hybrid           Semantic        │   │
│  │                                                      │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Outcome over Process**: What was achieved, not how
2. **Preserve Critical Info**: Actions that failed, state changes
3. **Consistent Format**: Predictable structure aids parsing
4. **Appropriate Length**: Balance compression with completeness

## Implementation Patterns

### Pattern 1: Outcome-Focused Summarization (Recommended)

**Use Case**: Default approach for most subgoal summarization

```typescript
async function summarizeOutcomeFocused(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>
): Promise<string> {
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: `
Subgoal: ${subgoal}

Actions taken:
${actions.map(a => `- ${a.action} → ${a.observation}`).join('\n')}

Summarize what was ACCOMPLISHED in ONE sentence.
Focus on the outcome, not individual actions.
Be specific about what changed or was achieved.
    `.trim(),
    temperature: 0,
  });

  return result.text.trim();
}
```

**Pros**:
- ✅ High compression (5-10×)
- ✅ Preserves essential outcomes
- ✅ Clean, readable context

**Cons**:
- ❌ Loses procedural details
- ❌ May miss failure context

**When to Use**: Default choice for completed subgoals

### Pattern 2: Status-Aware Summarization

**Use Case**: When tracking completion state matters

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

const StatusSummarySchema = z.object({
  summary: z.string(),
  status: z.enum(['completed', 'partial', 'failed']),
  blockers: z.array(z.string()).optional(),
});

async function summarizeWithStatus(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>,
  targetState: string
): Promise<z.infer<typeof StatusSummarySchema>> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: StatusSummarySchema,
    prompt: `
Subgoal: ${subgoal}
Target: ${targetState}

Actions:
${actions.map(a => `- ${a.action} → ${a.observation}`).join('\n')}

Summarize outcome and determine status:
- completed: Target fully achieved
- partial: Some progress made
- failed: Could not achieve target
    `.trim(),
  });

  return object;
}
```

**Pros**:
- ✅ Captures completion state
- ✅ Identifies blockers
- ✅ Enables retry logic

**Cons**:
- ❌ More complex output
- ❌ Requires target definition

**When to Use**: Error handling, retry workflows

### Pattern 3: Extractive Summarization (Zero Cost)

**Use Case**: Structured tasks with predictable patterns

```typescript
function extractiveSummarize(
  actions: Array<{ action: string; observation: string }>
): string {
  // Pattern 1: Repeated action type
  const baseAction = actions[0].action.split('_')[0];
  const allSameType = actions.every(a => a.action.startsWith(baseAction));

  if (allSameType) {
    const lastObs = actions[actions.length - 1].observation;
    return `Completed ${actions.length} ${baseAction} actions: ${lastObs}`;
  }

  // Pattern 2: Sequential steps (short)
  if (actions.length <= 3) {
    return actions.map(a => a.observation).join('; ');
  }

  // Pattern 3: General case
  const first = actions[0].observation;
  const last = actions[actions.length - 1].observation;
  return `${first} → ... → ${last} (${actions.length} steps)`;
}
```

**Pros**:
- ✅ Zero latency
- ✅ Zero cost
- ✅ Deterministic

**Cons**:
- ❌ Limited compression (3-5×)
- ❌ Less semantic
- ❌ Domain-specific patterns

**When to Use**: High-volume, cost-sensitive, structured tasks

### Pattern 4: Hybrid Approach

**Use Case**: Production systems balancing cost and quality

```typescript
class HybridSummarizer {
  async summarize(
    subgoal: string,
    actions: Array<{ action: string; observation: string }>
  ): Promise<string> {
    // Try extractive first (free)
    const extractive = extractiveSummarize(actions);

    // Use extractive if good enough
    if (extractive.length <= 100 && this.hasGoodPattern(actions)) {
      return extractive;
    }

    // Fall back to LLM for complex cases
    return await summarizeOutcomeFocused(subgoal, actions);
  }

  private hasGoodPattern(
    actions: Array<{ action: string; observation: string }>
  ): boolean {
    if (actions.length < 3) return true;

    const baseAction = actions[0].action.split('_')[0];
    return actions.every(a => a.action.startsWith(baseAction));
  }
}
```

**Pros**:
- ✅ Cost-effective
- ✅ High quality when needed
- ✅ Best of both approaches

**Cons**:
- ❌ More complex
- ❌ Variable latency

**When to Use**: Production systems, mixed complexity

## Framework Integration

### AI SDK v6 with Summarization

```typescript
import { generateText, tool, stepCountIs } from 'ai';
import { z } from 'zod';

const summarizer = new HybridSummarizer();
const memory = new HiAgentMemoryManager(task, goal);

// Tool that triggers summarization on subgoal completion
const executeAction = tool({
  description: 'Execute an action',
  inputSchema: z.object({
    action: z.string(),
  }),
  execute: async ({ action }) => {
    const observation = await performAction(action);
    memory.addAction(action, observation);

    // Check if subgoal completed
    if (await memory.isSubgoalComplete()) {
      const summary = await summarizer.summarize(
        memory.getCurrentSubgoal(),
        memory.getCurrentActions()
      );

      memory.archiveSubgoal(summary);
      console.log(`Archived: ${summary}`);
    }

    return { action, observation };
  },
});

async function runAgent(prompt: string) {
  const result = await generateText({
    model: openai('gpt-4o'),
    prompt: `${memory.getContext()}\n\n${prompt}`,
    tools: { executeAction },
    stopWhen: stepCountIs(10),
  });

  return result.text;
}
```

### Batch Summarization

For efficiency, summarize multiple subgoals in one LLM call:

```typescript
async function summarizeBatch(
  items: Array<{
    subgoal: string;
    actions: Array<{ action: string; observation: string }>;
  }>
): Promise<string[]> {
  const batchPrompt = items
    .map(
      (item, i) => `
${i + 1}. Subgoal: ${item.subgoal}
Actions: ${item.actions.map(a => `${a.action}→${a.observation}`).join('; ')}
    `
    )
    .join('\n');

  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: `
Summarize each subgoal in ONE sentence:
${batchPrompt}

Format: numbered list matching input order.
    `.trim(),
  });

  return result.text
    .split('\n')
    .filter(line => /^\d+\./.test(line))
    .map(line => line.replace(/^\d+\.\s*/, '').trim());
}
```

## Research & Benchmarks

### Compression Ratio Comparison

| Technique | Compression | Quality | Cost | Latency |
|-----------|-------------|---------|------|---------|
| **None** | 1:1 | 100% | $0 | 0ms |
| **Extractive** | 3:1 | 70% | $0 | <1ms |
| **Abstractive** | 10:1 | 85% | ~$0.001 | ~300ms |
| **Hybrid** | 8:1 | 88% | ~$0.0005 | ~150ms |

### HiAgent Performance Impact

| Metric | Without | With | Improvement |
|--------|---------|------|-------------|
| Tokens/task | 5,000-7,000 | 3,250-4,500 | -35% |
| Success rate | 21% | 42% | +100% |
| Quality loss | 0% | <15% | Minimal |

### Model Selection for Summarization

| Model | Speed | Quality | Cost ($/1M tokens) |
|-------|-------|---------|-------------------|
| **GPT-4o-mini** | 300ms | High | $0.15 |
| GPT-4o | 600ms | Highest | $2.50 |
| Claude 3.5 Haiku | 350ms | High | $0.25 |

**Recommendation**: GPT-4o-mini offers best balance for summarization tasks.

## When to Use This Pattern

### ✅ Use When:

1. **Subgoal completion**
   - Action sequence complete
   - Ready to archive for context

2. **Context approaching limits**
   - 80% capacity threshold
   - Need to compress older content

3. **Long-running tasks**
   - Multiple subgoals expected
   - Session will exceed context

### ❌ Don't Use When:

1. **Active subgoal**
   - Actions still in progress
   - May need full detail for debugging

2. **Very short sequences**
   - <3 actions
   - Summarization overhead exceeds benefit

3. **Audit requirements**
   - Full history legally required
   - Use persistent storage instead

### Decision Matrix

| Scenario | Summarization Approach |
|----------|------------------------|
| Repeated actions (e.g., loosen 4 nuts) | Extractive |
| Mixed actions (e.g., prepare tools) | Abstractive |
| High volume, structured | Hybrid (extractive first) |
| Critical tasks | Abstractive with status |

## Production Best Practices

### 1. Use Structured Prompts

Guide the LLM to consistent output:

```typescript
const SUMMARIZATION_PROMPT = `
Subgoal: {subgoal}
Actions: {actions}

Requirements:
- ONE sentence summary
- Focus on OUTCOME, not process
- Include specific entities/quantities
- Use past tense

Example: "Retrieved jack and wrench from boot"
NOT: "Opened boot, looked for jack, found jack, picked up jack..."
`;
```

### 2. Validate Summary Quality

Check summaries meet criteria:

```typescript
function validateSummary(summary: string): { valid: boolean; issues: string[] } {
  const issues: string[] = [];

  if (summary.split(' ').length < 3) {
    issues.push('Too short (<3 words)');
  }

  if (summary.split(' ').length > 30) {
    issues.push('Too long (>30 words)');
  }

  if (!/[.!?]$/.test(summary)) {
    issues.push('Missing punctuation');
  }

  return { valid: issues.length === 0, issues };
}
```

### 3. Handle Failures Gracefully

```typescript
async function safeSummarize(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>
): Promise<string> {
  try {
    return await summarizeOutcomeFocused(subgoal, actions);
  } catch (error) {
    // Fallback to extractive
    console.warn('LLM summarization failed, using extractive');
    return extractiveSummarize(actions);
  }
}
```

### 4. Common Pitfalls

#### ❌ Pitfall: Generic Summaries

**Problem**: "Some actions were performed" provides no useful context.

**Solution**: Require specific outcomes in prompts:
```
BAD:  "Worked on the task"
GOOD: "Loosened all 4 wheel nuts using wrench"
```

#### ❌ Pitfall: Over-compression

**Problem**: Summary loses critical failure information.

**Solution**: Include status and blockers:
```typescript
// Don't just say "Nuts loosened"
// Include: "3 of 4 nuts loosened (nut 2 stuck)"
```

## Key Takeaways

1. **Outcome-focused summaries achieve 10:1 compression** - Focus on what was accomplished
2. **Extractive is free, abstractive is better** - Use hybrid for best balance
3. **Summarization improves performance** - Cleaner context → better agent decisions
4. **Validate summary quality** - Check length, specificity, completeness
5. **Handle edge cases gracefully** - Fallbacks for LLM failures

**Quick Implementation Checklist**:

- [ ] Choose summarization strategy (extractive, abstractive, hybrid)
- [ ] Create structured prompts with examples
- [ ] Set length constraints (min 3, max 30 words)
- [ ] Implement validation for summary quality
- [ ] Add fallback for LLM failures
- [ ] Test compression ratios (target 8:1 - 12:1)

## References

1. **Hu, M. et al.** (2025). "HiAgent: Hierarchical Working Memory Management". *ACL 2025*. https://aclanthology.org/2025.acl-long.1575.pdf
2. **Mem0** (2025). "LLM Chat History Summarization Guide 2025". https://mem0.ai/blog/llm-chat-history-summarization-guide-2025
3. **Galileo AI** (2025). "Master LLM Summarization Strategies and their Implementations". https://galileo.ai/blog/llm-summarization-strategies
4. **Anthropic** (2025). "Effective Context Engineering for AI Agents". https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents
5. **LangChain** (2025). "Context Engineering for Agents". https://blog.langchain.dev/context-engineering-for-agents/
6. **Nawrot, P. et al.** (2024). "Dynamic Memory Compression". *ICML 2024*. https://arxiv.org/abs/2403.09636

**Related Topics**:

- [4.2.1 HiAgent Hierarchical Memory](./4.2.1-hiagent-hierarchical-memory.md)
- [4.2.3 Subgoal Detection](./4.2.3-subgoal-detection.md)
- [4.2.5 Compression Ratios](./4.2.5-compression-ratios.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
