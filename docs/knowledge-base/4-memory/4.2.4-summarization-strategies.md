# 4.2.4 Summarization Strategies for Agent Memory

**Status**: ✅ Complete  
**Last Updated**: 2025-11-18  
**Research Sources**: 15+ papers and frameworks (2024-2025)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Why Summarization Matters](#why-summarization-matters)
3. [Summarization Techniques](#summarization-techniques)
4. [LLM-Based Summarization](#llm-based-summarization)
5. [Extract vs Abstractive](#extractive-vs-abstractive)
6. [Implementation in TypeScript](#implementation-in-typescript)
7. [Quality and Validation](#quality-and-validation)
8. [References](#references)

---

## Executive Summary

**Summarization** converts detailed action-observation pairs into concise representations, enabling hierarchical memory compression. Effective summarization achieves **10:1 compression ratios** while preserving critical information.[^1][^2]

### Key Strategies

- **Outcome-Focused**: Summarize what was achieved, not how[^3]
- **LLM-Based**: Use GPT-4o-mini for intelligent compression[^4]
- **Structured Prompts**: Guide LLM to produce consistent summaries[^5]
- **Validation**: Ensure summaries contain actionable information[^6]

### Performance Impact (HiAgent)

| Metric | Without Summarization | With Summarization | Improvement |
|--------|----------------------|-------------------|-------------|
| **Avg Tokens/Task** | 5,000-7,000 | 3,250-4,500 | **-35%** |
| **Compression Ratio** | 1:1 (none) | 10:1 | **10x** |
| **Quality Loss** | 0% | <15% | **Minimal** |
| **Success Rate** | 21% | 42% | **+100%** |

*Results from ACL 2025 HiAgent paper*[^1]

---

## Why Summarization Matters

### Problem: Detailed History Overwhelms Context

**Without summarization**: Every action consumes tokens.[^7][^8]

```typescript
// Without summarization: 500+ tokens
const detailedHistory = [
  { action: 'open_boot', observation: 'Boot opened successfully' },
  { action: 'locate_jack', observation: 'Jack located in boot compartment' },
  { action: 'retrieve_jack', observation: 'Jack retrieved and placed on ground' },
  { action: 'locate_wrench', observation: 'Wrench found in tool kit' },
  { action: 'retrieve_wrench', observation: 'Wrench retrieved' }
];

const context = detailedHistory.map(h => 
  `action: ${h.action}, observation: ${h.observation}`
).join('\n');

// Tokens: ~120 tokens (24 tokens/action × 5 actions)
```

### Solution: Outcome-Focused Summary

**With summarization**: Preserve essence in fewer tokens.[^1][^9]

```typescript
// With summarization: 50 tokens
const summary = {
  subgoal: 'Prepare tools',
  outcome: 'Retrieved jack and wrench from boot',
  status: 'completed'
};

const context = `Prepare tools: Retrieved jack and wrench from boot`;

// Tokens: ~12 tokens (10x compression)
```

### Benefits

Research shows significant advantages:[^10][^11]

- ✅ **Token Efficiency**: 10x fewer tokens in context
- ✅ **Cognitive Clarity**: LLM focuses on outcomes, not minutiae  
- ✅ **Scalable**: Hundreds of actions → dozens of summaries
- ✅ **Cost Savings**: 90% reduction in processing costs[^12]
- ✅ **Better Performance**: Reduced context → less confusion → higher success rates[^1]

---

## Summarization Techniques

### 1. Outcome-Focused Summarization (Recommended)

**Focus on "what was achieved" rather than "how it was done".**[^3][^13]

```typescript
async function summarizeOutcomeFocused(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>
): Promise<string> {
  const prompt = `
Subgoal: ${subgoal}

Actions taken:
${actions.map(a => `- ${a.action} → ${a.observation}`).join('\n')}

Summarize what was ACCOMPLISHED in ONE sentence.
Focus on the outcome, not the individual actions.
Be specific about what changed or what was achieved.
  `.trim();

  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt
  });

  return result.text.trim();
}

// Example
const summary = await summarizeOutcomeFocused(
  'Loosen all nuts',
  [
    { action: 'loosen_nut1', observation: 'Nut 1 loosened' },
    { action: 'loosen_nut2', observation: 'Nut 2 loosened' },
    { action: 'loosen_nut3', observation: 'Nut 3 loosened' },
    { action: 'loosen_nut4', observation: 'Nut 4 loosened' }
  ]
);
// Output: "All four wheel nuts loosened successfully"
```

**Pros**:
- ✅ High compression (5-10x)
- ✅ Preserves outcomes
- ✅ Easy to understand

**Cons**:
- ❌ Loses action details
- ❌ May miss failure reasons

### 2. Status-Aware Summarization

**Include subgoal completion status in summary.**[^14][^15]

```typescript
async function summarizeWithStatus(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>,
  targetState: string
): Promise<{ summary: string; status: 'completed' | 'partial' | 'failed' }> {
  const StatusSchema = z.object({
    summary: z.string(),
    status: z.enum(['completed', 'partial', 'failed']),
    reasoning: z.string()
  });

  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: StatusSchema,
    prompt: `
Subgoal: ${subgoal}
Target state: ${targetState}

Actions taken:
${actions.map(a => `- ${a.action} → ${a.observation}`).join('\n')}

Summarize the outcome and determine if the subgoal was:
- completed: Fully achieved
- partial: Partially achieved
- failed: Not achieved

Provide a one-sentence summary and status.
    `.trim()
  });

  return {
    summary: object.summary,
    status: object.status
  };
}

// Example
const result = await summarizeWithStatus(
  'Loosen all nuts',
  [
    { action: 'loosen_nut1', observation: 'Nut 1 loosened' },
    { action: 'loosen_nut2', observation: 'Nut 2 stuck, could not loosen' },
    { action: 'loosen_nut3', observation: 'Nut 3 loosened' }
  ],
  'All 4 nuts loosened'
);
// Output: {
//   summary: "3 out of 4 nuts loosened (nut 2 stuck)",
//   status: "partial"
// }
```

**Pros**:
- ✅ Captures completion status
- ✅ Useful for error handling
- ✅ Enables retry logic

**Cons**:
- ❌ More complex
- ❌ Requires target state definition

### 3. Progressive Summarization

**Summarize in stages: action → subgoal → task.**[^16][^17]

```typescript
class ProgressiveSummarizer {
  // Level 1: Summarize action sequence
  async summarizeActions(
    actions: string[]
  ): Promise<string> {
    // "loosen_nut1, loosen_nut2, loosen_nut3, loosen_nut4"
    // → "Loosened 4 nuts"
    return this.extractPattern(actions);
  }

  // Level 2: Summarize subgoal
  async summarizeSubgoal(
    subgoal: string,
    actionSummary: string
  ): Promise<string> {
    // "Loosen all nuts: Loosened 4 nuts"
    // → "All wheel nuts loosened"
    return `${subgoal}: ${actionSummary}`;
  }

  // Level 3: Summarize task progress
  async summarizeTask(
    subgoalSummaries: string[]
  ): Promise<string> {
    // ["Tools retrieved", "Nuts loosened", "Car jacked"]
    // → "Prepared car for tire change"
    return this.compressSummaries(subgoalSummaries);
  }

  private extractPattern(actions: string[]): string {
    // Pattern detection logic
    const baseAction = actions[0].split('_')[0];
    if (actions.every(a => a.startsWith(baseAction))) {
      return `${baseAction} ${actions.length} items`;
    }
    return `Completed ${actions.length} actions`;
  }

  private async compressSummaries(summaries: string[]): Promise<string> {
    const prompt = `
Compress these subgoal summaries into ONE sentence:
${summaries.map(s => `- ${s}`).join('\n')}

Provide a high-level overview of what was accomplished.
    `.trim();

    const result = await generateText({
      model: openai('gpt-4o-mini'),
      prompt
    });

    return result.text.trim();
  }
}
```

**Pros**:
- ✅ Multi-level compression
- ✅ Flexible granularity
- ✅ Hierarchical structure

**Cons**:
- ❌ Complex to implement
- ❌ Multiple LLM calls

---

## LLM-Based Summarization

### Prompt Engineering for Summaries

**Key principles for effective summarization prompts:**[^5][^18]

1. **Be specific about format**
2. **Focus on outcomes, not processes**
3. **Set length constraints**
4. **Request actionable information**

```typescript
const SUMMARIZATION_PROMPTS = {
  // Concise (one sentence)
  concise: (subgoal: string, actions: string) => `
Subgoal: ${subgoal}
Actions: ${actions}

Summarize in ONE sentence what was accomplished.
  `.trim(),

  // Detailed (multi-sentence)
  detailed: (subgoal: string, actions: string) => `
Subgoal: ${subgoal}
Actions: ${actions}

Provide a 2-3 sentence summary:
1. What was the goal?
2. What was accomplished?
3. What's the current state?
  `.trim(),

  // Structured
  structured: (subgoal: string, actions: string) => `
Subgoal: ${subgoal}
Actions: ${actions}

Provide a structured summary:
- Goal: [what was trying to be achieved]
- Outcome: [what was accomplished]
- Status: [completed/partial/failed]
- Next: [what should happen next]
  `.trim()
};

// Usage
async function summarize(
  subgoal: string,
  actions: string,
  style: keyof typeof SUMMARIZATION_PROMPTS = 'concise'
): Promise<string> {
  const prompt = SUMMARIZATION_PROMPTS[style](subgoal, actions);
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt
  });
  return result.text.trim();
}
```

### Model Selection

Research on summarization model performance:[^19][^20]

| Model | Speed (ms) | Quality | Cost ($/1M tokens) | Use Case |
|-------|-----------|---------|-------------------|----------|
| **GPT-4o-mini** | 500 | High | $0.15 | **Recommended** |
| GPT-4o | 1000 | Highest | $2.50 | Critical summaries |
| GPT-3.5-turbo | 300 | Medium | $0.50 | Simple summaries |
| Claude 3.5 Haiku | 400 | High | $0.25 | Alternative |

**Recommendation**: Use **GPT-4o-mini** for most summaries (good balance of speed/quality/cost).[^4]

---

## Extractive vs Abstractive

### Extractive Summarization (Rule-Based)

**Select key observations directly from action history.**[^21]

```typescript
function extractiveSummarize(
  actions: Array<{ action: string; observation: string }>
): string {
  // Extract only observations (ignore actions)
  const observations = actions.map(a => a.observation);

  // Select last observation (usually most relevant)
  const finalObservation = observations[observations.length - 1];

  // Pattern: If repeated actions, count them
  if (actions.length > 3) {
    const baseAction = actions[0].action.split('_')[0];
    if (actions.every(a => a.action.startsWith(baseAction))) {
      return `Completed ${actions.length} ${baseAction} actions: ${finalObservation}`;
    }
  }

  return finalObservation;
}

// Example
const summary = extractiveSummarize([
  { action: 'loosen_nut1', observation: 'Nut 1 loosened' },
  { action: 'loosen_nut2', observation: 'Nut 2 loosened' },
  { action: 'loosen_nut3', observation: 'Nut 3 loosened' },
  { action: 'loosen_nut4', observation: 'Nut 4 loosened' }
]);
// Output: "Completed 4 loosen actions: Nut 4 loosened"
```

**Pros**:
- ✅ Fast (no LLM call)
- ✅ Preserves exact wording
- ✅ Zero cost

**Cons**:
- ❌ Limited compression
- ❌ Less semantic
- ❌ May be redundant

### Abstractive Summarization (LLM-Based)

**Generate new text that captures the essence.**[^22][^23]

```typescript
async function abstractiveSummarize(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>
): Promise<string> {
  const prompt = `
Summarize this sequence in a natural, concise way:

Subgoal: ${subgoal}
${actions.map(a => `${a.action} → ${a.observation}`).join('\n')}

Generate ONE sentence that captures the outcome.
Use natural language, not a list of actions.
  `.trim();

  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt
  });

  return result.text.trim();
}

// Example
const summary = await abstractiveSummarize(
  'Loosen all nuts',
  [...]
);
// Output: "All wheel nuts were successfully loosened in preparation for lifting the car"
```

**Pros**:
- ✅ Natural language
- ✅ High compression
- ✅ Semantic understanding

**Cons**:
- ❌ Slower (LLM call)
- ❌ Costs tokens
- ❌ May hallucinate

### Hybrid Approach (Recommended)

```typescript
async function hybridSummarize(
  subgoal: string,
  actions: Array<{ action: string; observation: string }>
): Promise<string> {
  // Try extractive first (fast, free)
  const extractive = extractiveSummarize(actions);

  // If extractive is too long, use abstractive
  if (extractive.length > 100) {
    return await abstractiveSummarize(subgoal, actions);
  }

  return extractive;
}
```

---

## Implementation in TypeScript

### Complete Summarizer

```typescript
// summarizer.ts
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export interface SummarizationConfig {
  model?: 'gpt-4o' | 'gpt-4o-mini' | 'gpt-3.5-turbo';
  style?: 'concise' | 'detailed' | 'structured';
  maxLength?: number;
}

export class ActionSummarizer {
  private config: SummarizationConfig;

  constructor(config: SummarizationConfig = {}) {
    this.config = {
      model: 'gpt-4o-mini',
      style: 'concise',
      maxLength: 100,
      ...config
    };
  }

  async summarize(
    subgoal: string,
    actions: Array<{ action: string; observation: string }>
  ): Promise<string> {
    // Quick exit for small action sets
    if (actions.length === 1) {
      return actions[0].observation;
    }

    // Try extractive for simple patterns
    const extractive = this.tryExtractive(actions);
    if (extractive && extractive.length <= this.config.maxLength!) {
      return extractive;
    }

    // Fallback to LLM abstractive
    return await this.abstractiveSummarize(subgoal, actions);
  }

  private tryExtractive(
    actions: Array<{ action: string; observation: string }>
  ): string | null {
    // Pattern 1: Repeated action
    const baseAction = actions[0].action.split('_')[0];
    const allSameType = actions.every(a => a.action.startsWith(baseAction));

    if (allSameType) {
      const lastObs = actions[actions.length - 1].observation;
      return `Completed ${actions.length} ${baseAction} actions: ${lastObs}`;
    }

    // Pattern 2: Sequential steps
    if (actions.length <= 3) {
      return actions.map(a => a.observation).join('; ');
    }

    return null;
  }

  private async abstractiveSummarize(
    subgoal: string,
    actions: Array<{ action: string; observation: string }>
  ): Promise<string> {
    const actionText = actions
      .map(a => `${a.action} → ${a.observation}`)
      .join('\n');

    const prompts = {
      concise: `
Subgoal: ${subgoal}
Actions:
${actionText}

Summarize what was accomplished in ONE sentence.
      `.trim(),

      detailed: `
Subgoal: ${subgoal}
Actions:
${actionText}

Provide a 2-3 sentence summary of what was accomplished.
      `.trim(),

      structured: `
Subgoal: ${subgoal}
Actions:
${actionText}

Provide:
- Outcome: [what was achieved]
- Status: [completed/partial]
- Next: [what's next]
      `.trim()
    };

    const prompt = prompts[this.config.style || 'concise'];

    const result = await generateText({
      model: openai(this.config.model || 'gpt-4o-mini'),
      prompt
    });

    return result.text.trim();
  }

  // Batch summarization (more efficient)
  async summarizeBatch(
    items: Array<{
      subgoal: string;
      actions: Array<{ action: string; observation: string }>;
    }>
  ): Promise<string[]> {
    // Process all in one LLM call
    const batchPrompt = items.map((item, i) => `
${i + 1}. Subgoal: ${item.subgoal}
Actions: ${item.actions.map(a => `${a.action} → ${a.observation}`).join('; ')}
    `).join('\n');

    const fullPrompt = `
Summarize each of these subgoals in ONE sentence each:
${batchPrompt}

Respond with numbered list:
1. [summary]
2. [summary]
...
    `.trim();

    const result = await generateText({
      model: openai(this.config.model || 'gpt-4o-mini'),
      prompt: fullPrompt
    });

    // Parse numbered list
    const lines = result.text.trim().split('\n');
    return lines.map(line => line.replace(/^\d+\.\s*/, '').trim());
  }
}
```

### Usage Example

```typescript
// example.ts
import { ActionSummarizer } from './summarizer';

async function demonstrateSummarization() {
  const summarizer = new ActionSummarizer({
    model: 'gpt-4o-mini',
    style: 'concise'
  });

  // Example 1: Simple repeated actions
  const summary1 = await summarizer.summarize(
    'Loosen all nuts',
    [
      { action: 'loosen_nut1', observation: 'Nut 1 loosened' },
      { action: 'loosen_nut2', observation: 'Nut 2 loosened' },
      { action: 'loosen_nut3', observation: 'Nut 3 loosened' },
      { action: 'loosen_nut4', observation: 'Nut 4 loosened' }
    ]
  );
  console.log('Summary 1:', summary1);
  // Output: "Completed 4 loosen actions: Nut 4 loosened" (extractive)

  // Example 2: Complex actions
  const summary2 = await summarizer.summarize(
    'Prepare tools',
    [
      { action: 'open_boot', observation: 'Boot opened' },
      { action: 'locate_jack', observation: 'Jack found in compartment' },
      { action: 'retrieve_jack', observation: 'Jack retrieved' },
      { action: 'get_wrench', observation: 'Wrench obtained' }
    ]
  );
  console.log('Summary 2:', summary2);
  // Output: "Retrieved jack and wrench from boot" (abstractive)

  // Example 3: Batch summarization
  const batchSummaries = await summarizer.summarizeBatch([
    {
      subgoal: 'Loosen nuts',
      actions: [...]
    },
    {
      subgoal: 'Jack up car',
      actions: [...]
    }
  ]);
  console.log('Batch:', batchSummaries);
}
```

---

## Quality and Validation

### Summary Quality Metrics

Research on evaluating summarization quality:[^24][^25]

```typescript
interface SummaryQualityMetrics {
  completeness: number;    // 0-1 (missing info → complete)
  conciseness: number;     // 0-1 (verbose → concise)
  accuracy: number;        // 0-1 (incorrect → accurate)
  actionability: number;   // 0-1 (vague → actionable)
}

async function evaluateSummaryQuality(
  summary: string,
  originalActions: Array<{ action: string; observation: string }>
): Promise<SummaryQualityMetrics> {
  const QualitySchema = z.object({
    completeness: z.number().min(0).max(1),
    conciseness: z.number().min(0).max(1),
    accuracy: z.number().min(0).max(1),
    actionability: z.number().min(0).max(1),
    reasoning: z.string()
  });

  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: QualitySchema,
    prompt: `
Original actions:
${originalActions.map(a => `${a.action} → ${a.observation}`).join('\n')}

Summary: "${summary}"

Evaluate the summary quality (0-1 scale):
- Completeness: Does it capture key outcomes?
- Conciseness: Is it brief without being vague?
- Accuracy: Is it factually correct?
- Actionability: Does it provide useful context?
    `.trim()
  });

  return {
    completeness: object.completeness,
    conciseness: object.conciseness,
    accuracy: object.accuracy,
    actionability: object.actionability
  };
}
```

### Validation Strategies

```typescript
class SummaryValidator {
  async validate(
    summary: string,
    originalActions: string[]
  ): Promise<{ valid: boolean; issues: string[] }> {
    const issues: string[] = [];

    // Check 1: Length (should be compressed)
    const originalLength = originalActions.join(' ').length;
    const compressionRatio = originalLength / summary.length;
    
    if (compressionRatio < 2) {
      issues.push('Insufficient compression (ratio < 2:1)');
    }

    // Check 2: Not too short (should have substance)
    if (summary.split(' ').length < 5) {
      issues.push('Summary too short (< 5 words)');
    }

    // Check 3: Contains actionable information
    if (!/\b(completed|achieved|retrieved|loosened|positioned)\b/i.test(summary)) {
      issues.push('Lacks action verbs');
    }

    return {
      valid: issues.length === 0,
      issues
    };
  }
}
```

---

## References

[^1]: Hu, M. et al. (2025). "HiAgent: Hierarchical Working Memory Management." *ACL 2025*. https://aclanthology.org/2025.acl-long.1575.pdf

[^2]: "LLM Chat History Summarization Guide 2025." *Mem0* (2025). https://mem0.ai/blog/llm-chat-history-summarization-guide-2025

[^3]: "Master LLM Summarization Strategies and their Implementations." *Galileo AI* (2025). https://galileo.ai/blog/llm-summarization-strategies

[^4]: "LLM Summarization: Techniques, Metrics, and Top Models." *ProjectPro* (2025). https://www.projectpro.io/article/llm-summarization/1082

[^5]: "LLM Summarization of Large Documents: How to Make It Work." *Belitsoft* (2025). https://belitsoft.com/llm-summarization

[^6]: "Evaluating LLMs for Text Summarization." *Carnegie Mellon SEI* (2025). https://www.sei.cmu.edu/blog/evaluating-llms-for-text-summarization-introduction

[^7]: "Context Engineering for Agents." *LangChain Blog* (2025). https://blog.langchain.com/context-engineering-for-agents/

[^8]: "Deep Dive into Context Engineering for Agents." *Galileo AI* (2025). https://galileo.ai/blog/context-engineering-for-agents

[^9]: "Effective Context Engineering for AI Agents." *Anthropic* (2025). https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents

[^10]: "Agent Memory: How to Build Agents that Learn and Remember." *Letta* (2025). https://www.letta.com/blog/agent-memory

[^11]: "Context Engineering - Short-Term Memory Management." *OpenAI Cookbook* (2025). https://cookbook.openai.com/examples/agents_sdk/session_memory

[^12]: "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference." *ICML 2024*. https://arxiv.org/abs/2403.09636

[^13]: "LLM Chat History Summarization: Best Practices." *Mem0* (2025). https://mem0.ai/blog/llm-chat-history-summarization-guide-2025

[^14]: "Task Memory Engine: Enhancing State Awareness." *arXiv:2504.08525* (2025). https://arxiv.org/abs/2504.08525

[^15]: "Optimus-1: Hybrid Multimodal Memory Empowered Agents." *arXiv:2408.03615* (2024). https://arxiv.org/abs/2408.03615

[^16]: "Scaling Long-Horizon LLM Agent via Context-Folding." *arXiv:2510.11967* (2025). https://arxiv.org/abs/2510.11967

[^17]: "ReCAP: Recursive Context-Aware Reasoning and Planning." *arXiv:2510.23822* (2025). https://arxiv.org/abs/2510.23822

[^18]: "Adapting LLMs for Efficient Context Processing through Soft Prompt Compression." *arXiv:2404.04997* (2024). https://arxiv.org/abs/2404.04997

[^19]: "TriSum: Learning Summarization Ability from LLMs with Structured Rationale." *NAACL 2024*. https://aclanthology.org/2024.naacl-long.154/

[^20]: "Prompt Compression in Large Language Models." *Medium* (2025). https://medium.com/@sahin.samia/prompt-compression-in-large-language-models-llms

[^21]: "Characterizing Prompt Compression Methods for Long Context Inference." *arXiv:2407.08892* (2024). https://arxiv.org/abs/2407.08892

[^22]: "A review of state-of-the-art techniques for large language model compression." *Springer* (2025). https://link.springer.com/article/10.1007/s40747-025-02019-z

[^23]: "A Comprehensive Survey on Process-Oriented Automatic Text Summarization." *arXiv:2403.02901* (2024). https://arxiv.org/abs/2403.02901

[^24]: "A Systematic Survey of Text Summarization: From Statistical Methods to LLMs." *arXiv:2406.11289* (2024). https://arxiv.org/pdf/2406.11289

[^25]: "Can Compressed LLMs Truly Act? Empirical Evaluation of Agentic Capabilities." *arXiv:2505.19433* (2024). https://arxiv.org/html/2505.19433v2

---

**Next Topic**: [4.2.5 Achieving 10:1 Compression Ratios](./4.2.5-compression-ratios.md)

**Related Topics**:
- [4.2.1 HiAgent Hierarchical Memory](./4.2.1-hiagent-hierarchical-memory.md)
- [4.2.2 Compression Triggers](./4.2.2-compression-triggers.md)
- [4.2.3 Subgoal Detection](./4.2.3-subgoal-detection.md)
