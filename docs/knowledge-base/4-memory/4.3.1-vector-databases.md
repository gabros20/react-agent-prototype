# 4.3.1 - Vector Databases for Long-Term Memory

## TL;DR

**Vector databases store high-dimensional embeddings and enable semantic similarity search, forming the foundation of AI agent long-term memory.** They allow agents to retrieve relevant information based on meaning rather than exact keyword matching, achieving 26% accuracy improvement and sub-100ms retrieval latency at scale.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [4.0.1 Memory Systems Overview](./4.0.1-memory-systems-overview.md), [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md)
- **Grounded In**: Mem0 (2025), Pinecone, LanceDB, Weaviate documentation (2024-2025)

## Table of Contents

- [Overview](#overview)
- [The Problem: Keyword Search Limitations](#the-problem-keyword-search-limitations)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

---

## Overview

Vector databases are specialized storage systems optimized for high-dimensional vector data (embeddings). Unlike traditional databases that match exact values, vector databases find similar items by computing distance in embedding space, enabling semantic search that understands meaning.

For AI agents, this means the difference between "find documents containing 'car'" and "find documents about vehicles, automobiles, or transportation"—the latter captures intent rather than just keywords.

**Key Research Findings (2024-2025)**:

- **Mem0**: 26% accuracy improvement over full-context retrieval
- **Pinecone**: Sub-50ms query latency at 100M+ vectors
- **LanceDB**: Zero-copy, embedded vector DB with 10x cost reduction
- **Cost**: $0.01-0.05 per query vs $0.10+ for full-context LLM calls

---

## The Problem: Keyword Search Limitations

### The Classic Challenge

Traditional search fails when users express the same concept differently:

```
User Query: "How do I fix my car?"

Keyword Search Results:
❌ "Vehicle maintenance guide" (no "car" or "fix")
❌ "Automobile repair manual" (no "car" or "fix")
❌ "Troubleshooting your sedan" (no "car" or "fix")
✅ "How to fix your car" (exact match)

Vector Search Results:
✅ "Vehicle maintenance guide" (semantically similar)
✅ "Automobile repair manual" (semantically similar)
✅ "Troubleshooting your sedan" (semantically similar)
✅ "How to fix your car" (exact + semantic match)
```

**Problems with Keyword Search**:

- ❌ **Synonym blindness**: "car" ≠ "vehicle" ≠ "automobile"
- ❌ **Context ignorance**: Can't understand intent or meaning
- ❌ **Relevance ranking**: Frequency-based, not meaning-based
- ❌ **Cross-lingual failure**: "coche" (Spanish) ≠ "car"

### Why This Matters for Agents

AI agents need to:
- Retrieve relevant memories even when phrased differently
- Find similar past conversations without exact matches
- Connect related concepts across sessions
- Build knowledge graphs from semantic relationships

---

## Core Concept

### What is a Vector Database?

A vector database stores data as high-dimensional vectors (embeddings) and enables efficient similarity search using distance metrics.

### How It Works

```
┌─────────────────────────────────────────────────────────────┐
│                    VECTOR DATABASE FLOW                      │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. INDEXING (Write Path)                                   │
│                                                              │
│     Text: "User prefers dark mode"                          │
│              ↓                                               │
│     Embedding Model (text-embedding-3-small)                │
│              ↓                                               │
│     Vector: [0.12, -0.34, 0.56, ..., 0.78]  (1536 dims)    │
│              ↓                                               │
│     Store in Index (HNSW, IVF, etc.)                        │
│                                                              │
│  2. QUERYING (Read Path)                                    │
│                                                              │
│     Query: "What are the user's preferences?"               │
│              ↓                                               │
│     Embedding Model                                          │
│              ↓                                               │
│     Query Vector: [0.15, -0.32, 0.54, ..., 0.80]           │
│              ↓                                               │
│     Similarity Search (cosine distance)                      │
│              ↓                                               │
│     Results: "User prefers dark mode" (0.92 similarity)     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Characteristics

| Property | Description |
|----------|-------------|
| **Embedding Storage** | Stores vectors (arrays of floats) |
| **Similarity Search** | Finds nearest neighbors by distance |
| **Indexing** | HNSW, IVF, PQ for fast retrieval |
| **Metadata Filtering** | Combine vector + attribute filters |
| **Scalability** | Millions to billions of vectors |

### Distance Metrics

| Metric | Formula | Best For |
|--------|---------|----------|
| **Cosine** | 1 - cos(θ) | Text embeddings, normalized vectors |
| **Euclidean (L2)** | √Σ(a-b)² | Image embeddings, absolute distance |
| **Dot Product** | Σ(a×b) | Recommendation systems, MaxSim |

---

## Implementation Patterns

### Pattern 1: LanceDB (Embedded, Zero-Cost)

**Use Case**: Local development, edge deployments, cost-sensitive applications

```typescript
import lancedb from 'lancedb';
import { OpenAI } from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Connect to local database (embedded mode - like SQLite)
const db = await lancedb.connect('./lancedb');

// Create table with schema
const table = await db.createTable('memories', [
  {
    id: 'mem_1',
    user_id: 'user_123',
    content: 'User prefers TypeScript over JavaScript',
    vector: await embed('User prefers TypeScript over JavaScript'),
    created_at: new Date().toISOString(),
    category: 'preference',
  },
]);

// Embed text using OpenAI
async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}

// Search with metadata filters
async function search(query: string, userId: string, limit = 5) {
  const queryVector = await embed(query);

  return await table
    .search(queryVector)
    .where(`user_id = '${userId}'`)
    .limit(limit)
    .execute();
}

// Usage
const results = await search('What programming language does the user like?', 'user_123');
// Returns: "User prefers TypeScript over JavaScript" (high similarity)
```

**Pros**:
- ✅ Zero infrastructure cost (embedded)
- ✅ No separate server required
- ✅ Fast local queries (<20ms)
- ✅ Multimodal support (text, images, audio)

**Cons**:
- ❌ Single-process only
- ❌ No enterprise SLA
- ❌ Limited to local storage

**When to Use**: Development, prototypes, desktop apps, edge deployments

---

### Pattern 2: Pinecone (Managed, Enterprise)

**Use Case**: Production deployments, multi-tenant systems, high availability

```typescript
import { Pinecone } from '@pinecone-database/pinecone';
import { OpenAI } from 'openai';

const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const index = pinecone.index('agent-memory');

// Upsert memories with namespace for multi-tenancy
async function storeMemory(userId: string, content: string, metadata: Record<string, any>) {
  const embedding = await embed(content);

  await index.namespace(userId).upsert([
    {
      id: `${userId}_${Date.now()}`,
      values: embedding,
      metadata: {
        content,
        created_at: new Date().toISOString(),
        ...metadata,
      },
    },
  ]);
}

// Query with metadata filtering
async function queryMemories(userId: string, query: string, topK = 5) {
  const queryEmbedding = await embed(query);

  const results = await index.namespace(userId).query({
    vector: queryEmbedding,
    topK,
    includeMetadata: true,
  });

  return results.matches.map(match => ({
    content: match.metadata?.content,
    score: match.score,
    metadata: match.metadata,
  }));
}

async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}

// Usage
await storeMemory('user_123', 'User allergic to peanuts', { category: 'medical' });
const results = await queryMemories('user_123', 'food allergies');
```

**Pros**:
- ✅ Fully managed (zero DevOps)
- ✅ 99.99% uptime SLA
- ✅ Scales to billions of vectors
- ✅ Multi-tenant isolation via namespaces

**Cons**:
- ❌ Cost at scale ($70+/month starting)
- ❌ Cloud-only (no self-hosted)
- ❌ Vendor lock-in

**When to Use**: Production SaaS, enterprise applications

---

### Pattern 3: PostgreSQL + pgvector (Hybrid)

**Use Case**: Existing PostgreSQL infrastructure, combined relational + vector queries

```typescript
import { Pool } from 'pg';
import { OpenAI } from 'openai';

const pool = new Pool({ connectionString: process.env.DATABASE_URL });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Setup (run once)
await pool.query(`
  CREATE EXTENSION IF NOT EXISTS vector;

  CREATE TABLE IF NOT EXISTS memories (
    id SERIAL PRIMARY KEY,
    user_id TEXT NOT NULL,
    content TEXT NOT NULL,
    embedding vector(1536),
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
  );

  CREATE INDEX ON memories USING ivfflat (embedding vector_cosine_ops);
`);

// Add memory
async function addMemory(userId: string, content: string, metadata: Record<string, any>) {
  const embedding = await embed(content);

  await pool.query(
    `INSERT INTO memories (user_id, content, embedding, metadata)
     VALUES ($1, $2, $3, $4)`,
    [userId, content, JSON.stringify(embedding), JSON.stringify(metadata)]
  );
}

// Semantic search with SQL
async function searchMemories(userId: string, query: string, limit = 5) {
  const queryEmbedding = await embed(query);

  const result = await pool.query(
    `SELECT content, metadata, 1 - (embedding <=> $1) AS similarity
     FROM memories
     WHERE user_id = $2
     ORDER BY embedding <=> $1
     LIMIT $3`,
    [JSON.stringify(queryEmbedding), userId, limit]
  );

  return result.rows;
}

async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}
```

**Pros**:
- ✅ Uses existing PostgreSQL
- ✅ ACID transactions
- ✅ Combined SQL + vector queries
- ✅ No additional infrastructure

**Cons**:
- ❌ Slower than dedicated vector DBs at scale
- ❌ Limited index options
- ❌ Requires tuning for performance

**When to Use**: Existing PostgreSQL users, need for transactions

---

### Pattern 4: Weaviate (Open Source, Hybrid Search)

**Use Case**: Hybrid search (vector + keyword), self-hosted deployments

```typescript
import weaviate from 'weaviate-ts-client';
import { OpenAI } from 'openai';

const client = weaviate.client({
  scheme: 'https',
  host: 'your-cluster.weaviate.network',
  apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY),
});

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Hybrid search (vector + keyword)
async function hybridSearch(query: string, userId: string, topK = 5) {
  const queryVector = await embed(query);

  const result = await client.graphql
    .get()
    .withClassName('UserMemory')
    .withFields('content category _additional { score }')
    .withHybrid({
      query: query,         // Keyword search
      vector: queryVector,  // Vector search
      alpha: 0.5,           // 0 = keyword only, 1 = vector only
    })
    .withWhere({
      path: ['userId'],
      operator: 'Equal',
      valueString: userId,
    })
    .withLimit(topK)
    .execute();

  return result.data.Get.UserMemory;
}

async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}
```

**Pros**:
- ✅ Best hybrid search (vector + BM25)
- ✅ Open source (self-hosted option)
- ✅ GraphQL API
- ✅ Multi-modal support

**Cons**:
- ❌ More complex setup
- ❌ Higher latency than Pinecone
- ❌ Requires DevOps for self-hosting

**When to Use**: Hybrid search requirements, open-source preference

---

## Framework Integration

### AI SDK v6 with Vector Memory Tools

```typescript
import { generateText, tool, stepCountIs } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import lancedb from 'lancedb';

const db = await lancedb.connect('./lancedb');
const memoriesTable = await db.openTable('memories');

// Memory retrieval tool
const retrieveMemories = tool({
  description: 'Search long-term memory for relevant information about the user',
  inputSchema: z.object({
    query: z.string().describe('Search query for memory retrieval'),
    userId: z.string().describe('User ID to filter memories'),
    limit: z.number().default(5).describe('Maximum results to return'),
  }),
  execute: async ({ query, userId, limit }) => {
    const queryEmbedding = await embed(query);

    const results = await memoriesTable
      .search(queryEmbedding)
      .where(`user_id = '${userId}'`)
      .limit(limit)
      .execute();

    return {
      memories: results.map(r => ({
        content: r.content,
        similarity: r._distance,
        category: r.category,
      })),
      count: results.length,
    };
  },
});

// Memory storage tool
const storeMemory = tool({
  description: 'Store important information in long-term memory',
  inputSchema: z.object({
    userId: z.string().describe('User ID'),
    content: z.string().describe('Information to remember'),
    category: z.enum(['preference', 'fact', 'event', 'relationship']),
  }),
  execute: async ({ userId, content, category }) => {
    const embedding = await embed(content);

    await memoriesTable.add([
      {
        id: `mem_${Date.now()}`,
        user_id: userId,
        content,
        vector: embedding,
        category,
        created_at: new Date().toISOString(),
      },
    ]);

    return { success: true, message: `Stored: "${content}"` };
  },
});

// Agent with memory tools
const { text, steps } = await generateText({
  model: openai('gpt-4o'),
  tools: { retrieveMemories, storeMemory },
  stopWhen: stepCountIs(10),
  prompt: 'The user said they prefer dark mode. Store this preference and confirm.',
});

async function embed(text: string): Promise<number[]> {
  const openaiClient = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const response = await openaiClient.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}
```

---

## Research & Benchmarks

### Vector Database Comparison (2025)

| Database | Query Latency (p95) | Scale | Cost/Month | Best For |
|----------|---------------------|-------|------------|----------|
| **LanceDB** | <20ms | 1B+ vectors | $0 (OSS) | Local/embedded |
| **Pinecone** | <50ms | 1B+ vectors | $70+ | Enterprise SaaS |
| **Weaviate** | <100ms | 100M+ vectors | $25+ | Hybrid search |
| **pgvector** | 30-100ms | 10M vectors | PostgreSQL cost | Existing PG |
| **Qdrant** | <5ms | 100M+ vectors | Self-hosted | High performance |
| **Chroma** | 10-30ms | 100M vectors | Self-hosted | Simple setup |

### Embedding Model Comparison

| Model | Dimensions | Cost/1M tokens | Quality |
|-------|------------|----------------|---------|
| **text-embedding-3-small** | 1536 | $0.02 | Good |
| **text-embedding-3-large** | 3072 | $0.13 | Excellent |
| **Cohere embed-v3** | 1024 | $0.10 | Excellent |
| **Voyage AI** | 1024 | $0.10 | Excellent |

### Memory System Performance (Mem0 Benchmarks)

| Metric | Full Context | Vector Memory | Improvement |
|--------|--------------|---------------|-------------|
| **Accuracy** | 52.9% | 66.9% | **+26%** |
| **Latency (p95)** | 17.12s | 1.44s | **-91%** |
| **Tokens/query** | 26K | 1.8K | **-93%** |

### Cost Comparison (10M Vectors, 1M Queries/Month)

| Database | Storage | Queries | Total/Month |
|----------|---------|---------|-------------|
| **LanceDB (OSS)** | $15 (S3) | $0 | ~$15 |
| **LanceDB Cloud** | $20 | $0.40 | ~$20 |
| **Weaviate Cloud** | $60 | $10 | ~$70 |
| **Pinecone** | $175 | Included | ~$175 |

---

## When to Use This Pattern

### ✅ Use Vector Databases When:

1. **Semantic search required**
   - Finding related content by meaning
   - "Show me documents about machine learning" → finds "AI", "neural networks"

2. **Long-term memory needed**
   - Persisting user preferences across sessions
   - Building knowledge over time

3. **RAG (Retrieval-Augmented Generation)**
   - Augmenting LLM with external knowledge
   - Reducing hallucinations

4. **Recommendation systems**
   - Finding similar items
   - Personalized suggestions

### ❌ Consider Alternatives When:

1. **Exact match required**
   - Use traditional SQL indexes
   - Faster for exact lookups

2. **Small dataset (<1000 items)**
   - In-memory search may be sufficient
   - Overhead not justified

3. **Real-time writes critical**
   - Vector indexing adds latency
   - Consider eventual consistency

### Database Selection Decision Matrix

| Scenario | LanceDB | Pinecone | pgvector | Weaviate |
|----------|---------|----------|----------|----------|
| **Development** | ✅ Best | ⚠️ Costly | ✅ Good | ⚠️ Complex |
| **Production SaaS** | ⚠️ Limited | ✅ Best | ✅ Good | ✅ Good |
| **Self-hosted** | ✅ Best | ❌ No | ✅ Yes | ✅ Best |
| **Hybrid search** | ❌ No | ✅ Yes | ❌ Limited | ✅ Best |
| **Cost-sensitive** | ✅ Best | ❌ Expensive | ✅ Good | ⚠️ Medium |

---

## Production Best Practices

### 1. Batch Embeddings (10x Cost Reduction)

```typescript
// ❌ Bad: One embedding per call (100 API calls)
for (const text of texts) {
  await openai.embeddings.create({ model: '...', input: text });
}

// ✅ Good: Batch embeddings (1 API call)
const response = await openai.embeddings.create({
  model: 'text-embedding-3-small',
  input: texts, // Array of up to 2048 inputs
});
```

### 2. Cache Embeddings

```typescript
const embeddingCache = new Map<string, number[]>();

async function embedWithCache(text: string): Promise<number[]> {
  const cacheKey = text.toLowerCase().trim();

  if (embeddingCache.has(cacheKey)) {
    return embeddingCache.get(cacheKey)!;
  }

  const embedding = await embed(text);
  embeddingCache.set(cacheKey, embedding);
  return embedding;
}
```

### 3. Multi-Tenancy with Namespaces

```typescript
// ✅ Good: Isolated namespaces per user (Pinecone)
const userIndex = index.namespace(`user_${userId}`);
await userIndex.upsert([...]);
await userIndex.query({ vector, topK: 5 });

// ❌ Bad: Single namespace with metadata filter (slower, risky)
await index.query({
  filter: { user_id: userId },
  vector,
  topK: 5,
});
```

### 4. Monitor Performance

```typescript
async function searchWithMetrics(query: string, userId: string) {
  const start = performance.now();

  const results = await search(query, userId);

  const latency = performance.now() - start;
  console.log({
    operation: 'vector_search',
    latency_ms: latency,
    results_count: results.length,
    user_id: userId,
  });

  return results;
}
```

### Common Pitfalls

#### ❌ Pitfall: Inconsistent Embedding Models

```typescript
// ❌ Bad: Different models produce incompatible vectors
const vector1 = await embed_ada002(text);    // 1536 dims
const vector2 = await embed_3_large(query);  // 3072 dims
// Can't compare!

// ✅ Good: Always use the same model
const EMBEDDING_MODEL = 'text-embedding-3-small';
```

#### ❌ Pitfall: Storing Too Much in Metadata

```typescript
// ❌ Bad: Full document in metadata (expensive)
const metadata = { full_document: '10,000 characters...' };

// ✅ Good: Store only references
const metadata = { document_id: 'doc123', summary: 'Brief summary...' };
```

#### ❌ Pitfall: No Cleanup Strategy

```typescript
// ❌ Bad: Never delete old data
// Costs accumulate forever!

// ✅ Good: Periodic cleanup
async function cleanupOldMemories(userId: string, daysOld = 90) {
  const cutoff = new Date(Date.now() - daysOld * 24 * 60 * 60 * 1000);
  await index.namespace(userId).deleteMany({
    filter: { timestamp: { $lt: cutoff.toISOString() } },
  });
}
```

---

## Key Takeaways

1. **Vector databases enable semantic search** - Find by meaning, not just keywords
2. **Choose based on scale and cost** - LanceDB for dev, Pinecone for enterprise
3. **Use text-embedding-3-small** - Best cost/quality ratio for most use cases
4. **Combine with metadata filtering** - Vector + SQL for precise results
5. **Batch and cache embeddings** - 10x cost reduction possible

**Quick Implementation Checklist**:

- [ ] Choose vector database (LanceDB dev, Pinecone prod)
- [ ] Select embedding model (text-embedding-3-small recommended)
- [ ] Design metadata schema (user_id, category, timestamp)
- [ ] Implement batch embedding for bulk operations
- [ ] Add embedding caching for repeated queries
- [ ] Set up multi-tenancy (namespaces or table separation)
- [ ] Plan cleanup strategy (TTL, periodic deletion)
- [ ] Monitor latency and costs

---

## References

1. **Mem0 Team** (2025). "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory". arXiv. https://arxiv.org/abs/2504.19413
2. **Pinecone** (2025). "Optimizing Pinecone for Agents". https://www.pinecone.io/blog/optimizing-pinecone/
3. **LanceDB** (2025). "The Future of AI-Native Development is Local". https://lancedb.com/blog/
4. **Weaviate** (2025). "Vector Database for AI Applications". https://weaviate.io/
5. **OpenAI** (2025). "Embeddings API Documentation". https://platform.openai.com/docs/guides/embeddings
6. **pgvector** (2025). "Open-source vector similarity search for Postgres". https://github.com/pgvector/pgvector
7. **LakeFS Blog** (2025). "Best Vector Databases for 2025". https://lakefs.io/blog/best-vector-databases/

**Related Topics**:

- [4.3.2 Semantic Search](./4.3.2-semantic-search.md)
- [4.3.3 Fact Extraction](./4.3.3-fact-extraction.md)
- [4.3.4 Cross-Session Retrieval](./4.3.4-cross-session-retrieval.md)
- [4.3.5 When to Use Long-Term Memory](./4.3.5-when-to-use.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
