# 4.3.1 Vector Databases (LanceDB, Pinecone, Weaviate)

**Layer**: 4 - Memory & State  
**Sublayer**: 4.3 - Episodic Memory (Long-Term)  
**Audience**: Intermediate to Advanced  
**Updated**: 2025-11-18

---

## Table of Contents

- [Overview](#overview)
- [Why Vector Databases?](#why-vector-databases)
- [Vector Database Architecture](#vector-database-architecture)
- [Top 3 Vector Databases for 2025](#top-3-vector-databases-for-2025)
  - [LanceDB - Embedded & Serverless](#lancedb---embedded--serverless)
  - [Pinecone - Managed Production](#pinecone---managed-production)
  - [Weaviate - Open Source Hybrid](#weaviate---open-source-hybrid)
- [Comparison Matrix](#comparison-matrix)
- [Implementation Examples](#implementation-examples)
  - [LanceDB Setup (TypeScript)](#lancedb-setup-typescript)
  - [Pinecone Setup (TypeScript)](#pinecone-setup-typescript)
  - [Weaviate Setup (TypeScript)](#weaviate-setup-typescript)
- [Production Patterns](#production-patterns)
  - [Multi-Tenancy with Namespaces](#multi-tenancy-with-namespaces)
  - [Hybrid Search Implementation](#hybrid-search-implementation)
  - [Cost Optimization Strategies](#cost-optimization-strategies)
- [When to Use Each Database](#when-to-use-each-database)
- [Performance Benchmarks](#performance-benchmarks)
- [Best Practices](#best-practices)
- [Common Pitfalls](#common-pitfalls)
- [Related Topics](#related-topics)
- [References](#references)

---

## Overview

Vector databases are specialized data stores optimized for storing, indexing, and querying high-dimensional vector embeddings. Unlike traditional databases that search for exact matches, vector databases enable **semantic similarity search** - finding data points that are conceptually similar based on their vector representations[^1].

**Why This Matters for AI Agents**:
- **Long-term Memory**: Store facts, experiences, and knowledge across sessions
- **RAG Systems**: Retrieve relevant context to augment LLM responses
- **Semantic Search**: Find information based on meaning, not exact keywords
- **Personalization**: Remember user preferences and interaction history
- **Cross-Session Context**: Maintain state and knowledge across multiple conversations

**Key Concepts**:
- **Vector Embeddings**: Numerical representations (e.g., 384, 768, 1536 dimensions) that capture semantic meaning
- **Similarity Search**: Finding nearest neighbors using distance metrics (cosine, dot product, Euclidean)
- **Indexing Algorithms**: HNSW, IVF, FAISS for efficient approximate nearest neighbor (ANN) search
- **Metadata Filtering**: Combining vector search with structured filters (user_id, timestamp, category)

---

## Why Vector Databases?

### The Problem with Traditional Databases

Traditional relational databases excel at exact keyword matching but fail at semantic understanding:

```sql
-- Traditional DB: Finds exact match only
SELECT * FROM documents WHERE title = 'refund policy';

-- Misses semantically similar content:
-- âŒ "money back guarantee"
-- âŒ "return procedure"
-- âŒ "cancellation terms"
```

### Vector Database Solution

Vector databases understand **meaning** through embeddings:

```typescript
// Vector DB: Finds semantically similar content
const query = await embed("refund policy"); // [0.23, -0.45, 0.87, ...]
const results = await vectorDB.search(query, { topK: 5 });

// âœ… Returns all semantically related documents:
// - "money back guarantee" (similarity: 0.92)
// - "return procedure" (similarity: 0.89)
// - "cancellation terms" (similarity: 0.87)
```

### Use Cases for AI Agents

1. **RAG-Powered Chatbots**: Retrieve relevant company documents to answer user questions[^2]
2. **Code Search**: Find similar code patterns or functions (Continue.dev uses LanceDB)[^3]
3. **Recommendation Systems**: Suggest similar products, content, or actions
4. **Multi-Session Memory**: Store user preferences, interaction history, and learned facts[^4]
5. **Agentic Workloads**: Millions of small namespaces (under 100k vectors each) for isolated tenant memory[^5]

---

## Vector Database Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vector Database                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. STORAGE LAYER                                        â”‚
â”‚     â€¢ Raw vectors (high-dimensional arrays)              â”‚
â”‚     â€¢ Metadata (JSON fields: user_id, timestamp, etc.)   â”‚
â”‚     â€¢ Original content (optional: text, images, audio)   â”‚
â”‚                                                          â”‚
â”‚  2. INDEXING LAYER                                       â”‚
â”‚     â€¢ HNSW (Hierarchical Navigable Small World)          â”‚
â”‚     â€¢ IVF (Inverted File Index)                          â”‚
â”‚     â€¢ FAISS (Facebook AI Similarity Search)              â”‚
â”‚                                                          â”‚
â”‚  3. QUERY LAYER                                          â”‚
â”‚     â€¢ Approximate Nearest Neighbor (ANN) search          â”‚
â”‚     â€¢ Metadata filtering                                 â”‚
â”‚     â€¢ Hybrid search (vector + keyword)                   â”‚
â”‚                                                          â”‚
â”‚  4. RETRIEVAL LAYER                                      â”‚
â”‚     â€¢ Top-K selection                                    â”‚
â”‚     â€¢ Reranking (optional)                               â”‚
â”‚     â€¢ Post-processing                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Similarity Metrics

1. **Cosine Similarity** (most common for text embeddings)[^6]:
   ```typescript
   cosine(A, B) = (A Â· B) / (||A|| Ã— ||B||)
   // Range: [-1, 1] where 1 = identical, 0 = orthogonal, -1 = opposite
   ```

2. **Dot Product** (faster, assumes normalized vectors):
   ```typescript
   dotProduct(A, B) = A Â· B
   // Higher value = more similar
   ```

3. **Euclidean Distance** (straight-line distance):
   ```typescript
   euclidean(A, B) = sqrt(Î£(Ai - Bi)Â²)
   // Lower distance = more similar
   ```

---

## Top 3 Vector Databases for 2025

### LanceDB - Embedded & Serverless

**Best For**: Local development, edge deployments, TypeScript/JavaScript projects[^7]

**Key Features**:
- **Embedded** (like SQLite): No separate server required, runs in your Node.js/Deno process
- **Serverless Cloud** option available for production
- **Zero Configuration**: Just `npm install lancedb` and start coding
- **Multimodal**: Store text, images, audio, video embeddings in one unified format (Lance columnar format)[^8]
- **Fast Random Access**: 100x faster than Parquet for point queries[^9]
- **Open Source**: Apache 2.0 license, 8k+ GitHub stars

**Pricing** (2025):
- **OSS (Self-Hosted)**: Free
- **LanceDB Cloud**: Serverless, pay-per-query (~$0.40 per 1M queries)[^10]

**Performance**:
- Query Latency: <20ms (SSD-based architecture)
- Scalability: Handles billions of vectors
- Indexing: Inverted File Index + Product Quantization

**Why Choose LanceDB**:
- âœ… Perfect for **prototyping** and **local development**
- âœ… **TypeScript-native** with excellent DX (Developer Experience)
- âœ… **Cost-effective** for small to medium-scale projects
- âœ… Works **offline** (embedded mode)
- âœ… **Multimodal** support (text, images, audio)

**When to Avoid**:
- âŒ Need enterprise SLA guarantees (99.99% uptime)
- âŒ Require advanced security features (SOC2, HIPAA)
- âŒ Need managed scaling for extreme loads (billions of queries/day)

---

### Pinecone - Managed Production

**Best For**: Enterprise production applications, high availability, minimal operational overhead[^11]

**Key Features**:
- **Fully Managed**: No infrastructure to manage, automatic scaling
- **Serverless Architecture**: Pay only for storage and queries, no idle costs[^12]
- **Low Latency**: p95 latency <50ms, sub-20ms median[^13]
- **99.99% Uptime SLA**: Production-grade reliability
- **Advanced Features**:
  - Hybrid search (vector + keyword)
  - Metadata filtering (150+ supported types)
  - Sparse-dense embeddings
  - Namespaces for multi-tenancy
  - Regional deployments (multi-region support)

**Pricing** (2025):
- **Starter Plan**: $70/month for ~2M vectors (768 dims)
- **Scale as You Go**: ~$35 per additional 1M vectors
- **Enterprise**: Custom pricing with dedicated support

**Performance**:
- Query Latency: <50ms p95 (serverless), <20ms (pod-based)
- Scalability: Auto-scales to billions of vectors
- Indexing: HNSW-based with custom optimizations

**Why Choose Pinecone**:
- âœ… **Best for production** (99.99% uptime SLA)
- âœ… **Zero DevOps**: Fully managed, no infrastructure headaches
- âœ… **Fast deployment**: From prototype to production in hours
- âœ… **Optimized for agentic workloads**: Millions of small namespaces[^14]
- âœ… **Enterprise-ready**: SOC2, GDPR, HIPAA compliance

**When to Avoid**:
- âŒ **Cost-sensitive** for large-scale projects (can get expensive at billions of vectors)
- âŒ Need **on-premises deployment** (cloud-only)
- âŒ Prefer **open-source** solutions for customization

---

### Weaviate - Open Source Hybrid

**Best For**: Flexible hybrid search, multi-modal data, self-hosted or managed options[^15]

**Key Features**:
- **Open Source**: BSD 3-Clause license, 14k+ GitHub stars
- **Hybrid Search**: Combines vector, keyword (BM25), and cross-encoder reranking
- **Built-in Vectorization**: Integrate with OpenAI, Cohere, HuggingFace models
- **GraphQL API**: Intuitive querying with rich filtering
- **Multi-tenancy**: Isolated namespaces for different users/tenants
- **Deployment Flexibility**: Self-hosted (Docker, Kubernetes) or managed cloud

**Pricing** (2025):
- **OSS (Self-Hosted)**: Free (+ infrastructure costs)
- **Weaviate Cloud**: Starting at $25/month for 1M vectors
- **Enterprise**: Custom pricing with support and SLA

**Performance**:
- Query Latency: <100ms p95 (self-hosted with proper hardware)
- Scalability: Designed for billions of vectors (requires proper infrastructure)
- Indexing: HNSW with multiple distance metrics

**Why Choose Weaviate**:
- âœ… **Best for hybrid search** (vector + keyword + reranking)
- âœ… **Open source** flexibility and customization
- âœ… **Multi-modal support** (text, images, audio)
- âœ… **GraphQL API** for complex queries
- âœ… **Self-hosted option** for data sovereignty

**When to Avoid**:
- âŒ Need **simplest setup** (more complex than Pinecone)
- âŒ Lack **DevOps expertise** for self-hosting
- âŒ Want **lowest latency** (Pinecone/LanceDB are faster)

---

## Comparison Matrix

| Feature                    | LanceDB          | Pinecone           | Weaviate           |
|----------------------------|------------------|--------------------|--------------------|
| **Deployment**             | Embedded/Cloud   | Cloud-only         | Self-hosted/Cloud  |
| **Setup Complexity**       | â­â­â­â­â­ (Easy)   | â­â­â­â­â­ (Easy)     | â­â­â­ (Moderate)      |
| **Query Latency (p95)**    | <20ms            | <50ms              | <100ms             |
| **Pricing (Starting)**     | Free (OSS)       | $70/month          | Free (OSS)         |
| **Cost at Scale**          | ğŸ’° Low           | ğŸ’°ğŸ’°ğŸ’° High          | ğŸ’°ğŸ’° Medium          |
| **Uptime SLA**             | None (OSS)       | 99.99%             | Custom (Cloud)     |
| **Hybrid Search**          | âŒ No             | âœ… Yes              | âœ… Yes (Best)       |
| **Metadata Filtering**     | âœ… Yes            | âœ… Yes (Advanced)   | âœ… Yes (Advanced)   |
| **Multi-tenancy**          | âœ… Via tables     | âœ… Namespaces       | âœ… Namespaces       |
| **Multimodal Support**     | âœ… Native         | âŒ No               | âœ… Yes              |
| **TypeScript SDK**         | âœ… Excellent      | âœ… Good             | âœ… Good             |
| **Open Source**            | âœ… Apache 2.0     | âŒ Proprietary      | âœ… BSD 3-Clause     |
| **Best For**               | Prototyping      | Production         | Hybrid Search      |

---

## Implementation Examples

### LanceDB Setup (TypeScript)

**Installation**:
```bash
npm install lancedb @lancedb/embeddings
npm install openai  # For generating embeddings
```

**Basic Usage**:
```typescript
import lancedb from "lancedb";
import { OpenAI } from "openai";

// 1. Initialize LanceDB (embedded mode)
const db = await lancedb.connect("./lancedb");

// 2. Create table with schema
const table = await db.createTable("user_memories", [
  {
    id: "1",
    text: "User prefers dark mode",
    vector: new Array(1536).fill(0), // OpenAI ada-002 dimensions
    metadata: {
      user_id: "user123",
      timestamp: new Date().toISOString(),
      category: "preference",
    },
  },
]);

// 3. Generate embeddings and insert data
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function addMemory(userId: string, text: string, category: string) {
  // Generate embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: text,
  });
  const vector = response.data[0].embedding;

  // Insert into LanceDB
  await table.add([
    {
      id: crypto.randomUUID(),
      text,
      vector,
      metadata: {
        user_id: userId,
        timestamp: new Date().toISOString(),
        category,
      },
    },
  ]);
}

// 4. Query by semantic similarity
async function searchMemories(query: string, userId: string, topK = 5) {
  // Generate query embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = response.data[0].embedding;

  // Search with metadata filter
  const results = await table
    .search(queryVector)
    .where(`metadata.user_id = '${userId}'`)
    .limit(topK)
    .execute();

  return results.map((r) => ({
    text: r.text,
    similarity: r._distance, // Lower distance = more similar
    metadata: r.metadata,
  }));
}

// Usage
await addMemory("user123", "User likes TypeScript over Python", "preference");
const memories = await searchMemories("programming language preferences", "user123");
console.log(memories);
```

**Advanced: Multimodal Storage**:
```typescript
// Store text + image embeddings in same table
interface MultimodalMemory {
  id: string;
  text: string;
  text_vector: number[];
  image_vector?: number[]; // Optional image embedding
  metadata: {
    user_id: string;
    timestamp: string;
    has_image: boolean;
  };
}

const multimodalTable = await db.createTable("multimodal_memories", [
  {
    id: "1",
    text: "User's profile photo",
    text_vector: new Array(768).fill(0),
    image_vector: new Array(512).fill(0), // CLIP image embedding
    metadata: {
      user_id: "user123",
      timestamp: new Date().toISOString(),
      has_image: true,
    },
  },
]);
```

---

### Pinecone Setup (TypeScript)

**Installation**:
```bash
npm install @pinecone-database/pinecone
npm install openai
```

**Basic Usage**:
```typescript
import { Pinecone } from "@pinecone-database/pinecone";
import { OpenAI } from "openai";

// 1. Initialize Pinecone
const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_API_KEY!,
});

// 2. Create or connect to index
const indexName = "user-memories";
const index = pinecone.index(indexName);

// 3. Initialize OpenAI for embeddings
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// 4. Upsert vectors with metadata
async function addMemory(
  userId: string,
  memoryId: string,
  text: string,
  category: string
) {
  // Generate embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: text,
  });
  const vector = response.data[0].embedding;

  // Upsert to Pinecone with namespace for multi-tenancy
  await index.namespace(userId).upsert([
    {
      id: memoryId,
      values: vector,
      metadata: {
        text,
        category,
        timestamp: new Date().toISOString(),
      },
    },
  ]);
}

// 5. Query with metadata filtering
async function searchMemories(
  query: string,
  userId: string,
  category?: string,
  topK = 5
) {
  // Generate query embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = response.data[0].embedding;

  // Search in user's namespace
  const results = await index.namespace(userId).query({
    vector: queryVector,
    topK,
    includeMetadata: true,
    filter: category ? { category: { $eq: category } } : undefined,
  });

  return results.matches.map((match) => ({
    id: match.id,
    score: match.score, // Higher score = more similar (cosine similarity)
    text: match.metadata?.text,
    category: match.metadata?.category,
    timestamp: match.metadata?.timestamp,
  }));
}

// Usage
await addMemory("user123", "mem_001", "User prefers dark mode", "preference");
const results = await searchMemories("UI preferences", "user123");
console.log(results);
```

**Production Pattern: Batch Upserts**:
```typescript
// Efficient batch upsert for multiple memories
async function batchAddMemories(
  userId: string,
  memories: Array<{ id: string; text: string; category: string }>
) {
  // Generate embeddings in batch (more efficient)
  const texts = memories.map((m) => m.text);
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: texts,
  });

  // Prepare vectors for batch upsert
  const vectors = memories.map((memory, idx) => ({
    id: memory.id,
    values: response.data[idx].embedding,
    metadata: {
      text: memory.text,
      category: memory.category,
      timestamp: new Date().toISOString(),
    },
  }));

  // Batch upsert (up to 1000 vectors per request)
  const BATCH_SIZE = 100;
  for (let i = 0; i < vectors.length; i += BATCH_SIZE) {
    const batch = vectors.slice(i, i + BATCH_SIZE);
    await index.namespace(userId).upsert(batch);
  }
}
```

---

### Weaviate Setup (TypeScript)

**Installation**:
```bash
npm install weaviate-ts-client
npm install openai
```

**Basic Usage**:
```typescript
import weaviate, { WeaviateClient } from "weaviate-ts-client";
import { OpenAI } from "openai";

// 1. Initialize Weaviate client
const client: WeaviateClient = weaviate.client({
  scheme: "https",
  host: "your-weaviate-cluster.weaviate.network",
  apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY!),
});

// 2. Create schema (class)
const className = "UserMemory";
await client.schema
  .classCreator()
  .withClass({
    class: className,
    vectorizer: "none", // We'll provide vectors manually
    properties: [
      { name: "text", dataType: ["text"] },
      { name: "userId", dataType: ["string"] },
      { name: "category", dataType: ["string"] },
      { name: "timestamp", dataType: ["date"] },
    ],
  })
  .do();

// 3. Initialize OpenAI
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// 4. Add data with vector
async function addMemory(userId: string, text: string, category: string) {
  // Generate embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: text,
  });
  const vector = response.data[0].embedding;

  // Insert into Weaviate
  await client.data
    .creator()
    .withClassName(className)
    .withProperties({
      text,
      userId,
      category,
      timestamp: new Date().toISOString(),
    })
    .withVector(vector)
    .do();
}

// 5. Hybrid search (vector + keyword)
async function hybridSearch(query: string, userId: string, topK = 5) {
  // Generate query embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = response.data[0].embedding;

  // Hybrid search with GraphQL
  const result = await client.graphql
    .get()
    .withClassName(className)
    .withFields("text category timestamp _additional { score }")
    .withHybrid({
      query: query, // Keyword search
      vector: queryVector, // Vector search
      alpha: 0.5, // 0 = pure keyword, 1 = pure vector, 0.5 = balanced
    })
    .withWhere({
      path: ["userId"],
      operator: "Equal",
      valueString: userId,
    })
    .withLimit(topK)
    .do();

  return result.data.Get[className];
}

// Usage
await addMemory("user123", "User loves TypeScript", "preference");
const results = await hybridSearch("programming preferences", "user123");
console.log(results);
```

**Advanced: Built-in Vectorization**:
```typescript
// Use Weaviate's built-in vectorization (no need for OpenAI API calls)
await client.schema
  .classCreator()
  .withClass({
    class: "AutoVectorizedMemory",
    vectorizer: "text2vec-openai", // Weaviate calls OpenAI internally
    moduleConfig: {
      "text2vec-openai": {
        model: "text-embedding-3-small",
      },
    },
    properties: [
      { name: "text", dataType: ["text"] },
      { name: "userId", dataType: ["string"] },
    ],
  })
  .do();

// Add data without generating vectors yourself
await client.data
  .creator()
  .withClassName("AutoVectorizedMemory")
  .withProperties({
    text: "User prefers dark mode",
    userId: "user123",
  })
  .do(); // Weaviate automatically vectorizes the text!
```

---

## Production Patterns

### Multi-Tenancy with Namespaces

**Problem**: Store memories for millions of users without data leakage

**Solution**: Use namespaces (Pinecone) or partitions (LanceDB, Weaviate)

**Pinecone Example**:
```typescript
// Each user gets their own namespace
const getUserIndex = (userId: string) => {
  return index.namespace(userId);
};

// User 1's memories (isolated)
await getUserIndex("user_001").upsert([
  { id: "mem1", values: vector1, metadata: { text: "Likes coffee" } },
]);

// User 2's memories (completely separate)
await getUserIndex("user_002").upsert([
  { id: "mem1", values: vector2, metadata: { text: "Likes tea" } },
]);

// Query only returns user's own memories
const results = await getUserIndex("user_001").query({
  vector: queryVector,
  topK: 5,
});
```

**LanceDB Example**:
```typescript
// Use separate tables per user
const getUserTable = async (userId: string) => {
  return await db.openTable(`memories_${userId}`);
};
```

**Weaviate Example**:
```typescript
// Filter by userId property
const results = await client.graphql
  .get()
  .withClassName("UserMemory")
  .withWhere({
    path: ["userId"],
    operator: "Equal",
    valueString: userId,
  })
  .do();
```

---

### Hybrid Search Implementation

**Why**: Vector search alone may miss exact keyword matches (e.g., product IDs, names)

**Example: Product Search**:
```typescript
// User query: "iPhone 15 Pro Max"
// Vector search alone might return: "iPhone 14 Pro", "Samsung Galaxy S24"
// Hybrid search ensures "iPhone 15 Pro Max" ranks highest

// Weaviate Hybrid Search
const results = await client.graphql
  .get()
  .withClassName("Product")
  .withFields("name description price")
  .withHybrid({
    query: "iPhone 15 Pro Max",
    vector: queryVector,
    alpha: 0.7, // Favor vector search (70%) over keyword (30%)
  })
  .withLimit(10)
  .do();
```

**DIY Hybrid Search for LanceDB/Pinecone**:
```typescript
// 1. Vector search
const vectorResults = await index.query({
  vector: queryVector,
  topK: 20,
  includeMetadata: true,
});

// 2. Keyword filter (post-processing)
const keywordFiltered = vectorResults.matches.filter((match) =>
  match.metadata?.name.toLowerCase().includes("iphone 15")
);

// 3. Combine scores (Reciprocal Rank Fusion)
const finalResults = keywordFiltered.slice(0, 10);
```

---

### Cost Optimization Strategies

**1. Use Smaller Embeddings** (Lower dimensions = lower costs)[^16]:
```typescript
// OpenAI text-embedding-3-small: 1536 dims â†’ $0.02/1M tokens
// OpenAI text-embedding-3-large: 3072 dims â†’ $0.13/1M tokens

// For most use cases, "small" is sufficient!
const response = await openai.embeddings.create({
  model: "text-embedding-3-small", // 8x cheaper than "large"
  input: text,
});
```

**2. Batch Embeddings** (Reduce API calls):
```typescript
// âŒ Bad: 100 API calls
for (const text of texts) {
  await openai.embeddings.create({ model: "...", input: text });
}

// âœ… Good: 1 API call
const response = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: texts, // Array of up to 2048 inputs
});
```

**3. Lazy Indexing** (Insert only when needed):
```typescript
// Don't index every message immediately
// Wait until user asks a question, then index recent conversation
if (query.requiresMemory) {
  await indexRecentMessages(conversationId);
}
```

**4. TTL (Time-to-Live) for Stale Data**:
```typescript
// Delete old memories after 90 days
const ninetyDaysAgo = new Date(Date.now() - 90 * 24 * 60 * 60 * 1000);

// Pinecone: Delete by metadata filter
await index.deleteMany({
  filter: {
    timestamp: { $lt: ninetyDaysAgo.toISOString() },
  },
});
```

**5. LanceDB for Cost Savings**:
```typescript
// Self-hosted LanceDB: No per-query costs
// Perfect for high-volume, low-budget projects
const db = await lancedb.connect("./lancedb"); // Free!
```

---

## When to Use Each Database

### Use LanceDB When:
- âœ… **Prototyping** or **local development**
- âœ… **TypeScript/JavaScript-first** projects
- âœ… **Cost-sensitive** (need free or low-cost solution)
- âœ… **Edge deployments** (embedded in mobile apps, desktop apps)
- âœ… **Multimodal data** (text + images + audio)
- âœ… **Offline-first** requirements

### Use Pinecone When:
- âœ… **Production applications** with high uptime requirements
- âœ… **Zero DevOps** preference (fully managed)
- âœ… **Fast time-to-market** (deploy in hours, not weeks)
- âœ… **Enterprise compliance** (SOC2, HIPAA, GDPR)
- âœ… **Agentic workloads** (millions of small namespaces)
- âœ… **Budget allows** ($70+/month is acceptable)

### Use Weaviate When:
- âœ… **Hybrid search** is critical (vector + keyword + reranking)
- âœ… **Open-source** preference
- âœ… **Self-hosted** or **on-premises** deployment required
- âœ… **Multi-modal** data (text, images, audio)
- âœ… **GraphQL** API preference
- âœ… **DevOps expertise** available for infrastructure management

---

## Performance Benchmarks

### Query Latency (p95) - 1M Vectors, 768 Dimensions[^17]

| Database  | Cold Start | Warm Cache | Notes                     |
|-----------|------------|------------|---------------------------|
| LanceDB   | 20-30ms    | 10-15ms    | SSD-based, local/embedded |
| Pinecone  | 40-50ms    | 20-30ms    | Serverless, network RTT   |
| Weaviate  | 80-100ms   | 30-50ms    | Self-hosted, proper HW    |
| Qdrant    | 1.6-3.5ms  | 1-2ms      | Rust, highly optimized    |

### Throughput (Queries Per Second)[^18]

| Database  | QPS (1M vectors) | Concurrent Users |
|-----------|------------------|------------------|
| LanceDB   | ~500-800         | ~50              |
| Pinecone  | ~1000-2000       | ~100             |
| Weaviate  | ~200-500         | ~25              |
| Qdrant    | ~1200+           | ~100+            |

### Cost Comparison (10M Vectors, 768 Dims, 1M Queries/Month)[^19]

| Database         | Storage   | Queries      | Total/Month |
|------------------|-----------|--------------|-------------|
| LanceDB (Cloud)  | $20       | $0.40        | ~$20        |
| Pinecone         | $175      | Included     | ~$175       |
| Weaviate (Cloud) | $60       | $10          | ~$70        |
| LanceDB (OSS)    | $15 (S3)  | $0 (compute) | ~$15        |

---

## Best Practices

### 1. Namespace Strategy for Multi-Tenancy
```typescript
// âœ… Good: One namespace per user
const userIndex = index.namespace(`user_${userId}`);

// âŒ Bad: Single namespace with metadata filtering
// (slower queries, risk of data leakage)
const results = await index.query({
  filter: { user_id: userId }, // Slower than namespace isolation
});
```

### 2. Metadata Design
```typescript
// âœ… Good: Flat, queryable metadata
const metadata = {
  user_id: "user123",
  category: "preference",
  timestamp: "2025-11-18T10:00:00Z",
  priority: "high",
};

// âŒ Bad: Nested or unstructured metadata (hard to filter)
const metadata = {
  data: {
    user: { id: "user123" }, // Nested = complex filters
  },
};
```

### 3. Embedding Model Consistency
```typescript
// âœ… Always use the SAME embedding model
const EMBEDDING_MODEL = "text-embedding-3-small"; // 1536 dims

// âŒ Mixing models breaks similarity search
// (e.g., ada-002 [1536] vs text-embedding-3-large [3072])
```

### 4. Error Handling & Retries
```typescript
async function upsertWithRetry(vectors: any[], maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      await index.upsert(vectors);
      return;
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await new Promise((resolve) => setTimeout(resolve, 2 ** i * 1000));
    }
  }
}
```

### 5. Monitoring & Observability
```typescript
// Track vector database performance
import { performance } from "perf_hooks";

async function monitoredQuery(query: string) {
  const start = performance.now();
  const results = await searchMemories(query, userId);
  const latency = performance.now() - start;

  // Log metrics
  console.log({
    operation: "vector_search",
    latency_ms: latency,
    results_count: results.length,
    user_id: userId,
  });

  return results;
}
```

---

## Common Pitfalls

### 1. âŒ Not Normalizing Vectors

```typescript
// âŒ Bad: Using dot product without normalization
const score = dotProduct(vectorA, vectorB); // Wrong if vectors not normalized

// âœ… Good: Normalize before dot product (or use cosine similarity)
const normalize = (v: number[]) => {
  const norm = Math.sqrt(v.reduce((sum, val) => sum + val ** 2, 0));
  return v.map((val) => val / norm);
};
```

### 2. âŒ Storing Too Much Metadata

```typescript
// âŒ Bad: Storing entire documents in metadata (increases costs)
const metadata = {
  full_document: "10,000 character document...", // Expensive!
};

// âœ… Good: Store only references
const metadata = {
  document_id: "doc123",
  summary: "Brief 100-char summary...",
};
```

### 3. âŒ Ignoring Cold Start Performance

```typescript
// âŒ Bad: Assuming all queries are fast (first query is slow)
const results = await index.query({ vector, topK: 5 }); // May be slow!

// âœ… Good: Warm up index on startup
await index.query({ vector: dummyVector, topK: 1 }); // Prime the cache
```

### 4. âŒ No Batch Deletion Strategy

```typescript
// âŒ Bad: Never deleting old data (costs accumulate)

// âœ… Good: Periodic cleanup
async function cleanupOldMemories() {
  const thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);
  await index.deleteMany({
    filter: { timestamp: { $lt: thirtyDaysAgo.toISOString() } },
  });
}
```

### 5. âŒ Over-Reliance on Vector Search Alone

```typescript
// âŒ Bad: Using only vector search for precise queries
const results = await index.query({ vector: queryVector, topK: 5 });
// May miss exact matches like "iPhone 15 Pro Max"

// âœ… Good: Hybrid search for best of both worlds
const results = await hybridSearch(query, queryVector);
```

---

## Related Topics

- **[4.3.2 Semantic Search](./4.3.2-semantic-search.md)** - Deep dive into similarity metrics and query strategies
- **[4.3.3 Fact Extraction & Storage](./4.3.3-fact-extraction.md)** - Extracting structured facts for long-term memory
- **[4.3.4 Cross-Session Retrieval](./4.3.4-cross-session.md)** - Maintaining context across multiple conversations
- **[4.3.5 When to Use vs Working Memory](./4.3.5-when-to-use.md)** - Choosing between short-term and long-term memory
- **[5.1 Vector Search Fundamentals](../../5-rag/5.1.1-embedding-documents.md)** - RAG patterns and embedding strategies
- **[0.3.2 Embedding Models](../../0-foundations/0.3.2-embedding-models.md)** - Choosing the right embedding model

---

## References

[^1]: "Best Vector Databases for 2025" - LakeFS Blog (2025): https://lakefs.io/blog/best-vector-databases/
[^2]: "Vector Databases For RAG: Pinecone Vs Weaviate Vs Chroma" - HowToBuySaaS (2025): https://www.howtobuysaas.com/blog/pinecone-vs-weaviate-vs-chroma/
[^3]: "The Future of AI-Native Development is Local: Inside Continue's LanceDB-Powered Evolution" - LanceDB Blog (2025): https://lancedb.com/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/
[^4]: "Building AI Agents That Actually Remember: A Developer's Guide to Memory Management in 2025" - Medium (2025): https://medium.com/@nomannayeem/building-ai-agents-that-actually-remember-a-developers-guide-to-memory-management-in-2025
[^5]: "Optimizing Pinecone for agents (and more)" - Pinecone Blog (2025): https://www.pinecone.io/blog/optimizing-pinecone/
[^6]: "Understanding vector databases for AI apps" - Vercel Guides (2025): https://vercel.com/guides/understanding-vector-databases-for-ai-apps
[^7]: "LanceDB | Vector Database for RAG, Agents & Hybrid Search" - LanceDB (2025): https://lancedb.com/
[^8]: "LanceDB â€” The Engine Powering Today's Multimodal AI" - Medium (2025): https://medium.com/@ameyakshirsagar02/deep-dive-lancedb-the-engine-powering-todays-multimodal-ai
[^9]: "What Makes a Good Vector Database? Comparing Pinecone and LanceDB" - 57Blocks (2024): https://57blocks.io/blog/what-makes-a-good-vector-database-comparing-pinecone-and-lancedb/
[^10]: "Serverless Vector DB with LanceDB | Serverless Office Hours" - AWS Events (2025): https://www.youtube.com/watch?v=a1r8K0x4WMo
[^11]: "Vector Databases Comparison 2025: Pinecone vs Weaviate vs Qdrant vs Milvus" - TensorBlue (2025): https://tensorblue.com/blog/vector-database-comparison-pinecone-weaviate-qdrant-milvus/
[^12]: "Reimagining the vector database to enable knowledgeable AI" - Pinecone Blog (2024): https://www.pinecone.io/blog/serverless-architecture/
[^13]: "Pinecone vs Weaviate 2025 - Vector Databases" - Aloa (2025): https://aloa.co/ai/comparisons/vector-database-comparison/pinecone-vs-weaviate
[^14]: "Evolving Pinecone's architecture to meet the demands of Knowledgeable AI" - Pinecone Blog (2025): https://www.pinecone.io/blog/evolving-pinecone-for-knowledgeable-ai/
[^15]: "Best Vector Database For RAG In 2025 (Pinecone Vs Weaviate Vs Qdrant Vs Milvus Vs Chroma)" - Digital One Agency (2025): https://digitaloneagency.com.au/best-vector-database-for-rag-in-2025-pinecone-vs-weaviate-vs-qdrant-vs-milvus-vs-chroma/
[^16]: "Top Embedding Models in 2025 â€” The Complete Guide" - ArtSmart AI Blog (2025): https://artsmart.ai/blog/top-embedding-models-in-2025/
[^17]: "Top Vector Databases for Enterprise AI in 2025" - Medium (2025): https://medium.com/@balarampanda.ai/top-vector-databases-for-enterprise-ai-in-2025
[^18]: "2025 Guide to Vector Databases for LLM Applications" - Abovo (2025): https://www.abovo.co/sean@abovo42.com/134573
[^19]: "Pinecone Production Architecture: Fix Common Issues & Best Practices" - ToolsTac (2025): https://toolstac.com/tool/pinecone

---

**Next**: [4.3.2 Semantic Search](./4.3.2-semantic-search.md) - Learn how to implement effective semantic search with embeddings and similarity metrics.

**Previous**: [4.2.5 Compression Ratios](./4.2.5-compression-ratios.md) - Achieving 10:1 compression in subgoal memory systems.
