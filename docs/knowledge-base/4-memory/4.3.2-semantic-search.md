# 4.3.2 Semantic Search

**Layer**: 4 - Memory & State  
**Sublayer**: 4.3 - Episodic Memory (Long-Term)  
**Audience**: Intermediate  
**Updated**: 2025-11-18

---

## Table of Contents

- [Overview](#overview)
- [Keyword Search vs Semantic Search](#keyword-search-vs-semantic-search)
- [How Semantic Search Works](#how-semantic-search-works)
- [Embedding Models for Semantic Search](#embedding-models-for-semantic-search)
- [Similarity Metrics](#similarity-metrics)
  - [Cosine Similarity](#cosine-similarity)
  - [Dot Product](#dot-product)
  - [Euclidean Distance](#euclidean-distance)
- [Implementation Examples](#implementation-examples)
  - [Basic Semantic Search](#basic-semantic-search)
  - [Hybrid Search (Vector + Keyword)](#hybrid-search-vector--keyword)
  - [Reranking for Improved Relevance](#reranking-for-improved-relevance)
- [Query Optimization Techniques](#query-optimization-techniques)
- [Production Patterns](#production-patterns)
- [Performance Benchmarks](#performance-benchmarks)
- [Best Practices](#best-practices)
- [Common Pitfalls](#common-pitfalls)
- [Related Topics](#related-topics)
- [References](#references)

---

## Overview

**Semantic search** is a search technique that understands the **meaning** and **intent** behind queries, rather than just matching exact keywords. It uses vector embeddings to represent text as high-dimensional numerical arrays, enabling searches based on conceptual similarity[^1].

**Key Differences from Traditional Search**:
- **Traditional Search**: "Find documents containing the exact words 'refund policy'"
- **Semantic Search**: "Find documents about getting money back, cancellations, returns, etc."

**Why This Matters for AI Agents**:
- **Context Understanding**: Retrieve relevant information even when phrasing differs
- **Multiliningual Support**: Find similar content across languages (embeddings capture meaning, not words)
- **Synonym Handling**: "car" matches "automobile", "vehicle", "auto"
- **Intent Recognition**: "How do I cancel?" matches "cancellation policy", "refund process"[^2]

**Real-World Applications**:
1. **RAG (Retrieval-Augmented Generation)**: Fetch relevant context for LLM responses
2. **Code Search**: Find similar functions or patterns (Continue.dev, GitHub Copilot)[^3]
3. **Customer Support**: Match user questions to knowledge base articles
4. **Document Search**: Enterprise search across internal documentation
5. **Recommendation Systems**: "Find products similar to this one"

---

## Keyword Search vs Semantic Search

### Traditional Keyword Search Failures

**Example 1: Synonym Blindness**
```sql
-- Query: "refund policy"
SELECT * FROM documents WHERE title LIKE '%refund policy%';

-- ✅ Finds: "Refund Policy"
-- ❌ Misses: "Money Back Guarantee"
-- ❌ Misses: "Return Procedure"
-- ❌ Misses: "Cancellation Terms"
```

**Example 2: Multilingual Mismatch**
```typescript
// User query in Spanish: "política de reembolso"
// Keyword search: 0 results (no Spanish documents)
// Semantic search: Finds English "refund policy" documents (same embedding space!)
```

**Example 3: Contextual Failure**
```typescript
// Query: "apple pie recipe"
// Keyword search: Also returns documents about "Apple Inc" (wrong context)
// Semantic search: Only returns cooking-related documents
```

### Semantic Search Advantages

**How It Works**:
1. Convert text to vector embeddings (numerical representations)
2. Measure similarity in high-dimensional space
3. Return documents closest to the query vector

**Visual Analogy**:
```
Traditional Search: Dictionary lookup (exact match)
Semantic Search: Library with smart librarian (understands what you need)
```

**Example**:
```typescript
// Query embedding: [0.23, -0.45, 0.87, ..., 0.12] (1536 dimensions)

// Document embeddings:
// "Refund Policy":          [0.25, -0.43, 0.85, ...] → Similarity: 0.98
// "Money Back Guarantee":   [0.22, -0.46, 0.88, ...] → Similarity: 0.95
// "Apple Inc History":      [0.91, 0.34, -0.12, ...] → Similarity: 0.12
```

---

## How Semantic Search Works

### Step-by-Step Process

```
┌─────────────────────────────────────────────────────────┐
│  1. INDEXING PHASE (One-time setup)                     │
├─────────────────────────────────────────────────────────┤
│  Documents → Chunking → Embeddings → Vector Database    │
│                                                          │
│  "Our refund policy..."  → [0.23, -0.45, ...]           │
│  "Money back guarantee..." → [0.22, -0.46, ...]         │
│  "Cancellation process..." → [0.21, -0.47, ...]         │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│  2. QUERY PHASE (Real-time)                             │
├─────────────────────────────────────────────────────────┤
│  User Query → Embedding → Similarity Search → Results   │
│                                                          │
│  "How do I get my money back?"                          │
│       ↓                                                  │
│  [0.24, -0.44, 0.86, ...]  (query vector)               │
│       ↓                                                  │
│  Cosine Similarity vs all document vectors              │
│       ↓                                                  │
│  Top 5 Most Similar Documents                           │
└─────────────────────────────────────────────────────────┘
```

### Key Components

1. **Chunking**: Split documents into digestible pieces (500-1000 tokens)
2. **Embedding Model**: Convert text to vectors (OpenAI, Cohere, SBERT)
3. **Vector Database**: Store and index embeddings (Pinecone, LanceDB, Weaviate)
4. **Similarity Metric**: Measure closeness (cosine similarity, dot product)
5. **Top-K Selection**: Return the K most similar results

---

## Embedding Models for Semantic Search

### Top Models (2025)

| Model                         | Dimensions | Cost (per 1M tokens) | Use Case                    | Quality  |
|-------------------------------|------------|----------------------|-----------------------------|----------|
| **OpenAI text-embedding-3-small** | 1536   | $0.02                | General-purpose, cost-effective | ⭐⭐⭐⭐    |
| **OpenAI text-embedding-3-large** | 3072   | $0.13                | High accuracy, enterprise   | ⭐⭐⭐⭐⭐   |
| **Cohere embed-v3**           | 1024       | $0.10                | Multilingual, reranking     | ⭐⭐⭐⭐⭐   |
| **SBERT (all-MiniLM-L6-v2)**  | 384        | Free (self-hosted)   | Budget projects, local dev  | ⭐⭐⭐     |
| **BGE (BAAI)**                | 768        | Free (self-hosted)   | Open source, high quality   | ⭐⭐⭐⭐    |
| **NV-Embed**                  | 4096       | -                    | Research, state-of-the-art  | ⭐⭐⭐⭐⭐   |

**Recommendation for 2025**[^4]:
- **Production**: OpenAI `text-embedding-3-small` (best cost/performance ratio)
- **High Accuracy**: OpenAI `text-embedding-3-large` or Cohere `embed-v3`
- **Budget/Local**: SBERT `all-MiniLM-L6-v2` or BGE

### Model Selection Criteria

```typescript
// 1. Dimensionality vs Accuracy vs Cost
// Lower dimensions = Faster queries, Lower storage costs, Slightly lower accuracy
// Higher dimensions = Higher accuracy, Slower queries, Higher costs

// 2. Example trade-offs:
// 384 dims (SBERT): 10ms query, $5/month storage for 1M vectors
// 1536 dims (OpenAI small): 20ms query, $20/month storage
// 3072 dims (OpenAI large): 40ms query, $40/month storage

// 3. Choose based on requirements:
if (needHighAccuracy && budgetAllows) {
  use("text-embedding-3-large"); // 3072 dims
} else if (needCostEfficiency) {
  use("text-embedding-3-small"); // 1536 dims
} else if (needLocalDeployment) {
  use("all-MiniLM-L6-v2"); // 384 dims, self-hosted
}
```

---

## Similarity Metrics

### Cosine Similarity

**Most Common** for text embeddings (measures angle between vectors)[^5]

**Formula**:
```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)
Range: [-1, 1] where 1 = identical, 0 = orthogonal, -1 = opposite
```

**TypeScript Implementation**:
```typescript
function cosineSimilarity(vecA: number[], vecB: number[]): number {
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  return dotProduct / (magnitudeA * magnitudeB);
}

// Example
const queryVec = [0.5, 0.8, 0.1];
const doc1Vec = [0.6, 0.7, 0.15];
const doc2Vec = [-0.3, 0.2, 0.9];

console.log(cosineSimilarity(queryVec, doc1Vec)); // 0.987 (very similar)
console.log(cosineSimilarity(queryVec, doc2Vec)); // 0.234 (not similar)
```

**Advantages**:
- ✅ **Normalized**: Always returns value between -1 and 1
- ✅ **Magnitude-independent**: Only cares about direction (good for text)
- ✅ **Standard**: Most embedding models are optimized for cosine similarity

**When to Use**: Default choice for text embeddings

---

### Dot Product

**Faster** than cosine similarity (no normalization), but requires **pre-normalized** vectors

**Formula**:
```
dot_product(A, B) = A · B = Σ(Ai × Bi)
Range: [-∞, ∞] (higher = more similar)
```

**TypeScript Implementation**:
```typescript
function dotProduct(vecA: number[], vecB: number[]): number {
  return vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
}

// For normalized vectors (||v|| = 1), dot product = cosine similarity
function normalizeVector(vec: number[]): number[] {
  const magnitude = Math.sqrt(vec.reduce((sum, v) => sum + v * v, 0));
  return vec.map((v) => v / magnitude);
}

const normalizedA = normalizeVector([0.5, 0.8, 0.1]);
const normalizedB = normalizeVector([0.6, 0.7, 0.15]);

// For normalized vectors, dot product = cosine similarity
console.log(dotProduct(normalizedA, normalizedB)); // 0.987 (same as cosine!)
```

**Advantages**:
- ✅ **Faster**: No square root calculation (30-50% faster than cosine)
- ✅ **Efficient**: Optimal for high-dimensional vectors (1536+)

**Disadvantages**:
- ❌ **Requires normalization**: Vectors must be normalized beforehand

**When to Use**: When you can pre-normalize vectors and need maximum speed

---

### Euclidean Distance

**Measures straight-line distance** between vectors (less common for text, more for images/audio)

**Formula**:
```
euclidean_distance(A, B) = sqrt(Σ(Ai - Bi)²)
Range: [0, ∞] (lower = more similar)
```

**TypeScript Implementation**:
```typescript
function euclideanDistance(vecA: number[], vecB: number[]): number {
  const sumSquaredDiff = vecA.reduce((sum, a, i) => sum + (a - vecB[i]) ** 2, 0);
  return Math.sqrt(sumSquaredDiff);
}

// Example
const vec1 = [1, 2, 3];
const vec2 = [1, 2, 10]; // Different in one dimension
const vec3 = [1, 2, 3.1]; // Very close

console.log(euclideanDistance(vec1, vec2)); // 7.0 (far)
console.log(euclideanDistance(vec1, vec3)); // 0.1 (close)
```

**Advantages**:
- ✅ **Intuitive**: Geometric interpretation (straight-line distance)
- ✅ **Good for images/audio**: Captures absolute differences

**Disadvantages**:
- ❌ **Magnitude-sensitive**: Two parallel vectors with different lengths score low
- ❌ **Not optimal for text**: Cosine similarity performs better

**When to Use**: Image similarity, audio fingerprinting, or when absolute distance matters

---

## Implementation Examples

### Basic Semantic Search

**Complete TypeScript Example**:
```typescript
import { OpenAI } from "openai";
import lancedb from "lancedb";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const db = await lancedb.connect("./lancedb");

// 1. Index documents
async function indexDocuments(documents: Array<{ id: string; text: string }>) {
  // Generate embeddings in batch
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: documents.map((d) => d.text),
  });

  // Prepare data for LanceDB
  const data = documents.map((doc, idx) => ({
    id: doc.id,
    text: doc.text,
    vector: response.data[idx].embedding,
  }));

  // Create table
  const table = await db.createTable("documents", data);
  return table;
}

// 2. Semantic search
async function semanticSearch(query: string, topK = 5) {
  // Generate query embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = response.data[0].embedding;

  // Search in vector database
  const table = await db.openTable("documents");
  const results = await table.search(queryVector).limit(topK).execute();

  return results.map((r) => ({
    id: r.id,
    text: r.text,
    similarity: 1 - r._distance, // Convert distance to similarity (0-1 scale)
  }));
}

// Usage
const docs = [
  { id: "1", text: "Our refund policy allows returns within 30 days" },
  { id: "2", text: "Money back guarantee for unsatisfied customers" },
  { id: "3", text: "Shipping takes 3-5 business days" },
  { id: "4", text: "Apple Inc was founded in 1976" },
];

await indexDocuments(docs);

const results = await semanticSearch("How do I get my money back?", 3);
console.log(results);
// [
//   { id: "2", text: "Money back guarantee...", similarity: 0.92 },
//   { id: "1", text: "Our refund policy...", similarity: 0.89 },
//   { id: "3", text: "Shipping takes...", similarity: 0.23 },
// ]
```

---

### Hybrid Search (Vector + Keyword)

**Problem**: Pure semantic search may miss exact keyword matches (e.g., product IDs, names)[^6]

**Solution**: Combine vector search with traditional keyword filtering

```typescript
import weaviate from "weaviate-ts-client";

const client = weaviate.client({
  scheme: "https",
  host: "your-cluster.weaviate.network",
  apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY!),
});

// Hybrid search: Vector (70%) + Keyword (30%)
async function hybridSearch(query: string, topK = 10) {
  // Generate embedding
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = response.data[0].embedding;

  // Weaviate hybrid search
  const result = await client.graphql
    .get()
    .withClassName("Document")
    .withFields("text title _additional { score }")
    .withHybrid({
      query: query, // BM25 keyword search
      vector: queryVector, // Semantic vector search
      alpha: 0.7, // 0 = pure keyword, 1 = pure vector, 0.5 = balanced
    })
    .withLimit(topK)
    .do();

  return result.data.Get.Document;
}

// Example: Finding "iPhone 15 Pro Max"
const results = await hybridSearch("iPhone 15 Pro Max", 5);
// Ensures "iPhone 15 Pro Max" ranks higher than "iPhone 14" or "Samsung Galaxy"
```

**DIY Hybrid Search (for LanceDB/Pinecone)**:
```typescript
async function diyHybridSearch(query: string, topK = 10) {
  // 1. Semantic search (vector)
  const vectorResults = await semanticSearch(query, topK * 2); // Get more results

  // 2. Keyword filtering (post-processing)
  const keywords = query.toLowerCase().split(" ");
  const keywordScores = vectorResults.map((result) => {
    const text = result.text.toLowerCase();
    const keywordMatches = keywords.filter((kw) => text.includes(kw)).length;
    return {
      ...result,
      keywordScore: keywordMatches / keywords.length, // % of keywords matched
    };
  });

  // 3. Combine scores (weighted)
  const VECTOR_WEIGHT = 0.7;
  const KEYWORD_WEIGHT = 0.3;

  const hybridScores = keywordScores.map((r) => ({
    ...r,
    hybridScore: r.similarity * VECTOR_WEIGHT + r.keywordScore * KEYWORD_WEIGHT,
  }));

  // 4. Re-rank and return top K
  hybridScores.sort((a, b) => b.hybridScore - a.hybridScore);
  return hybridScores.slice(0, topK);
}
```

---

### Reranking for Improved Relevance

**Problem**: Initial retrieval may have false positives (high similarity but irrelevant)

**Solution**: Use a reranking model to refine results[^7]

```typescript
import Anthropic from "@anthropic-ai/sdk";

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

// Step 1: Initial retrieval (get 20 candidates)
const candidates = await semanticSearch(query, 20);

// Step 2: Rerank using cross-encoder or LLM
async function rerankResults(
  query: string,
  candidates: Array<{ id: string; text: string }>,
  topK = 5
) {
  // Option 1: Use cross-encoder model (dedicated reranking model)
  // Example: Cohere Rerank API, SBERT Cross-Encoder
  const cohere = new CohereClient({ token: process.env.COHERE_API_KEY });
  const reranked = await cohere.rerank({
    model: "rerank-english-v3.0",
    query: query,
    documents: candidates.map((c) => c.text),
    topN: topK,
  });

  return reranked.results.map((r) => candidates[r.index]);
}

// Option 2: LLM-based reranking (slower but more flexible)
async function llmRerank(
  query: string,
  candidates: Array<{ id: string; text: string }>,
  topK = 5
) {
  const prompt = `
Given the query: "${query}"

Rank the following documents by relevance (1 = most relevant):

${candidates.map((c, i) => `${i + 1}. ${c.text}`).join("\n")}

Return only the top ${topK} document numbers in order (e.g., "3, 1, 5, 2, 4"):
`;

  const response = await anthropic.messages.create({
    model: "claude-3-5-sonnet-20241022",
    max_tokens: 100,
    messages: [{ role: "user", content: prompt }],
  });

  // Parse LLM response (e.g., "3, 1, 5, 2, 4")
  const ranking = response.content[0].text
    .trim()
    .split(",")
    .map((n) => parseInt(n.trim()) - 1);

  return ranking.map((idx) => candidates[idx]);
}

// Usage
const finalResults = await rerankResults(query, candidates, 5);
```

**Performance Gain**: Reranking can improve precision by 10-30%[^8]

---

## Query Optimization Techniques

### 1. Query Expansion

**Add synonyms and related terms to improve recall**:
```typescript
async function expandQuery(query: string): Promise<string> {
  const prompt = `Expand the following search query with 3-5 related terms or synonyms:
Query: "${query}"

Expanded query (single line):`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
    temperature: 0.3,
  });

  return response.choices[0].message.content!.trim();
}

// Example
const expanded = await expandQuery("refund policy");
// "refund policy return procedure money back guarantee cancellation terms"

// Use expanded query for better recall
const results = await semanticSearch(expanded, 10);
```

### 2. Query Decomposition

**Break complex queries into sub-queries**:
```typescript
async function decomposeQuery(query: string): Promise<string[]> {
  const prompt = `Break down the following complex query into 2-3 simpler sub-queries:
Query: "${query}"

Sub-queries (one per line):`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
  });

  return response.choices[0].message
    .content!.trim()
    .split("\n")
    .filter((q) => q.trim());
}

// Example
const subQueries = await decomposeQuery(
  "How do I return a damaged product and get a full refund?"
);
// [
//   "How to return damaged products?",
//   "Refund policy for defective items",
//   "Full refund eligibility criteria"
// ]

// Search for each sub-query and combine results
const allResults = await Promise.all(subQueries.map((q) => semanticSearch(q, 5)));
const uniqueResults = deduplicateResults(allResults.flat());
```

### 3. HyDE (Hypothetical Document Embeddings)

**Generate a hypothetical answer, then search for similar documents**[^9]:
```typescript
async function hydeSearch(query: string, topK = 5) {
  // Step 1: Generate hypothetical document
  const prompt = `Write a concise answer (2-3 sentences) to the following question:
"${query}"`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
  });

  const hypotheticalDoc = response.choices[0].message.content!;

  // Step 2: Embed hypothetical document
  const embedding = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: hypotheticalDoc,
  });

  // Step 3: Search using hypothetical embedding (not original query!)
  const results = await vectorDB.search(embedding.data[0].embedding, topK);
  return results;
}

// Why this works: Hypothetical answer is closer to actual documents than raw query
```

---

## Production Patterns

### 1. Caching Query Embeddings

```typescript
import NodeCache from "node-cache";

const queryCache = new NodeCache({ stdTTL: 3600 }); // 1 hour TTL

async function getCachedEmbedding(query: string): Promise<number[]> {
  const cacheKey = `embed:${query}`;
  const cached = queryCache.get<number[]>(cacheKey);

  if (cached) return cached;

  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const embedding = response.data[0].embedding;

  queryCache.set(cacheKey, embedding);
  return embedding;
}

// Saves ~50ms per cached query + reduces API costs
```

### 2. Metadata Filtering for Multi-Tenancy

```typescript
// Filter results by user_id to prevent data leakage
async function userScopedSearch(query: string, userId: string, topK = 5) {
  const queryVector = await getCachedEmbedding(query);

  // Pinecone with namespace
  const results = await index.namespace(userId).query({
    vector: queryVector,
    topK,
    includeMetadata: true,
  });

  // OR LanceDB with filter
  const results = await table
    .search(queryVector)
    .where(`user_id = '${userId}'`)
    .limit(topK)
    .execute();

  return results;
}
```

### 3. A/B Testing Similarity Metrics

```typescript
async function experimentalSearch(query: string, topK = 5) {
  const userId = getCurrentUserId();
  const experimentGroup = userId.charCodeAt(0) % 2; // 50/50 split

  if (experimentGroup === 0) {
    // Control: Cosine similarity
    return await semanticSearch(query, topK);
  } else {
    // Treatment: Hybrid search
    return await hybridSearch(query, topK);
  }
}

// Track metrics: click-through rate, user satisfaction, etc.
```

---

## Performance Benchmarks

### Query Latency (100k Documents, 1536 Dims)[^10]

| Operation                     | Latency (p50) | Latency (p95) |
|-------------------------------|---------------|---------------|
| Generate query embedding      | 50ms          | 80ms          |
| Vector search (LanceDB)       | 10ms          | 20ms          |
| Vector search (Pinecone)      | 20ms          | 50ms          |
| Reranking (Cohere, top 20)    | 100ms         | 150ms         |
| **Total (with reranking)**    | **160ms**     | **250ms**     |
| **Total (without reranking)** | **60ms**      | **100ms**     |

### Cost Comparison (1M Queries/Month)

| Component                     | Cost/Month  |
|-------------------------------|-------------|
| OpenAI embeddings (1M queries)| $20         |
| LanceDB Cloud (1M queries)    | $0.40       |
| Pinecone (1M queries)         | Included    |
| Cohere Rerank (100k rerankings)| $10        |
| **Total (LanceDB + Rerank)**  | **~$30**    |
| **Total (Pinecone + Rerank)** | **~$185**   |

---

## Best Practices

### 1. Consistent Embedding Model
```typescript
// ✅ Good: Same model for indexing and querying
const EMBEDDING_MODEL = "text-embedding-3-small";

await indexDocuments(docs, EMBEDDING_MODEL);
await search(query, EMBEDDING_MODEL);

// ❌ Bad: Different models (breaks similarity search!)
await indexDocuments(docs, "text-embedding-ada-002"); // 1536 dims
await search(query, "text-embedding-3-large"); // 3072 dims → WRONG!
```

### 2. Normalize Vectors for Dot Product
```typescript
function normalizeVector(vec: number[]): number[] {
  const magnitude = Math.sqrt(vec.reduce((sum, v) => sum + v * v, 0));
  return vec.map((v) => v / magnitude);
}

// Only necessary if using dot product instead of cosine similarity
```

### 3. Monitor Query Performance
```typescript
async function monitoredSearch(query: string) {
  const start = performance.now();
  const results = await semanticSearch(query, 5);
  const latency = performance.now() - start;

  console.log({
    operation: "semantic_search",
    query_length: query.length,
    results_count: results.length,
    latency_ms: latency,
  });

  // Alert if latency > 200ms
  if (latency > 200) {
    alertSlack("Semantic search latency spike!");
  }

  return results;
}
```

### 4. Fallback to Keyword Search
```typescript
async function robustSearch(query: string, topK = 5) {
  try {
    // Try semantic search first
    const results = await semanticSearch(query, topK);
    if (results.length > 0) return results;
  } catch (error) {
    console.error("Semantic search failed:", error);
  }

  // Fallback to keyword search
  return await keywordSearch(query, topK);
}
```

---

## Common Pitfalls

### 1. ❌ Not Chunking Large Documents

```typescript
// ❌ Bad: Embedding entire 10,000-word document
const embedding = await embed(largeDocument); // Information loss!

// ✅ Good: Chunk into 500-1000 token pieces
const chunks = chunkDocument(largeDocument, 500);
const embeddings = await embedBatch(chunks);
```

### 2. ❌ Ignoring Metadata

```typescript
// ❌ Bad: Only storing text
await table.add([{ text: "...", vector: [...] }]);

// ✅ Good: Include rich metadata for filtering
await table.add([
  {
    text: "...",
    vector: [...],
    metadata: {
      user_id: "user123",
      category: "support",
      timestamp: new Date().toISOString(),
      language: "en",
    },
  },
]);
```

### 3. ❌ No Query Preprocessing

```typescript
// ❌ Bad: Searching with raw user input
await search("HOW DO I GET MY MONEY BACK???");

// ✅ Good: Normalize query
function normalizeQuery(query: string): string {
  return query.toLowerCase().replace(/[^a-z0-9\s]/g, "").trim();
}

await search(normalizeQuery(query)); // "how do i get my money back"
```

### 4. ❌ Over-Reliance on Semantic Search

```typescript
// ❌ Bad: Using semantic search for exact ID lookup
await search("order #12345"); // May return irrelevant results

// ✅ Good: Use keyword/exact match for IDs
if (query.includes("#")) {
  const orderId = query.match(/#(\d+)/)?.[1];
  return await db.findOne({ orderId });
}
```

---

## Related Topics

- **[4.3.1 Vector Databases](./4.3.1-vector-databases.md)** - Choosing the right vector database (LanceDB, Pinecone, Weaviate)
- **[4.3.3 Fact Extraction & Storage](./4.3.3-fact-extraction.md)** - Extracting structured facts from unstructured text
- **[4.3.4 Cross-Session Retrieval](./4.3.4-cross-session.md)** - Maintaining context across multiple conversations
- **[5.1 Vector Search Fundamentals](../../5-rag/5.1.1-embedding-documents.md)** - Deep dive into RAG patterns
- **[0.3.2 Embedding Models](../../0-foundations/0.3.2-embedding-models.md)** - Choosing embedding models for your use case

---

## References

[^1]: "Understanding vector databases for AI apps" - Vercel Guides (2025): https://vercel.com/guides/understanding-vector-databases-for-ai-apps
[^2]: "Semantic Search Demystified: Architectures, Use Cases" - Medium (2025): https://medium.com/tellian-io/semantic-search-demystified-architectures-use-cases
[^3]: "The Future of AI-Native Development is Local: Inside Continue's LanceDB-Powered Evolution" - LanceDB Blog (2025): https://lancedb.com/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/
[^4]: "Top Embedding Models in 2025 — The Complete Guide" - ArtSmart AI Blog (2025): https://artsmart.ai/blog/top-embedding-models-in-2025/
[^5]: "Vector Similarity: Why It Matters for Real-World AI Applications" - Redis Blog (2025): https://redis.io/blog/vector-similarity/
[^6]: "AI Engineering Handbook: Similarity vs Semantic Search" - Exemplar (2025): https://handbook.exemplar.dev/ai_engineer/vector_dbs/similarity_semantic
[^7]: "Mastering Vector-Aware AI Agents: A Beginner's Guide" - SuperAGI (2025): https://superagi.com/mastering-vector-aware-ai-agents-a-beginners-guide-to-implementation-and-integration-in-2025/
[^8]: "Semantic Search & Chatbot Power" - CyFuture AI (2025): https://cyfuture.ai/blog/ai-vector-databases-semantic-search-chatbots
[^9]: "Understanding Semantic Search" - TigerData (2025): https://www.tigerdata.com/learn/understanding-semantic-search
[^10]: "When Large Language Models Meet Vector Databases: A Survey" - arXiv (2024): https://arxiv.org/abs/2402.01763

---

**Next**: [4.3.3 Fact Extraction & Storage](./4.3.3-fact-extraction.md) - Learn how to extract and store structured facts for long-term memory.

**Previous**: [4.3.1 Vector Databases](./4.3.1-vector-databases.md) - Comparison of LanceDB, Pinecone, and Weaviate for production AI agents.
