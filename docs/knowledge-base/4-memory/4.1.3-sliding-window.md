# 4.1.3 - Sliding Window Management

## TL;DR

**A sliding window is a fixed-size buffer that keeps only the most recent N items, automatically evicting older items when capacity is reached.** This prevents memory growth, controls token costs, and provides predictable performance regardless of conversation length.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-11
- **Prerequisites**: [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md)
- **Grounded In**: Azure OpenAI patterns (2025), LangChain, Strands Agents SDK

## Overview

Sliding windows are the fundamental mechanism for bounded memory in AI agents. Without them, memory grows linearly with conversation length, eventually exceeding context limits and causing costs to spiral. A properly sized sliding window maintains constant memory usage while preserving the most relevant recent context.

The pattern is universal across frameworks—Azure OpenAI recommends 3-5 turns, LangChain defaults to ~2000 tokens, and Strands SDK uses 20 messages. The key decision is choosing the right size for your use case.

**Key Research Findings (2024-2025)**:

- **Azure OpenAI**: 3-5 turn window recommended for multi-turn agents
- **LangChain**: Default 2000 token limit (~10 turns)
- **Strands SDK**: Default 20 message window
- **Cost Impact**: 96% reduction (2K vs 50K tokens for 100-turn conversation)

## The Problem: Infinite Memory Growth

### The Classic Challenge

Without bounded memory, conversations grow unbounded:

```
Turn 1: 2 messages, ~500 tokens
Turn 10: 20 messages, ~5K tokens
Turn 50: 100 messages, ~25K tokens
Turn 100: 200 messages, ~50K tokens  ← Exceeds many context windows
```

**Problems**:

- ❌ **Memory explosion**: 200+ messages after 100 turns
- ❌ **Token overflow**: Exceeds 128K context window
- ❌ **Cost spiral**: $0.0075+ per request (50K tokens)
- ❌ **Slow performance**: 5-10s latency processing 50K tokens

### Why This Matters

Every LLM call sends the full context. Without bounds:
- Costs grow linearly with conversation length
- Eventually exceed any context window
- Latency degrades as context grows
- Older context crowds out relevant recent information

## Core Concept

### What is a Sliding Window?

A sliding window maintains a fixed-size buffer of the most recent items. When a new item is added and the buffer is full, the oldest item is automatically evicted.

### Visual Representation

```
┌─────────────────────────────────────────────────────────────┐
│                    SLIDING WINDOW                            │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Window Size = 5                                             │
│                                                              │
│  Initial: [ ][ ][ ][ ][ ]  (empty)                          │
│                                                              │
│  +A:      [A][ ][ ][ ][ ]                                   │
│  +B:      [A][B][ ][ ][ ]                                   │
│  +C:      [A][B][C][ ][ ]                                   │
│  +D:      [A][B][C][D][ ]                                   │
│  +E:      [A][B][C][D][E]  (full)                           │
│                                                              │
│  +F:      [B][C][D][E][F]  ← A evicted                      │
│  +G:      [C][D][E][F][G]  ← B evicted                      │
│           ↑              ↑                                   │
│         oldest        newest                                 │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Characteristics

| Property | Description |
|----------|-------------|
| **Fixed Size** | Never exceeds maxSize |
| **FIFO Eviction** | Oldest removed first |
| **Automatic** | No manual cleanup needed |
| **O(1) Operations** | Constant-time add/evict |
| **Recency Bias** | Only keeps recent items |

## Implementation Patterns

### Pattern 1: Fixed Turn Count (Simplest)

**Use Case**: Most conversations, predictable context needs

Keep last N conversation turns (1 turn = user + assistant messages):

```
Window of 5 turns = 10 messages
Turn 1: [User, Assistant]
Turn 2: [User, Assistant]
...
Turn 5: [User, Assistant]
Turn 6: Evicts Turn 1, adds Turn 6
```

**Pros**:
- ✅ Simple and predictable
- ✅ Easy to reason about capacity
- ✅ Works for most use cases

**Cons**:
- ❌ Ignores message length (short vs long treated same)
- ❌ May drop important context if turns are info-dense

**When to Use**: Default choice, most applications

### Pattern 2: Token-Based (Dynamic)

**Use Case**: Variable message lengths, precise token control

Evict messages until total tokens are under limit:

```
Add message → Calculate total tokens → While over limit, evict oldest
```

**Pros**:
- ✅ Precise token control
- ✅ Adapts to message length
- ✅ Cost-efficient

**Cons**:
- ❌ Requires tokenization (slower)
- ❌ Less predictable message count
- ❌ May evict many short or few long messages

**When to Use**: Cost-critical applications, highly variable message lengths

### Pattern 3: Hybrid (Summary + Window)

**Use Case**: Long conversations requiring historical context

Combine sliding window with periodic summarization:

```
┌─────────────────────────────────────────────────────────────┐
│                    HYBRID MEMORY                             │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Long-term Summary (compressed)                              │
│  "User has been editing the Home page, prefers dark mode,   │
│   previously created 3 new sections..."                      │
│                                                              │
│  Sliding Window (last 5 turns)                              │
│  [Turn 6][Turn 7][Turn 8][Turn 9][Turn 10]                  │
│                                                              │
│  Token Budget:                                               │
│  Summary: ~500 tokens                                        │
│  Window: ~2000 tokens                                        │
│  Total: ~2500 tokens (constant)                              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Pros**:
- ✅ Preserves historical context
- ✅ Handles unlimited conversation length
- ✅ Controllable token budget

**Cons**:
- ❌ Additional LLM calls for summarization
- ❌ Summary loses detail
- ❌ More complex implementation

**When to Use**: Long-running sessions, customer support, research agents

## Framework Integration

### AI SDK v6 with Sliding Window

```typescript
import { generateText, tool, stepCountIs } from 'ai';
import { openai } from '@ai-sdk/openai';

class SlidingWindow<T> {
  private items: T[] = [];
  private readonly maxSize: number;

  constructor(maxSize: number) {
    this.maxSize = maxSize;
  }

  add(item: T): void {
    this.items.push(item);
    if (this.items.length > this.maxSize) {
      this.items.shift(); // Evict oldest
    }
  }

  getAll(): T[] {
    return [...this.items];
  }

  getRecent(n: number): T[] {
    return this.items.slice(-n);
  }
}

interface Message {
  role: 'user' | 'assistant';
  content: string;
}

// Create bounded message history
const messageWindow = new SlidingWindow<Message>(10); // 5 turns

async function chat(userMessage: string) {
  messageWindow.add({ role: 'user', content: userMessage });

  const result = await generateText({
    model: openai('gpt-4o'),
    messages: messageWindow.getAll(),
    tools: { /* ... */ },
    stopWhen: stepCountIs(10),
  });

  messageWindow.add({ role: 'assistant', content: result.text });
  return result.text;
}
```

### Entity Sliding Window

```typescript
interface Entity {
  type: string;
  id: number | string;
  label?: string;
  timestamp: number;
}

class EntityWindow extends SlidingWindow<Entity> {
  constructor(maxEntities = 20) {
    super(maxEntities);
  }

  get(type: string, id: number | string): Entity | undefined {
    return this.getAll().find(e => e.type === type && e.id === id);
  }

  getByType(type: string): Entity[] {
    return this.getAll().filter(e => e.type === type);
  }

  getMostRecent(type: string): Entity | undefined {
    const entities = this.getByType(type);
    return entities[entities.length - 1];
  }
}

// Usage with tool execution
const entityWindow = new EntityWindow(20);

const getPage = tool({
  description: 'Get page by ID',
  inputSchema: z.object({ pageId: z.number() }),
  execute: async ({ pageId }) => {
    const page = await db.getPage(pageId);

    // Add to sliding window
    entityWindow.add({
      type: 'page',
      id: page.id,
      label: page.title,
      timestamp: Date.now(),
    });

    return page;
  },
});
```

## Research & Benchmarks

### Window Size Recommendations

| Source | Recommended Size | Context |
|--------|------------------|---------|
| **Azure OpenAI** | 3-5 turns | Multi-turn assistants |
| **LangChain** | ~2000 tokens | Token-based default |
| **Strands SDK** | 20 messages | Message-based default |
| **Production Best** | 5-10 turns | Balances context vs cost |

### Cost Comparison (100-turn conversation)

| Approach | Tokens per Call | Cost per Call | Total Cost |
|----------|-----------------|---------------|------------|
| **No Window** | 50,000 | $0.0075 | $0.75 |
| **5-turn Window** | 2,000 | $0.0003 | $0.03 |
| **Savings** | **96%** | **96%** | **96%** |

### Latency Impact

| Context Size | Latency (p95) |
|--------------|---------------|
| 2K tokens | 0.5s |
| 10K tokens | 2s |
| 50K tokens | 8s |
| 100K tokens | 15s+ |

**Key Insight**: Sliding windows keep latency constant regardless of conversation length.

## Eviction Policies

### FIFO (First-In-First-Out) — Default

Always evict the oldest item.

**Pros**: Simple, predictable, fast
**Cons**: May drop important old items

### LRU (Least Recently Used)

Evict item that hasn't been accessed longest.

**Pros**: Keeps frequently accessed items
**Cons**: Requires access tracking, slower

### Priority-Based

Assign priorities, evict lowest priority first.

**Pros**: Keeps important items longer
**Cons**: Requires manual priority assignment

### Sticky Items

Protect certain items from eviction.

```
[System Prompt ←sticky] [Turn 1] [Turn 2] [Turn 3] [Turn 4]
                          ↑
                    evict this, not system prompt
```

**Pros**: Preserves critical context
**Cons**: May refuse eviction if all items sticky

## When to Use This Pattern

### ✅ Use Sliding Windows When:

1. **Multi-turn conversations**
   - Any conversation >5 turns
   - Predictable context needs

2. **Cost control required**
   - High-volume applications
   - Budget constraints

3. **Bounded latency needed**
   - Real-time applications
   - User-facing chat

4. **Simple implementation preferred**
   - MVP/prototype stage
   - Clear requirements

### ❌ Consider Alternatives When:

1. **Full history needed**
   - Audit trails, compliance
   - Use persistent storage + summarization

2. **Semantic retrieval required**
   - "Find what we discussed last week"
   - Use vector database + RAG

3. **Complex prioritization needed**
   - Some messages more important than others
   - Use priority-based or relevance-weighted eviction

### Window Size Decision Guide

| Conversation Type | Recommended Window | Reasoning |
|-------------------|-------------------|-----------|
| Quick Q&A | 3 turns | Minimal context needed |
| Task assistance | 5 turns | Balance of context and cost |
| Support chat | 10 turns | More context for complex issues |
| Research session | 15-20 turns | Extended exploration |
| Audit-required | Hybrid | Window + persistent storage |

## Production Best Practices

### 1. Size Window Appropriately

```
Rule of thumb:
- Simple tasks: 3-5 turns (6-10 messages)
- Standard tasks: 5-10 turns (10-20 messages)
- Complex tasks: 10-20 turns (20-40 messages)

Entity windows typically 2× message window (more entities than turns)
```

### 2. Combine with Summarization for Long Sessions

```
Turns 1-10: Full detail in sliding window
Turn 10: Generate summary, clear window
Turns 11-20: Summary + sliding window
Turn 20: Update summary, clear window
...
```

### 3. Monitor Window Utilization

Track these metrics:

| Metric | Target | Alert |
|--------|--------|-------|
| Average fill rate | 60-80% | >95% |
| Eviction rate | <20% | >40% |
| Token cost per conversation | Budget-dependent | Exceeds budget |
| Context relevance | Qualitative | User complaints |

### 4. Use Callbacks for Visibility

```typescript
const window = new SlidingWindow({
  maxSize: 10,
  onEvict: (item) => {
    console.log(`Evicted: ${item.role} message`);
    metrics.increment('window.evictions');
  },
  onAdd: (item) => {
    metrics.increment('window.additions');
  },
});
```

### Common Pitfalls

#### ❌ Pitfall: Window Too Small

**Problem**: Important context lost, agent seems forgetful.

**Solution**: Increase window or add summarization:
```
If users complain "you forgot what I said":
- Increase window from 3 → 5 turns
- Add hybrid summarization
```

#### ❌ Pitfall: Window Too Large

**Problem**: High costs, slow responses, context dilution.

**Solution**: Reduce window or use token-based sizing:
```
If costs are high or latency is slow:
- Reduce window from 20 → 10 turns
- Switch to token-based (2000 token limit)
```

## Key Takeaways

1. **Sliding windows prevent unbounded memory growth** - Essential for any multi-turn application
2. **5-turn window is a good default** - Azure recommendation, balances context vs cost
3. **FIFO is simplest and usually sufficient** - Only use LRU/priority for specific needs
4. **Combine with summarization for long sessions** - Preserves historical context
5. **Monitor utilization in production** - Tune window size based on real usage

**Quick Implementation Checklist**:

- [ ] Choose window type (turn-based, token-based, or hybrid)
- [ ] Set initial size (start with 5 turns / 10 messages)
- [ ] Implement FIFO eviction
- [ ] Add metrics/logging for eviction events
- [ ] Test with long conversations
- [ ] Tune size based on production data

## References

1. **Microsoft** (2025). "Azure OpenAI: Conversation Context Management Best Practices". https://learn.microsoft.com/azure/ai/openai/
2. **Strands Agents SDK** (2025). "Conversation Management". https://strandsagents.com/latest/documentation/
3. **LangChain** (2025). "Memory Management". https://js.langchain.com/docs/modules/memory/
4. **Google ADK** (2025). "Context Management Strategies". https://github.com/google/adk-python/discussions/826
5. **Sparkco AI** (2025). "Optimizing Context Windows in AI Agents". https://sparkco.ai/blog/optimizing-context-windows-in-ai-agents
6. **Factory AI** (2025). "The Context Window Problem". https://factory.ai/news/context-window-problem

**Related Topics**:

- [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md)
- [4.1.2 Entity Extraction](./4.1.2-entity-extraction.md)
- [4.1.4 Reference Resolution](./4.1.4-reference-resolution.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
