# 4.1.1 - Working Memory Concept

## TL;DR

**Working memory is an in-memory buffer that stores recently extracted entities and conversation context for immediate agent tasks—like RAM in computers, it provides fast access to current context without persistence overhead.** This enables reference resolution ("this page"), reduces token costs by 90%, and achieves 91% lower latency compared to full-context approaches.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-11
- **Prerequisites**: [4.0.1 Memory Systems Overview](./4.0.1-memory-systems-overview.md)
- **Grounded In**: Mem0 (2025), CoALA (2024), Azure OpenAI patterns, Letta/MemGPT

## Overview

Working memory is the cognitive scratchpad that holds information an agent is actively processing. While LLMs have no built-in state between API calls, working memory provides a structured way to track entities, maintain context, and enable natural language references within a session.

The concept draws from cognitive science—humans use working memory to hold phone numbers, follow conversations, and solve multi-step problems. AI agents need the same capability to handle tasks like "show me page 42, then update that page's title."

**Key Research Findings (2024-2025)**:

- **Mem0**: 26% accuracy improvement, 91% latency reduction, 90% token savings
- **Capacity**: Human working memory holds 7±2 items; agent working memory typically 10-50 entities
- **Access Speed**: 0.2ms in-memory vs 50-200ms database queries
- **Production Impact**: Enables reference resolution that would otherwise require expensive re-queries

## The Problem: Context Overload

### The Classic Challenge

Without working memory, agents must either:
1. **Send everything** - Include full conversation history (expensive, slow)
2. **Forget everything** - Lose context between turns (broken UX)

```
Turn 1: User: "Show me page 42"
Agent: [Fetches page 42] "Here's page 42: Home Page"

Turn 2: User: "Update that page's title"
Agent: "Which page do you mean?"  ← Lost context!
```

**Problems**:

- ❌ **Token Explosion**: 26K+ tokens for long conversations
- ❌ **High Latency**: 17.12s p95 vs 1.44s with memory (Mem0 benchmark)
- ❌ **Context Dilution**: Important info buried in irrelevant messages
- ❌ **Lost References**: Cannot resolve "this", "that", "it"

### Why This Matters

Every failed reference resolution forces:
- User to re-specify the entity
- Agent to re-query the database
- Increased frustration and task abandonment

## Core Concept

### What is Working Memory?

Working memory is an in-memory buffer that stores **recently extracted entities** and **conversation context** needed for immediate agent tasks.

### The RAM Analogy

```
┌─────────────────────────────────────────────────────────────┐
│              COMPUTER RAM vs AI WORKING MEMORY               │
├────────────────────────────┬────────────────────────────────┤
│       Computer RAM         │     AI Working Memory          │
├────────────────────────────┼────────────────────────────────┤
│ Volatile storage           │ Temporary context              │
│ Fast read/write (ns)       │ Fast entity lookup (0.2ms)     │
│ Limited capacity (16-64GB) │ Limited capacity (10-50 items) │
│ Stores running programs    │ Stores recent entities         │
│ Evicts old data when full  │ Evicts old entities when full  │
│ Lost on shutdown           │ Lost after session             │
└────────────────────────────┴────────────────────────────────┘
```

### Key Characteristics

| Property | Description |
|----------|-------------|
| **Short-Term** | Retains recent information (last 5-10 turns) |
| **Task-Relevant** | Stores entities agent is currently working with |
| **Volatile** | Does NOT persist across sessions |
| **Fast Access** | In-memory lookup without database queries |
| **Limited Capacity** | Fixed size to prevent token overflow |

### What Goes in Working Memory?

| Store ✅ | Don't Store ❌ |
|----------|---------------|
| Recently mentioned page IDs | Full page content |
| Current user's name | Historical user preferences |
| Active task context | Completed task history |
| Last 5 tool results | All tool execution logs |
| Pronouns to resolve ("this", "that") | Unrelated past conversations |

## Implementation Patterns

### Pattern 1: Sliding Window (Simplest)

**Use Case**: Short conversations, predictable context needs

Keep last N entities in FIFO (first-in, first-out) order.

```
Entity added → Check capacity → Evict oldest if full
     ↓
Memory: [entity_1, entity_2, ..., entity_N]
              ↑
         Oldest (evict first)
```

**Pros**:
- ✅ Simple to implement
- ✅ Predictable memory usage
- ✅ Good for 5-10 turn conversations

**Cons**:
- ❌ May drop important entities
- ❌ No semantic relevance (just recency)

**When to Use**: Prototypes, simple assistants, short task sequences

### Pattern 2: Relevance-Based Eviction

**Use Case**: Complex multi-step tasks, longer conversations

Score entities by recency AND frequency of access:

```
Score = (0.6 × recency) + (0.4 × frequency)

Where:
- recency = exp(-0.1 × age_in_minutes)
- frequency = min(access_count / 10, 1.0)
```

**Pros**:
- ✅ Keeps important entities longer
- ✅ Adapts to conversation flow
- ✅ Better for multi-step tasks

**Cons**:
- ❌ More complex to implement
- ❌ Requires tuning heuristics
- ❌ Harder to debug

**When to Use**: Production agents, complex workflows, CMS operations

### Pattern 3: Hybrid with Summarization

**Use Case**: Extended conversations exceeding working memory capacity

Combine entity tracking with conversation summarization:

```
┌─────────────────────────────────────────────────┐
│              HYBRID MEMORY                       │
├─────────────────────────────────────────────────┤
│                                                  │
│  Working Memory (entities)                       │
│  ┌─────────────────────────────────────────┐    │
│  │ [page_42] [entry_7] [user_123] [task_1] │    │
│  └─────────────────────────────────────────┘    │
│                    +                             │
│  Summary Buffer (compressed context)             │
│  ┌─────────────────────────────────────────┐    │
│  │ "User is editing Home page, requested   │    │
│  │  title change, prefers dark mode..."    │    │
│  └─────────────────────────────────────────┘    │
│                                                  │
└─────────────────────────────────────────────────┘
```

**Pros**:
- ✅ Handles unlimited conversation length
- ✅ Preserves both entities AND context
- ✅ Controllable token budget

**Cons**:
- ❌ Additional LLM calls for summarization
- ❌ Summary may lose important details
- ❌ More infrastructure complexity

**When to Use**: Long-running sessions, customer support, research agents

## Framework Integration

### AI SDK v6 with Working Memory

```typescript
import { generateText, tool, stepCountIs } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Entity {
  type: string;
  id: number | string;
  label?: string;
  timestamp: number;
}

class WorkingMemory {
  private entities: Map<string, Entity> = new Map();
  private readonly maxSize: number;

  constructor(maxSize = 20) {
    this.maxSize = maxSize;
  }

  add(entity: Omit<Entity, 'timestamp'>): void {
    const key = `${entity.type}:${entity.id}`;
    this.entities.set(key, { ...entity, timestamp: Date.now() });

    if (this.entities.size > this.maxSize) {
      // Evict oldest
      const oldest = [...this.entities.entries()]
        .sort((a, b) => a[1].timestamp - b[1].timestamp)[0];
      this.entities.delete(oldest[0]);
    }
  }

  getRecent(type?: string): Entity[] {
    return [...this.entities.values()]
      .filter(e => !type || e.type === type)
      .sort((a, b) => b.timestamp - a.timestamp);
  }
}

// Agent with working memory
const memory = new WorkingMemory(20);

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: userMessage,
  tools: {
    getPage: tool({
      description: 'Get a page by ID',
      inputSchema: z.object({ pageId: z.number() }),
      execute: async ({ pageId }, { experimental_context }) => {
        const page = await db.getPage(pageId);

        // Store in working memory
        memory.add({
          type: 'page',
          id: page.id,
          label: page.title,
        });

        return page;
      },
    }),
  },
  stopWhen: stepCountIs(10),
  experimental_context: { memory },
});
```

### Memory-Aware System Prompt

Include working memory state in the system prompt:

```typescript
function buildSystemPrompt(memory: WorkingMemory): string {
  const entities = memory.getRecent(5);

  return `You are a CMS assistant.

## Working Memory
Recently referenced entities:
${entities.map(e => `- ${e.type} "${e.label}" (ID: ${e.id})`).join('\n')}

When user says "this page" or "that entry", refer to the most recent entity of that type.
`;
}
```

## Research & Benchmarks

### Mem0 Performance (April 2025)

| Metric | Full Context | With Working Memory | Improvement |
|--------|--------------|---------------------|-------------|
| **Accuracy** | 52.9% | 66.9% | **+26%** |
| **Latency (p95)** | 17.12s | 1.44s | **-91%** |
| **Tokens/conversation** | 26K | 1.8K | **-93%** |
| **Median latency** | 9.87s | 0.20s | **-98%** |

### Access Speed Comparison

| Storage Type | Access Latency | Use Case |
|--------------|----------------|----------|
| Working Memory | 0.2ms | Current session entities |
| Vector DB | 50-200ms | Semantic search |
| SQL Database | 20-100ms | Indexed lookups |
| Full Context | 1-10s | Processing all history |

**Key Insight**: Working memory is 100-1000× faster than database queries for recent entity access.

## When to Use This Pattern

### ✅ Use Working Memory When:

1. **Multi-turn conversations** (5+ exchanges)
   - Users reference previous messages
   - Entity tracking needed across turns

2. **Reference resolution required**
   - "this page", "that entry", "it"
   - Natural language entity references

3. **Real-time response needed**
   - Sub-second latency requirements
   - Interactive chat applications

4. **Cost optimization priority**
   - High volume of requests
   - Token costs are significant

### ❌ Don't Use When:

1. **Single-shot queries**
   - No conversation context needed
   - Stateless Q&A

2. **Cross-session persistence required**
   - User preferences that must persist
   - Audit trails and compliance

3. **Complex semantic search**
   - "Find all pages about AI"
   - Requires vector similarity

### Decision Matrix

| Scenario | Working Memory | Long-term Memory |
|----------|----------------|------------------|
| "Update that page's title" | ✅ | ❌ |
| "What did I ask last week?" | ❌ | ✅ |
| "Find similar conversations" | ❌ | ✅ |
| "This page needs dark mode" | ✅ | ❌ |
| "Remember: I prefer dark mode" | ❌ | ✅ |

## Production Best Practices

### 1. Size Working Memory Appropriately

```
Rule of thumb: 1-2 entities per conversation turn

For 10-turn conversations:
  maxSize = 10 turns × 2 entities = 20 entities
  TTL = 5 minutes (session timeout)
```

### 2. Combine with Summarization for Long Conversations

After 10+ turns, summarize and compress:

```
Turns 1-10: Full detail in working memory
Turn 10: Generate summary, clear old entities
Turns 11-20: Summary + recent entities
```

### 3. Monitor Memory Utilization

Track these metrics:

| Metric | Target | Alert |
|--------|--------|-------|
| Memory fill rate | 50-80% | >95% |
| Eviction rate | <10% | >30% |
| Reference resolution success | >90% | <75% |
| Avg access latency | <1ms | >5ms |

### 4. Fallback to Database

When entity not in working memory:

```
1. Check working memory first (0.2ms)
2. If not found, query database (50-200ms)
3. Add result to working memory for next access
```

### Common Pitfalls

#### ❌ Pitfall: Storing Too Much

**Problem**: Storing full page content instead of just references bloats memory and slows access.

**Solution**: Store only identifiers and minimal metadata:
```
✅ { type: 'page', id: 42, label: 'Home' }
❌ { type: 'page', id: 42, content: '...10KB of HTML...' }
```

#### ❌ Pitfall: No Eviction Policy

**Problem**: Memory grows unbounded, eventually crashing or degrading performance.

**Solution**: Always set maxSize and implement eviction:
```
maxSize: 20-50 entities
TTL: 5-10 minutes
Policy: FIFO or relevance-based
```

## Key Takeaways

1. **Working memory is agent RAM** - Fast, volatile, limited capacity for current context
2. **Enables natural references** - "this page", "that entry" resolve to recent entities
3. **90% token savings** - Store entity IDs, not full content
4. **91% latency reduction** - In-memory access vs database queries
5. **Always set limits** - Max size + TTL prevents unbounded growth

**Quick Implementation Checklist**:

- [ ] Define entity types for your domain (page, entry, user, etc.)
- [ ] Set appropriate maxSize (typically 20-50)
- [ ] Implement eviction policy (FIFO or relevance-based)
- [ ] Add entities from tool results automatically
- [ ] Include memory state in system prompt
- [ ] Monitor utilization and eviction rates

## References

1. **Mem0 Team** (2025). "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory". arXiv. https://arxiv.org/abs/2504.19413
2. **Sumers et al.** (2024). "Cognitive Architectures for Language Agents". arXiv. https://arxiv.org/abs/2309.02427
3. **Microsoft** (2025). "Azure OpenAI: Conversation Context Management Best Practices". https://learn.microsoft.com/azure/ai/openai/
4. **Letta** (2025). "Memory Blocks: The Key to Agentic Context Management". https://www.letta.com/blog/memory-blocks
5. **Google ADK** (2025). "Context Management Strategies in AI Agents". https://github.com/google/adk-python/discussions/826
6. **Vercel AI SDK** (2025). "AI SDK v6 Documentation". https://v6.ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling

**Related Topics**:

- [4.0.1 Memory Systems Overview](./4.0.1-memory-systems-overview.md)
- [4.1.2 Entity Extraction](./4.1.2-entity-extraction.md)
- [4.1.3 Sliding Window Management](./4.1.3-sliding-window.md)
- [4.1.4 Reference Resolution](./4.1.4-reference-resolution.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
