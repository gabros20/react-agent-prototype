# 4.3.5 - Memory Type Decision Framework

## TL;DR

**Choose memory types based on information lifetime: working memory for current session context, episodic memory for recent cross-session facts, and semantic memory for permanent preferences and critical data.** A hybrid approach using all three tiers delivers 26% accuracy improvement with 91% latency reduction.

- **Status**: ‚úÖ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md), [4.3.1 Vector Databases](./4.3.1-vector-databases.md)
- **Grounded In**: Mem0 (2024), Google ADK session patterns, Microsoft Autogen memory tiers

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-choosing-the-wrong-memory-tier)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Selecting the right memory tier is one of the most impactful decisions in agent architecture. Store too much in long-term memory and costs spiral; store too little and your agent seems forgetful. The three-tier model (working, episodic, semantic) maps to human memory systems and provides clear decision criteria.

The core insight is **information lifetime**: how long does this information stay relevant? Current session context belongs in working memory (free, instant). Recent facts that might be useful next session go to episodic memory (cheap, fast). Permanent preferences and critical data get semantic memory (moderate cost, comprehensive).

**Key Research Findings (2024-2025)**:

- **Mem0**: 26% accuracy improvement, 91% latency reduction, 90% token savings with tiered memory
- **Microsoft Autogen**: Recommends three-tier architecture for production agents
- **Google ADK**: Session-based memory with configurable retention policies

## The Problem: Choosing the Wrong Memory Tier

### The Classic Challenge

Agents without proper memory tiering either:

1. **Store everything long-term** ‚Üí $500+/month in vector DB costs, slow retrieval, irrelevant context
2. **Store nothing long-term** ‚Üí Agent forgets preferences, repeats questions, frustrates users
3. **Mix tiers randomly** ‚Üí Unpredictable behavior, hard to debug, inconsistent personalization

**Problems**:

- ‚ùå **Cost explosion**: Storing session-specific data in vector DB wastes $0.01-0.05 per query
- ‚ùå **Context pollution**: Old, irrelevant facts dilute search results
- ‚ùå **Lost personalization**: Forgetting critical preferences (allergies, preferences, goals)
- ‚ùå **Performance degradation**: Searching large vector stores adds 50-100ms latency

### Why This Matters

Memory tier decisions compound over time. An agent with 10,000 users storing everything in semantic memory accumulates millions of facts, most irrelevant. Query costs grow linearly while accuracy drops due to noise. The right tier selection maintains O(1) cost per user while preserving the facts that matter.

## Core Concept

### Three Memory Tiers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MEMORY ARCHITECTURE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  TIER 1: WORKING MEMORY                                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Duration: Current session (30 min - 24 hours)          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Storage: In-memory / Redis                             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Retrieval: Instant (0ms), always in context            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cost: Free                                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Examples: Conversation history, temp references        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  TIER 2: EPISODIC MEMORY                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Duration: Recent sessions (7-30 days TTL)              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Storage: Vector DB with expiration                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Retrieval: Fast (20-50ms), semantic search             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cost: Low ($0.01-0.02/query)                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Examples: Recent goals, events, interactions           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  TIER 3: SEMANTIC MEMORY                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Duration: Permanent (months to years)                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Storage: Vector DB + optional Graph DB                 ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Retrieval: Moderate (50-100ms), comprehensive search   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cost: Medium ($0.02-0.10/query)                        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Examples: Core preferences, relationships, medical     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Decision Tree

```
Is this info needed AFTER session ends?
    ‚îÇ
    ‚îú‚îÄ‚îÄ NO ‚Üí WORKING MEMORY
    ‚îÇ        (free, in context)
    ‚îÇ
    ‚îî‚îÄ‚îÄ YES ‚Üí Is this a CORE preference or TEMPORARY goal?
              ‚îÇ
              ‚îú‚îÄ‚îÄ TEMPORARY (days/weeks) ‚Üí EPISODIC MEMORY
              ‚îÇ   (30-day TTL, auto-expires)
              ‚îÇ
              ‚îî‚îÄ‚îÄ PERMANENT (months/years) ‚Üí SEMANTIC MEMORY
                  (never expires, critical data)
```

### Feature Comparison

| Feature | Working Memory | Episodic Memory | Semantic Memory |
|---------|----------------|-----------------|-----------------|
| **Duration** | Minutes - Hours | Days - Weeks | Months - Years |
| **Persistence** | ‚ùå Ephemeral | ‚úÖ TTL-based | ‚úÖ Permanent |
| **Retrieval** | ‚ö° 0ms (in context) | üü° 20-50ms | üü† 50-100ms |
| **Cost/Query** | $0 | $0.01-0.02 | $0.02-0.10 |
| **Storage Cost** | $0/month | $5-20/month | $20-100/month |
| **Cross-Session** | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |
| **Auto-included** | ‚úÖ Yes | ‚ùå Query-based | ‚ùå Query-based |

### Key Decision Questions

1. **Lifetime**: "Will this be useful next week?" ‚Üí YES = Long-term
2. **Importance**: "Is this a core preference or casual context?" ‚Üí Core = Semantic
3. **Frequency**: "How often will this be referenced?" ‚Üí Every session = Semantic
4. **Cost Sensitivity**: "Is retrieval cost a concern?" ‚Üí YES = Prefer working memory

## Implementation Patterns

### Pattern 1: Hybrid Memory System

**Use Case**: Production agents needing all three tiers with automatic classification

```typescript
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import lancedb from 'lancedb';
import Redis from 'ioredis';

const MemoryClassification = z.object({
  type: z.enum(['working', 'episodic', 'semantic']),
  category: z.string().optional(),
  ttlDays: z.number().optional(),
  reason: z.string(),
});

class HybridMemorySystem {
  private redis: Redis;
  private vectorDB: any;
  private config: {
    workingMemorySize: number;
    episodicTTLDays: number;
    semanticCategories: string[];
  };

  constructor(config: typeof this.config) {
    this.config = config;
    this.redis = new Redis();
  }

  async add(userId: string, sessionId: string, content: string) {
    // Auto-classify using LLM
    const classification = await this.classify(content);

    switch (classification.type) {
      case 'working':
        await this.addWorking(sessionId, content);
        break;
      case 'episodic':
        await this.addEpisodic(userId, content, classification.ttlDays ?? 30);
        break;
      case 'semantic':
        await this.addSemantic(userId, content, classification.category!);
        break;
    }
  }

  private async classify(content: string) {
    const { object } = await generateObject({
      model: openai('gpt-4o-mini'),
      schema: MemoryClassification,
      prompt: `Classify this information for memory storage:
"${content}"

- working: Temporary (only needed this session). Examples: "order #12345", "this document"
- episodic: Recent context (may be useful next session, 7-30 days). Examples: "training for marathon", "on vacation"
- semantic: Permanent fact (core preference, medical, identity). Examples: "allergic to peanuts", "prefers dark mode"`,
    });
    return object;
  }

  private async addWorking(sessionId: string, content: string) {
    await this.redis.lpush(`session:${sessionId}:messages`, content);
    await this.redis.ltrim(`session:${sessionId}:messages`, 0, this.config.workingMemorySize - 1);
    await this.redis.expire(`session:${sessionId}:messages`, 3600);
  }

  private async addEpisodic(userId: string, content: string, ttlDays: number) {
    const embedding = await this.embed(content);
    const expiresAt = new Date(Date.now() + ttlDays * 24 * 60 * 60 * 1000);

    const table = await this.vectorDB.openTable('episodic_memory');
    await table.add([{
      id: crypto.randomUUID(),
      user_id: userId,
      content,
      vector: embedding,
      expires_at: expiresAt.toISOString(),
    }]);
  }

  private async addSemantic(userId: string, content: string, category: string) {
    const embedding = await this.embed(content);

    const table = await this.vectorDB.openTable('semantic_memory');
    await table.add([{
      id: crypto.randomUUID(),
      user_id: userId,
      content,
      category,
      vector: embedding,
      permanent: true,
    }]);
  }

  async retrieve(query: string, userId: string, sessionId: string) {
    // Tier 1: Working memory (always included, free)
    const working = await this.redis.lrange(`session:${sessionId}:messages`, 0, -1);

    // Tier 2: Episodic memory (recent, not expired)
    const episodicTable = await this.vectorDB.openTable('episodic_memory');
    const episodic = await episodicTable
      .search(await this.embed(query))
      .where(`user_id = '${userId}' AND expires_at > '${new Date().toISOString()}'`)
      .limit(5)
      .execute();

    // Tier 3: Semantic memory (permanent facts)
    const semanticTable = await this.vectorDB.openTable('semantic_memory');
    const semantic = await semanticTable
      .search(await this.embed(query))
      .where(`user_id = '${userId}'`)
      .limit(3)
      .execute();

    return {
      working,
      episodic: episodic.map(r => r.content),
      semantic: semantic.map(r => r.content),
    };
  }

  private async embed(text: string): Promise<number[]> {
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text,
    });
    return response.data[0].embedding;
  }
}
```

**Pros**:
- ‚úÖ Automatic tier selection via LLM classification
- ‚úÖ Cost-optimized (only stores what matters long-term)
- ‚úÖ TTL-based expiration for episodic tier

**Cons**:
- ‚ùå Classification adds ~100ms latency
- ‚ùå LLM may misclassify edge cases

**When to Use**: Default choice for production agents with personalization needs

### Pattern 2: Explicit Tier Selection

**Use Case**: When you know exactly which tier each fact belongs to

```typescript
class ExplicitMemory {
  async addWorking(sessionId: string, content: string) {
    // Session-specific, auto-expires
    await redis.lpush(`session:${sessionId}`, content);
  }

  async addEpisodic(userId: string, content: string, ttlDays = 30) {
    // Cross-session with TTL
    await this.storeWithExpiration(userId, content, ttlDays);
  }

  async addSemantic(userId: string, content: string) {
    // Permanent, never expires
    await this.storePermanent(userId, content);
  }
}

// Usage - explicit control
const memory = new ExplicitMemory();

// Working: Order context (session-specific)
await memory.addWorking(sessionId, 'Discussing order #12345');

// Episodic: Temporary goal (30 days)
await memory.addEpisodic(userId, 'Training for marathon', 90);

// Semantic: Permanent preference
await memory.addSemantic(userId, 'Allergic to peanuts');
```

**Pros**:
- ‚úÖ No classification overhead
- ‚úÖ Deterministic behavior
- ‚úÖ Easier to debug

**Cons**:
- ‚ùå Requires manual tier selection
- ‚ùå Developer must understand tier semantics

**When to Use**: Well-defined data flows where tier is known at write time

### Pattern 3: Promotion-Based Memory

**Use Case**: Start cheap, promote based on access patterns

```typescript
class PromotionMemory {
  private accessCounts = new Map<string, number>();

  async add(userId: string, sessionId: string, content: string) {
    // Always start in working memory (cheapest)
    await this.addWorking(sessionId, content);
  }

  async trackAccess(userId: string, content: string) {
    const key = `${userId}:${this.hash(content)}`;
    const count = (this.accessCounts.get(key) ?? 0) + 1;
    this.accessCounts.set(key, count);

    // Promote after 3 accesses
    if (count === 3) {
      await this.promoteToEpisodic(userId, content);
    }

    // Promote to semantic after 10 accesses across sessions
    if (count === 10) {
      await this.promoteToSemantic(userId, content);
    }
  }

  private async promoteToEpisodic(userId: string, content: string) {
    console.log(`Promoting to episodic: "${content}"`);
    await this.storeEpisodic(userId, content, 30);
  }

  private async promoteToSemantic(userId: string, content: string) {
    console.log(`Promoting to semantic: "${content}"`);
    await this.storeSemantic(userId, content);
  }
}
```

**Pros**:
- ‚úÖ Zero upfront classification cost
- ‚úÖ Only frequently-accessed facts get promoted
- ‚úÖ Self-optimizing based on actual usage

**Cons**:
- ‚ùå First access always misses long-term memory
- ‚ùå Requires access tracking infrastructure

**When to Use**: Unknown access patterns, cost-sensitive applications

## Framework Integration

### AI SDK v6 Memory Tools

```typescript
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

// Memory retrieval tool - searches all tiers
const retrieveMemory = tool({
  description: 'Retrieve relevant memories for context',
  parameters: z.object({
    query: z.string().describe('What to search for'),
  }),
  execute: async ({ query }, { userId, sessionId, memorySystem }) => {
    const memories = await memorySystem.retrieve(query, userId, sessionId);

    return {
      working: memories.working,
      episodic: memories.episodic,
      semantic: memories.semantic,
      totalRetrieved: memories.working.length + memories.episodic.length + memories.semantic.length,
    };
  },
});

// Memory storage tool - auto-classifies tier
const storeMemory = tool({
  description: 'Store a fact for future reference',
  parameters: z.object({
    content: z.string().describe('The fact to remember'),
    tier: z.enum(['auto', 'working', 'episodic', 'semantic'])
      .optional()
      .default('auto')
      .describe('Memory tier (auto = LLM classifies)'),
  }),
  execute: async ({ content, tier }, { userId, sessionId, memorySystem }) => {
    if (tier === 'auto') {
      await memorySystem.add(userId, sessionId, content);
    } else {
      await memorySystem.addExplicit(userId, sessionId, content, tier);
    }

    return { stored: true, content, tier };
  },
});

// Agent with memory integration
const result = await generateText({
  model: openai('gpt-4o'),
  system: `You have access to a three-tier memory system:
- Working: Current session context (auto-included)
- Episodic: Recent facts from past sessions (query-based)
- Semantic: Permanent preferences and critical data (query-based)

Before answering, retrieve relevant memories. Store important new facts.`,
  messages,
  tools: { retrieveMemory, storeMemory },
  maxSteps: 10,
});
```

### Context Injection Pattern

```typescript
// Inject memories into system prompt
async function buildContextWithMemories(
  userId: string,
  sessionId: string,
  query: string
) {
  const memories = await memorySystem.retrieve(query, userId, sessionId);

  const contextParts = [];

  // Always include semantic (critical facts)
  if (memories.semantic.length > 0) {
    contextParts.push(`## User Profile\n${memories.semantic.join('\n')}`);
  }

  // Include relevant episodic
  if (memories.episodic.length > 0) {
    contextParts.push(`## Recent Context\n${memories.episodic.join('\n')}`);
  }

  // Working memory included in messages, not system prompt

  return contextParts.join('\n\n');
}

const result = await generateText({
  model: openai('gpt-4o'),
  system: `You are a helpful assistant.\n\n${await buildContextWithMemories(userId, sessionId, query)}`,
  messages: workingMemory, // Working memory as conversation history
  tools,
});
```

## Research & Benchmarks

### Academic Research (2024-2025)

#### Mem0 Production Benchmarks

**Source**: Mem0 Documentation & Benchmarks (2024)

- **Accuracy**: 26.05% improvement over baseline (LOCOMO benchmark)
- **Latency**: 91.02% reduction with caching
- **Tokens**: 90.03% reduction through selective retrieval
- **Architecture**: Three-tier with automatic classification

#### LongMemEval Benchmark

**Source**: Microsoft Research (2024)

- Tests long-term memory across 500+ turns
- Best performers use tiered memory with TTL
- Single-tier systems degrade after ~100 turns
- Recommended: Working (10 turns) + Episodic (30 days) + Semantic (permanent)

### Cost-Benefit Analysis

**Scenario: E-commerce Chatbot (10,000 users)**

| Memory Strategy | Monthly Cost | Conversion Lift | ROI |
|-----------------|--------------|-----------------|-----|
| Working only | $0 | Baseline | - |
| + Episodic | $50 | +15% | 30x |
| + Semantic | $250 | +30% | 20x |

**Scenario: Personal Assistant (1,000 power users)**

| Memory Strategy | Monthly Cost | User Satisfaction | ROI |
|-----------------|--------------|-------------------|-----|
| Working only | $0 | 3.2/5 | - |
| + Episodic | $100 | 4.1/5 | 15x |
| + Semantic | $600 | 4.7/5 | 12x |

### Latency Comparison

| Memory Tier | Retrieval Latency | Cost per Query |
|-------------|-------------------|----------------|
| Working | 0ms (in context) | $0 |
| + Redis Cache | 1-5ms | $0.0001 |
| + Vector DB | 20-50ms | $0.01-0.02 |
| + Graph DB | 50-100ms | $0.05-0.10 |

## When to Use This Pattern

### ‚úÖ Use Three-Tier Memory When:

1. **Multi-session interactions**
   - Users return across days/weeks
   - Continuity matters for UX

2. **Personalization required**
   - Preferences affect responses
   - User-specific behavior needed

3. **Mixed information lifetimes**
   - Some facts are temporary
   - Some facts are permanent

4. **Cost-conscious applications**
   - Can't afford to store everything
   - Need selective retention

### ‚ùå Don't Use When:

1. **Single-session only**
   - Users never return
   - Working memory sufficient

2. **No personalization**
   - Same response for everyone
   - No user-specific context

3. **Unlimited budget**
   - Can afford to store everything
   - Simplicity over optimization

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| One-time support queries | Working memory only |
| Repeat customers, low volume | Working + Episodic |
| Power users, high personalization | All three tiers |
| Medical/legal (critical data) | Semantic for critical, others for context |
| Cost-constrained startup | Promotion-based (start cheap) |

## Production Best Practices

### 1. Set Appropriate TTLs

```typescript
// Tier-specific TTL recommendations
const TTL_CONFIG = {
  working: 3600,       // 1 hour (session)
  episodic: {
    shortGoals: 14,    // 2 weeks
    mediumGoals: 30,   // 1 month
    longGoals: 90,     // 3 months
  },
  semantic: Infinity,  // Never expires
};
```

**Why**: Prevents episodic memory bloat while keeping relevant context

### 2. Monitor Memory Growth

Track per-user memory metrics:

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| Episodic facts/user | <100 | >500 |
| Semantic facts/user | <20 | >100 |
| Retrieval latency p95 | <100ms | >200ms |
| Monthly cost/user | <$0.10 | >$0.50 |

### 3. Graceful Degradation

```typescript
async function retrieveWithFallback(query: string, userId: string, sessionId: string) {
  try {
    return await memorySystem.retrieve(query, userId, sessionId);
  } catch (error) {
    console.error('Memory retrieval failed:', error);
    // Fallback: working memory only (always available)
    return {
      working: await redis.lrange(`session:${sessionId}`, 0, -1),
      episodic: [],
      semantic: [],
    };
  }
}
```

### Common Pitfalls

#### ‚ùå Pitfall: Over-storing in Semantic Memory

**Problem**: Every fact goes to permanent storage

```typescript
// BAD: Storing casual preferences permanently
await memory.addSemantic(userId, 'I like coffee'); // Overkill
```

**Solution**: Reserve semantic for critical data

```typescript
// GOOD: Only critical facts are permanent
await memory.addSemantic(userId, 'Allergic to peanuts'); // Critical
await memory.addEpisodic(userId, 'I like coffee', 30);   // Casual
```

#### ‚ùå Pitfall: No TTL on Episodic Memory

**Problem**: Episodic facts accumulate forever

**Solution**: Always set TTL based on expected relevance window

```typescript
// Travel plans: 2 weeks after return date
await memory.addEpisodic(userId, 'Traveling to Japan', 14);

// Temporary diet: 3 months
await memory.addEpisodic(userId, 'On keto diet', 90);
```

#### ‚ùå Pitfall: Ignoring Working Memory

**Problem**: Using vector DB for everything, even session-specific context

**Solution**: Session-specific data stays in working memory (free)

```typescript
// BAD: Vector DB for session context
await vectorDB.add('Currently editing order #12345'); // Wasteful

// GOOD: Redis for session context
await redis.set(`session:${sessionId}:context`, 'Editing order #12345');
```

## Key Takeaways

1. **Match tier to lifetime** - Working (session), Episodic (days-weeks), Semantic (permanent)
2. **Start cheap, promote based on access** - Don't over-store in expensive tiers
3. **Always set TTL on episodic** - Prevents unbounded growth
4. **Reserve semantic for critical data** - Allergies, core preferences, identity
5. **Monitor per-user costs** - Memory costs scale with user engagement

**Quick Implementation Checklist**:

- [ ] Implement three-tier storage (Redis + Vector DB)
- [ ] Define classification criteria (manual or auto)
- [ ] Set TTL policies per tier
- [ ] Add retrieval with fallback
- [ ] Monitor growth and costs per user
- [ ] Test with multi-session scenarios

## References

1. **Mem0** (2024). "Memory Types & Architecture". https://docs.mem0.ai/features/memory-types
2. **Microsoft Research** (2024). "LongMemEval: Benchmarking Long-Term Memory in AI Agents". https://arxiv.org/abs/2410.10813
3. **Google ADK** (2025). "Session Management & Memory". https://google.github.io/adk-docs/sessions/
4. **Strands Agents SDK** (2025). "Conversation Memory Patterns". https://strandsagents.com/latest/user-guide/concepts/model-context/conversation-history/
5. **Vercel AI SDK** (2025). "Agent Memory Integration". https://ai-sdk.dev/docs/ai-sdk-core/agents

**Related Topics**:

- [4.1.1 Working Memory Concept](./4.1.1-working-memory-concept.md)
- [4.3.1 Vector Databases](./4.3.1-vector-databases.md)
- [4.3.4 Cross-Session Retrieval](./4.3.4-cross-session-retrieval.md)
- [4.4.1 Why Checkpoint](./4.4.1-why-checkpoint.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
