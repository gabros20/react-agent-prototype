# 4.4.3 - When to Checkpoint: Timing Strategies

## TL;DR

**Checkpoint at phase transitions, before expensive operations, and every √N steps (for N total steps).** The √N rule balances recovery time vs overhead—for a 100-step workflow, checkpoint every 10 steps (~0.5% overhead, <10s recovery). Keep total checkpoint overhead under 5%.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [4.4.1 Why Checkpoint](./4.4.1-why-checkpoint.md), [4.4.2 What to Save](./4.4.2-what-to-save.md)
- **Grounded In**: LangGraph persistence (2025), Restate durable execution, mathematical checkpoint optimization

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-checkpoint-timing)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Checkpoint timing balances two competing goals: checkpoint too often and execution slows from overhead; checkpoint too rarely and you lose significant progress on failure. The optimal strategy combines event-based triggers (phase transitions, expensive operations) with periodic backups (√N rule).

**Key Research Findings (2024-2025)**:

- **√N Rule**: For N steps, checkpoint every √N steps minimizes total cost
- **LangGraph**: Automatic checkpoint after each node (graph-based workflows)
- **Production Target**: Keep checkpoint overhead under 5% of execution time

## The Problem: Checkpoint Timing

### The Classic Challenge

```
Too Frequent (every step):
100 steps × 50ms checkpoint = 5,000ms overhead (5%)
✅ Zero progress lost on crash
❌ 5% performance penalty

Too Infrequent (every 50 steps):
100 steps × 2 checkpoints = 100ms overhead (0.1%)
✅ Minimal overhead
❌ Up to 49 steps lost on crash (49% of work!)
```

**Problems**:

- ❌ **Over-checkpointing**: 5%+ overhead degrades performance
- ❌ **Under-checkpointing**: Lose significant work on crash
- ❌ **Fixed frequency**: Ignores workflow structure (phases, costs)
- ❌ **No event triggers**: Miss critical save points

### Why This Matters

Optimal checkpoint timing maximizes the ratio of work preserved to overhead incurred. A well-timed checkpoint before an expensive API call can save $5; the same checkpoint after the call adds overhead with no benefit.

## Core Concept

### The √N Rule

For N total steps, checkpoint every √N steps to minimize total cost:

```
┌─────────────────────────────────────────────────────────────┐
│                    √N RULE DERIVATION                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  N = Total steps                                            │
│  k = Checkpoint frequency (every k steps)                   │
│                                                              │
│  Expected crash position: N/2 (uniform distribution)        │
│  Work lost per crash: k/2 (avg steps since last checkpoint) │
│  Number of checkpoints: N/k                                 │
│                                                              │
│  Total Cost = (Work lost) + (Checkpoint overhead)           │
│            = k/2 + N/k                                      │
│                                                              │
│  Minimize: d/dk (k/2 + N/k) = 0                             │
│           1/2 - N/k² = 0                                    │
│           k = √(2N) ≈ √N                                    │
│                                                              │
│  OPTIMAL: Checkpoint every √N steps                         │
│                                                              │
│  Examples:                                                   │
│  100 steps → every 10 steps                                 │
│  400 steps → every 20 steps                                 │
│  10,000 steps → every 100 steps                             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Six Checkpoint Triggers

```
┌─────────────────────────────────────────────────────────────┐
│                    CHECKPOINT TRIGGERS                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. PHASE TRANSITIONS                                       │
│     After: research → planning → execution → review         │
│     Why: Logical boundaries, easy resume points             │
│                                                              │
│  2. BEFORE EXPENSIVE OPERATIONS                             │
│     Before: LLM calls, external APIs, database writes       │
│     Why: Protect investment, enable retry                   │
│                                                              │
│  3. AFTER ERROR RECOVERY                                    │
│     After: Successful retry, fallback execution             │
│     Why: Capture recovered state, audit trail               │
│                                                              │
│  4. BEFORE HUMAN-IN-THE-LOOP                                │
│     Before: User approval, manual input                     │
│     Why: Long pauses need persistence                       │
│                                                              │
│  5. FIXED FREQUENCY (√N)                                    │
│     Every: √N steps as backup strategy                      │
│     Why: Guaranteed maximum loss bounds                     │
│                                                              │
│  6. TIME-BASED                                              │
│     Every: 1-3 minutes for long-running tasks               │
│     Why: Catch stuck/slow operations                        │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Frequency Guidelines

| Workflow Type | Duration | Checkpoint Frequency |
|--------------|----------|---------------------|
| **Fast** | <5 min | Every 5-10 steps |
| **Medium** | 5-30 min | Every 3-5 steps |
| **Long** | 30+ min | Every 1-3 steps |
| **Very Long** | Hours | Every 1-3 minutes |

## Implementation Patterns

### Pattern 1: Fixed Frequency

**Use Case**: Simple, uniform workflows

```typescript
const CHECKPOINT_FREQUENCY = 5;
let stepCount = 0;

async function executeStep(action: Action) {
  await performAction(action);
  stepCount++;

  if (stepCount % CHECKPOINT_FREQUENCY === 0) {
    await checkpoint.save({
      step: stepCount,
      state: currentState,
      timestamp: new Date().toISOString(),
    });
  }
}
```

**Pros**:
- ✅ Simple, predictable
- ✅ Easy to tune

**Cons**:
- ❌ Ignores workflow structure
- ❌ May checkpoint mid-operation

**When to Use**: Linear workflows with uniform step costs

### Pattern 2: Phase Transitions

**Use Case**: Multi-phase workflows with clear boundaries

```typescript
const workflow = [
  { phase: 'research', steps: ['search', 'analyze', 'summarize'] },
  { phase: 'plan', steps: ['outline', 'decompose', 'schedule'] },
  { phase: 'execute', steps: ['implement', 'test', 'review'] },
];

const completedPhases: string[] = [];

for (const { phase, steps } of workflow) {
  for (const step of steps) {
    await executeStep(step);
  }

  // Checkpoint after each phase
  completedPhases.push(phase);
  await checkpoint.save({
    phase,
    status: 'completed',
    completedPhases,
    timestamp: new Date().toISOString(),
  });
}
```

**Pros**:
- ✅ Logical save points
- ✅ Easy resume (from phase start)
- ✅ Lower checkpoint frequency

**Cons**:
- ❌ Long phases lose more work
- ❌ Requires clear phase boundaries

**When to Use**: Structured workflows with distinct phases

### Pattern 3: Before/After Expensive Operations

**Use Case**: Workflows with costly LLM or API calls

```typescript
async function performExpensiveAnalysis(document: string) {
  // Checkpoint BEFORE expensive operation
  await checkpoint.save({
    step: 'pre_analysis',
    documentId,
    timestamp: new Date().toISOString(),
  });

  try {
    // Expensive LLM call (~$5)
    const analysis = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: `Analyze: ${document}` }],
    });

    // Checkpoint AFTER success
    await checkpoint.save({
      step: 'post_analysis',
      documentId,
      analysis: analysis.choices[0].message.content,
      timestamp: new Date().toISOString(),
    });

    return analysis;
  } catch (error) {
    // On failure, can retry from pre_analysis checkpoint
    throw error;
  }
}
```

**Pros**:
- ✅ Protects expensive operations
- ✅ Enables precise retry
- ✅ Minimizes wasted costs

**Cons**:
- ❌ Higher checkpoint frequency
- ❌ Must identify expensive operations

**When to Use**: Workflows with costly external calls

### Pattern 4: Hybrid Scheduler

**Use Case**: Production systems combining multiple strategies

```typescript
interface CheckpointConfig {
  fixedFrequency: number;      // Every N steps
  timeIntervalMs: number;      // Every N milliseconds
  triggers: Set<string>;       // Event triggers
}

class HybridCheckpointScheduler {
  private stepCount = 0;
  private lastCheckpointTime = Date.now();
  private config: CheckpointConfig;

  constructor(config: CheckpointConfig) {
    this.config = config;
  }

  shouldCheckpoint(context: {
    phaseComplete?: boolean;
    beforeExpensive?: boolean;
    afterError?: boolean;
    beforeHITL?: boolean;
  }): boolean {
    // Event-based triggers (highest priority)
    if (context.phaseComplete && this.config.triggers.has('phase')) return true;
    if (context.beforeExpensive && this.config.triggers.has('expensive')) return true;
    if (context.afterError && this.config.triggers.has('error')) return true;
    if (context.beforeHITL && this.config.triggers.has('hitl')) return true;

    // Fixed frequency (backup)
    if (this.stepCount % this.config.fixedFrequency === 0) return true;

    // Time-based (catch long operations)
    const elapsed = Date.now() - this.lastCheckpointTime;
    if (elapsed >= this.config.timeIntervalMs) return true;

    return false;
  }

  markCheckpoint() {
    this.lastCheckpointTime = Date.now();
  }

  incrementStep() {
    this.stepCount++;
  }
}

// Usage
const scheduler = new HybridCheckpointScheduler({
  fixedFrequency: 5,
  timeIntervalMs: 2 * 60 * 1000, // 2 minutes
  triggers: new Set(['phase', 'expensive', 'hitl']),
});

async function executeWorkflow(steps: Action[]) {
  for (const step of steps) {
    // Check before execution
    if (scheduler.shouldCheckpoint({ beforeExpensive: step.isExpensive })) {
      await checkpoint.save(currentState);
      scheduler.markCheckpoint();
    }

    await executeStep(step);
    scheduler.incrementStep();

    // Check after execution
    if (scheduler.shouldCheckpoint({ phaseComplete: step.endsPhase })) {
      await checkpoint.save(currentState);
      scheduler.markCheckpoint();
    }
  }
}
```

**Pros**:
- ✅ Combines all strategies
- ✅ Configurable priorities
- ✅ Handles edge cases

**Cons**:
- ❌ More complex implementation
- ❌ Requires tuning

**When to Use**: Production systems with varied requirements

## Framework Integration

### AI SDK v6 with Checkpoint Hooks

```typescript
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

class CheckpointAwareAgent {
  private scheduler: HybridCheckpointScheduler;
  private checkpointStore: CheckpointStore;
  private threadId: string;

  async execute(messages: any[]) {
    const result = await generateText({
      model: openai('gpt-4o'),
      messages,
      tools: {
        expensiveAnalysis: tool({
          description: 'Perform expensive document analysis',
          parameters: z.object({ documentId: z.string() }),
          execute: async ({ documentId }) => {
            // Checkpoint before expensive call
            if (this.scheduler.shouldCheckpoint({ beforeExpensive: true })) {
              await this.checkpointStore.save(this.threadId, {
                step: 'pre_analysis',
                documentId,
              });
              this.scheduler.markCheckpoint();
            }

            const result = await analyzeDocument(documentId);

            // Checkpoint after success
            await this.checkpointStore.save(this.threadId, {
              step: 'post_analysis',
              documentId,
              result,
            });
            this.scheduler.markCheckpoint();

            return result;
          },
        }),
      },
      maxSteps: 10,
      onStepFinish: async ({ stepType }) => {
        this.scheduler.incrementStep();

        // Fixed frequency checkpoint
        if (this.scheduler.shouldCheckpoint({})) {
          await this.checkpointStore.save(this.threadId, {
            step: this.scheduler.stepCount,
            messages: result.messages,
          });
          this.scheduler.markCheckpoint();
        }
      },
    });

    return result;
  }
}
```

### LangGraph Automatic Checkpointing

```typescript
import { StateGraph } from '@langchain/langgraph';
import { PostgresSaver } from '@langchain/langgraph-checkpoint-postgres';
import { Pool } from 'pg';

const pool = new Pool({ connectionString: process.env.DATABASE_URL });
const checkpointer = new PostgresSaver(pool);

const workflow = new StateGraph({
  channels: {
    messages: { value: (x, y) => x.concat(y) },
    phase: { value: (x, y) => y },
  },
});

// Each node automatically triggers checkpoint
workflow.addNode('research', researchNode);  // → checkpoint
workflow.addNode('plan', planNode);          // → checkpoint
workflow.addNode('execute', executeNode);    // → checkpoint

const app = workflow.compile({ checkpointer });

// Run with thread ID - checkpoints after each node
await app.invoke(
  { messages: ['Start task'] },
  { configurable: { thread_id: 'thread_123' } }
);
```

## Research & Benchmarks

### Overhead Analysis

**Scenario: 100-step workflow, 1s per step, 50ms per checkpoint**

| Frequency | Checkpoints | Overhead | Total Time | Overhead % |
|-----------|-------------|----------|------------|------------|
| Every step | 100 | 5,000ms | 105s | 4.8% |
| Every 5 steps | 20 | 1,000ms | 101s | 1.0% |
| Every 10 steps (√N) | 10 | 500ms | 100.5s | 0.5% |
| Every 20 steps | 5 | 250ms | 100.25s | 0.25% |
| Every 50 steps | 2 | 100ms | 100.1s | 0.1% |

### Recovery Time Analysis

**If crash occurs at step 50 (midpoint)**:

| Frequency | Max Steps Lost | Recovery Time |
|-----------|----------------|---------------|
| Every step | 0 | 0s |
| Every 5 steps | 4 | 4s |
| Every 10 steps | 9 | 9s |
| Every 20 steps | 19 | 19s |
| Every 50 steps | 49 | 49s |

**Sweet Spot**: Every 5-10 steps (0.5-1% overhead, <10s recovery)

### Production Benchmarks

| Backend | Checkpoint Latency | Recommended Frequency |
|---------|-------------------|----------------------|
| Redis | 2-5ms | Every 3-5 steps |
| PostgreSQL | 5-10ms | Every 5-10 steps |
| SQLite | 10-20ms | Every 10-20 steps |
| S3 | 50-100ms | Phase transitions only |

## When to Use This Pattern

### ✅ Checkpoint At These Times:

1. **Phase transitions**
   - After completing research, planning, execution phases
   - Logical resume points

2. **Before expensive operations**
   - LLM calls ($0.01-$5 each)
   - External API calls
   - Database mutations

3. **Before human-in-the-loop**
   - User approval requests
   - Manual input waits

4. **After error recovery**
   - Successful retries
   - Fallback executions

5. **Every √N steps**
   - Backup strategy
   - Guaranteed maximum loss

### ❌ Don't Checkpoint:

1. **During atomic operations**
   - Mid-transaction
   - Partial state

2. **Between dependent steps**
   - When next step requires previous result
   - Would create inconsistent state

3. **For trivial operations**
   - <100ms tasks
   - No external dependencies

### Decision Matrix

| Situation | Strategy |
|-----------|----------|
| Linear workflow, uniform steps | Fixed frequency (√N) |
| Multi-phase workflow | Phase transitions + √N backup |
| Expensive API calls | Before/after each call |
| Long-running (hours) | Time-based (1-3 min) + events |
| Human approval needed | Before every HITL pause |

## Production Best Practices

### 1. Combine Event + Periodic Triggers

```typescript
// Event-based: Critical save points
if (phaseComplete || beforeExpensive || beforeHITL) {
  await checkpoint.save(state);
}

// Periodic: Backup strategy
if (stepCount % sqrtN === 0 || elapsedMs > timeInterval) {
  await checkpoint.save(state);
}
```

### 2. Monitor Checkpoint Performance

```typescript
const start = performance.now();
await checkpoint.save(state);
const duration = performance.now() - start;

metrics.record('checkpoint_duration_ms', duration);

if (duration > 100) {
  console.warn(`Slow checkpoint: ${duration}ms - consider reducing frequency`);
}
```

### 3. Adaptive Frequency Based on Stability

```typescript
let frequency = 5;
let consecutiveSuccesses = 0;

async function executeWithAdaptiveCheckpoint(action: Action) {
  try {
    await performAction(action);
    consecutiveSuccesses++;

    // Stable system → reduce frequency
    if (consecutiveSuccesses >= 50) {
      frequency = Math.min(frequency + 2, 20);
      consecutiveSuccesses = 0;
    }
  } catch (error) {
    // Unstable system → increase frequency
    frequency = Math.max(frequency - 2, 1);
    consecutiveSuccesses = 0;
    throw error;
  }
}
```

### Common Pitfalls

#### ❌ Pitfall: Checkpointing Too Frequently

**Problem**: 5%+ overhead degrades performance

```typescript
// BAD: Every operation
for (const item of items) {
  await process(item);
  await checkpoint.save(state); // 5% overhead!
}

// GOOD: Every 10 items
for (let i = 0; i < items.length; i++) {
  await process(items[i]);
  if (i % 10 === 0) await checkpoint.save(state);
}
```

#### ❌ Pitfall: Not Checkpointing Before Long Operations

**Problem**: Lose significant work on crash

```typescript
// BAD: No checkpoint before 10-minute operation
await longRunningProcess(); // 10 min, then crash → 10 min lost

// GOOD: Checkpoint before
await checkpoint.save({ step: 'pre_long_process' });
await longRunningProcess();
await checkpoint.save({ step: 'post_long_process' });
```

## Key Takeaways

1. **Use √N rule for fixed frequency** - 100 steps → every 10 steps
2. **Always checkpoint before expensive operations** - Protect your investment
3. **Checkpoint at phase transitions** - Natural resume points
4. **Keep overhead under 5%** - Balance protection vs performance
5. **Combine strategies** - Events + periodic for robust coverage

**Quick Implementation Checklist**:

- [ ] Calculate √N for your typical workflow length
- [ ] Identify phase transitions for event-based checkpoints
- [ ] Mark expensive operations for before/after checkpoints
- [ ] Implement time-based backup for long-running tasks
- [ ] Monitor checkpoint latency and adjust frequency
- [ ] Test recovery scenarios with different failure points

## References

1. **Sparkco AI** (2025). "Mastering LangGraph Checkpointing: Best Practices for 2025". https://sparkco.ai/blog/mastering-langgraph-checkpointing-best-practices-for-2025
2. **LangChain** (2024). "LangGraph v0.2: Increased customization with new checkpointer libraries". https://blog.langchain.com/langgraph-v0-2
3. **Youssef Hosni** (2025). "Building Agents with LangGraph Course #5: Persistence & Streaming". https://youssefh.substack.com/p/building-agents-with-langgraph-course-bd2
4. **Koog** (2025). "Agent Persistence Documentation". https://docs.koog.ai/agent-persistence/

**Related Topics**:

- [4.4.1 Why Checkpoint](./4.4.1-why-checkpoint.md)
- [4.4.2 What to Save](./4.4.2-what-to-save.md)
- [4.4.4 How to Resume](./4.4.4-how-to-resume.md)
- [4.4.5 Implementation](./4.4.5-implementation.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
