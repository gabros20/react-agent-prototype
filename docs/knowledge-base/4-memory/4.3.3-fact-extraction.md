# 4.3.3 Fact Extraction & Storage

**Layer**: 4 - Memory & State  
**Sublayer**: 4.3 - Episodic Memory (Long-Term)  
**Audience**: Intermediate to Advanced  
**Updated**: 2025-11-18

---

## Table of Contents

- [Overview](#overview)
- [Why Fact Extraction Matters](#why-fact-extraction-matters)
- [Mem0 Architecture](#mem0-architecture)
- [Fact Extraction Techniques](#fact-extraction-techniques)
- [Entity Recognition](#entity-recognition)
- [Knowledge Graphs for Facts](#knowledge-graphs-for-facts)
- [Implementation Examples](#implementation-examples)
- [Production Patterns](#production-patterns)
- [Performance Metrics](#performance-metrics)
- [Best Practices](#best-practices)
- [Common Pitfalls](#common-pitfalls)
- [Related Topics](#related-topics)
- [References](#references)

---

## Overview

**Fact extraction** is the process of identifying and extracting structured information (entities, relationships, preferences) from unstructured text (conversations, documents, user inputs). This enables AI agents to build persistent, queryable knowledge bases for long-term memory[^1].

**Key Difference from Vector Search**:
- **Vector Search**: Retrieves similar documents based on semantic meaning (fuzzy matching)
- **Fact Extraction**: Extracts discrete facts into structured format (precise knowledge)

**Example**:
```
User: "I prefer dark mode and I'm allergic to peanuts."

Vector Storage (Unstructured):
â†’ Store entire sentence as embedding: [0.23, -0.45, 0.87, ...]

Fact Extraction (Structured):
â†’ Extract facts:
   - preference: UI theme = "dark mode"
   - medical: allergy = "peanuts"
```

**Why Both Are Needed**:
1. **Vector Search**: For semantic retrieval ("What UI preferences does this user have?")
2. **Fact Extraction**: For precise queries ("Is this user allergic to peanuts?")

---

## Why Fact Extraction Matters

### Problem: Context Window Limitations

**Traditional Approach** (No Fact Extraction):
```typescript
// Session 1: User shares preferences
"I like TypeScript and dark mode"

// Session 50: Agent doesn't remember (too many tokens to pass entire history)
Agent: "What programming languages do you prefer?"
User: "I already told you! ğŸ˜¤"
```

**With Fact Extraction**:
```typescript
// Session 1: Extract and store facts
facts = {
  programming_language: "TypeScript",
  ui_theme: "dark mode"
}

// Session 50: Retrieve facts efficiently
const userPrefs = await getFacts(userId, "programming");
// Returns: { programming_language: "TypeScript" }
Agent: "Based on your TypeScript preference, here's a solution..."
```

### Real-World Benefits[^2]

**Mem0 Benchmarks** (2025):
- **26% higher accuracy** vs OpenAI baseline (LLM-as-a-Judge metric)
- **91% lower latency** (p95: 50ms vs 550ms)
- **90% token cost reduction** (storing facts vs full conversations)

**Use Cases**:
1. **Personalized AI Assistants**: Remember user preferences, habits, medical info
2. **Customer Support**: Track past issues, solutions, customer details
3. **Healthcare**: Extract patient symptoms, medications, allergies
4. **Compliance**: Track contractual terms, obligations, deadlines[^3]

---

## Mem0 Architecture

**Mem0** is an open-source memory layer for LLMs that provides scalable fact extraction and storage[^4].

### Two-Phase Memory Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: EXTRACTION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Input â†’ LLM (Few-Shot Prompt) â†’ Extract Facts      â”‚
â”‚                                                           â”‚
â”‚  "I prefer dark mode"                                     â”‚
â”‚       â†“                                                   â”‚
â”‚  LLM extracts:                                            â”‚
â”‚   - entity: "UI theme"                                    â”‚
â”‚   - value: "dark mode"                                    â”‚
â”‚   - category: "preference"                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: UPDATE                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  New Facts + Existing Facts â†’ LLM â†’ Reconcile + Store    â”‚
â”‚                                                           â”‚
â”‚  New: "UI theme = dark mode"                              â”‚
â”‚  Old: "UI theme = light mode"                             â”‚
â”‚       â†“                                                   â”‚
â”‚  LLM decides:                                             â”‚
â”‚   - UPDATE (replace old with new)                         â”‚
â”‚   - MERGE (combine both)                                  â”‚
â”‚   - KEEP (no change)                                      â”‚
â”‚       â†“                                                   â”‚
â”‚  Store in: Vector DB + Graph DB + Key-Value Store         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid Storage Architecture[^5]

Mem0 uses three storage types for optimal performance:

1. **Vector Store**: Semantic search over facts
   - Example: "Find all facts related to user preferences"
   - Storage: Qdrant, Pinecone, LanceDB

2. **Graph Store**: Entity relationships
   - Example: "Who is John's manager?" (relationship: reports_to)
   - Storage: Neo4j, Memgraph, Neptune

3. **Key-Value Store**: Fast lookup by ID
   - Example: "Get user_123's email address"
   - Storage: Redis, DynamoDB

---

## Fact Extraction Techniques

### 1. LLM-Based Extraction (Most Common)

**Approach**: Use LLM with structured prompt to extract facts

```typescript
import { OpenAI } from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function extractFacts(text: string): Promise<Array<{
  entity: string;
  value: string;
  category: string;
}>> {
  const prompt = `Extract all facts from the following text. Return as JSON array.

Text: "${text}"

Format:
[
  { "entity": "name", "value": "John Smith", "category": "personal" },
  { "entity": "age", "value": "30", "category": "personal" }
]

Facts:`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
    temperature: 0.0, // Deterministic extraction
    response_format: { type: "json_object" }, // Structured output
  });

  const content = response.choices[0].message.content!;
  return JSON.parse(content).facts || [];
}

// Example
const text = "My name is Alice, I'm 28 years old, and I prefer TypeScript over JavaScript.";
const facts = await extractFacts(text);
console.log(facts);
// [
//   { entity: "name", value: "Alice", category: "personal" },
//   { entity: "age", value: "28", category: "personal" },
//   { entity: "language_preference", value: "TypeScript", category: "professional" }
// ]
```

**Advantages**:
- âœ… **Flexible**: Works with any domain (no training required)
- âœ… **High accuracy**: GPT-4 achieves ~85-95% precision[^6]
- âœ… **Context-aware**: Understands nuance and ambiguity

**Disadvantages**:
- âŒ **Slow**: 200-500ms per extraction
- âŒ **Costly**: $0.10-0.30 per 1M input tokens (gpt-4o-mini)

---

### 2. Named Entity Recognition (NER)

**Approach**: Use specialized NER models to identify entities

```typescript
import { pipeline } from "@xenova/transformers";

// Load pre-trained NER model
const ner = await pipeline("ner", "Xenova/bert-base-NER");

async function extractEntities(text: string) {
  const entities = await ner(text);
  return entities;
}

// Example
const text = "Alice works at OpenAI in San Francisco.";
const entities = await extractEntities(text);
console.log(entities);
// [
//   { entity: "B-PER", word: "Alice", score: 0.99 },      // Person
//   { entity: "B-ORG", word: "OpenAI", score: 0.98 },    // Organization
//   { entity: "B-LOC", word: "San Francisco", score: 0.97 } // Location
// ]
```

**Advantages**:
- âœ… **Fast**: 10-50ms per extraction
- âœ… **Free**: Self-hosted models (BERT, SpaCy, Flair)
- âœ… **Efficient**: No API costs

**Disadvantages**:
- âŒ **Limited categories**: Only works for pre-trained entities (PER, ORG, LOC, DATE)
- âŒ **Lower accuracy**: 70-85% for complex domains
- âŒ **No relationships**: Only extracts entities, not facts/relationships

**When to Use**: For simple entity extraction (names, dates, locations) at scale

---

### 3. Hybrid Approach (Best of Both)

**Approach**: Use NER for entities + LLM for relationships/facts

```typescript
async function hybridExtraction(text: string) {
  // Step 1: Fast NER for entities
  const entities = await extractEntities(text);

  // Step 2: LLM for relationships and complex facts
  const prompt = `Given these entities: ${JSON.stringify(entities)}
Extract relationships and facts from: "${text}"`;

  const facts = await extractFacts(prompt);

  return { entities, facts };
}

// Example
const text = "Alice, a software engineer at OpenAI, prefers TypeScript.";
const result = await hybridExtraction(text);
console.log(result);
// {
//   entities: [
//     { type: "PERSON", value: "Alice" },
//     { type: "ORG", value: "OpenAI" }
//   ],
//   facts: [
//     { relation: "works_at", subject: "Alice", object: "OpenAI" },
//     { relation: "job_title", subject: "Alice", object: "software engineer" },
//     { relation: "prefers", subject: "Alice", object: "TypeScript" }
//   ]
// }
```

---

## Entity Recognition

### Mem0 Entity Extraction[^7]

Mem0's graph memory feature automatically extracts entities and relationships:

```typescript
import { MemoryClient } from "mem0ai";

const memory = new MemoryClient({ apiKey: process.env.MEM0_API_KEY });

// Add memory with automatic entity extraction
await memory.add(
  "John Smith works at Microsoft and prefers Python over Java.",
  {
    user_id: "user123",
    metadata: { source: "conversation", session_id: "sess_001" },
  }
);

// Mem0 automatically extracts:
// Entities:
//   - Person: "John Smith"
//   - Organization: "Microsoft"
//   - Programming Language: "Python", "Java"
//
// Relationships:
//   - (John Smith) --[WORKS_AT]--> (Microsoft)
//   - (John Smith) --[PREFERS]--> (Python)
//   - (John Smith) --[DISLIKES]--> (Java)

// Query by entity
const johnsMemories = await memory.search("What does John prefer?", {
  user_id: "user123",
  graph_query: true, // Use graph-based retrieval
});

console.log(johnsMemories);
// Returns: "John Smith prefers Python over Java"
```

---

## Knowledge Graphs for Facts

### Why Knowledge Graphs?

**Vector Search Limitation**:
```typescript
// Query: "Who is Alice's manager?"
// Vector search: Returns documents mentioning "Alice" and "manager"
// âŒ Problem: Can't answer explicit relationship questions
```

**Knowledge Graph Solution**:
```typescript
// Knowledge Graph:
// (Alice) --[REPORTS_TO]--> (Bob)
// (Bob) --[REPORTS_TO]--> (Carol)

// Query: "Who is Alice's manager?"
// Answer: "Bob" (direct relationship lookup)

// Query: "Who is Alice's manager's manager?"
// Answer: "Carol" (multi-hop traversal)
```

### Graph Storage with Neo4j

```typescript
import neo4j from "neo4j-driver";

const driver = neo4j.driver(
  "bolt://localhost:7687",
  neo4j.auth.basic("neo4j", "password")
);

// Store facts as graph
async function storeFact(subject: string, relation: string, object: string) {
  const session = driver.session();
  await session.run(
    `
    MERGE (s:Entity {name: $subject})
    MERGE (o:Entity {name: $object})
    MERGE (s)-[r:${relation}]->(o)
    `,
    { subject, object }
  );
  await session.close();
}

// Query graph
async function queryGraph(query: string) {
  const session = driver.session();
  const result = await session.run(query);
  await session.close();
  return result.records.map((r) => r.toObject());
}

// Usage
await storeFact("Alice", "WORKS_AT", "OpenAI");
await storeFact("Alice", "PREFERS", "TypeScript");

// Query: "Where does Alice work?"
const results = await queryGraph(`
  MATCH (s:Entity {name: 'Alice'})-[r:WORKS_AT]->(o:Entity)
  RETURN o.name AS workplace
`);
console.log(results); // [{ workplace: "OpenAI" }]
```

---

## Implementation Examples

### Complete Fact Extraction System

```typescript
import { OpenAI } from "openai";
import lancedb from "lancedb";
import neo4j from "neo4j-driver";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const db = await lancedb.connect("./lancedb");
const neo4jDriver = neo4j.driver("bolt://localhost:7687");

interface Fact {
  id: string;
  subject: string;
  predicate: string;
  object: string;
  timestamp: string;
  confidence: number;
}

// 1. Extract facts from text
async function extractFacts(text: string): Promise<Fact[]> {
  const prompt = `Extract all factual statements from the text. Return as JSON array.

Text: "${text}"

Format (subject-predicate-object triples):
[
  {
    "subject": "Alice",
    "predicate": "works_at",
    "object": "OpenAI",
    "confidence": 0.95
  }
]

Facts:`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
    temperature: 0.0,
    response_format: { type: "json_object" },
  });

  const content = JSON.parse(response.choices[0].message.content!);
  return content.facts.map((f: any) => ({
    id: crypto.randomUUID(),
    ...f,
    timestamp: new Date().toISOString(),
  }));
}

// 2. Store facts in vector DB + graph DB
async function storeFacts(facts: Fact[], userId: string) {
  // Generate embeddings for each fact
  const factTexts = facts.map(
    (f) => `${f.subject} ${f.predicate} ${f.object}`
  );
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: factTexts,
  });

  // Store in vector DB (for semantic search)
  const table = await db.openTable("facts");
  await table.add(
    facts.map((fact, idx) => ({
      id: fact.id,
      user_id: userId,
      subject: fact.subject,
      predicate: fact.predicate,
      object: fact.object,
      timestamp: fact.timestamp,
      confidence: fact.confidence,
      vector: response.data[idx].embedding,
    }))
  );

  // Store in graph DB (for relationship queries)
  const session = neo4jDriver.session();
  for (const fact of facts) {
    await session.run(
      `
      MERGE (s:Entity {name: $subject, user_id: $userId})
      MERGE (o:Entity {name: $object, user_id: $userId})
      MERGE (s)-[r:${fact.predicate} {
        confidence: $confidence,
        timestamp: $timestamp
      }]->(o)
      `,
      {
        subject: fact.subject,
        object: fact.object,
        userId,
        confidence: fact.confidence,
        timestamp: fact.timestamp,
      }
    );
  }
  await session.close();
}

// 3. Query facts (hybrid: vector + graph)
async function queryFacts(query: string, userId: string) {
  // Vector search for semantic retrieval
  const embeddingResponse = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = embeddingResponse.data[0].embedding;

  const table = await db.openTable("facts");
  const vectorResults = await table
    .search(queryVector)
    .where(`user_id = '${userId}'`)
    .limit(5)
    .execute();

  // Graph search for explicit relationships
  const session = neo4jDriver.session();
  const graphResults = await session.run(
    `
    MATCH (s:Entity {user_id: $userId})-[r]->(o:Entity)
    WHERE s.name CONTAINS $searchTerm OR o.name CONTAINS $searchTerm
    RETURN s.name AS subject, type(r) AS predicate, o.name AS object
    LIMIT 5
    `,
    { userId, searchTerm: query.split(" ")[0] }
  );
  await session.close();

  return {
    vector: vectorResults,
    graph: graphResults.records.map((r) => r.toObject()),
  };
}

// Usage
const text =
  "Alice works at OpenAI as a software engineer. She prefers TypeScript and lives in San Francisco.";
const facts = await extractFacts(text);
console.log(facts);
// [
//   { subject: "Alice", predicate: "works_at", object: "OpenAI", ... },
//   { subject: "Alice", predicate: "job_title", object: "software engineer", ... },
//   { subject: "Alice", predicate: "prefers", object: "TypeScript", ... },
//   { subject: "Alice", predicate: "lives_in", object: "San Francisco", ... }
// ]

await storeFacts(facts, "user123");

const results = await queryFacts("Where does Alice work?", "user123");
console.log(results);
// {
//   vector: [...similar facts...],
//   graph: [{ subject: "Alice", predicate: "works_at", object: "OpenAI" }]
// }
```

---

## Production Patterns

### 1. Fact Deduplication

**Problem**: Same fact extracted multiple times

```typescript
// User: "I like TypeScript"
// Later: "I prefer TypeScript over JavaScript"
// Result: Two duplicate facts about TypeScript preference
```

**Solution**: Deduplication with confidence scoring

```typescript
async function deduplicateFacts(newFacts: Fact[], existingFacts: Fact[]) {
  const deduped: Fact[] = [];

  for (const newFact of newFacts) {
    const duplicate = existingFacts.find(
      (ef) =>
        ef.subject === newFact.subject &&
        ef.predicate === newFact.predicate &&
        ef.object === newFact.object
    );

    if (duplicate) {
      // Update confidence if new fact is more confident
      if (newFact.confidence > duplicate.confidence) {
        await updateFact(duplicate.id, newFact);
      }
    } else {
      deduped.push(newFact);
    }
  }

  return deduped;
}
```

### 2. Fact Conflict Resolution

**Problem**: Contradictory facts

```typescript
// Session 1: "I prefer dark mode"
// Session 50: "I prefer light mode"
// Which is correct?
```

**Solution**: Mem0-style conflict resolution[^8]

```typescript
async function resolveFacts(oldFact: Fact, newFact: Fact): Promise<"UPDATE" | "MERGE" | "KEEP"> {
  const prompt = `Given two facts, decide how to reconcile them:

Old Fact: ${JSON.stringify(oldFact)}
New Fact: ${JSON.stringify(newFact)}

Options:
- UPDATE: Replace old with new (user changed preference)
- MERGE: Keep both (both are valid)
- KEEP: Ignore new fact (old is more reliable)

Decision:`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
    temperature: 0.0,
  });

  const decision = response.choices[0].message.content!.trim();
  return decision as "UPDATE" | "MERGE" | "KEEP";
}

// Usage
const oldFact = { subject: "Alice", predicate: "prefers_theme", object: "dark mode" };
const newFact = { subject: "Alice", predicate: "prefers_theme", object: "light mode" };

const decision = await resolveFacts(oldFact, newFact);
if (decision === "UPDATE") {
  await deleteFact(oldFact.id);
  await storeFact(newFact);
} else if (decision === "MERGE") {
  await storeFact(newFact); // Keep both
}
```

### 3. Temporal Facts

**Problem**: Facts change over time

```typescript
// 2024: "Alice works at OpenAI"
// 2025: "Alice works at Anthropic"
// Need to track history
```

**Solution**: Temporal knowledge graph

```typescript
interface TemporalFact extends Fact {
  valid_from: string;
  valid_until?: string; // null = still valid
}

async function storeTemporalFact(fact: TemporalFact, userId: string) {
  const session = neo4jDriver.session();

  // Invalidate old facts
  await session.run(
    `
    MATCH (s:Entity {name: $subject, user_id: $userId})-[r:${fact.predicate}]->(o:Entity)
    WHERE r.valid_until IS NULL
    SET r.valid_until = $now
    `,
    { subject: fact.subject, userId, now: new Date().toISOString() }
  );

  // Add new fact
  await session.run(
    `
    MERGE (s:Entity {name: $subject, user_id: $userId})
    MERGE (o:Entity {name: $object, user_id: $userId})
    CREATE (s)-[r:${fact.predicate} {
      valid_from: $validFrom,
      valid_until: $validUntil,
      confidence: $confidence
    }]->(o)
    `,
    {
      subject: fact.subject,
      object: fact.object,
      userId,
      validFrom: fact.valid_from,
      validUntil: fact.valid_until,
      confidence: fact.confidence,
    }
  );

  await session.close();
}

// Query facts at specific time
async function queryFactsAt(subject: string, predicate: string, timestamp: string) {
  const session = neo4jDriver.session();
  const result = await session.run(
    `
    MATCH (s:Entity {name: $subject})-[r:${predicate}]->(o:Entity)
    WHERE r.valid_from <= $timestamp
      AND (r.valid_until IS NULL OR r.valid_until > $timestamp)
    RETURN o.name AS value
    `,
    { subject, timestamp }
  );
  await session.close();
  return result.records.map((r) => r.get("value"));
}
```

---

## Performance Metrics

### Mem0 Benchmarks (2025)[^9]

| Metric                   | Mem0     | OpenAI Baseline | Improvement |
|--------------------------|----------|-----------------|-------------|
| **Response Accuracy**    | 87.3%    | 69.1%           | **+26%**    |
| **P95 Latency**          | 50ms     | 550ms           | **-91%**    |
| **Token Cost/Query**     | 150      | 1500            | **-90%**    |
| **Memory Precision**     | 92.4%    | 78.6%           | **+18%**    |

### Extraction Speed

| Method                   | Latency  | Cost (per 1M extractions) |
|--------------------------|----------|---------------------------|
| **LLM (GPT-4o-mini)**    | 200-500ms| $100-300                  |
| **NER (BERT)**           | 10-50ms  | Free (self-hosted)        |
| **Hybrid**               | 100-200ms| $50-150                   |

---

## Best Practices

### 1. Use Structured Outputs

```typescript
// âœ… Good: Structured output (JSON)
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: prompt }],
  response_format: { type: "json_object" }, // Ensures valid JSON
});

// âŒ Bad: Free-form text (requires parsing)
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: prompt }],
});
// May return: "Here are the facts: Alice works at OpenAI..." (hard to parse)
```

### 2. Confidence Scoring

```typescript
// Always include confidence scores for facts
const fact = {
  subject: "Alice",
  predicate: "works_at",
  object: "OpenAI",
  confidence: 0.95, // High confidence (explicitly stated)
};

// vs

const inferred = {
  subject: "Alice",
  predicate: "likes",
  object: "Python",
  confidence: 0.65, // Lower confidence (inferred from context)
};
```

### 3. Category Tagging

```typescript
// Tag facts by category for efficient filtering
const fact = {
  subject: "Alice",
  predicate: "allergy",
  object: "peanuts",
  category: "medical", // Enable: "Get all medical facts for user"
  sensitivity: "high", // Flag sensitive data
};
```

---

## Common Pitfalls

### 1. âŒ Over-Extraction

```typescript
// âŒ Bad: Extracting too many trivial facts
"I like coffee" â†’ fact: { subject: "User", predicate: "likes", object: "coffee" }
"I like tea too" â†’ fact: { subject: "User", predicate: "likes", object: "tea" }
"I like juice" â†’ fact: { subject: "User", predicate: "likes", object: "juice" }
// Result: 1000s of low-value facts

// âœ… Good: Extract only important facts
"I'm allergic to peanuts" â†’ fact: { subject: "User", predicate: "allergic_to", object: "peanuts" }
```

### 2. âŒ No Fact Expiration

```typescript
// âŒ Bad: Keeping outdated facts forever
// 2023: "Alice works at OpenAI"
// 2025: Still returning old fact even though Alice left

// âœ… Good: Add expiration or manual invalidation
const fact = {
  valid_until: "2025-12-31", // Auto-expire after 1 year
};
```

### 3. âŒ Ignoring Context

```typescript
// âŒ Bad: Extracting facts without context
"I don't like TypeScript" â†’ fact: { predicate: "likes", object: "TypeScript" } // WRONG!

// âœ… Good: Respect negations and qualifiers
"I don't like TypeScript" â†’ fact: { predicate: "dislikes", object: "TypeScript" }
"I sometimes use Python" â†’ fact: { predicate: "uses_occasionally", object: "Python" }
```

---

## Related Topics

- **[4.3.1 Vector Databases](./4.3.1-vector-databases.md)** - Storage backends for facts and embeddings
- **[4.3.2 Semantic Search](./4.3.2-semantic-search.md)** - Retrieving facts using semantic similarity
- **[4.3.4 Cross-Session Retrieval](./4.3.4-cross-session.md)** - Using extracted facts across multiple conversations
- **[4.1.2 Entity Extraction](./4.1.2-entity-extraction.md)** - Short-term entity extraction for working memory
- **[Zep: Temporal Knowledge Graphs](https://arxiv.org/abs/2501.13956)** - Advanced temporal fact storage

---

## References

[^1]: "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory" - arXiv (2025): https://arxiv.org/html/2504.19413v1
[^2]: "AI Memory Research: 26% Accuracy Boost for LLMs" - Mem0 Research (2025): https://mem0.ai/research
[^3]: "Graph Memory - Mem0" - Mem0 Documentation (2024): https://docs.mem0.ai/open-source/features/graph-memory
[^4]: "Mem0 - The Memory Layer for your AI Apps" - Mem0 (2025): https://mem0.ai/
[^5]: "Mem0: The Comprehensive Guide to Building AI with Persistent Memory" - Dev.to (2025): https://dev.to/yigit-konur/mem0-the-comprehensive-guide-to-building-ai-with-persistent-memory
[^6]: "Mem0: Building LLM-Powered Memory Systems â€” Part 1" - Medium (2024): https://medium.com/@liu170045/mem0-llm-memory-systems-deep-dive-part-1-fcf9f2
[^7]: "Memory Types - Mem0" - Mem0 Documentation (2024): https://docs.mem0.ai/v0x/core-concepts/memory-types
[^8]: "Mem0: The Missing Link in Long-Term AI Interactions" - Medium (2024): https://medium.com/@omkamal/mem0-the-missing-link-in-long-term-ai-interactions
[^9]: "AI Memory Research: 26% Accuracy Boost for LLMs" - Mem0 Research (2025): https://mem0.ai/research

---

**Next**: [4.3.4 Cross-Session Retrieval](./4.3.4-cross-session.md) - Maintaining context and facts across multiple conversations.

**Previous**: [4.3.2 Semantic Search](./4.3.2-semantic-search.md) - Understanding vector embeddings and similarity metrics.
