# 4.3.3 - Fact Extraction & Storage

## TL;DR

**Fact extraction transforms unstructured text into structured knowledge (entities, relationships, preferences) that AI agents can query precisely.** Unlike vector search which finds similar content, fact extraction enables exact queries like "Is this user allergic to peanuts?" achieving 26% higher accuracy and 90% token cost reduction.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [4.3.1 Vector Databases](./4.3.1-vector-databases.md), [4.3.2 Semantic Search](./4.3.2-semantic-search.md)
- **Grounded In**: Mem0 (2025), Neo4j, Knowledge Graph research (2024-2025)

## Table of Contents

- [Overview](#overview)
- [The Problem: Unstructured Memory](#the-problem-unstructured-memory)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

---

## Overview

Fact extraction identifies and extracts structured information (entities, relationships, attributes) from unstructured text. This enables AI agents to build queryable knowledge bases that persist across sessions.

**Key Difference from Vector Search**:

```
User: "I prefer dark mode and I'm allergic to peanuts."

Vector Storage (Unstructured):
→ Store entire sentence as embedding: [0.23, -0.45, 0.87, ...]

Fact Extraction (Structured):
→ Extract facts:
   - preference: UI theme = "dark mode"
   - medical: allergy = "peanuts"
```

**Key Research Findings (2024-2025)**:

- **Mem0**: 26% accuracy improvement with fact-based memory
- **Token Reduction**: 90% cost savings (structured facts vs full conversations)
- **Latency**: 91% reduction (50ms vs 550ms p95)
- **Precision**: 92.4% fact retrieval accuracy

---

## The Problem: Unstructured Memory

### The Classic Challenge

Without fact extraction, agents lose precision:

```
Session 1: User shares preferences
"I like TypeScript and dark mode"

Session 50: Vector search returns similar content
Query: "What programming languages does user prefer?"
Result: 5 documents mentioning "programming" and "languages"
Problem: ❌ No precise answer, agent must infer from context

With Fact Extraction:
Query: "What programming languages does user prefer?"
Result: { programming_language: "TypeScript" }
Precise: ✅ Exact answer in 1ms
```

**Problems**:

- ❌ **No precision**: Vector search returns "similar" not "exact"
- ❌ **Context loss**: Important facts buried in conversation history
- ❌ **No relationships**: Can't answer "Who is Alice's manager?"
- ❌ **High cost**: Passing full conversation history for context

### Why Both Are Needed

| Use Case | Best Approach |
|----------|---------------|
| "What UI preferences does this user have?" | Vector search (semantic) |
| "Is this user allergic to peanuts?" | Fact extraction (exact) |
| "Find documents about TypeScript" | Vector search |
| "What is user's preferred language?" | Fact extraction |

---

## Core Concept

### Two-Phase Memory Pipeline (Mem0 Architecture)

```
┌─────────────────────────────────────────────────────────────┐
│                 PHASE 1: EXTRACTION                          │
├─────────────────────────────────────────────────────────────┤
│  User Input → LLM (Structured Prompt) → Extract Facts       │
│                                                              │
│  "I prefer dark mode"                                        │
│       ↓                                                      │
│  LLM extracts:                                               │
│   - subject: "user"                                          │
│   - predicate: "prefers_theme"                               │
│   - object: "dark mode"                                      │
│   - confidence: 0.95                                         │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                 PHASE 2: UPDATE                              │
├─────────────────────────────────────────────────────────────┤
│  New Facts + Existing Facts → LLM → Reconcile + Store       │
│                                                              │
│  New: "UI theme = dark mode"                                 │
│  Old: "UI theme = light mode"                                │
│       ↓                                                      │
│  LLM decides:                                                │
│   - UPDATE (replace old with new)                            │
│   - MERGE (combine both)                                     │
│   - KEEP (no change)                                         │
│       ↓                                                      │
│  Store in: Vector DB + Graph DB + Key-Value Store            │
└─────────────────────────────────────────────────────────────┘
```

### Hybrid Storage Architecture

| Storage Type | Purpose | Example Query |
|--------------|---------|---------------|
| **Vector Store** | Semantic search | "Find all facts about preferences" |
| **Graph Store** | Relationships | "Who is Alice's manager?" |
| **Key-Value** | Fast lookup | "Get user_123's email" |

---

## Implementation Patterns

### Pattern 1: LLM-Based Extraction

**Use Case**: Flexible fact extraction from any domain

```typescript
import { OpenAI } from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

interface Fact {
  id: string;
  subject: string;
  predicate: string;
  object: string;
  confidence: number;
  timestamp: string;
}

async function extractFacts(text: string): Promise<Fact[]> {
  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [{
      role: 'user',
      content: `Extract all factual statements from the text as subject-predicate-object triples.

Text: "${text}"

Return JSON:
{
  "facts": [
    { "subject": "Alice", "predicate": "works_at", "object": "OpenAI", "confidence": 0.95 }
  ]
}`,
    }],
    temperature: 0.0,
    response_format: { type: 'json_object' },
  });

  const content = JSON.parse(response.choices[0].message.content!);
  return content.facts.map((f: any) => ({
    id: crypto.randomUUID(),
    ...f,
    timestamp: new Date().toISOString(),
  }));
}

// Usage
const text = 'Alice works at OpenAI as a software engineer. She prefers TypeScript.';
const facts = await extractFacts(text);
// [
//   { subject: "Alice", predicate: "works_at", object: "OpenAI", confidence: 0.95, ... },
//   { subject: "Alice", predicate: "job_title", object: "software engineer", confidence: 0.90, ... },
//   { subject: "Alice", predicate: "prefers", object: "TypeScript", confidence: 0.85, ... }
// ]
```

**Pros**:
- ✅ Flexible (any domain)
- ✅ High accuracy (85-95%)
- ✅ Context-aware

**Cons**:
- ❌ Slow (200-500ms)
- ❌ Costly ($0.10-0.30 per 1M tokens)

---

### Pattern 2: Named Entity Recognition (NER)

**Use Case**: Fast entity extraction at scale

```typescript
import { pipeline } from '@xenova/transformers';

const ner = await pipeline('ner', 'Xenova/bert-base-NER');

async function extractEntities(text: string) {
  const entities = await ner(text);

  return entities.map((e: any) => ({
    type: e.entity.replace('B-', '').replace('I-', ''),
    value: e.word,
    score: e.score,
  }));
}

// Usage
const entities = await extractEntities('Alice works at OpenAI in San Francisco.');
// [
//   { type: "PER", value: "Alice", score: 0.99 },
//   { type: "ORG", value: "OpenAI", score: 0.98 },
//   { type: "LOC", value: "San Francisco", score: 0.97 }
// ]
```

**Pros**:
- ✅ Fast (10-50ms)
- ✅ Free (self-hosted)
- ✅ No API costs

**Cons**:
- ❌ Limited categories (PER, ORG, LOC, DATE)
- ❌ No relationships
- ❌ Lower accuracy (70-85%)

---

### Pattern 3: Knowledge Graph Storage

**Use Case**: Relationship queries and multi-hop reasoning

```typescript
import neo4j from 'neo4j-driver';

const driver = neo4j.driver(
  'bolt://localhost:7687',
  neo4j.auth.basic('neo4j', 'password')
);

// Store fact as graph relationship
async function storeFact(fact: Fact, userId: string) {
  const session = driver.session();
  await session.run(`
    MERGE (s:Entity {name: $subject, user_id: $userId})
    MERGE (o:Entity {name: $object, user_id: $userId})
    MERGE (s)-[r:${fact.predicate} {
      confidence: $confidence,
      timestamp: $timestamp
    }]->(o)
  `, {
    subject: fact.subject,
    object: fact.object,
    userId,
    confidence: fact.confidence,
    timestamp: fact.timestamp,
  });
  await session.close();
}

// Query relationships
async function queryRelationship(subject: string, predicate: string, userId: string) {
  const session = driver.session();
  const result = await session.run(`
    MATCH (s:Entity {name: $subject, user_id: $userId})-[r:${predicate}]->(o:Entity)
    RETURN o.name AS object, r.confidence AS confidence
  `, { subject, userId });
  await session.close();

  return result.records.map(r => ({
    object: r.get('object'),
    confidence: r.get('confidence'),
  }));
}

// Multi-hop query: "Who is Alice's manager's manager?"
async function multiHopQuery(start: string, relation: string, hops: number, userId: string) {
  const session = driver.session();
  const result = await session.run(`
    MATCH (s:Entity {name: $start, user_id: $userId})-[:${relation}*${hops}]->(target:Entity)
    RETURN target.name AS result
  `, { start, userId });
  await session.close();

  return result.records.map(r => r.get('result'));
}
```

**Pros**:
- ✅ Relationship queries
- ✅ Multi-hop reasoning
- ✅ Precise answers

**Cons**:
- ❌ Additional infrastructure
- ❌ More complex setup

---

### Pattern 4: Hybrid Extraction + Storage

**Use Case**: Production-ready fact system

```typescript
import { OpenAI } from 'openai';
import lancedb from 'lancedb';
import neo4j from 'neo4j-driver';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const vectorDb = await lancedb.connect('./lancedb');
const graphDriver = neo4j.driver('bolt://localhost:7687');

class FactStore {
  // 1. Extract facts from text
  async extract(text: string): Promise<Fact[]> {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{
        role: 'user',
        content: `Extract facts as JSON: ${text}`,
      }],
      response_format: { type: 'json_object' },
    });

    return JSON.parse(response.choices[0].message.content!).facts;
  }

  // 2. Store in both vector and graph DB
  async store(facts: Fact[], userId: string) {
    // Vector DB for semantic search
    const factTexts = facts.map(f => `${f.subject} ${f.predicate} ${f.object}`);
    const embeddings = await this.embed(factTexts);

    const table = await vectorDb.openTable('facts');
    await table.add(facts.map((fact, idx) => ({
      ...fact,
      user_id: userId,
      vector: embeddings[idx],
    })));

    // Graph DB for relationship queries
    const session = graphDriver.session();
    for (const fact of facts) {
      await session.run(`
        MERGE (s:Entity {name: $subject, user_id: $userId})
        MERGE (o:Entity {name: $object, user_id: $userId})
        MERGE (s)-[:${fact.predicate}]->(o)
      `, { subject: fact.subject, object: fact.object, userId });
    }
    await session.close();
  }

  // 3. Query facts (hybrid)
  async query(query: string, userId: string) {
    // Try exact relationship query first
    const graphResult = await this.queryGraph(query, userId);
    if (graphResult.length > 0) return { type: 'exact', results: graphResult };

    // Fall back to semantic search
    const vectorResult = await this.queryVector(query, userId);
    return { type: 'semantic', results: vectorResult };
  }

  private async embed(texts: string[]): Promise<number[][]> {
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: texts,
    });
    return response.data.map(d => d.embedding);
  }
}

// Usage
const store = new FactStore();
const facts = await store.extract('Alice works at OpenAI and prefers TypeScript.');
await store.store(facts, 'user_123');

const result = await store.query('Where does Alice work?', 'user_123');
// { type: 'exact', results: [{ object: 'OpenAI' }] }
```

---

## Framework Integration

### AI SDK v6 with Fact Extraction Tool

```typescript
import { generateText, tool, stepCountIs } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const factStore = new FactStore();

// Fact extraction tool
const extractAndStoreFacts = tool({
  description: 'Extract and store facts from user statement',
  inputSchema: z.object({
    text: z.string().describe('Text to extract facts from'),
    userId: z.string().describe('User ID'),
  }),
  execute: async ({ text, userId }) => {
    const facts = await factStore.extract(text);
    await factStore.store(facts, userId);

    return {
      success: true,
      factsExtracted: facts.length,
      facts: facts.map(f => `${f.subject} ${f.predicate} ${f.object}`),
    };
  },
});

// Fact query tool
const queryFacts = tool({
  description: 'Query stored facts about a user',
  inputSchema: z.object({
    query: z.string().describe('What to look up'),
    userId: z.string().describe('User ID'),
  }),
  execute: async ({ query, userId }) => {
    const result = await factStore.query(query, userId);
    return {
      queryType: result.type,
      results: result.results,
    };
  },
});

// Agent with fact tools
const { text, steps } = await generateText({
  model: openai('gpt-4o'),
  tools: { extractAndStoreFacts, queryFacts },
  stopWhen: stepCountIs(10),
  prompt: 'The user said they work at Google and prefer Python. Store this information.',
});
```

---

## Research & Benchmarks

### Mem0 Performance (2025)

| Metric | Mem0 | OpenAI Baseline | Improvement |
|--------|------|-----------------|-------------|
| **Response Accuracy** | 87.3% | 69.1% | **+26%** |
| **P95 Latency** | 50ms | 550ms | **-91%** |
| **Token Cost/Query** | 150 | 1500 | **-90%** |
| **Fact Precision** | 92.4% | 78.6% | **+18%** |

### Extraction Method Comparison

| Method | Latency | Cost/1M | Accuracy | Use Case |
|--------|---------|---------|----------|----------|
| **LLM (GPT-4o-mini)** | 200-500ms | $100-300 | 85-95% | Complex facts |
| **NER (BERT)** | 10-50ms | Free | 70-85% | Simple entities |
| **Hybrid** | 100-200ms | $50-150 | 90%+ | Production |

### Storage Backend Comparison

| Backend | Query Type | Latency | Best For |
|---------|------------|---------|----------|
| **Vector (LanceDB)** | Semantic | 10-50ms | Similar facts |
| **Graph (Neo4j)** | Relationship | 5-20ms | "Who is X's Y?" |
| **Key-Value (Redis)** | Exact | <5ms | Direct lookup |

---

## When to Use This Pattern

### ✅ Use Fact Extraction When:

1. **Precise queries needed**
   - "Is this user allergic to peanuts?"
   - "What is user's preferred theme?"

2. **Relationship tracking**
   - Org charts, social connections
   - "Who reports to Alice?"

3. **Preference persistence**
   - User settings, habits, history
   - Personalization across sessions

4. **Compliance requirements**
   - Audit trails, contractual terms
   - Precise record keeping

### ❌ Consider Alternatives When:

1. **Semantic similarity sufficient**
   - "Find similar documents"
   - Use vector search only

2. **Temporary context only**
   - Single-session conversations
   - Use working memory

3. **High-volume, low-value facts**
   - Every minor preference
   - Overhead not justified

### Decision Matrix

| Query Type | Best Approach |
|------------|---------------|
| "What did user say about TypeScript?" | Vector search |
| "Does user prefer TypeScript or JavaScript?" | Fact extraction |
| "Find documents mentioning preferences" | Vector search |
| "Is user allergic to peanuts?" | Fact extraction |
| "What are user's dietary restrictions?" | Both (fact + vector) |

---

## Production Best Practices

### 1. Confidence Scoring

```typescript
// Always include confidence scores
const fact = {
  subject: 'Alice',
  predicate: 'works_at',
  object: 'OpenAI',
  confidence: 0.95, // Explicitly stated
};

const inferred = {
  subject: 'Alice',
  predicate: 'likes',
  object: 'Python',
  confidence: 0.65, // Inferred from context
};

// Query with threshold
async function queryWithConfidence(query: string, minConfidence = 0.8) {
  const results = await factStore.query(query);
  return results.filter(r => r.confidence >= minConfidence);
}
```

### 2. Fact Conflict Resolution

```typescript
async function resolveFacts(oldFact: Fact, newFact: Fact): Promise<'UPDATE' | 'MERGE' | 'KEEP'> {
  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [{
      role: 'user',
      content: `Old: ${JSON.stringify(oldFact)}
New: ${JSON.stringify(newFact)}
Should we UPDATE (replace), MERGE (keep both), or KEEP (ignore new)?`,
    }],
  });

  return response.choices[0].message.content!.trim() as any;
}

// Usage: User changed preference
const old = { subject: 'Alice', predicate: 'prefers_theme', object: 'dark mode' };
const new_ = { subject: 'Alice', predicate: 'prefers_theme', object: 'light mode' };
const decision = await resolveFacts(old, new_); // "UPDATE"
```

### 3. Temporal Facts

```typescript
interface TemporalFact extends Fact {
  valid_from: string;
  valid_until?: string; // null = still valid
}

async function storeTemporalFact(fact: TemporalFact, userId: string) {
  // Invalidate old version
  await session.run(`
    MATCH (s:Entity {name: $subject})-[r:${fact.predicate}]->(o:Entity)
    WHERE r.valid_until IS NULL
    SET r.valid_until = $now
  `, { subject: fact.subject, now: new Date().toISOString() });

  // Add new version
  await session.run(`
    MERGE (s:Entity {name: $subject})
    MERGE (o:Entity {name: $object})
    CREATE (s)-[:${fact.predicate} {
      valid_from: $validFrom,
      valid_until: null
    }]->(o)
  `, { subject: fact.subject, object: fact.object, validFrom: fact.valid_from });
}
```

### 4. Category Tagging

```typescript
const fact = {
  subject: 'Alice',
  predicate: 'allergy',
  object: 'peanuts',
  category: 'medical',     // Enable filtering by type
  sensitivity: 'high',     // Flag for access control
};

// Query by category
async function getFactsByCategory(userId: string, category: string) {
  return await table
    .search()
    .where(`user_id = '${userId}' AND category = '${category}'`)
    .execute();
}
```

### Common Pitfalls

#### ❌ Pitfall: Over-Extraction

```typescript
// ❌ Bad: Extracting trivial facts
"I like coffee" → fact: { predicate: "likes", object: "coffee" }
"I like tea too" → fact: { predicate: "likes", object: "tea" }
// Result: 1000s of low-value facts

// ✅ Good: Extract only important facts
"I'm allergic to peanuts" → fact: { predicate: "allergic_to", object: "peanuts" }
```

#### ❌ Pitfall: Ignoring Negations

```typescript
// ❌ Bad: Missing negation
"I don't like TypeScript" → { predicate: "likes", object: "TypeScript" }

// ✅ Good: Respect negations
"I don't like TypeScript" → { predicate: "dislikes", object: "TypeScript" }
```

#### ❌ Pitfall: No Expiration

```typescript
// ❌ Bad: Facts never expire
// 2023: "Alice works at OpenAI"
// 2025: Still returning outdated fact

// ✅ Good: Add expiration
const fact = {
  valid_until: '2025-12-31', // Review after 1 year
};
```

---

## Key Takeaways

1. **Fact extraction enables precise queries** - Exact answers vs similar documents
2. **Use hybrid storage** - Vector for semantic, graph for relationships
3. **Include confidence scores** - Not all facts are equally reliable
4. **Handle conflicts** - Same fact may change over time
5. **Tag by category** - Enable efficient filtering and access control

**Quick Implementation Checklist**:

- [ ] Choose extraction method (LLM for complex, NER for simple)
- [ ] Set up hybrid storage (vector + graph or key-value)
- [ ] Implement confidence scoring
- [ ] Add conflict resolution logic
- [ ] Plan for temporal facts (versioning)
- [ ] Tag facts by category and sensitivity
- [ ] Set up fact expiration/review process

---

## References

1. **Mem0 Team** (2025). "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory". arXiv. https://arxiv.org/abs/2504.19413
2. **Mem0** (2025). "Graph Memory Documentation". https://docs.mem0.ai/open-source/features/graph-memory
3. **Neo4j** (2025). "Knowledge Graphs for AI Applications". https://neo4j.com/
4. **Zep** (2025). "Temporal Knowledge Graphs for AI Memory". arXiv. https://arxiv.org/abs/2501.13956
5. **LangChain** (2025). "Entity Memory Documentation". https://python.langchain.com/docs/modules/memory/
6. **Hugging Face** (2025). "Named Entity Recognition Models". https://huggingface.co/models?pipeline_tag=ner

**Related Topics**:

- [4.3.1 Vector Databases](./4.3.1-vector-databases.md)
- [4.3.2 Semantic Search](./4.3.2-semantic-search.md)
- [4.3.4 Cross-Session Retrieval](./4.3.4-cross-session-retrieval.md)
- [4.1.2 Entity Extraction](./4.1.2-entity-extraction.md)

**Layer Index**: [Layer 4: Memory & State](../AI_KNOWLEDGE_BASE_TOC.md#layer-4-memory--state)
