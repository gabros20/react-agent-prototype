# 1.3.3 Prompt Versioning & Caching

## TL;DR

Prompt versioning tracks changes over time (enabling rollbacks, A/B testing, and audit trails), while caching reduces costs by 60-90% and latency by 80%; together they form the foundation of production-ready LLM applications—2025 best practices include Langfuse for versioning and Anthropic's native prompt caching for cost optimization.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [1.3.1 Template Engines](./1.3.1-template-engines.md), [1.3.2 Conditional Sections](./1.3.2-conditional-sections.md)
- **Grounded In**: Langfuse, PromptLayer, Anthropic Prompt Caching, LaunchDarkly, Production LLM Best Practices

## Table of Contents

- [Overview](#overview)
- [The Problem: Unmanaged Prompts](#the-problem-unmanaged-prompts)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Prompt versioning** tracks prompt changes over time, enabling rollbacks, A/B testing, and audit trails—treating prompts like code. **Prompt caching** stores frequently used prompts and their responses, reducing costs by 60-90% and latency by 80% in production systems.

**Key Insight** (2024-2025): Every production LLM application needs versioning (for reliability) and caching (for cost optimization).

**Platform Landscape**:

| Platform | Versioning | Caching | Best For |
|----------|------------|---------|----------|
| **Langfuse** | ✅ Git-style | ❌ | Open-source versioning |
| **PromptLayer** | ✅ Visual | ❌ | Team collaboration |
| **LaunchDarkly** | ✅ Feature flags | ❌ | Gradual rollouts |
| **Anthropic** | ❌ | ✅ Native | Claude cost optimization |
| **Redis** | ❌ | ✅ Distributed | High-scale applications |

**Date Verified**: 2025-12-03

## The Problem: Unmanaged Prompts

### The Classic Challenge

Without versioning, prompt changes are untraceable:

```typescript
// Version 1 (no tracking)
const prompt = "You are an AI assistant.";

// Later... (what changed? when? why?)
const prompt = "You are an autonomous AI assistant using ReAct.";

// Even later... (how do we rollback?)
const prompt = "You are an autonomous AI assistant. Think step-by-step.";
```

Without caching, every request is expensive:

```
Every request pays for full prompt:
Request 1: 2,000 token system prompt + 100 token user input = 2,100 tokens ($0.021)
Request 2: 2,000 token system prompt + 150 token user input = 2,150 tokens ($0.0215)
Request 3: 2,000 token system prompt + 80 token user input = 2,080 tokens ($0.0208)

Total: 6,330 tokens ($0.0633)
```

**Problems**:

- ❌ **No history**: Can't identify when issues started
- ❌ **No rollback**: Can't revert bad changes
- ❌ **No audit trail**: Compliance requirements unmet
- ❌ **High costs**: Re-processing same prompt every request
- ❌ **High latency**: No benefit from repeated content

### Why This Matters

- Versioning ensures reliability and recoverability
- Caching ensures cost efficiency
- Together: Production-ready LLM applications

## Core Concept

### Semantic Versioning for Prompts

**Format**: `MAJOR.MINOR.PATCH`

| Change Type | Version Bump | Example |
|-------------|--------------|---------|
| **Breaking** | MAJOR (1.0→2.0) | Switch from standard to ReAct pattern |
| **New features** | MINOR (1.0→1.1) | Add working memory section |
| **Bug fixes** | PATCH (1.0.0→1.0.1) | Fix typo in instructions |

**Example Evolution**:

```
v1.0.0 - Initial CMS agent prompt
v1.0.1 - Fixed typo in tool description
v1.1.0 - Added working memory section
v1.2.0 - Added few-shot examples
v1.2.1 - Clarified deletion confirmation rule
v2.0.0 - Switched to modular architecture (breaking)
v2.1.0 - Added production/dev conditionals
```

### Caching Strategies

**1. Prefix Caching**: Cache static system prompt, vary user input
**2. Semantic Caching**: Cache responses for similar queries
**3. Native Caching**: Use provider's built-in caching (Anthropic)
**4. Distributed Caching**: Redis for scaled applications

## Implementation Patterns

### Pattern 1: Langfuse Versioning (Open Source)

**Use Case**: Git-style version control with performance tracking

```typescript
import { Langfuse } from 'langfuse';

const langfuse = new Langfuse({
  publicKey: process.env.LANGFUSE_PUBLIC_KEY,
  secretKey: process.env.LANGFUSE_SECRET_KEY
});

// Create new prompt version
await langfuse.createPrompt({
  name: 'react-agent',
  prompt: reactPromptTemplate,
  labels: ['staging'],
  config: {
    model: 'gpt-4o-mini',
    temperature: 0.7
  }
});

// Fetch prompt by label
const prompt = await langfuse.getPrompt('react-agent', { label: 'production' });

// Render with variables
const rendered = prompt.compile({
  workingMemory: context.memory,
  tools: context.tools
});

// Track usage
const generation = langfuse.generation({
  name: 'react-execution',
  prompt: prompt,
  model: 'gpt-4o-mini'
});
```

**Labeling Strategy**:

```typescript
// Development flow
await langfuse.createPrompt({ name: 'react', labels: ['dev'] });
await langfuse.createPrompt({ name: 'react', labels: ['staging'] });
await langfuse.createPrompt({ name: 'react', labels: ['production'] });

// A/B testing
await langfuse.createPrompt({ name: 'react', labels: ['experiment-cot'] });
await langfuse.createPrompt({ name: 'react', labels: ['experiment-react'] });

// Rollback
await langfuse.updatePrompt({
  name: 'react',
  version: 12, // Previous stable version
  labels: ['production']
});
```

### Pattern 2: File-Based Versioning (Git)

**Use Case**: Simple approach for small teams

```
prompts/
├── react-agent/
│   ├── v1.0.0.hbs         # Initial version
│   ├── v1.1.0.hbs         # Added examples
│   ├── v2.0.0.hbs         # Modular architecture
│   ├── v2.1.0.hbs         # Current production
│   └── changelog.md       # Version history
└── registry.json          # Version metadata
```

**registry.json**:

```json
{
  "react-agent": {
    "current": "v2.1.0",
    "production": "v2.1.0",
    "staging": "v2.2.0-beta",
    "versions": {
      "v2.1.0": {
        "file": "react-agent/v2.1.0.hbs",
        "created": "2025-03-15T10:30:00Z",
        "author": "team@company.com",
        "changelog": "Added production/dev conditionals",
        "metrics": {
          "accuracy": 0.92,
          "latency_p95": 1200
        }
      }
    }
  }
}
```

**Loader**:

```typescript
class PromptRegistry {
  private registry: any;
  private cache = new Map<string, HandlebarsTemplateDelegate>();

  constructor(registryPath: string) {
    this.registry = JSON.parse(fs.readFileSync(registryPath, 'utf-8'));
  }

  getPrompt(name: string, options?: { version?: string; label?: string }): string {
    const promptConfig = this.registry[name];
    if (!promptConfig) throw new Error(`Prompt not found: ${name}`);

    const targetVersion = options?.version
      || (options?.label && promptConfig[options.label])
      || promptConfig.current;

    const versionInfo = promptConfig.versions[targetVersion];
    if (!versionInfo) throw new Error(`Version not found: ${targetVersion}`);

    // Load and compile template
    const cacheKey = `${name}:${targetVersion}`;
    if (!this.cache.has(cacheKey)) {
      const templateSource = fs.readFileSync(versionInfo.file, 'utf-8');
      this.cache.set(cacheKey, Handlebars.compile(templateSource));
    }

    return cacheKey;
  }

  render(name: string, context: any, options?: { version?: string; label?: string }): string {
    const cacheKey = this.getPrompt(name, options);
    const template = this.cache.get(cacheKey)!;
    return template(context);
  }

  rollback(name: string, label: string = 'production'): void {
    const promptConfig = this.registry[name];
    const versions = Object.keys(promptConfig.versions).sort().reverse();
    const currentVersion = promptConfig[label];
    const currentIndex = versions.indexOf(currentVersion);
    const previousVersion = versions[currentIndex + 1];

    if (!previousVersion) throw new Error('No previous version');

    promptConfig[label] = previousVersion;
    console.log(`Rolled back ${name} ${label} to ${previousVersion}`);
  }
}
```

### Pattern 3: Anthropic Native Caching

**Use Case**: Cost optimization for Claude (90% cost reduction)

```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

// Define cached system prompt
const cachedSystemPrompt = {
  type: "text",
  text: `You are an autonomous AI assistant...

  [2,000 tokens of static instructions, tools, examples]`,
  cache_control: { type: "ephemeral" } // Cache this block
};

// Request 1: Full processing (creates cache)
const response1 = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  max_tokens: 1024,
  system: [cachedSystemPrompt],
  messages: [
    { role: "user", content: "Create a new page titled 'Getting Started'" }
  ]
});

// Requests 2-N: Use cached system prompt (90% cost reduction)
const response2 = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  max_tokens: 1024,
  system: [cachedSystemPrompt], // Cached!
  messages: [
    { role: "user", content: "Update homepage content" }
  ]
});

// Usage statistics
console.log(response1.usage);
// { cache_creation_tokens: 2000, cache_read_tokens: 0 }

console.log(response2.usage);
// { cache_creation_tokens: 0, cache_read_tokens: 2000 } // 90% cheaper!
```

**Performance** (Anthropic 2025 data):

| Use Case | Latency Reduction | Cost Reduction |
|----------|-------------------|----------------|
| Chat with book (100K tokens) | 79% | 90% |
| Many-shot prompting (10K tokens) | 31% | 86% |
| Multi-turn conversations | 75% | 53% |

### Pattern 4: Redis Distributed Cache

**Use Case**: High-scale applications with multiple instances

```typescript
import Redis from 'ioredis';
import crypto from 'crypto';

class PromptCache {
  private redis: Redis;

  constructor() {
    this.redis = new Redis(process.env.REDIS_URL);
  }

  private hash(prompt: string): string {
    return crypto.createHash('sha256').update(prompt).digest('hex');
  }

  async get(prompt: string): Promise<string | null> {
    const key = `prompt:${this.hash(prompt)}`;
    return await this.redis.get(key);
  }

  async set(prompt: string, response: string, ttl: number = 3600): Promise<void> {
    const key = `prompt:${this.hash(prompt)}`;
    await this.redis.setex(key, ttl, response);
  }

  async invalidate(pattern: string): Promise<void> {
    const keys = await this.redis.keys(`prompt:${pattern}*`);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }
}

// Usage
const cache = new PromptCache();

async function generateWithCache(prompt: string) {
  const cached = await cache.get(prompt);
  if (cached) {
    console.log('Cache hit!');
    return cached;
  }

  const response = await generateText({ prompt });
  await cache.set(prompt, response, 3600); // Cache for 1 hour

  return response;
}

// Invalidate when prompt version changes
await cache.invalidate('react-agent-v2.1.0');
```

### Pattern 5: Combined Versioning + Caching

**Use Case**: Production-ready with both reliability and cost optimization

```typescript
class VersionedCachedPromptEngine {
  constructor(
    private registry: PromptRegistry,
    private cache: PromptCache
  ) {}

  async render(
    name: string,
    context: any,
    options: { version?: string; label?: string } = {}
  ): Promise<string> {
    // Resolve version
    const version = this.registry.resolveVersion(name, options);

    // Build cache key (includes version)
    const contextHash = this.hashContext(context);
    const cacheKey = `prompt:${name}:${version}:${contextHash}`;

    // Check cache
    const cached = await this.cache.get(cacheKey);
    if (cached) {
      metrics.increment('prompt.cache.hit', { name, version });
      return cached;
    }

    // Render prompt
    const rendered = this.registry.render(name, context, { version });

    // Cache result (24 hour TTL)
    await this.cache.set(cacheKey, rendered, 86400);

    metrics.increment('prompt.cache.miss', { name, version });
    return rendered;
  }

  async deployVersion(name: string, version: string, label: string): Promise<void> {
    // Update registry
    await this.registry.setLabel(name, version, label);

    // Invalidate all cached prompts for this name
    await this.cache.invalidatePattern(`prompt:${name}:*`);

    console.log(`Deployed ${name} ${version} as ${label}, cache invalidated`);
  }

  private hashContext(context: any): string {
    const cacheableFields = {
      environment: context.environment,
      userTier: context.user?.tier,
      modelFamily: context.model?.family
    };
    return crypto
      .createHash('sha256')
      .update(JSON.stringify(cacheableFields))
      .digest('hex')
      .substring(0, 16);
  }
}
```

## When to Use This Pattern

### ✅ Use Versioning When

1. **Production systems** - Need rollback capability
2. **Team collaboration** - Multiple people editing prompts
3. **Compliance requirements** - Audit trail needed
4. **A/B testing** - Comparing prompt variants
5. **Performance tracking** - Metrics per version

### ✅ Use Caching When

1. **Long system prompts** - Static content repeated
2. **High volume** - Many requests with same prompt
3. **Cost sensitive** - LLM API costs are significant
4. **Latency sensitive** - Need faster responses
5. **Repeated queries** - FAQ-style applications

### Decision Matrix

| Your Situation | Versioning | Caching |
|----------------|------------|---------|
| Prototype | Git only | None |
| Small production | File-based | Redis |
| Large production | Langfuse | Redis + Anthropic |
| Enterprise | PromptLayer + LaunchDarkly | Full stack |

## Production Best Practices

### 1. Monitor Cache Hit Rate

```typescript
class MonitoredCache {
  private hits = 0;
  private misses = 0;

  async get(key: string): Promise<string | null> {
    const value = await this.redis.get(key);
    if (value) {
      this.hits++;
      metrics.increment('cache.hit');
    } else {
      this.misses++;
      metrics.increment('cache.miss');
    }
    return value;
  }

  getHitRate(): number {
    const total = this.hits + this.misses;
    return total > 0 ? this.hits / total : 0;
  }
}

// Alert if hit rate drops
setInterval(() => {
  const hitRate = cache.getHitRate();
  if (hitRate < 0.5) {
    console.warn(`Low cache hit rate: ${hitRate}`);
  }
}, 60000);
```

### 2. Version Deployment Pipeline

```typescript
// Development → Staging → Production
async function deployPromptVersion(name: string, version: string) {
  // 1. Deploy to staging
  await engine.deployVersion(name, version, 'staging');

  // 2. Run automated tests
  const testResults = await runPromptTests(name, 'staging');
  if (!testResults.passed) {
    throw new Error(`Tests failed: ${testResults.errors}`);
  }

  // 3. Gradual rollout (via LaunchDarkly)
  await launchDarkly.gradualRollout(name, version, {
    stages: [0.05, 0.25, 0.5, 1.0],
    interval: '24h'
  });

  // 4. Monitor metrics
  await monitorPromptMetrics(name, version, '7d');

  // 5. Full production
  await engine.deployVersion(name, version, 'production');
}
```

### 3. Invalidation on Deploy

```typescript
async function deployVersion(name: string, version: string, label: string) {
  // Update registry
  await registry.setLabel(name, version, label);

  // Invalidate cache
  await cache.invalidatePattern(`${name}:*`);

  console.log(`Deployed ${name} ${version}, cache invalidated`);
}
```

### Common Pitfalls

**❌ No rollback plan**: Deploy without ability to revert.

**❌ Cache without invalidation**: Stale prompts after version change.

**❌ Missing metrics**: No tracking of version performance.

**❌ Over-caching**: Caching dynamic content that shouldn't be cached.

## Key Takeaways

1. **Versioning for reliability** - Track changes, enable rollbacks
2. **Caching for cost** - 60-90% reduction possible
3. **Semantic versioning** - MAJOR.MINOR.PATCH for prompts
4. **Invalidate on deploy** - Clear cache when version changes
5. **Monitor hit rate** - Track caching effectiveness

**Quick Implementation Checklist**:

- [ ] Choose versioning platform (Langfuse, file-based)
- [ ] Implement semantic versioning
- [ ] Set up cache layer (Redis, Anthropic native)
- [ ] Add cache invalidation on deploy
- [ ] Implement monitoring for hit rate
- [ ] Create rollback procedures

## References

1. **Langfuse** (2025). "Prompt Management Documentation". https://langfuse.com/docs/prompts
2. **Anthropic** (2025). "Prompt Caching Guide". https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
3. **PromptLayer** (2025). "Prompt Version Control". https://promptlayer.com/docs
4. **LaunchDarkly** (2025). "Feature Flags for ML". https://launchdarkly.com/solutions/machine-learning/
5. **Redis** (2025). "Caching Patterns". https://redis.io/docs/manual/patterns/

**Related Topics**:

- [1.3.1 Template Engines](./1.3.1-template-engines.md)
- [1.3.2 Conditional Sections](./1.3.2-conditional-sections.md)
- [1.2.5 Modular Architecture](./1.2.5-modular-architecture.md)

**Layer Index**: [Layer 1: Prompt Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-1-prompt-engineering)
