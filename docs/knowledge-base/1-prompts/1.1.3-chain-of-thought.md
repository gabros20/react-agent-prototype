# 1.1.3 Chain-of-Thought (CoT) Prompting

## TL;DR

Chain-of-Thought prompting dramatically improves LLM performance on complex reasoning tasks (50-400% on math/logic benchmarks) by encouraging models to "think step-by-step" before answering; it works via in-context examples showing reasoning steps or zero-shot triggers like "Let's think step by step."

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [1.1.1 Instruction Design](./1.1.1-instruction-design.md), [1.1.2 Few-Shot Learning](./1.1.2-few-shot.md)
- **Grounded In**: Wei et al. (2022), Kojima et al. (2022), Google Research

## Table of Contents

- [Overview](#overview)
- [The Problem: LLMs Jump to Wrong Conclusions](#the-problem-llms-jump-to-wrong-conclusions)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Chain-of-Thought (CoT)** prompting is a breakthrough technique that guides models through intermediate reasoning steps, similar to showing your work in math class. Instead of jumping to conclusions, CoT externalizes working memory as text.

**Key Paper**: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., 2022)

**Impact on Benchmarks**:

| Benchmark | Without CoT | With CoT | Improvement |
|-----------|-------------|----------|-------------|
| **GSM8K** (Math) | 17% | 57% | +235% |
| **SVAMP** (Math) | 69% | 79% | +14% |
| **StrategyQA** (Logic) | 54% | 67% | +24% |
| **CommonsenseQA** | 68% | 75% | +10% |

**Key Finding**: Larger models benefit more (emergence at 10B+ parameters).

**Date Verified**: 2025-12-03

## The Problem: LLMs Jump to Wrong Conclusions

### The Classic Challenge

Without reasoning steps, LLMs pattern-match to superficially similar problems:

```
Standard Prompting:
Q: The cafeteria had 23 apples. They used 20 for lunch and bought 6 more.
   How many apples do they have?
A: 9 apples ❌

Model jumped to answer without reasoning through the steps.
```

**With Chain-of-Thought**:

```
Q: The cafeteria had 23 apples. They used 20 for lunch and bought 6 more.
   How many apples do they have?

A: The cafeteria started with 23 apples.
   They used 20 to make lunch: 23 - 20 = 3 apples left.
   Then they bought 6 more: 3 + 6 = 9 apples.
   The answer is 9 apples. ✅
```

### Why Step-by-Step Matters

**Without CoT**:
- ❌ Model pattern-matches to superficially similar problems
- ❌ Misses intermediate dependencies
- ❌ Can't correct errors mid-reasoning
- ❌ "Black box"—can't debug wrong answers

**With CoT**:
- ✅ Forces decomposition into logical steps
- ✅ Each step builds on previous (chain)
- ✅ Errors visible and fixable
- ✅ Transparent, debuggable reasoning

## Core Concept

### Mechanism: Computational Intermediate Steps

**Theory** (from research):

1. LLMs have limited "working memory" in a single forward pass
2. Complex problems exceed this capacity
3. CoT externalizes working memory as text
4. Each reasoning step becomes input for the next step
5. This creates a "chain" of thought

**Analogy**: Like using scratch paper for long division instead of doing it all in your head.

### Two Variants

#### 1. Few-Shot CoT

Provide examples with reasoning steps:

```
Q: Roger has 5 tennis balls. He buys 2 cans of 3 balls each.
   How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans × 3 balls = 6 balls.
   5 + 6 = 11. The answer is 11.

Q: [new problem]
A: [model generates step-by-step reasoning]
```

#### 2. Zero-Shot CoT

Just add: "Let's think step by step"

```
Q: The cafeteria had 23 apples. They used 20 for lunch and bought 6 more.

Let's think step by step:
```

**Surprisingly effective!** This simple addition improves performance 20-50% on many tasks.

## Implementation Patterns

### Pattern 1: Few-Shot with Reasoning Examples

**Use Case**: Consistent reasoning pattern for similar problems

```
Solve these math word problems:

Q: There are 15 trees in the grove. After planting, there are 21. How many planted?
A: Originally 15 trees. After planting: 21 trees.
   Planted = 21 - 15 = 6 trees. The answer is 6.

Q: If there are 3 cars and 2 more arrive, how many cars?
A: Already had 3 cars. 2 more arrive. 3 + 2 = 5 cars. The answer is 5.

Q: Leah had 32 chocolates. Her sister had 42. They ate 35. How many left?
A:
```

**Pros**:
- ✅ Models learn from reasoning pattern
- ✅ Highest accuracy on complex problems

**Cons**:
- ❌ Requires crafting example reasoning
- ❌ Higher token usage

### Pattern 2: Zero-Shot with Magic Phrase

**Use Case**: Quick reasoning boost without examples

```
Q: A juggler can juggle 16 balls. Half are golf balls.
   How many golf balls?

Let's think step by step:
```

**Result**:
```
1. Total balls: 16
2. Half are golf balls
3. Half of 16 = 16 ÷ 2 = 8
4. Therefore, 8 golf balls
```

**Magic Phrases That Work**:
- "Let's think step by step" (most effective)
- "Let's work this out step by step"
- "Let's break this down"
- "Let's solve this systematically"

### Pattern 3: Structured Reasoning Template

**Use Case**: Consistent format across complex problems

```
Problem: [state problem]

Given Information:
- [fact 1]
- [fact 2]

Step-by-Step Solution:
1. [step 1]
2. [step 2]
3. [step 3]

Verification:
[check answer]

Final Answer: [answer]
```

**Example**:
```
Problem: $3 notebooks, 20% off for 5+. How much do 6 cost?

Given Information:
- Regular price: $3 per notebook
- Quantity: 6 notebooks
- Discount: 20% off for 5+

Step-by-Step Solution:
1. Base price: 6 × $3 = $18
2. Discount: 20% of $18 = $3.60
3. Final price: $18 - $3.60 = $14.40

Verification:
6 > 5, so discount applies ✓
20% off = 80% of original: $18 × 0.80 = $14.40 ✓

Final Answer: $14.40
```

### Pattern 4: Self-Ask (Decomposition)

**Use Case**: Multi-hop questions requiring intermediate lookups

```
Q: Who was U.S. president when the first iPhone was released?

Self-Ask CoT:
- Q1: When was the first iPhone released?
  A1: June 2007

- Q2: Who was U.S. president in June 2007?
  A2: George W. Bush (2001-2009)

Final Answer: George W. Bush
```

**Pros**:
- ✅ Explicit decomposition of multi-hop questions
- ✅ Each sub-question is simpler

**Cons**:
- ❌ More verbose output

## When to Use This Pattern

### ✅ Best For

1. **Multi-step reasoning**: Math word problems, logic puzzles
2. **Code debugging**: Tracing through execution step-by-step
3. **Logical deduction**: Syllogisms, constraint satisfaction
4. **Common sense reasoning**: Cause-effect chains
5. **Symbolic manipulation**: Algebra, variable substitution

**Examples**:
```
✅ "If a train travels 120 km in 2 hours, how far in 5 hours?"
✅ "Why does this code return undefined?"
✅ "All birds can fly. Penguins are birds. Can penguins fly?"
```

### ❌ Skip For

1. **Simple lookups**: "What is the capital of France?"
2. **Single-step problems**: "What is 5 + 3?"
3. **Creative tasks**: "Write a poem about sunset"
4. **When speed > accuracy**: High-throughput, low-stakes queries

## Production Best Practices

### 1. Combine with Structured Output

```typescript
import { z } from 'zod'

const schema = z.object({
  reasoning_steps: z.array(z.string()),
  intermediate_calculations: z.array(z.number()),
  final_answer: z.number(),
  confidence: z.number().min(0).max(1)
})

const result = await generateObject({
  model: openai("gpt-4o-mini", { structuredOutputs: true }),
  schema,
  prompt: `Solve this problem with step-by-step reasoning: ${problem}`
})
```

### 2. Add Verification Step

```
After showing your reasoning, verify by:
1. Checking your arithmetic
2. Ensuring all percentages add to 100%
3. Confirming units are consistent
4. Flagging any assumptions made

If you find errors, correct them before final answer.
```

### 3. Force Explicit Final Answer

```
After your reasoning, write your final answer on a new line:

FINAL ANSWER: [answer here]
```

### Common Pitfalls

**❌ Model skips reasoning**: Make instruction more explicit

```
❌ Weak: "Let's think step by step"

✅ Strong: "Before answering, write out your complete reasoning process.
   Show all calculations and intermediate steps. Do not skip any steps."
```

**❌ Reasoning loops**: Use numbered steps with structure

```
Follow this exact structure:
Step 1: Identify what we know
Step 2: Identify what we need to find
Step 3: Calculate or deduce
Step 4: Verify
Final Answer: [result]
```

**❌ Too verbose**: Constrain output length

```typescript
await generateText({
  model: openai("gpt-4o-mini"),
  prompt: `${question}\n\nLet's think step by step.`,
  maxTokens: 300  // Force conciseness
})
```

## Key Takeaways

1. **Externalize reasoning** - CoT forces models to "think out loud"
2. **50-400% improvement** - Dramatic gains on math, logic, common sense
3. **Two variants** - Few-shot (with examples) and zero-shot ("Let's think step by step")
4. **Transparent debugging** - Errors are visible in reasoning chain
5. **Combines with other techniques** - Few-shot, structured outputs, self-consistency

**Quick Implementation Checklist**:

- [ ] Is the problem multi-step or requiring reasoning?
- [ ] Have you tried zero-shot CoT first ("Let's think step by step")?
- [ ] For consistent results, use few-shot examples with reasoning?
- [ ] Is there a verification step?
- [ ] Is the final answer clearly marked?

## References

1. **Wei et al.** (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". _NeurIPS 2022_. https://arxiv.org/abs/2201.11903
2. **Kojima et al.** (2022). "Large Language Models are Zero-Shot Reasoners". _NeurIPS 2022_. https://arxiv.org/abs/2205.11916
3. **Wang et al.** (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models". https://arxiv.org/abs/2203.11171
4. **Zhou et al.** (2022). "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models". https://arxiv.org/abs/2205.10625
5. **Google Research** (2024). "Chain-of-Thought Prompting Guide". Research Blog.

**Related Topics**:

- [1.1.2 Few-Shot Learning](./1.1.2-few-shot.md)
- [1.1.4 Zero-Shot CoT](./1.1.4-zero-shot-cot.md)
- [1.1.5 Self-Consistency](./1.1.5-self-consistency.md)

**Layer Index**: [Layer 1: Prompt Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-1-prompt-engineering)
