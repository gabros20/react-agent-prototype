# 1.1.4 Zero-Shot Chain-of-Thought (Zero-Shot CoT)

## TL;DR

Zero-Shot CoT is a remarkably simple technique—just adding "Let's think step by step" to any prompt improves reasoning accuracy by 20-50% on complex tasks without requiring examples; it's the most cost-effective upgrade to standard prompting, adding only 4 tokens for significant accuracy gains.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [1.1.3 Chain-of-Thought](./1.1.3-chain-of-thought.md)
- **Grounded In**: Kojima et al. (2022), Plan-and-Solve Research (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: Few-Shot Examples Aren't Always Available](#the-problem-few-shot-examples-arent-always-available)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Zero-Shot CoT** unlocks reasoning abilities already present in LLMs with a single phrase—no examples needed.

**Key Paper**: "Large Language Models are Zero-Shot Reasoners" (Kojima et al., 2022)

**The Discovery**:

```
Standard Zero-Shot:
Q: Roger has 5 tennis balls. He buys 2 cans of 3 balls each. How many now?
A: [model guesses or pattern-matches]

Zero-Shot CoT:
Q: Roger has 5 tennis balls. He buys 2 cans of 3 balls each. How many now?
A: Let's think step by step.
→ Roger started with 5. 2 cans × 3 = 6 balls. 5 + 6 = 11.
→ The answer is 11. ✅
```

**Impact on Benchmarks** (GPT-3):

| Task | Zero-Shot | Zero-Shot CoT | Improvement |
|------|-----------|---------------|-------------|
| **MultiArith** | 17.7% | 78.7% | +345% |
| **GSM8K** | 10.4% | 40.7% | +291% |
| **SVAMP** | 63.7% | 71.8% | +13% |
| **AQuA** | 24.4% | 38.0% | +56% |

**Date Verified**: 2025-12-03

## The Problem: Few-Shot Examples Aren't Always Available

### The Classic Challenge

Few-shot CoT requires crafting examples with reasoning—time-consuming and sometimes impossible:

- No time to create high-quality examples
- Questions vary too much for fixed examples
- Token budget is tight (context window full)
- Need immediate improvement without setup

**Problems**:

- ❌ **Example creation burden**: Manual effort for each task type
- ❌ **Token overhead**: Each example costs ~100-200 tokens
- ❌ **Limited applicability**: Fixed examples don't match all inputs
- ❌ **Quick prototyping**: Can't iterate fast with example requirements

### Why Zero-Shot CoT Matters

**Solution**: 4 tokens ("Let's think step by step") unlock reasoning without examples:

```
Token overhead comparison:
Few-Shot CoT: 500-1000 tokens (examples)
Zero-Shot CoT: 4 tokens (magic phrase)

→ 99% reduction in prompt overhead
→ Similar accuracy improvement
```

## Core Concept

### Why It Works

**Theory** (from research):

1. LLMs are trained on internet text containing step-by-step solutions
2. "Let's think step by step" activates these learned patterns
3. Model enters "reasoning mode" instead of "answer mode"
4. Each step becomes prompt for next step (chain)
5. Errors can self-correct mid-reasoning

**Analogy**: Telling someone "talk me through it" versus "give me the answer now"—the process changes.

### Two-Stage Process

Zero-Shot CoT actually works in two stages:

**Stage 1: Reasoning Extraction**
```
Q: [question]
A: Let's think step by step.
→ [model generates reasoning steps]
```

**Stage 2: Answer Extraction**
```
Q: [question]
A: [reasoning from stage 1]
Therefore, the answer is:
→ [model extracts final answer]
```

**Modern models** (GPT-4o, Claude) often combine both stages automatically.

### The Magic Phrases (Ranked)

Kojima et al. tested dozens of phrases:

1. **"Let's think step by step"** (most effective)
2. **"Let's work this out step by step"** (similar)
3. **"Let's break this down"** (good)
4. **"Let's solve this systematically"** (good)
5. **"First, let's consider..."** (guided start)

**What doesn't work**:
- ❌ "Think about it" (too vague)
- ❌ "Be careful" (no action)
- ❌ "Answer quickly" (discourages reasoning)

## Implementation Patterns

### Pattern 1: Basic Zero-Shot CoT

**Use Case**: Quick reasoning upgrade for any task

```typescript
async function zeroShotCoT(question: string) {
  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `${question}\n\nLet's think step by step.`
  })
}
```

**Pros**:
- ✅ Minimal implementation
- ✅ Works on diverse tasks

**Cons**:
- ❌ May be verbose
- ❌ Less consistent than few-shot

### Pattern 2: Two-Stage Implementation

**Use Case**: Reliable answer extraction

```typescript
async function twoStageCoT(question: string) {
  // Stage 1: Get reasoning
  const reasoning = await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `Q: ${question}\nA: Let's think step by step.`
  })

  // Stage 2: Extract answer
  const answer = await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `Q: ${question}\nA: ${reasoning.text}\n\nTherefore, the answer is:`
  })

  return { reasoning: reasoning.text, answer: answer.text }
}
```

**Pros**:
- ✅ Clean separation of reasoning and answer
- ✅ More reliable extraction

**Cons**:
- ❌ Two API calls (cost/latency)

### Pattern 3: Plan-and-Solve (PS+)

**Use Case**: Complex problems requiring structured approach

```
Q: [question]

A: Let's devise a plan and solve this step by step:
1. Extract relevant information
2. Devise a calculation plan
3. Execute the plan with accurate calculations
4. Verify the result
```

**Results** (Arithmetic Reasoning):

| Method | Accuracy |
|--------|----------|
| Standard Zero-Shot | 70.4% |
| Zero-Shot CoT | 73.2% |
| Plan-and-Solve (PS) | 75.1% |
| Plan-and-Solve+ (PS+) | 76.7% |

```typescript
async function planAndSolve(problem: string) {
  const prompt = `${problem}

Let's devise a plan and solve this step by step:
1. Extract the relevant information from the problem
2. Devise a calculation plan
3. Execute the plan with careful calculations
4. Verify the result makes sense

Plan and solution:`

  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt
  })
}
```

### Pattern 4: Chain of Draft (Concise CoT)

**Use Case**: Token-constrained scenarios

Standard CoT can be verbose (150+ tokens). Chain of Draft forces conciseness:

```
Standard CoT:
"First, we have the equation 3x + 7 = 22.
To isolate the term with x, we need to subtract 7 from both sides..."
[150 tokens]

Chain of Draft:
3x + 7 = 22
3x = 15  (subtract 7)
x = 5    (divide by 3)
[15 tokens, 90% reduction]
```

**Prompt**:
```
Solve this problem with concise reasoning.
Write only essential steps without explanation.

Q: [problem]
A: Let's solve concisely.
```

**Benefit**: 7.6% of standard CoT tokens, same accuracy.

## When to Use This Pattern

### ✅ Perfect For

1. **Quick reasoning enhancement** - No time for examples
2. **Diverse question types** - Questions vary too much for fixed examples
3. **Token-constrained** - Context window nearly full
4. **Arithmetic & math** - "Let's think step by step" dramatically helps
5. **Common sense reasoning** - Cause-effect chains
6. **Logical puzzles** - Syllogisms, deduction

### ❌ Skip For

1. **Simple lookups** - "What is the capital of France?"
2. **Creative tasks** - "Write a poem about sunset"
3. **When you have good few-shot examples** - Few-shot CoT > Zero-shot CoT
4. **Speed-critical** - Zero-shot CoT adds latency (more output tokens)

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| No time for examples | Basic Zero-Shot CoT |
| Complex multi-step | Plan-and-Solve (PS+) |
| Token-constrained | Chain of Draft |
| Need consistent output | Two-stage implementation |
| Have good examples | Use Few-Shot CoT instead |

## Production Best Practices

### 1. Combine with Structured Output

```typescript
import { z } from 'zod'

const schema = z.object({
  reasoning_steps: z.array(z.string()),
  final_answer: z.string(),
  confidence: z.number().min(0).max(1)
})

const result = await generateObject({
  model: openai("gpt-4o-mini", { structuredOutputs: true }),
  schema,
  prompt: `${question}\n\nLet's think step by step and provide structured output.`
})
```

### 2. Handle Model Skipping Reasoning

If model jumps to answer despite trigger:

```
❌ Weak: "Let's think step by step"

✅ Strong: "Before answering, write out your complete reasoning process.
   Show all intermediate steps. Do not skip any steps."

✅ Structured:
   "Let's solve this in exactly 3 steps:
   Step 1: [what to do]
   Step 2: [what to do]
   Step 3: [final calculation]"
```

### 3. Cost-Benefit Analysis

**Scenario**: 10k queries on math problems

| Approach | Tokens | Cost | Accuracy |
|----------|--------|------|----------|
| Direct | 50 | $0.30 | 70% |
| Zero-Shot CoT | 200 | $1.20 | 85% |

**Analysis**:
- 4x higher cost
- 15% absolute accuracy gain (21% relative)
- Worth it for: High-stakes, exams, critical calculations
- Not worth it for: High-volume, low-stakes queries

### Common Pitfalls

**❌ Model still jumps to answer**: Use stronger instruction or structured template

**❌ Reasoning is circular**: Add numbered steps with explicit structure

**❌ Too verbose**: Use Chain of Draft or set `maxTokens`

**❌ Wrong answer despite good reasoning**: Use two-stage extraction

## Key Takeaways

1. **4-token upgrade** - "Let's think step by step" unlocks reasoning
2. **20-50% accuracy gains** - On math, logic, common sense tasks
3. **No examples needed** - Zero overhead for quick prototyping
4. **Plan-and-Solve** - +3-4% over basic zero-shot CoT
5. **Chain of Draft** - 90% fewer tokens, same accuracy

**Quick Implementation Checklist**:

- [ ] Is the problem reasoning-intensive?
- [ ] Have you tried the basic magic phrase first?
- [ ] For complex problems, use Plan-and-Solve?
- [ ] Token-constrained? Use Chain of Draft
- [ ] Is final answer clearly extracted?

## References

1. **Kojima et al.** (2022). "Large Language Models are Zero-Shot Reasoners". _NeurIPS 2022_. https://arxiv.org/abs/2205.11916
2. **Wang et al.** (2023). "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning". https://arxiv.org/abs/2305.04091
3. **Xu et al.** (2024). "Chain of Draft: Thinking Faster by Writing Less". Research Paper.
4. **Wei et al.** (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". https://arxiv.org/abs/2201.11903
5. **Google Research** (2024). "Zero-Shot Reasoning in Language Models". Research Blog.

**Related Topics**:

- [1.1.3 Chain-of-Thought Prompting](./1.1.3-chain-of-thought.md)
- [1.1.5 Self-Consistency](./1.1.5-self-consistency.md)
- [3.1.2 ReAct Pattern](../3-agents/3.1.2-react-pattern.md)

**Layer Index**: [Layer 1: Prompt Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-1-prompt-engineering)
