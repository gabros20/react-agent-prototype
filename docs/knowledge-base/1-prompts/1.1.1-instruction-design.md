# 1.1.1 Instruction Design: Writing Clear and Effective Prompts

## Overview

Instruction design is the foundation of prompt engineering—the art of crafting clear, specific, and well-structured prompts that guide LLMs to produce desired outputs. While LLMs are powerful, they're only as good as the instructions they receive. This guide covers principles, techniques, and anti-patterns for designing effective prompts.

**Core Principle**: Clarity over cleverness. Specific instructions yield better results than vague requests.

## The CLEAR Framework

Developed by Dr. Leo Lo, CLEAR provides structure for effective prompts:

### C - Concise
Use simple language, focus on essentials, omit irrelevant details.

**Bad**:
```
So like, I was thinking maybe you could help me with something... 
if you have time... I need to analyze some data but I'm not sure 
exactly what I need... it's complicated...
```

**Good**:
```
Analyze the sales data from Q4 2024 and identify the top 3 
performing products by revenue.
```

### L - Logical
Structure prompts in a coherent sequence, follow natural flow.

**Bad** (random order):
```
Write a blog post. Make it 500 words. About climate change. 
For teenagers. Don't use jargon.
```

**Good** (logical structure):
```
Write a 500-word blog post about climate change for teenagers.
Use simple language without scientific jargon. Structure: 
1) Hook with relatable example
2) Explain the problem
3) Show impact on their generation
4) Call to action
```

### E - Explicit
Provide specific instructions about format, tone, length, and constraints.

**Bad** (vague):
```
Summarize this article.
```

**Good** (explicit):
```
Summarize this article in 3 bullet points, each under 20 words.
Focus on key findings, not methodology. Use neutral tone.
```

### A - Adaptive
Refine prompts based on model responses, break down complex tasks.

**Initial**:
```
Create a marketing campaign for our new product.
```

**Adaptive refinement**:
```
Create an email marketing campaign for our eco-friendly water bottle.
Target audience: 25-35 year old urban professionals.
Campaign includes: 
1) Subject lines (5 options)
2) Email body (250 words)
3) Call-to-action button text
Tone: Conversational but professional. Emphasize sustainability.
```

### R - Reflective
Assess accuracy of outputs, continuously improve prompt crafting.

**Workflow**:
1. Write initial prompt
2. Evaluate output quality
3. Identify gaps or errors
4. Refine prompt with more specificity
5. Repeat until optimal

## Core Principles

### 1. Be Specific

**Principle**: Vague prompts produce vague results. Specificity guides the model.

**Examples**:

❌ **Vague**: "Tell me about dogs"
- Model doesn't know: history, breeds, care, behavior, health?
- Result: Generic, unfocused response

✅ **Specific**: "Explain the top 5 health considerations when adopting a senior dog (age 8+). Include costs and time commitments."
- Clear scope: health, senior dogs, 5 considerations
- Explicit requirements: costs, time
- Result: Focused, actionable advice

**Practice**:
```
Bad:  "Write code for a website"
Good: "Write HTML/CSS for a responsive navigation bar with logo,
      4 menu items, and mobile hamburger menu. Include hover effects."

Bad:  "Explain machine learning"
Good: "Explain supervised learning to a non-technical manager in 3 
      paragraphs, using business examples like email spam filtering."
```

### 2. Provide Context

**Principle**: LLMs perform better when they understand the situation, audience, and purpose.

**Context Elements**:
- **Role/Persona**: "You are a pediatrician..."
- **Audience**: "Explain to a 10-year-old child..."
- **Purpose**: "For a scientific presentation..."
- **Background**: "Given that our app targets students..."
- **Constraints**: "Within 200 tokens, without technical jargon..."

**Example**:

❌ **No Context**:
```
How do I fix this error?
[error message]
```

✅ **With Context**:
```
I'm debugging a Next.js 14 app. Getting this TypeScript error 
when importing a server component into a client component:

[error message]

My setup: React Server Components, App Router, TypeScript 5.x
What's causing this and how do I resolve it?
```

**Your Codebase Example** (`server/prompts/react.xml`):
```xml
<agent>
You are an autonomous AI assistant using the ReAct pattern.

{{{workingMemory}}}  <!-- Context: Recently accessed resources -->

**CORE LOOP:**
Think → Act → Observe → Repeat until completion
</agent>
```

Context provided:
- Role: "Autonomous AI assistant"
- Framework: "ReAct pattern"
- Working memory: Dynamic context injection
- Expected behavior: Think → Act → Observe loop

### 3. Specify Output Format

**Principle**: Explicitly define how you want the response structured.

**Format Options**:
- Bullet points
- Numbered lists
- Tables
- JSON
- Code blocks
- Paragraphs with headings
- Step-by-step instructions

**Examples**:

❌ **No Format**:
```
Compare GPT-4 and Claude 3.5
```

✅ **With Format**:
```
Compare GPT-4 and Claude 3.5 in a table with these columns:
| Feature | GPT-4 | Claude 3.5 |

Rows: Pricing, Context Window, Coding Ability, Speed, Best Use Cases
```

**JSON Output Example**:
```
Extract product information from this description and return as JSON:

{
  "name": string,
  "price": number,
  "features": string[],
  "availability": boolean
}

Description: [product text]
```

**Your Codebase** uses structured outputs:
```typescript
// orchestrator.ts
model: openai("gpt-4o-mini", { structuredOutputs: true })
```

### 4. Set Constraints and Boundaries

**Principle**: Constraints focus the model and prevent unwanted outputs.

**Types of Constraints**:
- **Length**: "in 3 sentences", "under 500 words"
- **Tone**: "professional", "casual", "empathetic"
- **Scope**: "only discuss X, Y, Z"
- **Exclusions**: "don't include opinions", "avoid jargon"
- **Format**: "markdown only", "no code blocks"
- **Domain**: "stick to factual information", "cite sources"

**Example**:

❌ **No Constraints**:
```
Write a product description for noise-cancelling headphones.
```

✅ **With Constraints**:
```
Write a product description for noise-cancelling headphones.

Constraints:
- Exactly 100 words
- Professional but friendly tone
- Target: commuters and remote workers
- Focus: comfort, battery life, noise reduction
- Exclude: technical specs, pricing
- Format: 2 short paragraphs
```

**Your Codebase Constraints** (`react.xml`):
```xml
**CRITICAL RULES:**
1. **THINK before acting** - Explain your reasoning for each step
2. **EXECUTE immediately** - Don't ask unnecessary clarifying questions
3. **CHAIN operations** - Complete multi-step tasks in one conversation turn
```

### 5. Use Clear Separators

**Principle**: Separate instructions from content using delimiters.

**Recommended Separators**:
- Triple quotes: `"""content"""`
- Triple backticks: ` ```content``` `
- XML-style tags: `<content>...</content>`
- Triple hashes: `### content ###`
- Markdown headers: `## Instructions` vs `## Input Data`

**Example**:

❌ **Unclear Boundaries**:
```
Summarize this article: Climate scientists warn that rising 
temperatures are causing [continues with full article text]
```

✅ **Clear Separation**:
```
Summarize the following article in 3 bullet points:

"""
Climate scientists warn that rising temperatures are causing...
[full article text]
"""

Focus on: main findings, geographic impact, timeline
```

**Your Codebase Example**:
```xml
**AVAILABLE TOOLS:** {{toolCount}} tools

{{toolsFormatted}}  <!-- Clear boundary for tool list -->

**SESSION INFO:**
- Session ID: {{sessionId}}
```

### 6. Define Role and Persona

**Principle**: Assigning a role helps the model adopt appropriate expertise and tone.

**Role Types**:
- **Expert**: "You are a senior software architect..."
- **Educator**: "You are a patient teacher explaining..."
- **Specialist**: "You are a financial advisor..."
- **Assistant**: "You are a helpful research assistant..."
- **Analyst**: "You are a data analyst reviewing..."

**Examples**:

Basic Role:
```
You are a Python expert. Review this code for security vulnerabilities.
```

Detailed Persona:
```
You are a senior DevOps engineer with 10 years of Kubernetes experience.
Review this deployment configuration for a high-traffic production app.
Consider: security, scalability, cost optimization, disaster recovery.
Use industry best practices from CNCF guidelines.
```

**Your Codebase**:
```xml
You are an autonomous AI assistant using the ReAct (Reasoning and Acting) pattern.
```

Clear, focused role that sets expectations for behavior.

## Advanced Techniques

### 1. Task Decomposition

Break complex requests into smaller, manageable steps.

**Example**:

❌ **Monolithic**:
```
Build me a landing page for a SaaS product.
```

✅ **Decomposed**:
```
Task 1: Outline landing page structure
- List 5-7 key sections (hero, features, pricing, etc.)
- Brief description of each section's purpose

Task 2: Write hero section copy
- Headline (under 10 words)
- Subheadline (under 20 words)
- 2 CTA button options

[Continue with each section...]
```

### 2. Conditional Instructions

Provide different paths based on input characteristics.

**Example**:
```
Analyze the following code snippet:

IF the code is Python:
  - Check PEP 8 compliance
  - Suggest type hints

IF the code is TypeScript:
  - Check ESLint rules
  - Suggest interface improvements

IF the code is SQL:
  - Check for injection vulnerabilities
  - Suggest index optimizations

Code:
[code snippet]
```

### 3. Multi-Step Reasoning

Explicitly request step-by-step thinking (preview of Chain-of-Thought).

**Example**:
```
Solve this problem step-by-step:

Problem: A company has 150 employees. 60% work remote, 
25% are in management. What percentage of remote workers 
are managers if 30% of all managers work remote?

Show your work:
1. Calculate number of remote workers
2. Calculate number of managers
3. Calculate number of remote managers
4. Find percentage
```

### 4. Output Verification

Ask the model to verify its own output.

**Example**:
```
Task: Calculate the ROI of this marketing campaign.

After your calculation, verify by:
1. Checking your arithmetic
2. Ensuring all percentages add to 100%
3. Confirming units are consistent
4. Flagging any assumptions made

If you find errors, correct them before final answer.
```

### 5. Example-Driven Design

Provide examples of desired output (leads into Few-Shot Learning).

**Example**:
```
Convert product names to URL slugs using this pattern:

Input: "Premium Noise-Cancelling Headphones"
Output: "premium-noise-cancelling-headphones"

Input: "MacBook Pro 16\" (2024)"
Output: "macbook-pro-16-inch-2024"

Now convert: "Ultra HD 4K Smart TV - 55\""
```

## Common Anti-Patterns

### ❌ 1. The Overly Brief Prompt

**Problem**: Expects model to read your mind.

```
Fix this code.
```

**Why it fails**:
- Which aspect to fix? (bugs, style, performance, security)
- What's the expected behavior?
- What environment/language version?

**Fix**: Be explicit about what's wrong and desired outcome.

### ❌ 2. The Kitchen Sink

**Problem**: Cramming too many requests into one prompt.

```
Write a blog post about AI, then create 5 social media captions,
design an email sequence, outline a webinar, generate FAQs,
write meta descriptions, and create a podcast script.
```

**Why it fails**:
- Model loses focus
- Quality suffers across all outputs
- Can't optimize for any single task

**Fix**: Break into separate, focused prompts.

### ❌ 3. The Assumption Trap

**Problem**: Assuming the model knows your context.

```
Update the docs with the new API changes from last week.
```

**Why it fails**:
- Model doesn't know what "last week" means
- Doesn't have access to your API changes
- Can't update docs it can't see

**Fix**: Provide all necessary information explicitly.

### ❌ 4. The Jargon Overload

**Problem**: Using ambiguous or domain-specific terms without definition.

```
Refactor the monorepo's turborepo config to use the new
remote caching paradigm with DAG optimization for better DX.
```

**Why it fails**:
- Assumes model knows your exact setup
- Jargon can be ambiguous (DX = Developer Experience or Data Exchange?)
- Missing concrete goals

**Fix**: Define terms or use simpler language.

### ❌ 5. The Implicit Expectation

**Problem**: Expecting specific format/style without stating it.

```
Tell me about climate change.
```

**Then frustrated when you get**:
- An essay instead of bullet points
- Scientific detail instead of simple explanation
- 3 paragraphs instead of 1

**Fix**: State format, tone, length, and audience explicitly.

## Prompt Templates

### General Task Template

```
Role: [Who should the model be?]
Context: [Relevant background information]
Task: [Specific action to perform]
Constraints: [Length, format, tone, exclusions]
Output Format: [How to structure the response]

Input:
[Your data/content]

Additional Notes:
[Any special considerations]
```

### Analysis Template

```
Analyze the following [type of content]:

Focus Areas:
1. [Aspect 1]
2. [Aspect 2]
3. [Aspect 3]

For each area, provide:
- Current state
- Issues/concerns
- Recommendations

Content:
"""
[content to analyze]
"""

Format response as a table.
```

### Code Review Template

```
Review this [language] code for:
- Bugs and logic errors
- Security vulnerabilities
- Performance issues
- Best practices compliance

Context:
- Framework: [framework]
- Environment: [production/dev]
- Scale: [expected traffic/data volume]

Code:
```[language]
[code here]
```

Provide:
1. Issues found (severity: high/medium/low)
2. Specific fix recommendations
3. Updated code for critical issues
```

### Content Creation Template

```
Create [type of content] for [target audience].

Parameters:
- Length: [specific length]
- Tone: [professional/casual/friendly/etc.]
- Purpose: [inform/persuade/educate/entertain]
- Key Points: [list of required topics]
- Exclusions: [topics to avoid]
- Format: [structure specifications]

Background:
[relevant context]

Examples of desired style:
"""
[example 1]
"""
```

## Testing Your Prompts

### The 5-Question Test

Before finalizing a prompt, ask:

1. **Can someone else read this and know what I want?**
   - If not, add more context

2. **Is the desired output format clear?**
   - If not, provide examples or structure

3. **Are there any ambiguous terms?**
   - If yes, define or replace them

4. **What could go wrong?**
   - Add constraints to prevent it

5. **Could this be more concise without losing clarity?**
   - Remove unnecessary words

### A/B Testing Prompts

Test variations to find what works best:

**Version A** (Direct):
```
List 5 benefits of meditation.
```

**Version B** (Structured):
```
List 5 benefits of meditation for busy professionals.
For each benefit:
- One-line explanation
- Real-world example
- Time required to see results
```

Compare outputs, measure: relevance, completeness, usability.

## Your Codebase Analysis

Your ReAct prompt (`server/prompts/react.xml`) demonstrates excellent instruction design:

**✅ Strengths**:

1. **Clear Role**: "autonomous AI assistant using the ReAct pattern"
2. **Explicit Rules**: 5 critical rules with examples
3. **Structured Flow**: Think → Act → Observe → Repeat
4. **Concrete Examples**: Full session examples showing desired behavior
5. **Context Injection**: Working memory via `{{{workingMemory}}}`
6. **Format Specification**: "Include 'FINAL_ANSWER:' prefix"
7. **Tool Documentation**: `{{toolsFormatted}}` for available actions
8. **Edge Cases**: Destruction operations, confirmation patterns
9. **Optimization Guidance**: Granular fetching strategies

**Techniques Used**:
- Role definition
- Conditional instructions (IF destructive operation...)
- Step-by-step examples
- Clear separators (XML tags, markdown sections)
- Variable injection (session ID, date, tools)
- Constraints (NEVER auto-confirm deletions)

**Minor Improvements**:
```xml
<!-- Could add explicit token optimization note -->
**TOKEN EFFICIENCY:**
- Prefer granular fetching (saves 40-96% tokens)
- Example: 500 tokens vs 2000 tokens for same task
- Use includeContent only when necessary
```

## Practical Exercises

### Exercise 1: Clarify Vague Prompts

Transform these vague prompts into clear instructions:

1. "Summarize this article"
2. "Help me with Python"
3. "Write an email"
4. "Explain blockchain"
5. "Review my website"

### Exercise 2: Add Constraints

Add 3-5 constraints to make these prompts more focused:

1. "Create a logo for my startup"
2. "Write product descriptions"
3. "Generate test cases for my app"

### Exercise 3: Structure a Complex Prompt

Create a full prompt for: "Build a content marketing strategy for a B2B SaaS company"

Include: role, context, tasks, constraints, format, examples

## Key Takeaways

**Core Principles**:
1. **Clarity over cleverness**: Simple, specific instructions win
2. **Context is king**: Provide role, audience, purpose, constraints
3. **Format matters**: Explicitly define output structure
4. **Iterate and refine**: First draft rarely optimal
5. **Test variations**: A/B test to find best approach

**The CLEAR Framework**:
- **C**oncise: Focus on essentials
- **L**ogical: Coherent structure
- **E**xplicit: Specific requirements
- **A**daptive**: Refine based on results
- **R**eflective**: Assess and improve

**Common Mistakes to Avoid**:
- Vague instructions
- Missing context
- Undefined format
- Too many tasks in one prompt
- Assuming model knows your situation

**Your Codebase**:
- ReAct prompt is excellent example of structured instruction design
- Uses role definition, explicit rules, examples, and constraints
- Implements context injection and variable templates
- Shows production-quality prompt engineering

## Navigation

- [← Previous: Layer 0 Complete](../0-foundations/0.2.4-tradeoffs.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: 1.1.2 Few-Shot Learning](./1.1.2-few-shot.md)

---

*Part of Layer 1: Prompt Engineering - Foundation of effective AI communication*
