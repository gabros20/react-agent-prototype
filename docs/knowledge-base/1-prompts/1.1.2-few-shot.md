# 1.1.2 Few-Shot Learning - Teaching Through Examples

## TL;DR

Few-shot learning teaches LLMs by providing 2-10 input-output examples directly in the prompt, enabling in-context learning without fine-tuning; research shows 5-10 well-chosen examples outperform 20 mediocre ones, and even a single example can improve accuracy by 20-40% on complex tasks.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [1.1.1 Instruction Design](./1.1.1-instruction-design.md)
- **Grounded In**: Brown et al. (2020) GPT-3, Wei et al. (2022), In-Context Learning Research (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: Inconsistent Zero-Shot Outputs](#the-problem-inconsistent-zero-shot-outputs)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Few-shot learning** provides examples in the prompt to guide model behavior—"show, don't tell." Instead of fine-tuning (expensive, slow) or zero-shot (hoping the model understands), few-shot teaches the model by demonstration.

**The Spectrum**:

```
Zero-Shot       One-Shot        Few-Shot        Fine-Tuning
(no examples) → (1 example) → (2-10 examples) → (1000s of examples)
   Fast             ↓              ↓              Slow
   Cheap            ↓              ↓              Expensive
   Less accurate    ↓              ↓              Most accurate
```

**Key Research Findings**:

- **In-context learning**: LLMs learn patterns from examples without parameter updates
- **Quality > Quantity**: 5-10 good examples > 20 mediocre examples (Wei et al., 2022)
- **Single example impact**: Even 1 well-chosen example improves accuracy 20-40%
- **Modern robustness**: GPT-4/Claude are less sensitive to example ordering than GPT-3

**Date Verified**: 2025-12-03

## The Problem: Inconsistent Zero-Shot Outputs

### The Classic Challenge

Zero-shot prompting relies on the model understanding your intent without examples:

```
Prompt: "Extract city and date from travel queries."
Input: "Flying to Tokyo next Wednesday"

❌ Zero-shot outputs might vary:
Run 1: "City: Tokyo, Date: next Wednesday"
Run 2: "Tokyo, Wednesday"
Run 3: {"city": "Tokyo", "date": "Wed"}
```

**Problems**:

- ❌ **Format inconsistency**: Output structure varies between runs
- ❌ **Edge case failures**: Model doesn't know how to handle ambiguous inputs
- ❌ **Style drift**: Tone and structure unpredictable
- ❌ **Missing patterns**: Model doesn't learn your specific conventions

### Why This Matters

Without examples:

- Downstream systems can't parse inconsistent outputs
- Classification tasks have unpredictable category labels
- Code generation doesn't follow your patterns
- Production reliability suffers

## Core Concept

### How In-Context Learning Works

LLMs learn patterns from examples provided in the same prompt—no training required:

1. You provide examples in the prompt
2. Model identifies patterns from examples
3. Model applies learned pattern to new input
4. No parameter updates or fine-tuning

**Example**:

```
Translate English to French:

English: Hello, how are you?
French: Bonjour, comment allez-vous?

English: I love programming.
French: J'aime la programmation.

English: Where is the library?
French: [model generates: Où est la bibliothèque?]
```

The model learns: task type (translation), source language, target language, format (question→answer pairs).

### Example Selection Criteria

#### 1. Diversity

Cover different patterns in your task:

```
❌ Redundant (all same category):
cat → mammal, dog → mammal, horse → mammal

✅ Diverse (covers categories):
cat → mammal, eagle → bird, salmon → fish, python → reptile
```

#### 2. Representativeness

Match real inputs the model will see:

```
❌ Artificial: "Help me" → General, "Question" → General

✅ Realistic:
"Can't log in after password reset" → Authentication
"Charged twice for same order" → Billing
"Feature request: dark mode" → Product Feedback
```

#### 3. Difficulty Range

Include easy, medium, and hard examples:

```
Easy:   Q: 5 + 3 = ?  A: 8
Medium: Q: 4 packs × 6 pencils = ?  A: 24 pencils
Hard:   Q: 6 × $3 with 20% off for 5+ = ?  A: $14.40
```

#### 4. Edge Cases

Teach nuance and exceptions:

```
"Thanks for the refund!" → Positive (gratitude, not request)
"I still haven't received my refund?" → Refund Request
"When can I expect my refund to process?" → Refund Inquiry (timeline question)
```

#### 5. Consistent Formatting

Examples must match exact desired output format:

```
❌ Inconsistent:
"Great product!" → positive
"Worst ever" → This is negative sentiment
"It's okay" → NEUTRAL

✅ Consistent:
"Great product!" → Positive
"Worst ever" → Negative
"It's okay" → Neutral
```

## Implementation Patterns

### Pattern 1: Standard Few-Shot

**Use Case**: Classification, extraction, formatting tasks

```
Task: Classify sentiment of movie reviews

Example 1:
Input: "Absolute masterpiece! Best film of the year."
Output: Positive

Example 2:
Input: "Boring, predictable plot. Wasted 2 hours."
Output: Negative

Example 3:
Input: "Some good moments, but overall disappointing."
Output: Mixed

Now classify:
Input: "Incredible performances but slow pacing."
Output:
```

**Pros**:
- ✅ Simple, reliable structure
- ✅ Works for most tasks
- ✅ Easy to iterate on examples

**Cons**:
- ❌ Token cost increases with examples
- ❌ Fixed examples may not match all inputs

### Pattern 2: Few-Shot with Reasoning (Hybrid CoT)

**Use Case**: Tasks requiring explanation alongside output

```
Solve math word problems:

Q: If 3 apples cost $6, how much do 5 apples cost?
Reasoning: $6 ÷ 3 apples = $2 per apple. 5 × $2 = $10
A: $10

Q: A train travels 120 miles in 2 hours. How far in 5 hours?
Reasoning: 120 ÷ 2 = 60 mph. 60 × 5 = 300 miles
A: 300 miles

Q: [new problem]
Reasoning:
A:
```

**Pros**:
- ✅ Teaches reasoning pattern, not just output
- ✅ More robust on complex problems

**Cons**:
- ❌ Higher token usage per example

### Pattern 3: Dynamic Few-Shot with RAG

**Use Case**: Variable input types requiring relevant examples

```typescript
async function buildFewShotPrompt(task: string, numExamples = 5) {
  // Search for similar examples from vector database
  const similarExamples = await vectorIndex.search({
    query: task,
    limit: numExamples
  })

  // Build prompt with retrieved examples
  const examplesText = similarExamples
    .map(ex => `Input: ${ex.input}\nOutput: ${ex.output}`)
    .join('\n\n')

  return `${examplesText}\n\nNow process:\nInput: ${task}\nOutput:`
}
```

**Pros**:
- ✅ Always uses most relevant examples
- ✅ Scales to large example libraries
- ✅ Adapts to input automatically

**Cons**:
- ❌ Requires vector database infrastructure
- ❌ Additional latency for retrieval

### Pattern 4: Contrastive Examples

**Use Case**: Clarifying nuances with good/bad pairs

```
Summarize articles (good vs bad examples):

Article: [long text about climate change]

❌ Bad Summary: "The article talks about climate stuff and mentions some statistics."

✅ Good Summary: "Global temperatures have risen 1.1°C since pre-industrial times,
primarily due to fossil fuel emissions. Scientists warn of irreversible damage beyond
1.5°C warming, urging immediate policy action."

Now summarize this article:
[new article]
```

**Pros**:
- ✅ Clarifies what NOT to do
- ✅ Teaches quality standards

**Cons**:
- ❌ Uses more tokens for contrast

## When to Use This Pattern

### ✅ Use Few-Shot When:

1. **Format/style requirements** - Output must match specific structure
2. **Zero-shot results are inconsistent** - Need reliability across runs
3. **Custom classification** - Non-standard categories or labels
4. **Niche domain tasks** - Model lacks training data for your use case
5. **Tone/voice matching** - Must follow brand guidelines

### ❌ Skip Few-Shot When:

1. **Simple, common tasks** - Summarization, basic Q&A work zero-shot
2. **Zero-shot already works** - Don't add unnecessary tokens
3. **Examples are hard to create** - Dynamic content, subjective outputs
4. **Token budget is very limited** - Each example costs tokens
5. **Speed is critical** - Examples add latency

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| Consistent formatting needed | 3-5 standard examples |
| Complex reasoning task | Few-shot + CoT hybrid |
| Diverse input types | Dynamic RAG-based examples |
| Clarifying quality standards | Contrastive examples |
| Token-constrained | 1-2 high-quality examples |

## Production Best Practices

### 1. Example Ordering Strategies

**Modern LLMs** (GPT-4, Claude 3+) are robust to random ordering.

| Strategy | Best For |
|----------|----------|
| **Simple → Complex** | Teaching new concepts |
| **Typical → Edge Cases** | Classification, handling exceptions |
| **Chronological** | Time-series, historical analysis |
| **Random** | When no clear progression exists |
| **Similarity-Based** | Dynamic retrieval systems |

### 2. Token Optimization

```
❌ Verbose (200 tokens × 10 examples = 2000 tokens)
✅ Concise (50 tokens × 5 examples = 250 tokens)

→ 87.5% reduction, similar accuracy
```

**Strategies**:

- Reduce to 3-5 examples (often enough)
- Trim unnecessary context in examples
- Use dynamic selection (RAG) to pick most relevant
- For 100+ examples, consider fine-tuning instead

### 3. Prevent Verbatim Copying

If model copies examples directly:

```
Examples:
[examples]

Important: Use these examples as a guide, but generate original responses
for the new input. Do not copy examples directly.

New input: [task]
```

### 4. Explicit Format Enforcement

Add format instruction after examples:

```
[Examples]

Important: Always output in this exact format:
Sentiment: [Positive/Negative/Neutral]
Confidence: [0-100]%
```

### Common Pitfalls

**❌ Too many examples**: Diminishing returns after 5-10, increases cost

**❌ Inconsistent example format**: Mixed formats confuse the model

**❌ Examples don't match real inputs**: Artificial examples hurt generalization

**❌ Conflicting instructions**: If instructions say one thing but examples show another

## Key Takeaways

1. **Show, don't tell** - Examples are more powerful than instructions alone
2. **Quality over quantity** - 5 good examples > 20 mediocre
3. **Diversity matters** - Cover different patterns, edge cases, difficulty levels
4. **Consistency is critical** - Examples must match exact desired format
5. **Dynamic selection** - RAG-based retrieval gives best results for varied inputs
6. **Modern robustness** - GPT-4/Claude less sensitive to example ordering

**Quick Implementation Checklist**:

- [ ] Are examples diverse (covering different patterns)?
- [ ] Do examples match real-world inputs?
- [ ] Is formatting consistent across all examples?
- [ ] Are edge cases included?
- [ ] Is token budget reasonable (3-5 examples)?
- [ ] Have you tested with zero-shot first (baseline)?

## References

1. **Brown et al.** (2020). "Language Models are Few-Shot Learners". _NeurIPS 2020_. https://arxiv.org/abs/2005.14165
2. **Wei et al.** (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". _NeurIPS 2022_. https://arxiv.org/abs/2201.11903
3. **Liu et al.** (2024). "What Makes Good In-Context Examples for GPT-3?". _DeepMind Research_.
4. **Anthropic** (2025). "Prompt Engineering: Few-Shot Examples". Documentation.
5. **OpenAI** (2025). "Best Practices for GPT Prompts". https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api
6. **Google DeepMind** (2024). "In-Context Learning in Large Language Models". Research Blog.

**Related Topics**:

- [1.1.1 Instruction Design](./1.1.1-instruction-design.md)
- [1.1.3 Chain-of-Thought Prompting](./1.1.3-chain-of-thought.md)
- [5.1.1 Embedding Documents for RAG](../5-rag/5.1.1-embedding-documents.md)

**Layer Index**: [Layer 1: Prompt Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-1-prompt-engineering)
