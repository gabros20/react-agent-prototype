# 1.1.5 Self-Consistency - Improving Reliability Through Voting

## TL;DR

Self-consistency improves answer reliability by generating 3-10 reasoning paths for the same question and selecting the most common answer through majority voting; it achieves +7-18% accuracy on math benchmarks at 3-10x cost, making it ideal for high-stakes decisions where error cost exceeds compute cost.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [1.1.3 Chain-of-Thought](./1.1.3-chain-of-thought.md), [1.1.4 Zero-Shot CoT](./1.1.4-zero-shot-cot.md)
- **Grounded In**: Wang et al. (2022), Universal Self-Consistency Research (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: Single Reasoning Paths Can Fail](#the-problem-single-reasoning-paths-can-fail)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Self-consistency** generates multiple reasoning paths and selects the most common answer through voting—"asking a panel of experts" where the model generates diverse solutions and picks the consensus.

**Key Paper**: "Self-Consistency Improves Chain of Thought Reasoning in Language Models" (Wang et al., 2022)

**Impact on Benchmarks**:

| Benchmark | Single Path CoT | Self-Consistency (5) | Improvement |
|-----------|-----------------|---------------------|-------------|
| **GSM8K** (Math) | 57% | 75% | +18% |
| **SVAMP** (Math) | 69% | 80% | +11% |
| **AQuA** (Math) | 38% | 50% | +12% |
| **StrategyQA** (Logic) | 67% | 74% | +7% |

**Trade-off**: 3-10x cost/latency for significantly better reliability.

**Date Verified**: 2025-12-03

## The Problem: Single Reasoning Paths Can Fail

### The Classic Challenge

Even with Chain-of-Thought, a single reasoning path can have errors:

```
Q: Trees: 15 originally, planted some, now 21. How many planted?

Single Path (might have error):
"Originally 15 trees. Now 21. That's 21 + 15 = 36 planted." ❌

What if we generate multiple paths?
Path 1: 21 - 15 = 6 ✅
Path 2: 21 - 15 = 6 ✅
Path 3: 21 + 15 = 36 ❌ (error)
Path 4: 21 - 15 = 6 ✅
Path 5: 15 + 6 = 21, so 6 planted ✅

Vote: 6 appears 4/5 times → Answer: 6 (high confidence)
```

**Problems**:

- ❌ **Single point of failure**: One reasoning path can have random errors
- ❌ **No confidence signal**: Can't tell if answer is reliable
- ❌ **Ambiguous problems**: Different valid interpretations

### Why This Matters

**The Wisdom of Crowds Effect**:

- Correct reasoning is consistent across diverse paths
- Errors are random and don't align
- Majority vote filters out noise

## Core Concept

### Mechanism: Marginalizing Over Reasoning Paths

**Mathematical Intuition**:
```
Traditional: P(answer | question, single_path)
Self-Consistency: P(answer | question) = Σ P(answer | path_i) × P(path_i)
```

**Plain English**:
- Each reasoning path might have errors
- Correct reasoning tends to be consistent
- Errors are random and distributed
- Majority vote converges on correct answer

### The Process

```
Question
    ↓
Generate Path 1 → Answer A
Generate Path 2 → Answer A
Generate Path 3 → Answer B (outlier)
Generate Path 4 → Answer A
Generate Path 5 → Answer A
    ↓
Vote: A = 4, B = 1
    ↓
Final: A (80% confidence)
```

## Implementation Patterns

### Pattern 1: Basic Self-Consistency

**Use Case**: Reliable answers for verifiable tasks

```typescript
async function selfConsistency(
  question: string,
  paths: number = 5
): Promise<{ answer: string; confidence: number }> {

  // Step 1: Generate multiple reasoning paths
  const results = await Promise.all(
    Array(paths).fill(null).map(() =>
      generateText({
        model: openai("gpt-4o-mini"),
        prompt: `${question}\n\nLet's think step by step.`,
        temperature: 0.7  // Diversity via sampling
      })
    )
  )

  // Step 2: Extract answers
  const answers = results.map(r => extractAnswer(r.text))

  // Step 3: Count votes
  const votes = countVotes(answers)
  const winner = votes[0]

  return {
    answer: winner.value,
    confidence: winner.count / paths
  }
}

function extractAnswer(reasoning: string): string {
  const match = reasoning.match(/(?:answer is|equals?|=)\s*(\d+)/i)
  return match ? match[1] : reasoning.split('\n').pop()?.trim() || ''
}

function countVotes(answers: string[]): Array<{value: string; count: number}> {
  const counts = new Map<string, number>()
  answers.forEach(a => counts.set(a, (counts.get(a) || 0) + 1))

  return Array.from(counts.entries())
    .map(([value, count]) => ({ value, count }))
    .sort((a, b) => b.count - a.count)
}
```

**Pros**:
- ✅ Simple majority voting
- ✅ Works for numeric/categorical answers

**Cons**:
- ❌ Requires clean answer extraction
- ❌ Doesn't work for free-form text

### Pattern 2: Weighted Voting

**Use Case**: Quality-weighted consensus

```typescript
function scoreReasoning(text: string): number {
  let score = 1.0

  // Bonus for showing calculations
  if (/\d+\s*[+\-×÷]\s*\d+\s*=\s*\d+/.test(text)) score += 0.5

  // Bonus for verification step
  if (/verify|check|confirm/i.test(text)) score += 0.3

  // Penalty for very short reasoning
  if (text.length < 100) score -= 0.3

  // Bonus for structured format
  if (/Step \d+:|First,|Then,|Finally,/.test(text)) score += 0.2

  return Math.max(0.1, score)
}

async function weightedSelfConsistency(question: string, paths = 5) {
  const results = await generatePaths(question, paths)

  const scored = results.map(r => ({
    answer: extractAnswer(r.text),
    score: scoreReasoning(r.text)
  }))

  // Weighted vote
  const votes = new Map<string, number>()
  scored.forEach(s => {
    votes.set(s.answer, (votes.get(s.answer) || 0) + s.score)
  })

  const winner = Array.from(votes.entries())
    .sort((a, b) => b[1] - a[1])[0]

  return { answer: winner[0], weightedScore: winner[1] }
}
```

**Pros**:
- ✅ Higher quality paths count more
- ✅ More robust to low-quality outliers

**Cons**:
- ❌ Scoring heuristics need tuning

### Pattern 3: Universal Self-Consistency (USC)

**Use Case**: Free-form answers where voting is ambiguous

```typescript
async function universalSelfConsistency(question: string, paths = 5) {
  // Step 1: Generate paths
  const results = await generatePaths(question, paths)

  // Step 2: Ask model to select best
  const candidates = results.map((r, i) =>
    `Option ${i + 1}:\nReasoning: ${r.text}\nAnswer: ${extractAnswer(r.text)}`
  ).join('\n\n')

  const selection = await generateText({
    model: openai("gpt-4o-mini"),
    prompt: `Given these ${paths} solutions to the same problem,
select the most accurate answer and explain why.

${candidates}

Which option is most correct and why?`
  })

  return { selected: selection.text, allPaths: results }
}
```

**Pros**:
- ✅ Works for any answer type
- ✅ Model explains selection

**Cons**:
- ❌ Extra API call for selection
- ❌ Model might pick wrong option

### Pattern 4: Multi-Model Ensemble

**Use Case**: Maximum reliability through model diversity

```typescript
async function multiModelConsistency(question: string) {
  const prompt = `${question}\n\nLet's think step by step.`

  const results = await Promise.all([
    generateText({ model: openai("gpt-4o-mini"), prompt }),
    generateText({ model: openai("gpt-4o-mini"), prompt }),
    generateText({ model: anthropic("claude-3-5-haiku"), prompt }),
    generateText({ model: anthropic("claude-3-5-haiku"), prompt }),
    generateText({ model: google("gemini-1.5-flash"), prompt }),
  ])

  return majorityVote(results.map(r => extractAnswer(r.text)))
}
```

**Pros**:
- ✅ Different model biases cancel out
- ✅ Most robust consensus

**Cons**:
- ❌ Very expensive
- ❌ Complex setup

## When to Use This Pattern

### ✅ Best For

1. **High-stakes decisions** - Medical, legal, financial where errors are costly
2. **Ambiguous problems** - Multiple valid interpretations
3. **Verifiable answers** - Math, code output, classification
4. **Reducing hallucinations** - Cross-checking facts

**Cost-Benefit Example**:
```
Medical diagnosis (1000 queries/month):
Without SC: $0.12/mo, 85% accuracy, 150 errors
With SC:    $0.48/mo, 95% accuracy, 50 errors

Additional cost: $0.36/mo
Errors prevented: 100
Cost per error prevented: $0.0036

→ Worth it for high-stakes applications
```

### ❌ Skip For

1. **Simple, deterministic tasks** - "What is 2 + 2?"
2. **Creative/subjective outputs** - "Write a poem"
3. **Budget-constrained** - When 3-10x cost is unacceptable
4. **Speed-critical** - Real-time, low-latency requirements

### Diminishing Returns

| Paths | Accuracy | Cost | Improvement per Path |
|-------|----------|------|---------------------|
| 1 | 57% | 1x | baseline |
| 3 | 70% | 3x | +6.5% each |
| 5 | 75% | 5x | +2.5% each |
| 10 | 77% | 10x | +0.4% each |
| 20 | 78% | 20x | +0.1% each |

**Sweet spot**: 5-7 paths for most applications.

## Production Best Practices

### 1. Temperature Tuning

```typescript
// Optimal temperature for diversity
const temperatures = [0.5, 0.7, 0.9, 0.7, 0.5]

const results = await Promise.all(
  temperatures.map(temp =>
    generateText({
      model: openai("gpt-4o-mini"),
      prompt,
      temperature: temp
    })
  )
)
```

**Research finding**: Temperature 0.7 gives best diversity/quality balance.

### 2. Confidence Thresholds

```typescript
const result = await selfConsistency(question, 5)

if (result.confidence >= 0.6) {  // 3/5 agreement
  return result.answer
} else {
  return "Uncertain - manual review needed"
}
```

### 3. Parallel vs Sequential

**Sequential** (lower cost, higher latency):
```
Path 1: 2s → Path 2: 2s → ... → Total: 10s
```

**Parallel** (same cost, lower latency):
```
All paths: 2s (parallelized) → Total: 2s
```

Choose based on requirements and budget.

### Common Pitfalls

**❌ Answer extraction fails**: Use structured outputs or USC

**❌ All paths make same error**: Self-consistency can't fix systematic bias

**❌ Too few paths**: 3 minimum, 5-7 recommended

**❌ Same temperature**: Vary temperature for diversity

## Key Takeaways

1. **Wisdom of crowds** - Multiple paths filter random errors
2. **+7-18% accuracy** - Significant gains on reasoning tasks
3. **3-10x cost** - Worth it when error cost > compute cost
4. **Sweet spot: 5-7 paths** - Diminishing returns after that
5. **Use for high-stakes** - Medical, legal, financial, exams

**Quick Implementation Checklist**:

- [ ] Is error cost high enough to justify 3-10x compute?
- [ ] Are answers extractable for voting?
- [ ] Using temperature 0.7 for diversity?
- [ ] Have you set a confidence threshold?
- [ ] Considered USC for free-form answers?

## References

1. **Wang et al.** (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models". https://arxiv.org/abs/2203.11171
2. **Chen et al.** (2024). "Universal Self-Consistency for Large Language Model Generation". https://arxiv.org/abs/2311.17311
3. **Wei et al.** (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". https://arxiv.org/abs/2201.11903
4. **Google Research** (2024). "Ensemble Methods for Language Models". Research Blog.
5. **Anthropic** (2025). "Reliability Techniques for Claude". Documentation.

**Related Topics**:

- [1.1.3 Chain-of-Thought Prompting](./1.1.3-chain-of-thought.md)
- [1.1.4 Zero-Shot CoT](./1.1.4-zero-shot-cot.md)
- [3.2.2 Multi-Agent Architectures](../3-agents/3.2.2-multi-agent.md)

**Layer Index**: [Layer 1: Prompt Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-1-prompt-engineering)
