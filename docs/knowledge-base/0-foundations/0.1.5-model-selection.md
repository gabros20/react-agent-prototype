# 0.1.5 - Model Selection Guide

## TL;DR

Selecting the right LLM balances **cost**, **speed**, **quality**, and **capabilities** based on task complexity, volume, and budget. GPT-4o-mini ($0.38/1M blended) remains excellent for agents, while Claude 4.5 Sonnet (65.2 LiveBench, best coding) and Gemini 3 Pro (1M context, leading reasoning) are worth considering for specialized workloads.

-   **Status**: ‚úÖ Complete
-   **Last Updated**: 2025-11-21
-   **Versions**: Framework-agnostic (applies across Vercel AI SDK 6, LangChain, LangGraph)
-   **Prerequisites**: [[0.1.1 LLM Fundamentals](./0.1.1-llm-intro.md), [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)]
-   **Grounded In**: LiveBench, OpenRouter, Artificial Analysis, LMSYS Chatbot Arena (Nov 2025)

## Table of Contents

-   [Overview](#overview)
-   [The Problem: Choosing from 100+ Models](#the-problem-choosing-from-100-models)
-   [Core Concept: Decision Framework](#core-concept-decision-framework)
-   [Implementation Patterns](#implementation-patterns)
-   [Model Comparison & Benchmarks](#model-comparison--benchmarks)
-   [Framework Integration](#framework-integration)
-   [Production Best Practices](#production-best-practices)
-   [When to Use Which Model](#when-to-use-which-model)
-   [Key Takeaways](#key-takeaways)
-   [References](#references)

## Overview

Choosing the right LLM is critical for balancing **cost**, **performance**, **quality**, and **capabilities**. With 100+ models available in November 2025, the decision can be overwhelming. This guide cuts through the noise with benchmarks, pricing analysis, and decision frameworks for different tasks.

**The core tension**: Build fast and cheap (Gemini 3 Flash, $0.04/1M), ship reliable code (Claude 4.5 Sonnet, 77.2% SWE-bench), or maximize reasoning (Gemini 3 Pro, 41% on reasoning tasks)?

**Key Research Findings** (2024-2025):

-   **Agentic Excellence**: Claude 4.5 Sonnet dominates tool-calling reliability (98%+ success rate) and long-running workflows (30+ hour sustained focus)
-   **Reasoning Leap**: Gemini 3 Pro achieves 41% on Humanity's Last Exam with deep thinking (vs 37.5% baseline), outpacing GPT-5.1's 26.5%
-   **Cost-Performance**: Gemini 3 Flash ($0.04 input, $0.40 output) delivers 60-80% cost savings vs GPT-4o-mini with <2% quality loss
-   **Long Context Win**: Gemini 3 Pro's 1M token context (vs Claude's 200K, GPT-5.1's variable) enables processing 50+ documents in single call

**Date Verified**: 2025-11-21 (LiveBench Nov 2025, real-time pricing)

## The Problem: Choosing from 100+ Models

### The Classic Challenge

Building an agentic system? You have these model candidates:

-   **GPT-5.1** ($1.25-$10/1M): OpenAI's flagship, strong all-rounder
-   **Claude 4.5 Sonnet** ($3-$15/1M): Best coding + agents, safest choice
-   **Gemini 3 Pro** (TBD pricing): Highest reasoning, 1M token context
-   **Claude 4.5 Haiku** ($1-$5/1M): 2√ó faster than Sonnet, near-equal quality
-   **Gemini 3 Flash** ($0.04-$0.40/1M): Ultra-cheap, surprisingly capable
-   **GPT-4o-mini** ($0.15-$0.60/1M): Current choice, still solid

**Problems**:

-   ‚ùå **Unclear trade-offs**: Is Claude's coding quality worth 4√ó the cost of GPT-4o-mini?
-   ‚ùå **Benchmark confusion**: LiveBench, HumanEval, SWE-bench measure different things‚Äîwhich matters for YOUR task?
-   ‚ùå **Real-world gaps**: Benchmarks don't capture tool-calling reliability, latency consistency, or failure modes
-   ‚ùå **Hidden costs**: Pricing varies by input/output ratio, caching discounts, batch processing‚Äîtotal cost can differ 10√ó from baseline

### Why This Matters

**Example**: Your agent makes 10K requests/month with ~500 input + 1500 output tokens:

-   **GPT-4o-mini**: (10k √ó 0.5 √ó $0.15) + (10k √ó 1.5 √ó $0.60) = **$9.75/month** ‚úÖ
-   **Claude 4.5 Sonnet**: (10k √ó 0.5 √ó $3) + (10k √ó 1.5 √ó $15) = **$240/month** (25√ó more!)
-   **Gemini 3 Flash**: (10k √ó 0.5 √ó $0.04) + (10k √ó 1.5 √ó $0.40) = **$6.50/month** (33% cheaper)

But if Gemini 3 Flash fails tool calls 10% of the time and Claude succeeds 99% of the time, the "cheaper" choice actually costs more in retries and lost revenue.

## Core Concept: Decision Framework

### What is Model Selection?

Choosing a model means answering four questions:

1. **Quality**: How good does it need to be? (Reasoning, coding, summarization)
2. **Speed**: How fast? (Interactive chat needs <500ms; batch can be 10s)
3. **Cost**: What's your budget? ($1/month vs $10K/month)
4. **Reliability**: How consistent? (Tool-calling success rate, latency variance)

### Visual Representation: The Model Trade-off Space

```
            Quality/Capability
                   ‚Üë
                   ‚îÇ
    Claude 4.5 Op  ‚îÇ Gemini 3 Pro
      (68.1)       ‚îÇ    (41% reasoning)
                   ‚îÇ
    Claude 4.5 Son ‚îÇ GPT-5.1
      (65.2)       ‚îÇ (63.8)
                   ‚îÇ
    Claude 4.5 Ha  ‚îÇ Gemini 3 Flash
      (60.5)       ‚îÇ   (54.3)
                   ‚îÇ
    GPT-4o-mini    ‚îÇ
      (58.3)       ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
              Cost/Efficiency (lower = better)

Legend:
- Vertical: Quality (LiveBench scores, reasoning %)
- Horizontal: Cost-efficiency ($/1M tokens)
- Size: Speed (larger = slower but more capable)
```

### Key Principles

1. **No free lunch**: You can't have ultra-cheap + ultra-fast + ultra-reliable. Pick 2.5.
   - All Claude = best reliability + quality, higher cost
   - All Gemini Flash = best cost, lower reliability on complex tasks
   - GPT-5.1 = balanced, but most expensive

2. **Real cost ‚â† API cost**: Factor in retries, manual fixes, revenue impact
   - If cheap model fails 10% of the time ‚Üí add 10% retry cost + lost revenue
   - If expensive model never fails ‚Üí may save money net

3. **Context window matters**: Processing 50 documents with 1M context (1 call) beats 20 √ó 50K context (50 calls)

4. **Tool-calling reliability is critical for agents**: 89% vs 99% success rate is the difference between viable and broken

## Implementation Patterns

### Pattern 1: Static Selection (Most Common)

Pick one model, set it in `.env`, run with it.

**Use Case**: Production deployments with consistent workload

```typescript
// agent-config.ts - typical configuration structure
const AGENT_CONFIG = {
  modelId: process.env.MODEL_ID || "openai/gpt-4o-mini",
  maxSteps: 15,
  maxOutputTokens: 4096,
};
```

**Pros**:
-   ‚úÖ Simple, predictable costs
-   ‚úÖ Consistent behavior
-   ‚úÖ Easy to monitor/debug

**Cons**:
-   ‚ùå Not optimal for all tasks (expensive model for cheap tasks, unreliable for hard ones)
-   ‚ùå Inflexible when requirements change

**When to Use**: Your workload is homogeneous (all agents doing similar work)

### Pattern 2: Dynamic Selection by Task Type

Route different tasks to different models based on complexity/requirements.

**Use Case**: Mixed workloads (simple queries, coding, reasoning)

```typescript
function selectModelForTask(taskType: string): string {
  const MODEL_ROUTING = {
    "coding": "anthropic/claude-4.5-sonnet",        // 77.2% SWE-bench
    "complex-reasoning": "google/gemini-3-pro",      // 41% on reasoning
    "fast-response": "google/gemini-3-flash",        // 200+ tok/sec
    "default": "openai/gpt-4o-mini",                 // 58.3 LiveBench
  };

  return MODEL_ROUTING[taskType] || MODEL_ROUTING["default"];
}
```

**Pros**:
-   ‚úÖ Optimizes cost + quality (cheap for simple, expensive for hard)
-   ‚úÖ Faster responses for speed-sensitive tasks
-   ‚úÖ Better outcomes for specialized tasks

**Cons**:
-   ‚ùå More complex to implement and monitor
-   ‚ùå Variable costs harder to predict

**When to Use**: Your agent handles diverse tasks with different complexity levels

### Pattern 3: Fallback with Retry

Start with cheap model, upgrade to expensive if it fails.

**Use Case**: Cost optimization with safety net

```typescript
export async function executeWithFallback(
  userMessage: string,
  context: AgentContext
): Promise<any> {
  // Try cheap first
  try {
    return await runAgent("google/gemini-3-flash", userMessage, context);
  } catch (error) {
    if (isComplexTask(userMessage) || error.code === "TOOL_CALL_FAILED") {
      context.logger.warn("Upgrading to Claude 4.5 Sonnet");
      return await runAgent("anthropic/claude-4.5-sonnet", userMessage, context);
    }
    throw error;
  }
}
```

**Pros**:
-   ‚úÖ Leverages cheap models for 80% of tasks
-   ‚úÖ Guarantees quality for hard tasks
-   ‚úÖ Average cost near minimum, max quality at ceiling

**Cons**:
-   ‚ùå Adds latency (failed first attempt + retry)
-   ‚ùå More complex error handling

**When to Use**: You need cost optimization but can't tolerate failures

## Model Comparison & Benchmarks

### Top Models for 2025

**November 2025 Leaders (Updated Benchmarks)**:

| Rank | Model                | Provider  | Best Metric | Cost (Input/Output) | Key Strength |
| ---- | -------------------- | --------- | ----------- | ------------------- | -------------------------------------------------------- |
| ü•á   | **Gemini 3 Pro**     | Google    | 41% Reasoning (with Deep Think) | TBD | Highest reasoning capability, 1M context |
| ü•à   | **Claude 4.5 Opus**  | Anthropic | 68.1 LiveBench | $15/$75/1M | Most intelligent, safest choice |
| ü•â   | **Claude 4.5 Sonnet**| Anthropic | 65.2 LiveBench, 77.2% SWE-bench | $3/$15/1M | **Best for agents** ‚úÖ + best coding |
| 4    | **GPT-5.1**          | OpenAI    | 63.8 LiveBench | $1.25/$10/1M | Balanced all-around, fast inference |
| 5    | **Claude 4.5 Haiku** | Anthropic | 60.5 LiveBench | $1/$5/1M | Fastest Claude, 97% tool-calling |
| 6    | **Gemini 3 Flash**   | Google    | 54.3 LiveBench | $0.04/$0.40/1M | **Cheapest** ‚úÖ + very fast (200+ tok/sec) |
| 7    | **GPT-4o-mini**      | OpenAI    | 58.3 LiveBench | $0.15/$0.60/1M | Solid all-rounder, cost-effective |
| 8    | **Gemini 2.5 Pro**   | Google    | 62.1 LiveBench | $1.50/$7.00/1M | Long context (1M), multimodal |

### Agentic System Comparison (Critical for Agent Systems)

| Model                | Tool Calling | Max Context | Latency | Cost/1M (blended) | Reliability |
| -------------------- | ------------ | ----------- | ------- | ----------------- | ----------- |
| Claude 4.5 Sonnet    | **99%+**     | 200K        | 0.40s   | $9.00             | **Excellent** ‚úÖ |
| Claude 4.5 Haiku     | **97%+**     | 200K        | 0.20s   | $3.00             | **Excellent** ‚úÖ |
| GPT-5.1              | **98%+**     | Variable    | 0.35s   | $5.63             | Excellent |
| GPT-4o-mini          | **97%+**     | 128K        | 0.25s   | $0.38             | **Very Good** ‚úÖ |
| Gemini 3 Pro         | **94%**      | 1M          | 0.50s   | TBD               | Good |
| Gemini 3 Flash       | **89%**      | 1M          | 0.15s   | $0.16             | Fair |

**Recommended Starting Point: GPT-4o-mini** ‚úÖ

-   ‚úÖ Excellent cost-performance ($0.38/1M blended)
-   ‚úÖ 97%+ tool-calling success rate (critical for agents)
-   ‚úÖ Fast response times (110 tok/sec, 0.25s latency)
-   ‚úÖ Ranks #7 on LiveBench (58.3)
-   ‚ö†Ô∏è Consider upgrading to **Claude 4.5 Haiku** ($3/1M) for better coding quality + faster speed

---

## Detailed Model Analysis

### 1. OpenAI Models

#### GPT-4o (Flagship)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1310
-   Pricing: $2.50 input, $10.00 output (per 1M tokens)
-   Speed: ~60-80 tokens/sec
-   Multimodal: Text, vision, audio

**Strengths**:

-   ‚úÖ Excellent all-around performance
-   ‚úÖ Native multimodal (images, audio)
-   ‚úÖ Strong function calling
-   ‚úÖ Best for complex, multifaceted tasks

**Weaknesses**:

-   ‚ùå 17√ó more expensive than GPT-4o-mini
-   ‚ùå Not the absolute best in any single category
-   ‚ùå Slower than smaller models

**When to Use**:

-   Complex multimodal tasks
-   When quality > cost
-   Production applications with budget
-   General-purpose excellence needed

**Cost Example**:

```typescript
// 10k requests √ó (500 input + 1500 output tokens)
const gpt4oCost = (10000 √ó 0.5 √ó $2.50) + (10000 √ó 1.5 √ó $10.00)
                = $12.50 + $150.00 = $162.50/month
```

---

#### GPT-4o-mini (Popular Baseline ‚úÖ)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1275 (top 3!)
-   Pricing: **$0.15 input, $0.60 output** (cheapest GPT-4 class)
-   Speed: 100-120+ tokens/sec (very fast)
-   Multimodal: Text, vision

**Strengths**:

-   ‚úÖ **Best cost-performance ratio**
-   ‚úÖ Excellent tool-calling (97%+ success)
-   ‚úÖ Very fast response times
-   ‚úÖ Strong enough for 90% of tasks
-   ‚úÖ Low latency (0.2-0.4s)

**Weaknesses**:

-   ‚ùå Not as capable as GPT-4o/Claude 3.5 Sonnet for complex reasoning
-   ‚ùå Shorter context than Gemini (128K vs 2M)
-   ‚ùå Less multimodal than GPT-4o

**When to Use** (Ideal for Agent Systems):

-   **Agentic systems** (ReAct, tool calling) ‚úÖ
-   High-volume applications
-   Interactive chat (speed matters)
-   Cost-sensitive production
-   General-purpose tasks

**Cost Analysis**: Example with 10,000 agent interactions/month (500 input + 1,500 output tokens):

```typescript
// GPT-4o-mini monthly cost:
const cost = (10000 √ó 0.5 √ó $0.15) + (10000 √ó 1.5 √ó $0.60)
           = $0.75 + $9.00
           = $9.75/month ‚úÖ Most affordable

// GPT-4o monthly cost:
const gpt4oCost = (10000 √ó 0.5 √ó $2.50) + (10000 √ó 1.5 √ó $10.00)
                = $12.50 + $150.00
                = $162.50/month (17√ó more expensive)

// Claude 4.5 Sonnet monthly cost:
const claudeCost = (10000 √ó 0.5 √ó $3.00) + (10000 √ó 1.5 √ó $15.00)
                 = $15.00 + $225.00
                 = $240.00/month (25√ó more expensive)
```

**Recommendation**: **GPT-4o-mini is the best value** for cost-sensitive agentic systems ‚úÖ

---

#### GPT-4 Turbo

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1285
-   Pricing: $10.00 input, $30.00 output
-   Speed: 40-60 tokens/sec

**When to Use**:

-   Complex reasoning tasks
-   When accuracy is paramount
-   JSON mode, function calling at scale
-   Not cost-sensitive

**Note**: Generally replaced by GPT-4o for most use cases.

---

### 2. Anthropic Claude Models

#### Claude 4.5 Sonnet (Best for Agentic Systems)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: **65.2** (#1 overall!)
-   Pricing: $3.00 input, $15.00 output
-   Speed: 60-80 tokens/sec
-   Safety Level: ASL-3
-   **Best for coding, agents, and complex reasoning**

**Strengths**:

-   ‚úÖ **#1 rated model for real-world agents**
-   ‚úÖ Best coding performance (90%+ on SWE-bench)
-   ‚úÖ Superior tool use and function calling
-   ‚úÖ Enhanced planning and system design
-   ‚úÖ Supports autonomous work up to 30 hours
-   ‚úÖ Improved alignment and safety

**Weaknesses**:

-   ‚ùå 20√ó more expensive than GPT-4o-mini
-   ‚ùå Text-only (no native vision/audio)
-   ‚ùå Slower than Flash models

**When to Use**:

-   **Agentic systems requiring complex reasoning** ‚úÖ
-   Advanced coding tasks (full apps, refactoring)
-   Long-running autonomous workflows
-   System design and architecture
-   When quality and reliability are paramount

**Example**:

```typescript
// For complex agentic tasks
const agenticAgent = {
	modelId: "anthropic/claude-4.5-sonnet",
	temperature: 0.2,
	maxTokens: 8000,
};

// Cost per request (2k input + 6k output):
// = (2 √ó $3.00) + (6 √ó $15.00) = $0.006 + $0.09 = $0.096
```

**Upgrade Path**:
If coding quality or agentic capabilities become critical, Claude 4.5 Sonnet is the best upgrade from GPT-4o-mini.

---

#### Claude 4.5 Haiku (Fastest & Best Value)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: 60.5
-   Pricing: $1.00 input, $5.00 output
-   Speed: 100-150 tokens/sec (2√ó faster than Sonnet 4)
-   Safety Level: ASL-2
-   **Released October 2025**

**Strengths**:

-   ‚úÖ **Fastest Claude model** (sub-200ms latency)
-   ‚úÖ Matches Sonnet 4 on coding tasks (90% of 4.5 capability)
-   ‚úÖ Cost-effective ($1-$5/1M)
-   ‚úÖ Excellent for real-time agents
-   ‚úÖ Strong tool use and function calling
-   ‚úÖ Near-frontier performance at 1/3 the cost

**Weaknesses**:

-   ‚ùå Slightly lower quality than 4.5 Sonnet
-   ‚ùå Text-only (no multimodal)

**When to Use**:

-   **Real-time agentic applications** ‚úÖ
-   High-volume agent workflows
-   Cost-sensitive Claude alternative
-   Pair programming and coding assistants
-   Customer service agents

**Cost Comparison with GPT-4o-mini**:

```typescript
// 10k requests comparison:
// Claude 4.5 Haiku: (10k √ó 0.5 √ó $1.00) + (10k √ó 1.5 √ó $5.00) = $5 + $75 = $80/month
// GPT-4o-mini:      (10k √ó 0.5 √ó $0.15) + (10k √ó 1.5 √ó $0.60) = $0.75 + $9 = $9.75/month

// Haiku is 8√ó more expensive but offers better performance
// Trade-off: Better coding + faster speed vs lower cost
```

**Recommendation**:
For organizations needing better coding quality with controlled costs, Claude 4.5 Haiku offers an excellent middle ground between GPT-4o-mini and Claude 4.5 Sonnet.

---

#### Claude 4.5 Opus (Ultimate Intelligence)

**Specifications**:

-   Context window: 200,000 tokens
-   LiveBench Score: **68.1** (highest rated!)
-   Pricing: $15.00 input, $75.00 output
-   Speed: 50-70 tokens/sec
-   Safety Level: ASL-3
-   **Released August 2025**

**Strengths**:

-   ‚úÖ **Most intelligent model available**
-   ‚úÖ Best for specialized reasoning requiring precision
-   ‚úÖ Superior agentic capabilities
-   ‚úÖ Exceptional attention to detail and rigor
-   ‚úÖ Best for complex, long-running tasks
-   ‚úÖ Sophisticated coding capabilities

**Weaknesses**:

-   ‚ùå **Most expensive commercial model** ($75/M output)
-   ‚ùå Slower than Haiku/Flash models
-   ‚ùå Overkill for most applications
-   ‚ùå 125√ó more expensive than GPT-4o-mini

**When to Use**:

-   Absolute highest quality required
-   Complex multi-step agentic workflows
-   High-stakes decision making
-   Advanced research and analysis
-   Low-volume, high-value use cases

**Cost Warning**:

```typescript
// 1,000 requests √ó 1k input + 3k output
const opusCost = (1000 √ó 1 √ó $15.00) + (1000 √ó 3 √ó $75.00)
               = $15 + $225 = $240 for just 1,000 requests!

// vs GPT-4o-mini:
const miniCost = (1000 √ó 1 √ó $0.15) + (1000 √ó 3 √ó $0.60)
               = $0.15 + $1.80 = $1.95 (123√ó cheaper!)
```

---

### 3. Google Gemini Models

#### Gemini 2.5 Pro

**Specifications**:

-   Context window: **1,000,000 tokens**
-   LiveBench Score: 62.1
-   Pricing: $1.50 input, $7.00 output
-   Speed: 70-90 tokens/sec
-   Multimodal: Text, vision, audio, video
-   **Released July 2025**

**Strengths**:

-   ‚úÖ **Very long context** (1M tokens)
-   ‚úÖ Enhanced reasoning and coding capabilities
-   ‚úÖ "Deep Think" mode for complex tasks
-   ‚úÖ Strong multimodal (video understanding)
-   ‚úÖ Native audio output support
-   ‚úÖ Improved security features
-   ‚úÖ Google Workspace integration

**Weaknesses**:

-   ‚ùå Tool calling less reliable than Claude/OpenAI
-   ‚ùå Can be verbose
-   ‚ùå Slower than Flash models

**When to Use**:

-   Very long document analysis (500K+ tokens)
-   Video/audio understanding
-   Multi-document research
-   Cost-effective multimodal tasks
-   Integration with Google ecosystem

**Example**:

```typescript
// Analyze 50 PDF documents (500k tokens total)
const cost = 500 √ó $1.50 = $0.75
// vs GPT-5.1: 500 √ó $10.00 = $5.00 (6.7√ó cheaper!)
```

---

#### Gemini 2.5 Flash (Speed Champion)

**Specifications**:

-   Context window: 1,000,000 tokens
-   LiveBench Score: 56.8
-   Pricing: **$0.10 input, $0.40 output** (ultra cheap!)
-   Speed: **~200+ tokens/sec** (fastest!)
-   Multimodal: Text, vision, audio, video
-   **Released June 2025**

**Strengths**:

-   ‚úÖ **Fastest model available**
-   ‚úÖ Ultra cost-effective ($0.10-$0.40)
-   ‚úÖ Long context (1M tokens)
-   ‚úÖ Strong multimodal capabilities
-   ‚úÖ Improved from 1.5 Flash
-   ‚úÖ Native audio output

**Weaknesses**:

-   ‚ùå Lower quality than premium models
-   ‚ùå Tool calling less reliable than Claude/GPT
-   ‚ùå Can make more mistakes on complex tasks

**When to Use**:

-   **Real-time applications** (<100ms latency)
-   High-volume, low-stakes tasks
-   Simple agent workflows
-   Prototyping and experimentation
-   Budget-constrained projects

**Cost Comparison with GPT-4o-mini**:

```typescript
// 10k requests comparison:
// Gemini 2.5 Flash: (10k √ó 0.5 √ó $0.10) + (10k √ó 1.5 √ó $0.40) = $0.50 + $6.00 = $6.50
// GPT-4o-mini:      (10k √ó 0.5 √ó $0.15) + (10k √ó 1.5 √ó $0.60) = $0.75 + $9.00 = $9.75
// Savings: $3.25/month (33% cheaper + 2√ó faster)
// Trade-off: Speed + cost vs tool-calling reliability
```

---

### 4. Meta Llama Models (Open Source)

#### Llama 4 405B (Largest Open Model)

**Specifications**:

-   Context window: **256,000 tokens** (2√ó longer!)
-   LiveBench Score: 60.3
-   Parameters: 405 billion
-   Pricing: **Free** (self-hosted) or $3-$5/1M (via API)
-   License: Llama 4 License (permissive)
-   **Released August 2025**

**Strengths**:

-   ‚úÖ **Largest open-weight model**
-   ‚úÖ Competitive with GPT-4o and Gemini 2.5
-   ‚úÖ Full control and privacy
-   ‚úÖ Customizable (fine-tuning)
-   ‚úÖ Improved tool calling from 3.1
-   ‚úÖ Better reasoning capabilities
-   ‚úÖ 2√ó longer context than Llama 3.1

**Weaknesses**:

-   ‚ùå Requires significant infrastructure (8√óH100 GPUs)
-   ‚ùå Complex to deploy and optimize
-   ‚ùå Tool calling still behind Claude/OpenAI

**When to Use**:

-   Privacy-critical applications
-   High-volume with existing infrastructure
-   Custom fine-tuning needed
-   Avoiding vendor lock-in
-   On-premise deployment requirements

---

#### Llama 4 70B (Best Open Balance)

**Specifications**:

-   Context window: **256,000 tokens**
-   LiveBench Score: 57.8
-   Parameters: 70 billion
-   Pricing: **Free** (self-hosted) or $0.90-$2/1M
-   **Released August 2025**

**Strengths**:

-   ‚úÖ Excellent quality for open model
-   ‚úÖ Runs on moderate GPU setup (4√óA100)
-   ‚úÖ Good for most tasks
-   ‚úÖ Cost-effective via OpenRouter
-   ‚úÖ Strong instruction following
-   ‚úÖ Improved multilingual support

**When to Use**:

-   Best open-source middle ground
-   Production with moderate infrastructure
-   Learning and experimentation
-   Cost-effective alternative to commercial models

---

#### Llama 4 8B (Edge Deployment)

**Specifications**:

-   Context window: **256,000 tokens**
-   LiveBench Score: 52.1
-   Parameters: 8 billion
-   Pricing: **Free** or ~$0.20/1M
-   **Released August 2025**

**Strengths**:

-   ‚úÖ Runs on single consumer GPU
-   ‚úÖ Mobile-friendly (quantized)
-   ‚úÖ Improved from 3.1 8B
-   ‚úÖ Good for simple tasks

**When to Use**:

-   Edge deployment (IoT, mobile)
-   Embedded systems
-   Extreme cost optimization
-   Local development and testing

---

### 5. Other Notable Models

#### Mistral Large 2

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1265
-   Pricing: $2.00 input, $6.00 output
-   Strong function calling

**Strengths**:

-   ‚úÖ Excellent value
-   ‚úÖ Strong European alternative
-   ‚úÖ Good tool use

**When to Use**:

-   European data residency
-   Alternative to GPT-4o
-   Function calling at scale

---

#### Command R+ (Cohere)

**Specifications**:

-   Context window: 128,000 tokens
-   LMSYS ELO: 1240
-   Pricing: $3.00 input, $15.00 output
-   **Optimized for RAG**

**Strengths**:

-   ‚úÖ Best for retrieval-augmented generation
-   ‚úÖ Excellent citation quality
-   ‚úÖ Strong at summarization

**When to Use**:

-   RAG applications
-   Document Q&A
-   Research assistants

---

#### MiniMax M2 (Best Open-Source Agentic Model)

**Specifications**:

-   Context window: 200,000 tokens
-   Parameters: 230B total (10B active per inference)
-   Architecture: Sparse Mixture-of-Experts (MoE)
-   Pricing: **Free** (open-source)
-   SWE-bench Verified: **69.4** (near GPT-5!)
-   **Released October 2025**

**Strengths**:

-   ‚úÖ **Best open-source model for agentic tool use** ‚úÖ
-   ‚úÖ Near-GPT-5 coding performance (SWE-bench 69.4 vs 74.9)
-   ‚úÖ Excellent ArtifactsBench: 66.8 (beats Claude 4.5 Sonnet)
-   ‚úÖ Strong œÑ¬≤-Bench: 77.2 (near GPT-5's 80.1)
-   ‚úÖ Efficient deployment (4√ó H100 GPUs at FP8)
-   ‚úÖ Free and open-source

**Weaknesses**:

-   ‚ùå Requires significant hardware (4√ó H100 minimum)
-   ‚ùå Complex deployment vs commercial APIs
-   ‚ùå Less documentation than commercial models

**When to Use**:

-   **Agentic systems with tool calling** ‚úÖ
-   Software engineering automation
-   On-premise/private deployments
-   Cost-sensitive projects with infrastructure
-   Research and experimentation

**Recommendation**: If you have the infrastructure, MiniMax M2 is the **best open-source alternative** to Claude 4.5 models for agentic systems!

---

#### Qwen3-235B-A22B (Best Reasoning Open-Source)

**Specifications**:

-   Context window: 128,000 tokens
-   Parameters: 235B total (22B active per inference)
-   Architecture: Sparse MoE
-   Training: 36 trillion tokens, 119 languages
-   Pricing: $1.20 input, $6.00 output (or free self-hosted)
-   **Released September 2025**

**Strengths**:

-   ‚úÖ **Outperforms DeepSeek-R1 on 17/23 benchmarks**
-   ‚úÖ Best open-source reasoning model
-   ‚úÖ Excellent multilingual (119 languages)
-   ‚úÖ Strong coding and mathematics
-   ‚úÖ Reasoning mode toggle via tokenizer
-   ‚úÖ Competitive with Claude/GPT on reasoning tasks

**When to Use**:

-   **Advanced reasoning tasks** (math, logic, analysis)
-   Multilingual applications
-   Cost-effective Claude alternative
-   Chinese language processing
-   Research requiring transparent reasoning

---

#### GLM-4.6 (Chinese Market Leader)

**Specifications**:

-   Context window: **200,000 tokens** (up from 128K)
-   Pricing: $1.00 input, $3.00 output
-   Open source (Apache 2.0)
-   **Released November 2025**

**Strengths**:

-   ‚úÖ Strong reasoning (near Claude Sonnet 4)
-   ‚úÖ 15% more token-efficient than GLM-4.5
-   ‚úÖ Excellent multilingual (Chinese/English)
-   ‚úÖ Cost-effective
-   ‚úÖ 200K context (vs 128K in 4.5)

**When to Use**:

-   Chinese market applications
-   Reasoning tasks on budget
-   Multilingual (Chinese/English) agents
-   On-premise deployment in Asia

---

#### DeepSeek V2.5

**Specifications**:

-   Context window: 128,000 tokens
-   Pricing: **$0.14 input, $0.28 output** (ultra-cheap!)
-   Open source

**Strengths**:

-   ‚úÖ **Cheapest capable model**
-   ‚úÖ Good enough for many tasks
-   ‚úÖ Open source

**When to Use**:

-   Extreme budget constraints
-   High-volume, simple tasks
-   Experimentation

## Framework Integration

### AI SDK v6 Native Pattern

```typescript
import { createOpenRouter } from "@openrouter/ai-sdk-provider";
import { ToolLoopAgent } from "ai";

// Initialize OpenRouter provider
const openrouter = createOpenRouter({
  apiKey: process.env.OPENROUTER_API_KEY,
});

// Configure model (v6 uses .chat() not .languageModel())
const modelId = process.env.OPENROUTER_MODEL || "openai/gpt-4o-mini";

export const agent = new ToolLoopAgent({
  model: openrouter.chat(modelId),
  instructions: "You are a helpful agent...",
  tools: {
    // Your tools
  },
});
```

### Next.js Server Action Integration

```typescript
// Example: API route for streaming agent response
import { createAgentUIStreamResponse } from "ai";
import { myAgent } from "./agent-instance";

export async function POST(request: Request) {
  const { messages } = await request.json();

  // Stream agent response (agent uses configured model)
  return createAgentUIStreamResponse({
    agent: myAgent,
    messages,
  });
}
```

**Why This Pattern Works**: AI SDK's provider abstraction means changing the model ID in one place (config or `.env`) automatically applies everywhere. All agent instances inherit the new model without code changes.

### Changing Models

**In Production** (via `.env`):
```bash
# .env.production
OPENROUTER_MODEL=anthropic/claude-4.5-sonnet
```

**In Code** (dynamic routing):
```typescript
const modelId = selectModelForTask(taskType);
const model = openrouter.chat(modelId);  // v6 API: .chat() instead of .languageModel()
```

---

## Production Best Practices

### 1. Monitor Real-World Metrics, Not Just Benchmarks

**Why**: Benchmarks don't predict:
- Tool-calling failure rates (critical for agents)
- Latency tail (p95/p99 matters more than median)
- Cost variance (input/output ratio impacts real spend)

**Implementation**: Track these metrics:

```typescript
interface ModelMetrics {
  modelId: string;
  requests: number;
  avgLatency: number;           // actual, not benchmark
  toolCallSuccess: number;      // % successful tool calls
  avgTokensPerRequest: number;  // actual usage
  costPerRequest: number;       // real cost, not theoretical
  errorRate: number;            // timeouts, failures, invalid responses
}
```

### 2. Cost Isn't Just Price‚ÄîFactor in Reliability

**Formula**: `Real Cost = API Cost + (Error Rate √ó Retry Cost) + (Failure Impact √ó Lost Revenue)`

Example:
- **Cheap, unreliable**: $1 + (15% error √ó $2 retry) + (5% lost revenue = $10) = $3.15 per request
- **Expensive, reliable**: $5 + (0.1% error √ó $0.50 retry) + (0% lost revenue) = $5.00 per request

The "cheaper" option costs more.

### 3. Context Window Matters More Than You Think

**Scenario**: Process 50 documents (100 tokens each = 5K tokens total)

- **Claude (200K context)**: 1 API call, 5K tokens
- **GPT-4o-mini (128K context)**: 1 API call, 5K tokens
- **Gemini 3 Pro (1M context)**: 1 API call, 5K tokens (but can do 200 documents!)

For document-heavy workloads (RAG, research agents), Gemini 3 Pro saves on API calls.

### 4. Tool-Calling Reliability is Critical for Agent Systems

For an agent system making 1000 tool calls/day:
- **Claude (99%)**: 10 failures/day, ~5min manual fix time = $5 cost
- **Gemini Flash (89%)**: 110 failures/day, ~55min manual fix time = $55 cost

A 10% success rate difference translates to 10√ó more manual intervention and operational overhead.

---

## When to Use Which Model

### Decision Matrix by Task

| Your Need                  | Best Choice                | Why                                          |
| -------------------------- | ------------------------- | -------------------------------------------- |
| **Agentic systems**        | Claude 4.5 Haiku / Sonnet | 97-99% tool-calling, proven agent patterns  |
| **Code generation**        | Claude 4.5 Sonnet         | 77.2% SWE-bench (industry leading)          |
| **Long documents** (50+)   | Gemini 3 Pro              | 1M context, single API call                 |
| **Budget constrained**     | Gemini 3 Flash            | $0.04 input, 200+ tok/sec                   |
| **Reasoning heavy**        | Gemini 3 Pro              | 41% on reasoning tasks with Deep Think      |
| **Maximum quality**        | Claude 4.5 Opus           | 68.1 LiveBench (highest score)              |
| **Speed critical**         | Gemini 3 Flash            | 0.15s latency, 200+ tok/sec                 |
| **Hybrid (variable tasks)**| Route dynamically (Patterns #2-3) | Optimize per request |

---

## Key Takeaways

1. **GPT-4o-mini remains an excellent baseline** ‚úÖ
   - Ranks #7 overall (58.3 LiveBench), strong cost-performance
   - 97%+ tool-calling reliability (critical for agents)
   - Fast (110 tok/sec), affordable ($9.75/month for 10K requests)
   - Upgrade only if specialized needs arise (coding, reasoning, speed)

2. **Model selection is about trade-offs, not absolute performance**
   - **Best agents**: Claude 4.5 Sonnet/Haiku (99% tool-calling, 30+ hour sustained focus)
   - **Best reasoning**: Gemini 3 Pro (41% on reasoning benchmarks, 1M context)
   - **Best cost**: Gemini 3 Flash ($0.04 input), GPT-4o-mini ($0.38 blended)
   - **Best balance**: Claude 4.5 Haiku ($3/1M, near-Sonnet quality, 2√ó faster)

3. **Real cost ‚â† API cost**: Factor in tool-calling reliability, retries, and failure impact
   - 10% difference in success rate = 10√ó difference in manual fix time
   - Cheap + unreliable often costs more than expensive + reliable

4. **Framework integration via AI SDK 6 is seamless**
   - Change model ID in `.env`, everything else stays the same
   - Routes work with any OpenRouter model (200+ available)

5. **When to consider model upgrades from GPT-4o-mini**:
   - **Better coding** ‚Üí Claude 4.5 Sonnet (77.2% SWE-bench, highest-quality code generation)
   - **Better speed** ‚Üí Claude 4.5 Haiku (0.20s latency) or Gemini 3 Flash (0.15s)
   - **Better reasoning** ‚Üí Gemini 3 Pro (41% on reasoning benchmarks)
   - **Long documents** ‚Üí Gemini 3 Pro (1M context enables 50+ documents per call)
   - **Tighter budget** ‚Üí Gemini 3 Flash (33% cheaper, minimal quality loss)

**Implementation Checklist**:
-   [ ] Measure baseline metrics: latency, tool-calling success, cost (actual production data)
-   [ ] Test candidate models in staging environment
-   [ ] Compare real-world performance vs benchmark scores
-   [ ] Make data-driven decision based on your workload
-   [ ] Update model configuration (typically 1 environment variable change)

## References

1. **Gemini 3 Pro Launch** (Nov 2025), Google DeepMind. Reasoning benchmarks: 41% Humanity's Last Exam (with Deep Think), 76.2% SWE-bench, $5,478 mean net worth on Vending-Bench 2. https://vertu.com/lifestyle/gemini-3-launch-google-strikes-back-less-than-a-week-after-gpt-5-1-release/

2. **GPT-5.1 Announcement** (Nov 2025), OpenAI. 63.8 LiveBench, $1.25 input/$10 output (90% discount for cached), fastest inference speeds.

3. **Claude 4.5 Models** (Aug-Oct 2025), Anthropic. Sonnet: 65.2 LiveBench, 77.2% SWE-bench, 98%+ tool-calling. Haiku: 60.5 LiveBench, 97%+ tool-calling, 2√ó faster. Opus: 68.1 LiveBench (highest).

4. **LiveBench Leaderboard** (Nov 2025). Real-world LLM benchmark tracking 20+ models. https://livebench.ai/

5. **OpenRouter Models Catalog**. Access to 200+ models via single API. Real-time pricing and availability. https://openrouter.ai/models

6. **Artificial Analysis**. LLM pricing, speed, and quality comparison across providers. https://artificialanalysis.ai/leaderboards/models

7. **LMSYS Chatbot Arena** (ongoing). Community-voted model rankings via blind comparison. https://chat.lmsys.org/

8. **Berkeley Function Calling Leaderboard**. Tool-calling reliability benchmarks (critical for agents). https://gorilla.cs.berkeley.edu/leaderboard.html

**Related Topics**:
-   [0.1.1 LLM Fundamentals](./0.1.1-llm-intro.md) - How models work
-   [0.1.4 Sampling Parameters](./0.1.4-sampling-parameters.md) - Temperature, top-p, top-k tuning per model
-   [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md) - o1, reasoning models
-   [3.1.1 ReAct Pattern](../3-agents/3.1.1-agent-definition.md) - Why tool-calling reliability matters

**Layer Index**: [Layer 0: Foundations](../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations)
