# 0.3.3 Vector Similarity Metrics

## TL;DR

Vector similarity metrics measure how related two embeddings are; cosine similarity (angle-based) is the industry standard for text embeddings, while dot product, Euclidean, and Manhattan distances each have specific use cases depending on whether magnitude matters and data characteristics.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [0.3.2 Embedding Models](./0.3.2-embedding-models.md)
- **Grounded In**: Pinecone, Weaviate, Qdrant, Microsoft Azure AI, Google ML documentation

## Table of Contents

- [Overview](#overview)
- [The Problem: Measuring Semantic Similarity](#the-problem-measuring-semantic-similarity)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Once you have embeddings (vectors), you need to **measure similarity** between them. This is fundamental to semantic search, clustering, recommendations, and RAG systems.

```
Query: "contact form"
  ↓ Embed
Query vector: [0.3, 0.8, -0.2, ...]
  ↓ Compare with indexed vectors
Results: "Get in Touch Section" (similarity: 0.89) ✅
```

**Four Common Metrics**:

| Metric | Measures | Range | Best For |
|--------|----------|-------|----------|
| **Cosine Similarity** | Angle (direction) | -1 to 1 | Text embeddings |
| **Dot Product** | Direction + magnitude | -∞ to ∞ | Normalized vectors |
| **Euclidean Distance** | Straight-line distance | 0 to ∞ | Spatial data |
| **Manhattan Distance** | Grid-like distance | 0 to ∞ | Sparse/high-dim data |

**Key Insight**: For text embeddings, use **cosine similarity** (or dot product with normalized vectors).

**Date Verified**: 2025-12-03

## The Problem: Measuring Semantic Similarity

### The Classic Challenge

Given two embedding vectors, how do we quantify their similarity?

```
Vector A: [0.3, 0.8, -0.2, 0.5]  ("contact form")
Vector B: [0.4, 0.7, -0.1, 0.6]  ("get in touch")

How similar are these? Need a number!
```

**Problems**:

- ❌ **No inherent meaning**: Vectors are just numbers
- ❌ **Different lengths**: Should longer documents score higher?
- ❌ **Scale sensitivity**: Do magnitudes matter or just directions?
- ❌ **High dimensions**: 1536 dimensions—can't visualize

### Why This Matters

Choosing the wrong similarity metric can:

- Return irrelevant search results
- Bias toward longer/shorter documents
- Miss semantically similar but different-magnitude vectors
- Degrade ANN index performance

## Core Concept

### Cosine Similarity (Recommended for Text)

**What it measures**: The **angle** between vectors, ignoring magnitude.

**Formula**:

```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)

Where:
- A · B = dot product = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ
- ||A|| = magnitude = √(a₁² + a₂² + ... + aₙ²)
```

**Range**: -1 to 1

- **1.0**: Same direction (identical meaning)
- **0.0**: Perpendicular (unrelated)
- **-1.0**: Opposite directions (antonyms)

**Why ignore magnitude?**

```
A = [0.3, 0.8, -0.2]  # "cat sleeps" (short)
B = [0.6, 1.6, -0.4]  # "cat sleeps softly" (longer)

B is just 2× A (same direction, different magnitude)
They should be equally similar to a query!

cosine_similarity(A, B) = 1.0  ✅ Correct
euclidean_distance(A, B) = 0.92  ❌ Treats as different
```

### Dot Product

**What it measures**: Direction **and** magnitude combined.

**Formula**: `dot_product(A, B) = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ`

**Range**: -∞ to +∞

**Key insight**: For **normalized vectors** (||v|| = 1), dot product = cosine similarity!

```python
# Normalized vectors
A_norm = A / ||A||  # Unit vector
B_norm = B / ||B||  # Unit vector

dot_product(A_norm, B_norm) = cosine_similarity(A, B)  # Identical!
```

**Advantage**: 3× faster than cosine (no square root needed).

### Euclidean Distance

**What it measures**: Straight-line **distance** in space.

**Formula**: `euclidean(A, B) = √Σ(aᵢ - bᵢ)²`

**Range**: 0 to ∞ (smaller = more similar)

**Note**: Distance metric (smaller = more similar), not similarity (larger = more similar).

**Problem in high dimensions**: The "curse of dimensionality"—all distances converge, losing discriminative power.

### Manhattan Distance

**What it measures**: Grid-like distance (sum of absolute differences).

**Formula**: `manhattan(A, B) = Σ|aᵢ - bᵢ|`

**Range**: 0 to ∞ (smaller = more similar)

**Advantages**:

- Faster (no square root)
- Robust to outliers
- Better for sparse data

## Implementation Patterns

### Pattern 1: Cosine Similarity (Most Common)

```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0
  let magnitudeA = 0
  let magnitudeB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    magnitudeA += a[i] * a[i]
    magnitudeB += b[i] * b[i]
  }

  return dotProduct / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB))
}

// Example
const query = [0.3, 0.8, -0.2, 0.5]
const doc = [0.4, 0.7, -0.1, 0.6]

console.log(cosineSimilarity(query, doc))  // 0.991 (very similar!)
```

### Pattern 2: Dot Product (Fastest for Normalized)

```typescript
function dotProduct(a: number[], b: number[]): number {
  return a.reduce((sum, val, i) => sum + val * b[i], 0)
}

function l2Normalize(v: number[]): number[] {
  const magnitude = Math.sqrt(v.reduce((s, x) => s + x * x, 0))
  return v.map(x => x / magnitude)
}

// For normalized vectors, dot product = cosine similarity
const aNorm = l2Normalize(a)
const bNorm = l2Normalize(b)

console.log(dotProduct(aNorm, bNorm))  // Same as cosineSimilarity!
```

### Pattern 3: Vector Database Integration

```typescript
// LanceDB (default: cosine)
const results = await table.search(queryEmbedding).limit(10)

// Explicit metric selection
await table.search(queryEmbedding).metric('cosine').limit(10)
await table.search(queryEmbedding).metric('dot').limit(10)    // Faster
await table.search(queryEmbedding).metric('l2').limit(10)     // Euclidean
```

## When to Use This Pattern

### Metric Selection Guide

| Your Situation | Recommended Metric | Reason |
|----------------|-------------------|--------|
| **Text embeddings** | Cosine Similarity | Ignores document length |
| **Normalized embeddings** | Dot Product | Same as cosine, 3× faster |
| **Spatial/image data** | Euclidean Distance | Position matters |
| **Sparse/high-dim data** | Manhattan Distance | Robust, efficient |
| **Magnitude matters** | Dot Product | Captures both direction + scale |

### Score Interpretation (Cosine)

| Score Range | Meaning | Action |
|-------------|---------|--------|
| **> 0.9** | Excellent match | Highly confident |
| **0.7 - 0.9** | Good match | Relevant results |
| **0.5 - 0.7** | Weak match | Consider filtering |
| **< 0.5** | Poor match | Likely irrelevant |

## Production Best Practices

### 1. Match Training Metric

Use the same metric your embedding model was trained with:

```
OpenAI text-embedding-3: Trained with cosine → Use cosine
SBERT models: Trained with cosine → Use cosine
```

### 2. Normalize for Speed

If using cosine frequently, pre-normalize vectors:

```typescript
// At index time: normalize once
const normalizedEmbedding = l2Normalize(embedding)
await index.add(normalizedEmbedding)

// At search time: use dot product (3× faster)
await table.search(queryEmbedding).metric('dot')
```

### 3. Consistent Normalization

Never mix normalized and unnormalized vectors:

```typescript
// ❌ BAD: Inconsistent
const queryNorm = l2Normalize(queryEmbedding)
const docRaw = docEmbedding  // Not normalized!
dotProduct(queryNorm, docRaw)  // Meaningless!

// ✅ GOOD: Consistent
const queryNorm = l2Normalize(queryEmbedding)
const docNorm = l2Normalize(docEmbedding)
dotProduct(queryNorm, docNorm)  // Valid!
```

### 4. Monitor Score Distribution

```typescript
async function searchWithMonitoring(query: string) {
  const results = await vectorIndex.search(query, 100)
  const scores = results.map(r => r.score)

  const avg = scores.reduce((a, b) => a + b) / scores.length

  if (avg < 0.3) {
    console.warn('⚠️ Low average similarity - check embeddings!')
  }

  return results.filter(r => r.score > 0.7)  // Filter low-confidence
}
```

### Common Pitfalls

**❌ Wrong metric for data type**:

Euclidean on text embeddings penalizes longer documents unfairly.

**❌ Comparing different embedding models**:

OpenAI and SBERT embeddings live in different vector spaces—can't compare.

**❌ Ignoring normalization**:

Dot product on unnormalized vectors gives magnitude-biased results.

## Key Takeaways

1. **Cosine similarity for text** - Ignores magnitude, focuses on direction
2. **Dot product for speed** - 3× faster with normalized vectors
3. **Euclidean for spatial** - When absolute position matters
4. **Normalize consistently** - Never mix normalized/unnormalized
5. **Match training metric** - Use what the model was trained with

**Quick Reference**:

```
Text/semantic search      → Cosine Similarity ✅
Normalized + speed        → Dot Product
Image/spatial data        → Euclidean Distance
Sparse/high-dimensional   → Manhattan Distance
```

## References

1. **Pinecone** (2025). "Vector Similarity Explained". https://www.pinecone.io/learn/vector-similarity/
2. **Weaviate** (2025). "Distance Metrics in Vector Search". https://weaviate.io/blog/distance-metrics-in-vector-search
3. **Qdrant** (2025). "Distance Metrics". https://qdrant.tech/course/essentials/day-1/distance-metrics/
4. **Microsoft** (2025). "Azure AI Search: Vector Relevance and Ranking". https://learn.microsoft.com/en-us/azure/search/vector-search-ranking
5. **Google** (2025). "Measuring Similarity from Embeddings". https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity
6. **Milvus** (2025). "Vector Normalization and Metrics". https://milvus.io/ai-quick-reference/what-is-the-relationship-between-vector-normalization-and-the-choice-of-metric

**Related Topics**:

- [0.3.2 Embedding Models](./0.3.2-embedding-models.md)
- [0.3.4 Dimensionality Trade-offs](./0.3.4-dimensionality.md)
- [5.1.2 Similarity Metrics in RAG](../5-rag/5.1.2-similarity-metrics.md)

**Layer Index**: [Layer 0: Foundations](../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations-prerequisites)
