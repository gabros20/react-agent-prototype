# 0.3.2 Embedding Models & Vector Spaces

**Layer**: 0 - Foundations  
**Prerequisites**: [0.3.1 Tokenization](./0.3.1-tokenization.md)  
**Next**: [0.3.3 Vector Similarity](./0.3.3-vector-similarity.md)

---

## Overview

**Embeddings** are dense, fixed-size vector representations of text that capture semantic meaning. They enable machines to understand that "king" and "monarch" are similar, even though they share no characters.

```
Text: "artificial intelligence" 
  ↓ Tokenization
Tokens: ["artificial", "intelligence"]
  ↓ Embedding Model
Vector: [0.23, -0.45, 0.67, ..., 0.12]  (1536 dimensions)
         ↓
   Semantic meaning captured in numbers
```

**Why embeddings matter**:
1. **Semantic search**: Find "dog" when user searches "puppy"
2. **Similarity comparison**: Calculate how related two texts are
3. **Clustering**: Group similar content automatically
4. **Classification**: Categorize text by meaning
5. **RAG systems**: Retrieve relevant context for LLMs

**In your codebase**: Used for fuzzy resource search (LanceDB vector index)

**Source**: [Stack Overflow: Intuitive Introduction to Text Embeddings](https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/)

---

## What Are Embeddings?

### From Words to Vectors

**Problem**: Computers can't understand "queen" and "king" are related

**Solution**: Map text to high-dimensional space where similar meanings → nearby vectors

```
Traditional (one-hot encoding):
"king"  → [1, 0, 0, 0, 0, ...]  (50k dimensions, mostly zeros)
"queen" → [0, 1, 0, 0, 0, ...]  (no relationship captured!)

Embedding:
"king"  → [0.3, 0.8, -0.2, 0.5, ...]  (768 dims, dense, meaningful)
"queen" → [0.4, 0.7, -0.1, 0.6, ...]  (similar values!)
                ↓
        Cosine similarity: 0.91 (very similar!)
```

### Key Properties

1. **Dense**: Most values are non-zero (vs sparse one-hot)
2. **Fixed-size**: Always same dimensionality (e.g., 384, 768, 1536)
3. **Semantic**: Similar meanings → similar vectors
4. **Learned**: Trained on massive datasets to capture patterns

**Analogy**: Embeddings are like GPS coordinates for meaning.
- "king" at (0.3, 0.8, -0.2, ...)
- "queen" at (0.4, 0.7, -0.1, ...)
- "democracy" at (-0.5, 0.1, 0.9, ...)

Nearby in space = similar in meaning!

**Source**: [Toolify: Sentence Embedding Power](https://www.toolify.ai/ai-news/unleash-the-power-of-sentence-embedding-sentence-similarity-semantic-search-and-clustering)

---

## Popular Embedding Models

### 1. OpenAI text-embedding-3 (2024)

**Latest generation** from OpenAI, successor to ada-002.

**Variants**:
| Model | Dimensions | Cost (per 1M tokens) | Use Case |
|-------|------------|---------------------|----------|
| **text-embedding-3-small** | 1536 | $0.02 | General-purpose, cost-effective |
| **text-embedding-3-large** | 3072 | $0.13 | Highest quality, expensive |

**Key Features**:
- ✅ **Multilingual**: 100+ languages
- ✅ **High quality**: SOTA performance on MTEB benchmark
- ✅ **Flexible dimensions**: Can reduce to 512/1024 (same model!)
- ✅ **Long context**: 8,191 tokens max input

**Performance** (MTEB Benchmark):
- text-embedding-3-small: 62.3% average (best cost/performance)
- text-embedding-3-large: 64.6% average (best quality)
- ada-002 (previous): 61.0% (superseded)

**Example Usage**:
```typescript
// Your codebase: server/services/vector-index.ts
async embedText(text: string): Promise<number[]> {
  const response = await fetch('https://openrouter.ai/api/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'openai/text-embedding-3-small',  // 1536 dims, $0.02/1M
      input: text
    })
  })
  
  const { data } = await response.json()
  return data[0].embedding  // [0.23, -0.45, ..., 0.12] (1536 numbers)
}
```

**Dimensionality Reduction** (optional):
```typescript
// Request 512-dim vectors instead of 1536 (faster search, same model)
{
  model: 'openai/text-embedding-3-small',
  input: text,
  dimensions: 512  // Reduce from 1536 → 512 (3x smaller!)
}

// Trade-off: ~2-3% accuracy loss, 3x faster similarity search
```

**Source**: [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)

---

### 2. Sentence Transformers (SBERT)

**Open-source** models optimized for sentence-level semantic similarity.

**Most Popular Models**:

| Model | Dimensions | Size | Performance | Use Case |
|-------|------------|------|-------------|----------|
| **all-MiniLM-L6-v2** | 384 | 23 MB | 68.7% | Fast, lightweight |
| **all-mpnet-base-v2** | 768 | 420 MB | 69.6% | Balanced |
| **all-MiniLM-L12-v2** | 384 | 34 MB | 68.4% | Slightly better quality |

**Key Features**:
- ✅ **Free**: No API costs (run locally or on your server)
- ✅ **Fast**: Optimized for CPU/GPU inference
- ✅ **Flexible**: 6,000+ community models on Hugging Face
- ✅ **Privacy**: Data never leaves your infrastructure

**Example Usage** (Python):
```python
from sentence_transformers import SentenceTransformer

# Load model (downloads ~23 MB once)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed sentences
sentences = [
  "The cat sits on the mat.",
  "A kitten rests on the rug.",
  "The weather is sunny today."
]

embeddings = model.encode(sentences)
# Shape: (3, 384)  - 3 sentences, 384 dimensions each

# Compare similarity
from sklearn.metrics.pairwise import cosine_similarity

similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
# 0.85 (very similar: cat/kitten, sits/rests, mat/rug)

similarity = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
# 0.12 (not similar: cat vs weather)
```

**Node.js Integration** (for your codebase):
```typescript
// Option 1: Call Python microservice
const response = await fetch('http://localhost:5000/embed', {
  method: 'POST',
  body: JSON.stringify({ text: "Your text here" })
})
const { embedding } = await response.json()

// Option 2: Use transformers.js (ONNX in Node)
import { pipeline } from '@xenova/transformers'

const embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2')
const output = await embedder("Your text here", { pooling: 'mean', normalize: true })
const embedding = Array.from(output.data)  // 384-dim vector
```

**Source**: [Sentence Transformers Documentation](https://sbert.net/)

---

### 3. BGE (BAAI General Embedding)

**State-of-the-art** Chinese-developed models (2024).

**Variants**:
- **bge-small-en-v1.5**: 384 dims (33 MB) - Fast, competitive
- **bge-base-en-v1.5**: 768 dims (109 MB) - Better quality
- **bge-large-en-v1.5**: 1024 dims (326 MB) - Best quality

**Performance**: Often outperforms OpenAI on retrieval tasks

**Example** (Python):
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-small-en-v1.5')

# For retrieval tasks, prefix queries with instruction:
query = "What is the capital of France?"
query_embedding = model.encode(f"Represent this sentence for searching relevant passages: {query}")

# Documents don't need prefix
doc = "Paris is the capital of France."
doc_embedding = model.encode(doc)

similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
# Improved retrieval accuracy vs models without query instructions
```

**Source**: [Hugging Face: BGE Models](https://huggingface.co/BAAI)

---

### 4. Embeddings in Large Language Models

**Context**: Some LLMs expose their internal embeddings.

**GPT-3/4 Embeddings**:
```
Input: "Hello world"
  ↓ Token embeddings (internal)
[768-dim vector for "Hello"]
[768-dim vector for "world"]
  ↓ Transformer layers (attention, MLP)
Final hidden states (not directly accessible via API)
```

**Dedicated embedding models** (text-embedding-3) are **better** for:
- Semantic search (optimized for similarity)
- Retrieval (trained on query-document matching)
- Clustering (learned metric space)

**LLM embeddings** (GPT-4) are **better** for:
- Task-specific fine-tuning
- Next-token prediction
- Text generation

**Recommendation**: Use dedicated embedding models (text-embedding-3, SBERT) for search/RAG.

---

## Embedding Model Comparison

| Model | Dims | Cost | Speed | Quality | Multilingual | Local |
|-------|------|------|-------|---------|--------------|-------|
| **text-embedding-3-small** | 1536 | $0.02/1M | Fast | Excellent | ✅ | ❌ API |
| **text-embedding-3-large** | 3072 | $0.13/1M | Slow | Best | ✅ | ❌ API |
| **all-MiniLM-L6-v2** | 384 | Free | Very Fast | Good | ⚠️ EN-focus | ✅ Yes |
| **all-mpnet-base-v2** | 768 | Free | Fast | Very Good | ⚠️ EN-focus | ✅ Yes |
| **bge-small-en-v1.5** | 384 | Free | Very Fast | Very Good | ⚠️ EN-focus | ✅ Yes |
| **bge-large-en-v1.5** | 1024 | Free | Moderate | Excellent | ⚠️ EN-focus | ✅ Yes |

**Recommendation for Your Codebase**:
- **Current** (OpenAI via OpenRouter): Good for multilingual, no infrastructure
- **Alternative** (SBERT self-hosted): Free, private, requires Python service

**Source**: [Simplr: Comparing Embedding Models](https://dev.to/simplr_sh/comparing-popular-embedding-models-choosing-the-right-one-for-semantic-similarity-and-rag-4nej)

---

## How Embeddings Work

### Training Process

**Contrastive Learning** (most common approach):

```
1. Take sentence pairs:
   Positive pair (similar):
     - "The cat sleeps."
     - "A feline rests."
   
   Negative pair (dissimilar):
     - "The cat sleeps."
     - "The stock market crashed."

2. Pass through encoder:
   "The cat sleeps." → [0.3, 0.8, -0.2, ...]
   "A feline rests." → [0.4, 0.7, -0.1, ...]  (should be close!)
   "Stock crashed."  → [-0.5, 0.1, 0.9, ...] (should be far!)

3. Loss function:
   - Pull similar pairs together (minimize distance)
   - Push dissimilar pairs apart (maximize distance)

4. Repeat millions of times on huge datasets
   → Model learns semantic relationships
```

**Key Insight**: Model never explicitly "understands" meaning - it learns from patterns in data where similar things appear in similar contexts.

### Dimensionality

**Why high-dimensional?** (384-3072 dimensions)

**Intuition**: More dimensions → more capacity to distinguish concepts.

**Analogy**: Map coordinates
- 1D: Can only distinguish "left" vs "right"
- 2D: Can distinguish "north", "south", "east", "west"
- 3D: Add "up" and "down"
- 384D: Can distinguish millions of nuanced concepts!

**Trade-offs**:

| Dimensions | Pros | Cons |
|------------|------|------|
| **Small** (384) | ✅ Fast search | ⚠️ Less nuanced |
| **Medium** (768) | ✅ Balanced | ✅ Balanced |
| **Large** (1536-3072) | ✅ Most accurate | ⚠️ Slower search |

**Practical guidance**:
- **384**: Good enough for most applications (95% of quality at 25% cost)
- **768**: Sweet spot for quality/speed
- **1536+**: When accuracy is critical (legal, medical)

---

## Semantic Search in Your Codebase

### Current Implementation (LanceDB + OpenAI)

```typescript
// server/services/vector-index.ts

class VectorIndexService {
  private table: any  // LanceDB table
  
  // Embed text using OpenAI
  async embedText(text: string): Promise<number[]> {
    const response = await fetch('https://openrouter.ai/api/v1/embeddings', {
      method: 'POST',
      body: JSON.stringify({
        model: 'openai/text-embedding-3-small',  // 1536 dimensions
        input: text
      })
    })
    return (await response.json()).data[0].embedding
  }
  
  // Index a CMS resource
  async indexResource(resource: {
    id: string
    type: 'page' | 'section_def' | 'collection' | 'entry'
    name: string
    slug?: string
  }) {
    // Create searchable text
    const searchText = `${resource.name} ${resource.slug || ''}`
    
    // Generate embedding
    const embedding = await this.embedText(searchText)
    
    // Store in LanceDB
    await this.table.add([{
      id: resource.id,
      type: resource.type,
      name: resource.name,
      slug: resource.slug,
      embedding: embedding  // 1536-dim vector
    }])
  }
  
  // Semantic search
  async search(query: string, limit: number = 5) {
    // Embed query
    const queryEmbedding = await this.embedText(query)
    
    // Vector similarity search (LanceDB computes cosine similarity)
    const results = await this.table
      .search(queryEmbedding)
      .limit(limit)
      .execute()
    
    // Return with similarity scores
    return results.map(r => ({
      id: r.id,
      type: r.type,
      name: r.name,
      slug: r.slug,
      score: r._distance  // 0-1, higher = more similar
    }))
  }
}
```

**Example Queries**:
```typescript
// User searches: "hero banner"
await vectorIndex.search("hero banner", 3)

// Returns (by semantic similarity):
// 1. "Hero Section" (score: 0.92) - exact match
// 2. "Banner Component" (score: 0.81) - related
// 3. "Header Image Section" (score: 0.73) - conceptually similar

// Even though user didn't type exact name, embeddings understand:
// "hero banner" ≈ "Hero Section"
```

**Why this works**:
```
Embedding space learned relationships:
- "hero" ≈ "banner" ≈ "header"
- "section" ≈ "component" ≈ "block"
- "page" ≈ "document" ≈ "article"

When user types "blog post", system finds:
- "Blog Collection" (high similarity)
- "Article Entry" (related concept)
```

**Related Files**:
- [`server/services/vector-index.ts`](../../server/services/vector-index.ts) - Embedding integration
- [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - search.vector tool

---

## Use Cases

### 1. Semantic Search (Your Codebase)

**Problem**: User searches "contact form" but you have "Get in Touch Section"

**Solution**: Embeddings understand they're the same concept.

```typescript
// search.vector tool
async vectorSearch({ query, limit }: { query: string, limit: number }) {
  // Embed query
  const queryVec = await embedText(query)
  
  // Find similar resources
  const results = await lancedb.search(queryVec).limit(limit)
  
  // Results ranked by semantic similarity
  return results  // Returns "Get in Touch" for "contact form" query
}
```

**Accuracy**: 95%+ vs 60% with keyword search (handles synonyms, typos, paraphrases).

### 2. Duplicate Detection

**Problem**: Identify similar content across your CMS.

```typescript
// Find duplicate pages
const pages = await db.pages.findMany()

for (const page of pages) {
  const embedding = await embedText(page.name + page.slug)
  const similar = await vectorIndex.search(embedding, 5)
  
  // Filter out self, keep high similarity (>0.9)
  const duplicates = similar.filter(s => 
    s.id !== page.id && s.score > 0.9
  )
  
  if (duplicates.length > 0) {
    console.warn(`Possible duplicate: ${page.name}`, duplicates)
  }
}
```

### 3. Clustering & Organization

**Problem**: Automatically group 100+ pages by topic.

```python
from sklearn.cluster import KMeans
import numpy as np

# Get all page embeddings
page_embeddings = [embed(page.name) for page in pages]

# Cluster into 5 groups
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(page_embeddings)

# Results:
# Cluster 0: Product pages
# Cluster 1: Blog posts
# Cluster 2: Legal docs
# Cluster 3: Support articles
# Cluster 4: Landing pages
```

### 4. Content Recommendations

**Problem**: "Users who viewed this page also liked..."

```typescript
// Get embedding for current page
const currentPageEmb = await embedText(currentPage.name)

// Find similar pages
const similar = await vectorIndex.search(currentPageEmb, 10)
  .filter(p => p.type === 'page' && p.id !== currentPage.id)

// Show top 5 recommendations
return similar.slice(0, 5)
```

**Source**: [Sentence Transformers: Usage](https://sbert.net/docs/sentence_transformer/usage/usage.html)

---

## Best Practices

### 1. Normalize Text Before Embedding

```typescript
function normalizeForEmbedding(text: string): string {
  return text
    .toLowerCase()                    // "Page" → "page"
    .replace(/[^\w\s-]/g, ' ')       // Remove special chars
    .replace(/\s+/g, ' ')            // Collapse whitespace
    .trim()                          // Remove leading/trailing spaces
}

// Usage
const text = "  Hello,    WORLD!!!  "
const normalized = normalizeForEmbedding(text)  // "hello world"
const embedding = await embedText(normalized)
```

**Why**: Reduces noise, improves consistency, saves tokens.

### 2. Chunk Long Text (>8k Tokens)

```typescript
function chunkText(text: string, maxTokens: number = 8000): string[] {
  const enc = tiktoken.get_encoding('cl100k_base')
  const tokens = enc.encode(text)
  
  if (tokens.length <= maxTokens) {
    return [text]
  }
  
  // Split into chunks of maxTokens
  const chunks: string[] = []
  for (let i = 0; i < tokens.length; i += maxTokens) {
    const chunkTokens = tokens.slice(i, i + maxTokens)
    chunks.push(enc.decode(chunkTokens))
  }
  
  return chunks
}

// Usage
const longText = getPageContent()  // 20k tokens
const chunks = chunkText(longText, 8000)

// Embed each chunk
const embeddings = await Promise.all(
  chunks.map(chunk => embedText(chunk))
)

// Strategy 1: Use first chunk (summary often at start)
const primaryEmbedding = embeddings[0]

// Strategy 2: Average all chunk embeddings
const avgEmbedding = embeddings[0].map((_, i) => 
  embeddings.reduce((sum, emb) => sum + emb[i], 0) / embeddings.length
)
```

### 3. Cache Embeddings (Save Costs)

```typescript
// Cache in database
const embeddingCache = new Map<string, number[]>()

async function getEmbeddingCached(text: string): Promise<number[]> {
  // Check cache
  if (embeddingCache.has(text)) {
    return embeddingCache.get(text)!
  }
  
  // Generate embedding (costs API call)
  const embedding = await embedText(text)
  
  // Cache for future
  embeddingCache.set(text, embedding)
  
  return embedding
}

// For your CMS: Store embeddings in pages table
// ALTER TABLE pages ADD COLUMN embedding BLOB;

// Only recompute when page name/slug changes
if (page.nameChanged || page.slugChanged) {
  page.embedding = await embedText(page.name + page.slug)
}
```

**Savings**: 1M page views with search → $20 API cost if cached, $20,000 if not!

### 4. Monitor Embedding Quality

```typescript
// Sanity check: Similar texts should have high similarity
async function testEmbeddingQuality() {
  const tests = [
    { text1: "dog", text2: "puppy", expectedSim: 0.8 },
    { text1: "car", text2: "automobile", expectedSim: 0.9 },
    { text1: "happy", text2: "sad", expectedSim: 0.3 },  // Antonyms
  ]
  
  for (const test of tests) {
    const emb1 = await embedText(test.text1)
    const emb2 = await embedText(test.text2)
    const similarity = cosineSimilarity(emb1, emb2)
    
    console.log(`${test.text1} <-> ${test.text2}: ${similarity.toFixed(2)}`)
    
    if (Math.abs(similarity - test.expectedSim) > 0.2) {
      console.warn('⚠️ Unexpected similarity - check embedding model!')
    }
  }
}
```

---

## Advanced Techniques

### 1. Query Augmentation

**Problem**: Short queries have less context than documents.

**Solution**: Expand query before embedding.

```typescript
// Original query
const query = "contact"

// Augmented query (provides more context)
const augmented = `
  User is searching for: ${query}
  This likely refers to: contact form, contact page, get in touch section
`

const embedding = await embedText(augmented)
// Better matches "Get in Touch" section!
```

### 2. Hybrid Search (Vector + Keyword)

**Best of both worlds**: Semantic understanding + exact matching.

```typescript
async function hybridSearch(query: string, limit: number = 10) {
  // 1. Vector search (semantic)
  const vectorResults = await vectorIndex.search(query, limit * 2)
  
  // 2. Keyword search (exact)
  const keywordResults = await db.pages.findMany({
    where: {
      OR: [
        { name: { contains: query } },
        { slug: { contains: query } }
      ]
    },
    take: limit * 2
  })
  
  // 3. Merge and deduplicate
  const merged = new Map()
  
  // Add vector results (score = similarity)
  for (const r of vectorResults) {
    merged.set(r.id, { ...r, score: r.score * 0.7 })  // 70% weight
  }
  
  // Add keyword results (score = 1.0 for exact match)
  for (const r of keywordResults) {
    const existing = merged.get(r.id)
    if (existing) {
      existing.score += 0.3  // Boost if in both results
    } else {
      merged.set(r.id, { ...r, score: 0.3 })
    }
  }
  
  // Sort by combined score
  return Array.from(merged.values())
    .sort((a, b) => b.score - a.score)
    .slice(0, limit)
}
```

**Benefit**: Handles both semantic ("contact" → "get in touch") and exact ("hero-section" slug).

### 3. Fine-Tuning Embeddings

**When to fine-tune**:
- Domain-specific terminology (legal, medical, technical)
- Poor retrieval quality on your data
- High-volume application (ROI on fine-tuning cost)

**Example** (Sentence Transformers):
```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# Load base model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Prepare training data (positive pairs)
train_examples = [
  InputExample(texts=['hero section', 'banner component'], label=1.0),
  InputExample(texts=['contact form', 'get in touch'], label=1.0),
  InputExample(texts=['blog post', 'article entry'], label=1.0),
  # ... 1000+ examples from your CMS
]

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Fine-tune
train_loss = losses.CosineSimilarityLoss(model)
model.fit(
  train_objectives=[(train_dataloader, train_loss)],
  epochs=3,
  warmup_steps=100
)

# Save fine-tuned model
model.save('cms-embeddings-v1')
```

**Result**: 10-20% accuracy improvement on domain-specific queries.

---

## Key Takeaways

1. **Embeddings = semantic vectors**: Text → numbers capturing meaning
2. **Dimensions matter**: 384 (fast) vs 1536 (accurate) vs 3072 (best)
3. **OpenAI text-embedding-3**: Current SOTA, $0.02/1M tokens, 1536 dims
4. **Sentence Transformers**: Free, open-source, 384-768 dims, self-hosted
5. **Your codebase**: Uses OpenAI embeddings + LanceDB for semantic search
6. **Best practices**: Normalize text, chunk long content, cache embeddings
7. **Hybrid search**: Combine vector (semantic) + keyword (exact) for best results
8. **Token limit**: 8,191 tokens max input for text-embedding-3

---

## Further Reading

### Documentation
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Sentence Transformers Docs](https://sbert.net/)
- [Hugging Face: Sentence Transformers Models](https://huggingface.co/sentence-transformers)

### Research Papers
- [Sentence-BERT: Sentence Embeddings using Siamese BERT Networks](https://arxiv.org/abs/1908.10084)
- [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816)

### Practical Guides
- [Stack Overflow: Intuitive Introduction to Text Embeddings](https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/)
- [Simplr: Comparing Embedding Models](https://dev.to/simplr_sh/comparing-popular-embedding-models-choosing-the-right-one-for-semantic-similarity-and-rag-4nej)

---

## Codebase Integration

**Files Using Embeddings**:
- [`server/services/vector-index.ts`](../../server/services/vector-index.ts) - OpenAI embeddings + LanceDB
- [`server/tools/all-tools.ts`](../../server/tools/all-tools.ts) - search.vector tool
- [`.env`](../../.env.example) - EMBEDDINGS_PROVIDER, EMBEDDING_MODEL config

**Related Topics**:
- [0.3.1 Tokenization](./0.3.1-tokenization.md) - Input processing for embeddings
- [0.3.3 Vector Similarity](./0.3.3-vector-similarity.md) - Comparing embeddings
- [5.1.1 Embedding Documents](../5-rag/5.1.1-embedding-documents.md) - RAG patterns

---

**Next Topic**: [0.3.3 Vector Similarity Metrics](./0.3.3-vector-similarity.md)  

---

**Last Updated**: 2025-11-16  
**Status**: ✅ Complete  
**Sources**: 10+ authoritative references including OpenAI, Sentence Transformers, Hugging Face
