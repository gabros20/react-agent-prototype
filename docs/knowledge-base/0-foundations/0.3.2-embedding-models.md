# 0.3.2 Embedding Models & Vector Spaces

## TL;DR

Embeddings are dense, fixed-size vector representations that capture semantic meaning, enabling machines to understand that "king" and "monarch" are similar despite sharing no characters; production systems use models like OpenAI text-embedding-3, Voyage AI, or open-source SBERT for semantic search, RAG, and clustering.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [0.3.1 Tokenization](./0.3.1-tokenization.md)
- **Grounded In**: MTEB Benchmark (2025), OpenAI, Voyage AI, Cohere, Sentence Transformers

## Table of Contents

- [Overview](#overview)
- [The Problem: Semantic Understanding](#the-problem-semantic-understanding)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Embeddings** are dense, fixed-size vector representations of text that capture semantic meaning. They enable machines to understand that "puppy" and "dog" are related, even though they share few characters.

```
Text: "artificial intelligence"
  ↓ Tokenization
Tokens: ["artificial", "intelligence"]
  ↓ Embedding Model
Vector: [0.23, -0.45, 0.67, ..., 0.12]  (1536 dimensions)
         ↓
   Semantic meaning captured in numbers
```

**Why embeddings matter**:

1. **Semantic search**: Find "dog" when user searches "puppy"
2. **RAG systems**: Retrieve relevant context for LLMs
3. **Similarity comparison**: Calculate how related two texts are
4. **Clustering**: Group similar content automatically
5. **Classification**: Categorize text by meaning

**Key Research Findings (2025)**:

- **Voyage-3-large**: Current MTEB retrieval leader, wide gap over competitors
- **OpenAI text-embedding-3-large**: Strong second tier, 3072 dimensions, $0.13/1M tokens
- **Cohere Embed v3**: 100+ languages, good for multilingual
- **MTEB Caution**: Self-reported results often inflated; test on your data

**Date Verified**: 2025-12-03

## The Problem: Semantic Understanding

### The Classic Challenge

Computers can't understand that "queen" and "king" are related using traditional text matching:

```
Traditional (one-hot encoding):
"king"  → [1, 0, 0, 0, 0, ...]  (50k dimensions, mostly zeros)
"queen" → [0, 1, 0, 0, 0, ...]  (no relationship captured!)

Cosine similarity: 0.0 (orthogonal vectors!)
```

**Problems**:

- ❌ **No semantic relationship**: "dog" and "puppy" are completely unrelated
- ❌ **Vocabulary explosion**: Each word needs its own dimension
- ❌ **Sparse representations**: Mostly zeros, inefficient storage
- ❌ **Exact matching only**: "contact form" won't match "get in touch"

### Why This Matters

Without semantic understanding:

- Search fails on synonyms ("puppy" won't find "dog" content)
- RAG retrieves irrelevant context (hurts LLM quality)
- Clustering requires manual labeling
- Duplicate detection misses paraphrases

## Core Concept

### From Words to Vectors

**The solution**: Map text to high-dimensional space where similar meanings → nearby vectors.

```
Embedding space:
"king"  → [0.3, 0.8, -0.2, 0.5, ...]  (768 dims, dense)
"queen" → [0.4, 0.7, -0.1, 0.6, ...]  (similar values!)
                ↓
        Cosine similarity: 0.91 (very similar!)
```

**Key Properties**:

1. **Dense**: Most values are non-zero (vs sparse one-hot)
2. **Fixed-size**: Always same dimensionality (384, 768, 1536, 3072)
3. **Semantic**: Similar meanings → nearby vectors
4. **Learned**: Trained on massive datasets via contrastive learning

**Analogy**: Embeddings are GPS coordinates for meaning—nearby coordinates = similar concepts.

### Model Comparison (2025)

| Model | Dims | Cost/1M | MTEB Score | Best For |
|-------|------|---------|------------|----------|
| **Voyage-3-large** | 1024 | $0.18 | #1 | Highest relevance |
| **text-embedding-3-large** | 3072 | $0.13 | Top tier | General purpose |
| **text-embedding-3-small** | 1536 | $0.02 | Good | Cost-effective |
| **Cohere Embed v3** | 1024 | $0.10 | Good | Multilingual (100+) |
| **all-MiniLM-L6-v2** | 384 | Free | Good | Fast, local |
| **BGE-large-en-v1.5** | 1024 | Free | Excellent | Open-source best |

**Important**: MTEB results are self-reported and often inflated. Always test on your actual data.

### Dimensionality Trade-offs

| Dimensions | Pros | Cons |
|------------|------|------|
| **Small** (384) | Fast search, less storage | Less nuanced |
| **Medium** (768-1024) | Balanced | Sweet spot |
| **Large** (1536-3072) | Most accurate | Slower search, more storage |

**Practical guidance**:

- **384**: Good enough for most apps (95% quality at 25% cost)
- **768-1024**: Production sweet spot
- **1536+**: When accuracy is critical (legal, medical, financial)

## Implementation Patterns

### Pattern 1: OpenAI Embeddings (Recommended)

**Use Case**: Production systems needing reliability and multilingual support

```typescript
async function embedText(text: string): Promise<number[]> {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'text-embedding-3-small',  // 1536 dims, $0.02/1M
      input: text
    })
  })

  const { data } = await response.json()
  return data[0].embedding  // [0.23, -0.45, ..., 0.12]
}
```

**Dimensionality reduction** (same model, smaller vectors):

```typescript
{
  model: 'text-embedding-3-small',
  input: text,
  dimensions: 512  // Reduce 1536 → 512 (3x smaller!)
}

// Trade-off: ~2-3% accuracy loss, 3x faster search
```

**Pros**:

- ✅ High quality, multilingual (100+ languages)
- ✅ Flexible dimensions (512, 1024, 1536)
- ✅ No infrastructure needed

**Cons**:

- ❌ API costs at scale
- ❌ Data sent to OpenAI

### Pattern 2: Voyage AI (Highest Quality)

**Use Case**: RAG systems where retrieval quality is paramount

```typescript
async function embedWithVoyage(text: string): Promise<number[]> {
  const response = await fetch('https://api.voyageai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.VOYAGE_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'voyage-3-large',
      input: text
    })
  })

  const { data } = await response.json()
  return data[0].embedding
}
```

**Pros**:

- ✅ MTEB retrieval leader (wide gap over competitors)
- ✅ "Tricky negatives" training—pushes similar-but-wrong items apart
- ✅ Stanford AI research backing

**Cons**:

- ❌ Higher cost ($0.18/1M tokens)
- ❌ Newer company (less track record)

### Pattern 3: Self-Hosted (SBERT)

**Use Case**: Privacy-sensitive apps, high-volume, cost optimization

```typescript
import { pipeline } from '@xenova/transformers'

const embedder = await pipeline(
  'feature-extraction',
  'Xenova/all-MiniLM-L6-v2'
)

async function embedLocal(text: string): Promise<number[]> {
  const output = await embedder(text, {
    pooling: 'mean',
    normalize: true
  })
  return Array.from(output.data)  // 384-dim vector
}
```

**Pros**:

- ✅ Free (no API costs)
- ✅ Private (data never leaves your infra)
- ✅ Fast (no network latency)

**Cons**:

- ❌ Lower quality than API models
- ❌ Requires compute resources
- ❌ English-focused (multilingual models exist but weaker)

### Pattern 4: Hybrid Search (Vector + Keyword)

**Use Case**: Best of both worlds—semantic understanding + exact matching

```typescript
async function hybridSearch(query: string, limit: number = 10) {
  // 1. Vector search (semantic)
  const vectorResults = await vectorIndex.search(query, limit * 2)

  // 2. Keyword search (exact)
  const keywordResults = await db.pages.findMany({
    where: {
      OR: [
        { name: { contains: query } },
        { slug: { contains: query } }
      ]
    },
    take: limit * 2
  })

  // 3. Merge with weighted scoring
  const merged = new Map()

  for (const r of vectorResults) {
    merged.set(r.id, { ...r, score: r.score * 0.7 })  // 70% weight
  }

  for (const r of keywordResults) {
    const existing = merged.get(r.id)
    if (existing) {
      existing.score += 0.3  // Boost if in both
    } else {
      merged.set(r.id, { ...r, score: 0.3 })
    }
  }

  return Array.from(merged.values())
    .sort((a, b) => b.score - a.score)
    .slice(0, limit)
}
```

**Benefit**: Handles both semantic ("contact" → "get in touch") and exact ("hero-section" slug).

## When to Use This Pattern

### ✅ Use OpenAI/Voyage When:

1. **Multilingual content**: 100+ languages supported
2. **No infrastructure**: Don't want to manage GPU servers
3. **Quality priority**: Best retrieval accuracy matters
4. **RAG systems**: Feeding context to LLMs

### ✅ Use Self-Hosted (SBERT) When:

1. **Privacy required**: Data can't leave your infrastructure
2. **High volume**: 10M+ embeddings (API costs prohibitive)
3. **Low latency**: Need <10ms embedding time
4. **English-only**: Majority of self-hosted models are English-focused

### ✅ Use Hybrid Search When:

1. **Mixed query types**: Some semantic, some exact-match
2. **Structured + unstructured**: Technical IDs + natural language
3. **Safety net**: Keyword backup for edge cases

### Embedding Model Selection

| Your Situation | Recommended Model |
|----------------|-------------------|
| General production | text-embedding-3-small ($0.02/1M) |
| Quality-critical RAG | Voyage-3-large ($0.18/1M) |
| Multilingual | Cohere Embed v3 ($0.10/1M) |
| Budget-sensitive | all-MiniLM-L6-v2 (free) |
| Open-source best | BGE-large-en-v1.5 (free) |
| Max privacy | Self-hosted SBERT |

## Production Best Practices

### 1. Cache Embeddings (Massive Cost Savings)

```typescript
async function getEmbeddingCached(text: string): Promise<number[]> {
  const hash = crypto.createHash('md5').update(text).digest('hex')

  // Check cache
  const cached = await redis.get(`emb:${hash}`)
  if (cached) return JSON.parse(cached)

  // Generate and cache
  const embedding = await embedText(text)
  await redis.set(`emb:${hash}`, JSON.stringify(embedding), 'EX', 86400)

  return embedding
}

// Savings: 1M queries → $20 if cached, $20,000 if not!
```

### 2. Chunk Long Text (8k Token Limit)

```typescript
function chunkText(text: string, maxTokens: number = 8000): string[] {
  const enc = tiktoken.get_encoding('cl100k_base')
  const tokens = enc.encode(text)

  if (tokens.length <= maxTokens) return [text]

  const chunks: string[] = []
  for (let i = 0; i < tokens.length; i += maxTokens) {
    chunks.push(enc.decode(tokens.slice(i, i + maxTokens)))
  }

  return chunks
}

// Embedding strategy for chunks:
// Option 1: Use first chunk (summary often at start)
// Option 2: Average all chunk embeddings
// Option 3: Store each chunk separately
```

### 3. Normalize Before Embedding

```typescript
function normalizeForEmbedding(text: string): string {
  return text
    .toLowerCase()
    .replace(/[^\w\s-]/g, ' ')   // Remove special chars
    .replace(/\s+/g, ' ')        // Collapse whitespace
    .trim()
}

// More consistent embeddings, fewer edge cases
```

### 4. Test on Your Data (Not Just Benchmarks)

```typescript
async function testEmbeddingQuality() {
  const tests = [
    { text1: "dog", text2: "puppy", expected: 0.8 },
    { text1: "car", text2: "automobile", expected: 0.9 },
    { text1: "happy", text2: "sad", expected: 0.3 },  // Antonyms
  ]

  for (const test of tests) {
    const similarity = await computeSimilarity(test.text1, test.text2)
    console.log(`${test.text1} <-> ${test.text2}: ${similarity.toFixed(2)}`)

    if (Math.abs(similarity - test.expected) > 0.2) {
      console.warn('⚠️ Unexpected result - test on your actual data!')
    }
  }
}
```

**Critical**: MTEB benchmarks are often inflated. Models fine-tuned on MTEB datasets show artificially high scores. Always validate on your production data.

### Common Pitfalls

**❌ Trusting benchmarks blindly**:

Many MTEB results are self-reported and inflated. Test on your actual queries.

**❌ Over-dimensioning**:

3072-dim embeddings aren't always better than 384. Measure quality vs cost for your use case.

**❌ Ignoring token limits**:

text-embedding-3 silently truncates at 8191 tokens. Always check length first.

**❌ Not caching**:

Re-embedding the same text costs money every time. Cache aggressively.

## Key Takeaways

1. **Embeddings = semantic vectors** - Text → numbers capturing meaning
2. **Voyage-3-large leads 2025** - Widest gap on retrieval benchmarks
3. **OpenAI is reliable** - $0.02/1M, 1536 dims, 100+ languages
4. **MTEB caution** - Self-reported, often inflated; test on your data
5. **Cache everything** - 1000x cost savings at scale

**Quick Implementation Checklist**:

- [ ] Choose model based on quality/cost/privacy needs
- [ ] Implement embedding caching (Redis, database)
- [ ] Handle text chunking for >8k tokens
- [ ] Normalize text before embedding
- [ ] Test on your actual queries, not just benchmarks
- [ ] Consider hybrid search for production

## References

1. **Pinecone** (2025). "Choosing an Embedding Model". https://www.pinecone.io/learn/series/rag/embedding-models-rundown/
2. **Document360** (2025). "Text Embedding Models Compared". https://document360.com/blog/text-embedding-model-analysis/
3. **Elephas** (2025). "13 Best Embedding Models in 2025". https://elephas.app/blog/best-embedding-models
4. **DataStax** (2025). "Best Embedding Models for Information Retrieval". https://dev.to/datastax/the-best-embedding-models-for-information-retrieval-in-2025-3dp5
5. **Hugging Face** (2025). "MTEB: Massive Text Embedding Benchmark". https://huggingface.co/blog/mteb
6. **OpenAI** (2025). "Embeddings Guide". https://platform.openai.com/docs/guides/embeddings
7. **Sentence Transformers** (2025). Documentation. https://sbert.net/
8. **Voyage AI** (2025). Official Documentation. https://docs.voyageai.com/

**Related Topics**:

- [0.3.1 Tokenization](./0.3.1-tokenization.md)
- [0.3.3 Vector Similarity Metrics](./0.3.3-vector-similarity.md)
- [5.1.1 Embedding Documents for RAG](../5-rag/5.1.1-embedding-documents.md)

**Layer Index**: [Layer 0: Foundations](../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations-prerequisites)
