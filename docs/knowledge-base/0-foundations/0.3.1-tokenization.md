# 0.3.1 Tokenization - BPE, WordPiece, and SentencePiece

## TL;DR

Tokenization converts text into discrete subword units that language models can process; the three dominant algorithms—BPE, WordPiece, and SentencePiece—balance vocabulary size with sequence length, enabling models to handle unknown words, morphology, and multiple languages effectively.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [0.1.1 What is a Large Language Model?](./0.1.1-llm-intro.md)
- **Grounded In**: OpenAI tiktoken, Hugging Face Tokenizers, Google SentencePiece, GitHub BPE Research (2024-2025)

## Table of Contents

- [Overview](#overview)
- [The Problem: Text to Numbers](#the-problem-text-to-numbers)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Tokenization** is the bridge between human-readable text and numerical representations that neural networks understand. Every LLM API call, every embedding, every cost calculation depends on how text is converted to tokens.

```
"Hello world!" → Tokenization → [15496, 995, 0] → LLM
```

**Why it matters**:

1. **Cost**: API pricing is per token ($0.15-$15 per 1M tokens)
2. **Context limits**: Token counts determine what fits in context windows
3. **Performance**: Efficient tokenization → smaller sequences → faster inference
4. **Multilingual**: Quality of non-English support depends on tokenizer design

**Key Research Findings (2024-2025)**:

- **GitHub's new BPE**: 4x faster than tiktoken, O(n) complexity vs O(n²)
- **tiktoken**: 3-6x faster than comparable open-source tokenizers
- **1 token ≈ 0.75 English words** (average across modern tokenizers)
- **Token efficiency**: Optimized tokenizers reduce inference energy consumption

**Date Verified**: 2025-12-03

## The Problem: Text to Numbers

### The Classic Challenge

LLMs operate on numbers, not text. How do we convert arbitrary text to a fixed vocabulary of integers?

**Naive approaches fail**:

```
Word-level: "ChatGPT" → OOV error (not in 170k word vocabulary)
Character-level: "Hello" → 5 tokens (sequences 5-10x longer)
```

**Problems**:

- ❌ **Word-level**: Massive vocabulary (170k+ English words), can't handle new words
- ❌ **Character-level**: Extremely long sequences, O(n²) attention cost
- ❌ **Language-specific**: Space-based splitting fails for Chinese, Japanese
- ❌ **Morphology ignored**: "run", "running", "runner" treated as unrelated

### Why This Matters

Token count directly impacts:

- **Cost**: 2x tokens = 2x API cost
- **Latency**: Longer sequences = slower inference
- **Context**: Inefficient tokenization wastes precious context window
- **Quality**: Poor tokenization hurts model understanding

## Core Concept

### Subword Tokenization

**The solution**: Break text into **subword units**—smaller than words, larger than characters.

```
"unhappiness" → ["un", "happi", "ness"]
                  ↓       ↓        ↓
               prefix   root    suffix

Model learns compositional meaning:
- "un" = negation
- "happi" = emotion root
- "ness" = noun suffix
```

**Benefits**:

- ✅ Reasonable vocabulary (30k-100k tokens)
- ✅ Handles unknown words (decompose into known subwords)
- ✅ Captures morphology (prefixes, suffixes, roots)
- ✅ Works across languages
- ✅ Balances sequence length and semantic meaning

### The Three Algorithms

```
┌─────────────────────────────────────────────────────────────┐
│                    Subword Tokenization                     │
├─────────────────┬──────────────────┬───────────────────────┤
│      BPE        │    WordPiece     │    SentencePiece      │
│  (GPT, Llama)   │     (BERT)       │    (T5, mT5)          │
├─────────────────┼──────────────────┼───────────────────────┤
│ Frequency-based │ Likelihood-based │ Language-agnostic     │
│ Greedy merging  │ Semantic units   │ No pre-tokenization   │
│ Fully lossless  │ ## continuation  │ ▁ space character     │
└─────────────────┴──────────────────┴───────────────────────┘
```

### Comparison Table

| Feature | BPE | WordPiece | SentencePiece |
|---------|-----|-----------|---------------|
| **Used By** | GPT-2/3/4, Llama, Mistral | BERT, DistilBERT | T5, ALBERT, mT5 |
| **Algorithm** | Greedy frequency merge | Likelihood merge | BPE or Unigram |
| **Pre-tokenization** | Required (space split) | Required | NOT required ✅ |
| **Lossless** | Fully (all spaces) | Lossy (spaces lost) | Partial (1 space) |
| **Multilingual** | Good | Good | Excellent ✅ |
| **Typical Vocab** | 50k-128k | 30k | 32k-256k |
| **Special Tokens** | None | `##` continuation | `▁` space char |

## Implementation Patterns

### Pattern 1: Byte Pair Encoding (BPE)

**Use Case**: GPT models, Llama, Mistral—most production LLMs

**How it works**:

```
Training Process:
1. Start: Character vocabulary {a, b, c, ..., z, space}
2. Count: Most frequent adjacent pairs in corpus
3. Merge: ("l","o") → "lo" (most frequent)
4. Repeat: Until vocabulary reaches target size (50k-128k)

Result: Vocabulary of subwords ordered by frequency
```

**Tokenization example**:

```
Text: "lower lowest"

Step 1: Characters ["l","o","w","e","r"," ","l","o","w","e","s","t"]
Step 2: Apply merges (lo→, low→, lowe→, lower, etc.)
Final:  ["lower", " ", "low", "est"]
```

**Pros**:

- ✅ Simple, fast, deterministic
- ✅ Fully lossless (all spaces preserved)
- ✅ Handles any text (byte-level variant)

**Cons**:

- ❌ Greedy (may not find optimal tokenization)
- ❌ Sensitive to text normalization

**Key Variant: Byte-Level BPE (GPT-4)**

Operates on 256 bytes instead of 140k+ Unicode characters:

```
Base vocabulary: {0x00, 0x01, ..., 0xFF} → 256 symbols
Never OOV: Any byte sequence can be tokenized
```

### Pattern 2: WordPiece

**Use Case**: BERT family models, classification tasks

**Difference from BPE**: Chooses merges that maximize corpus likelihood (semantic coherence), not just frequency.

```
BPE:       Merge most frequent pair
WordPiece: Merge pair that increases P(corpus) most

Score(pair) = P(pair) / (P(symbol1) × P(symbol2))
```

**Tokenization notation**:

```
"unbelievable" → ["un", "##believ", "##able"]
                   ↑       ↑          ↑
                prefix  continuation continuation

## prefix indicates "continuation of previous token"
```

**Pros**:

- ✅ Semantically meaningful units
- ✅ Better morphology handling

**Cons**:

- ❌ Lossy (spaces between tokens lost)
- ❌ Slower training (likelihood computation)

### Pattern 3: SentencePiece

**Use Case**: Multilingual models (T5, mT5), language-agnostic applications

**Key innovation**: No pre-tokenization required.

```
BPE/WordPiece require pre-tokenization:
  English: "Hello world" → ["Hello", "world"] → BPE
  Chinese: "你好世界" → ??? (no spaces!)

SentencePiece treats input as raw sequence:
  English: "Hello world" → [H,e,l,l,o,▁,w,o,r,l,d] → Subword
  Chinese: "你好世界" → [你,好,世,界] → Subword
           Works identically for all languages!
```

**Space handling**: Uses `▁` (Unicode U+2581) to represent spaces as regular characters.

**Pros**:

- ✅ Language-agnostic (same code for all languages)
- ✅ Optional stochastic sampling (training augmentation)
- ✅ Supports both BPE and Unigram algorithms

**Cons**:

- ❌ Partially lossless (multiple spaces → single space)

## When to Use This Pattern

### ✅ Choose BPE/tiktoken When:

1. **Using OpenAI models**: GPT-3.5, GPT-4, GPT-4o
2. **Using Llama/Mistral**: Most open-source LLMs
3. **Need exact token counting**: Cost estimation, context limits
4. **Speed is critical**: tiktoken is 3-6x faster than alternatives

### ✅ Choose WordPiece When:

1. **Using BERT-family**: Classification, NER, sentiment
2. **Semantic segmentation matters**: Morphology-heavy tasks
3. **Fixed vocabulary requirement**: Production embeddings

### ✅ Choose SentencePiece When:

1. **Multilingual applications**: Non-English or mixed-language
2. **Training custom tokenizers**: Need training + inference
3. **Language-agnostic pipeline**: Same code for all languages

### Token Estimation Rules

| Content Type | Tokens per Unit |
|--------------|-----------------|
| English text | 1 word ≈ 1.33 tokens |
| English characters | 4 chars ≈ 1 token |
| Code | 1 line ≈ 5-10 tokens |
| Chinese | 1 character ≈ 1-2 tokens |
| JSON (compact) | 30% fewer than pretty-printed |

## Production Best Practices

### 1. Count Tokens Before API Calls

Estimate costs and check context limits:

```typescript
import tiktoken from 'tiktoken'

const enc = tiktoken.get_encoding('cl100k_base') // GPT-4

function countTokens(text: string): number {
  return enc.encode(text).length
}

// Estimate before expensive call
const promptTokens = countTokens(systemPrompt + userMessage)
const estimatedCost = (promptTokens / 1_000_000) * 0.15 // GPT-4o-mini input
```

### 2. Minimize Token Usage

**Compact JSON** (30% savings):

```typescript
// ❌ Pretty JSON (more tokens)
JSON.stringify(data, null, 2)  // 150 tokens

// ✅ Compact JSON (fewer tokens)
JSON.stringify(data)  // 105 tokens
```

**Structured over verbose** (80% savings):

```typescript
// ❌ Verbose (200 tokens)
"The page 'About Us' was successfully created with slug 'about-us'..."

// ✅ Structured (40 tokens)
{ "page": "About Us", "slug": "about-us", "status": "created" }
```

### 3. Handle Token Limits

Truncate safely to token boundaries:

```typescript
function truncateToTokenLimit(text: string, maxTokens: number): string {
  const enc = tiktoken.get_encoding('cl100k_base')
  const tokens = enc.encode(text)

  if (tokens.length <= maxTokens) return text

  return enc.decode(tokens.slice(0, maxTokens))
}

// Safe truncation for embeddings (8191 token limit)
const safeText = truncateToTokenLimit(content, 8000)
```

### 4. Monitor Token Usage

Track usage for cost optimization:

```typescript
// Log token stats per request
console.log({
  systemTokens: countTokens(systemPrompt),
  toolTokens: countTokens(JSON.stringify(tools)),
  conversationTokens: countTokens(messages),
  totalInput: totalTokens,
  contextUsage: `${(totalTokens / 128000 * 100).toFixed(1)}%`
})
```

### Common Pitfalls

**❌ Different models = different token counts**:

Same text tokenizes differently across models:

```
"Hello world" →
  GPT-4 (cl100k_base): 2 tokens
  BERT (WordPiece): 2 tokens
  Gemini (SentencePiece): 2 tokens

But complex text can vary 20-30%!
```

**❌ Non-English uses more tokens**:

Chinese/Japanese typically use 1.5-2x more tokens than English for equivalent content.

**❌ Embedding truncation is silent**:

Embedding APIs (8191 token limit) truncate without warning. Always check length first.

## Key Takeaways

1. **Subword is optimal** - Balances vocabulary size, sequence length, and OOV handling
2. **BPE dominates production** - GPT, Llama, Mistral all use byte-level BPE
3. **1 token ≈ 0.75 English words** - Use for quick estimation
4. **Token count = cost** - Optimize tokenization to reduce API spend
5. **Different models, different counts** - Always use model-specific tokenizer

**Quick Implementation Checklist**:

- [ ] Install tiktoken for token counting
- [ ] Estimate tokens before API calls (cost/context checks)
- [ ] Use compact JSON for structured data
- [ ] Truncate long content before embedding
- [ ] Monitor token usage in production

## References

1. **OpenAI** (2025). "tiktoken: Fast BPE tokeniser". https://github.com/openai/tiktoken
2. **Karpathy, Andrej** (2025). "minbpe: Minimal BPE implementation". https://github.com/karpathy/minbpe
3. **GitHub Engineering** (2024). "Introducing a faster, more flexible byte-pair tokenizer". https://github.blog/ai-and-ml/llms/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer/
4. **Hugging Face** (2025). "Tokenizer Summary". https://huggingface.co/docs/transformers/en/tokenizer_summary
5. **Google Research** (2018). "SentencePiece: A simple and language independent approach". https://arxiv.org/abs/1808.06226
6. **Sennrich et al.** (2016). "Neural Machine Translation of Rare Words with Subword Units". https://arxiv.org/abs/1508.07909
7. **Raschka, Sebastian** (2025). "Implementing BPE From Scratch". https://sebastianraschka.com/blog/2025/bpe-from-scratch.html
8. **fast.ai** (2025). "Let's Build the GPT Tokenizer". https://www.fast.ai/posts/2025-10-16-karpathy-tokenizers.html

**Related Topics**:

- [0.1.3 Context Windows & Token Limits](./0.1.3-context-windows.md)
- [0.3.2 Embedding Models & Vector Spaces](./0.3.2-embedding-models.md)
- [0.2.4 Cost-Latency-Quality Trade-offs](./0.2.4-tradeoffs.md)

**Layer Index**: [Layer 0: Foundations](../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations-prerequisites)
