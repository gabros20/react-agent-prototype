# 0.2.2 Reasoning Models Deep Dive

## Overview

Reasoning models represent a paradigm shift in AI: instead of answering immediately, they "think" through problems step-by-step, similar to human System 2 thinking. OpenAI's o1 and o3 models pioneered this approach using reinforcement learning to train models that generate hidden chains of thought before producing final answers.

**Key Innovation**: Models generate internal reasoning steps (thinking tokens) that are invisible to users but dramatically improve performance on complex tasks requiring logic, mathematics, and multi-step problem solving.

## How Reasoning Models Work

### 1. Architecture Fundamentals

**Standard LLM Flow**:

```
User Prompt → Token Generation → Immediate Answer
(Single-pass generation, no intermediate reasoning)
```

**Reasoning Model Flow**:

```
User Prompt → Hidden Chain of Thought → Final Answer
              (Thinking Tokens)         (Output Tokens)
```

### 2. Chain of Thought (CoT) Process

Reasoning models break problems into sequential steps:

**Example: Math Problem**

```
Problem: If John has 8 apples and buys 3 more bags with 4 apples each, how many total?

Standard LLM: "20 apples" (may be wrong)

Reasoning Model (Hidden Thoughts):
1. Starting amount: 8 apples
2. Buys 3 bags with 4 apples each
3. Calculate bags: 3 × 4 = 12 apples
4. Add to starting: 8 + 12 = 20 apples
5. Verify: 8 (original) + 12 (new) = 20 ✓

Final Answer: "20 apples"
```

**Six-Step Reasoning Process** (from research):

1. **Problem Analysis** - Reformulate and identify constraints
2. **Task Decomposition** - Break into manageable sub-problems
3. **Systematic Execution** - Build solution step-by-step
4. **Error Detection** - Identify mistakes in reasoning
5. **Alternative Exploration** - Try multiple solution paths
6. **Verification** - Validate final answer

### 3. Training Process

**Stage 1: Standard Pretraining**

-   Start with base LLM (GPT-4o class model)
-   Trained on trillions of tokens (text from internet)

**Stage 2: Chain-of-Thought Fine-tuning**

-   Dataset of problems with **step-by-step reasoning**
-   Example format:
    ```
    Problem: Solve 2x + 5 = 13
    Reasoning:
    - Subtract 5 from both sides: 2x = 8
    - Divide both sides by 2: x = 4
    - Verify: 2(4) + 5 = 13 ✓
    Answer: x = 4
    ```

**Stage 3: Reinforcement Learning (RL)**

-   **Process Supervision**: Reward each reasoning step, not just final answer
-   Model explores thousands of reasoning paths
-   Learns which strategies lead to correct solutions
-   Self-corrects mistakes during reasoning

**Key Training Signal**: Process rewards > Outcome rewards

-   ❌ Old: Reward only if final answer correct
-   ✅ New: Reward logical intermediate steps even if final answer wrong

### 4. Thinking Tokens (Hidden Reasoning)

**What Are Thinking Tokens?**

-   Internal "monologue" generated before final answer
-   Not visible in API response or ChatGPT interface
-   Can range from 100s to 10,000s of tokens per query
-   Used for working memory, exploration, error correction

**Token Flow in o1**:

```
User Input Tokens:  "Solve this calculus problem..."
                           ↓
Thinking Tokens:    [Hidden reasoning - 5,000 tokens]
                    - Try integration by parts
                    - Check substitution method
                    - Verify with chain rule
                    - Calculate derivative to confirm
                           ↓
Output Tokens:      "The solution is: [detailed answer]"
```

**Why Hidden?**

-   Prevents gaming the system (users can't "coach" reasoning)
-   Protects OpenAI's training methodology
-   Reduces API response size (only send final answer)
-   Allows messy exploration without confusing users

**Cost Implications** (2025 Pricing):

-   **o1-preview**: $15/M input + $60/M output (includes thinking tokens in output cost)
-   **o1-mini**: $3/M input + $12/M output
-   Thinking tokens charged as output tokens (expensive!)
-   Typical query: 200 input + 5,000 thinking + 500 output = $0.36 per query (o1-preview)

## OpenAI o1 vs o3: Technical Comparison

### o1 Model (Released September 2024)

**Architecture**:

-   Built on GPT-4o foundation
-   Large-scale RL training with process supervision
-   Adaptive reasoning depth based on problem complexity

**Benchmark Performance**:
| Benchmark | o1-preview | GPT-4o | Improvement |
|-----------|-----------|--------|-------------|
| **AIME 2024** (Math) | 83.3% | 13.4% | 6.2x better |
| **Codeforces** (Coding) | 1673 rating (89th %ile) | - | Top 11% globally |
| **GPQA Diamond** (Science) | 78% | 50.6% | 27.4% absolute |
| **ARC-AGI** (Visual Reasoning) | 32% | 5% | 6.4x better |

**Thinking Token Usage**:

-   Average: 2,000-5,000 tokens per complex query
-   Simple queries: 200-500 tokens
-   Hard math/coding: 10,000+ tokens
-   Cost per query: $0.10-$0.60 (typical)

### o3 Model (Released December 2024)

**Architecture**:

-   Next-generation reasoning model
-   Enhanced with "program synthesis" approach
-   Creates mini-programs to solve tasks
-   Three compute modes: low, medium, high

**Benchmark Performance**:
| Benchmark | o3 (low) | o3 (high) | o1 | Improvement |
|-----------|---------|-----------|-----|-------------|
| **ARC-AGI** (Visual) | 75.7% | 87.5% | 32% | 2.7x better |
| **AIME 2024** (Math) | 96.7% | - | 83.3% | 13.4% absolute |
| **SWE-bench** (Coding) | 71.7% | - | 48.9% | 22.8% absolute |
| **Codeforces** (Programming) | 2727 ELO | - | 1673 | Top 0.1% globally |

**Computing Modes**:

```
Low Compute:  ~$20 per query   → 75.7% ARC-AGI (efficient)
Medium:       ~$200 per query  → 80-85% estimated
High Compute: ~$3,000 per query → 87.5% ARC-AGI (maximum accuracy)
```

**Key Innovation**: Adaptive thinking time

-   Simple problems: minimal thinking tokens (cost-efficient)
-   Complex problems: deep reasoning (expensive but accurate)
-   User can tune compute budget via API

### What Makes o3 Different?

**1. Program Synthesis**

-   Doesn't just reason textually
-   Generates small programs/algorithms to solve puzzles
-   Example: ARC-AGI visual tasks
    ```python
    # o3 might generate:
    def solve_puzzle(grid):
        # Detect pattern: diagonal fill
        for i in range(len(grid)):
            grid[i][i] = 1
        return grid
    ```

**2. Stronger RL Training**

-   Trained on harder verification tasks
-   More compute during training
-   Better at detecting reasoning errors

**3. Efficiency Optimization**

-   o1: Always uses ~5k thinking tokens
-   o3: Scales thinking tokens dynamically (200-20,000 based on difficulty)

## ARC-AGI Benchmark: The Reasoning Test

**What is ARC-AGI?**

-   Visual reasoning benchmark by François Chollet
-   Tests ability to understand novel patterns (not memorization)
-   400 public training examples, harder private evaluation set
-   Designed to require human-like abstraction

**Example Task**:

```
Input Grid:    Output Grid:
[1 0 0]        [1 1 1]
[0 0 0]   →    [1 0 0]
[0 0 0]        [1 0 0]

Pattern: Fill first column and first row with 1s
```

**Why It's Hard for AI**:

-   Can't brute force (infinite possible patterns)
-   Requires understanding spatial relationships
-   Needs abstraction (what is a "corner"? a "diagonal"?)
-   Tests adaptive intelligence, not training data recall

**Historical Progress**:
| Year | Best Model | Score | Method |
|------|-----------|-------|--------|
| 2019 | GPT-3 | 0% | Standard prompting |
| 2023 | GPT-4 | 5% | Few-shot learning |
| 2024 | o1 | 32% | Reasoning model |
| 2024 | o3-low | 75.7% | Program synthesis + RL |
| 2024 | o3-high | 87.5% | Maximum compute |
| Human | Average | 60% | Natural intelligence |

**Significance**: o3-high surpasses average human performance on abstract reasoning (87.5% vs 60%).

## Reinforcement Learning Deep Dive

### Training Methodology

**1. Reward Model**

-   Trained to evaluate reasoning quality
-   Gives +1 for correct intermediate steps
-   Gives -1 for logical errors
-   Trained on domains with ground truth (math, code, chess)

**2. RL Algorithm**

-   Likely uses Proximal Policy Optimization (PPO)
-   Model generates reasoning chains
-   Reward model scores each step
-   Update policy to favor high-reward reasoning paths

**3. Process Supervision**

```python
# Pseudocode for process supervision

def train_reasoning_model():
    for problem in training_set:
        # Generate reasoning chain
        reasoning_steps = model.generate_thoughts(problem)

        # Score each step (not just final answer)
        step_rewards = []
        for step in reasoning_steps:
            reward = reward_model.score_step(step, problem)
            step_rewards.append(reward)

        # Update model to favor high-reward paths
        loss = compute_rl_loss(reasoning_steps, step_rewards)
        model.update(loss)
```

**4. Self-Play & Iteration**

-   Model solves problems multiple ways
-   Learns from successful reasoning strategies
-   Prunes ineffective approaches
-   Scales to millions of RL iterations

### Challenges & Limitations

**1. Motivated Reasoning**

-   Models may justify wrong answers convincingly
-   Example: Asked "Is 2+2=5?", might generate reasoning to defend it
-   Solution: External verification, not just plausible-sounding reasoning

**2. Domain Specificity**

-   Excels in verifiable domains (math, code, logic)
-   Struggles in subjective areas (creative writing, opinion essays)
-   Works best when "ground truth" exists

**3. Cost-Quality Trade-off**

```
Standard LLM:  $0.001 per query  → 60% accuracy
o1:            $0.30 per query   → 85% accuracy (300x cost)
o3-high:       $3.00 per query   → 95% accuracy (3,000x cost)
```

**When NOT to use reasoning models**:

-   ❌ Simple factual questions ("What's the capital of France?")
-   ❌ Creative tasks (write a poem, brainstorm ideas)
-   ❌ Fast response required (< 5 seconds)
-   ❌ Budget-constrained applications

**When TO use reasoning models**:

-   ✅ Complex math/physics problems
-   ✅ Advanced coding challenges (algorithms, debugging)
-   ✅ Multi-step logical deduction
-   ✅ Scientific research assistance
-   ✅ Tasks where accuracy > speed/cost

## Implementation Considerations

### 1. API Integration

**Current Codebase** (`server/agent/orchestrator.ts`):

```typescript
// Your agent uses GPT-4o-mini (standard model)
model: openai("gpt-4o-mini", { structuredOutputs: true });
```

**Adding o1 Support**:

```typescript
// Option 1: Dynamic model selection based on task
function selectModel(task: Task) {
	if (task.requiresDeepReasoning) {
		return openai("o1-preview"); // Reasoning model
	}
	return openai("gpt-4o-mini"); // Fast standard model
}

// Option 2: Hybrid approach
async function solveComplexProblem(problem: string) {
	// Use o1 for initial reasoning
	const reasoning = await generateText({
		model: openai("o1-preview"),
		prompt: `Solve this step-by-step: ${problem}`,
	});

	// Use fast model for followup formatting
	const formatted = await generateText({
		model: openai("gpt-4o-mini"),
		prompt: `Format this solution: ${reasoning.text}`,
	});

	return formatted.text;
}
```

### 2. Cost Management

**Strategy 1: Selective Reasoning**

```typescript
// Only use reasoning for hard problems
if (task.complexity > 0.7 || task.previousAttemptsFailed) {
	model = openai("o1-mini"); // $0.10 per query
} else {
	model = openai("gpt-4o-mini"); // $0.001 per query
}
```

**Strategy 2: Budget Limits**

```typescript
// Set max thinking tokens (o3 feature)
const response = await generateText({
	model: openai("o3-mini"),
	maxReasoningTokens: 2000, // Cap thinking tokens
	prompt: problem,
});
```

**Strategy 3: Caching**

```typescript
// Cache reasoning for similar problems
const cachedReasoning = await redis.get(`reasoning:${problemHash}`);
if (cachedReasoning) {
	return cachedReasoning; // Save $0.30
}
```

### 3. Quality Monitoring

**Track Reasoning Effectiveness**:

```typescript
interface ReasoningMetrics {
	thinkingTokens: number;
	outputTokens: number;
	cost: number;
	accuracyScore: number;
	latencyMs: number;
}

// Log for analysis
await logReasoningQuery({
	model: "o1-preview",
	thinkingTokens: 4500,
	outputTokens: 800,
	cost: 0.32,
	accuracyScore: 0.95,
	latencyMs: 12000,
});
```

### 4. Your Codebase Integration

**Current Agent Architecture** (from `orchestrator.ts`):

-   Uses ReAct pattern (Reasoning + Acting)
-   Already has chain-of-thought in system prompt
-   Tools for code execution, file operations

**Enhancement Opportunity**:

```typescript
// Add reasoning model for complex planning
async function planComplexTask(userGoal: string): Promise<Plan> {
	// Use o1 for deep planning
	const plan = await generateText({
		model: openai("o1-mini"), // Cost-efficient reasoning
		prompt: `
      Create detailed step-by-step plan for: ${userGoal}
      
      Consider:
      - Edge cases and error handling
      - Dependencies between steps
      - Resource constraints
      - Alternative approaches
      
      Provide reasoning for each step.
    `,
	});

	// Execute plan with standard model (fast)
	for (const step of plan.steps) {
		await executeStep(step, openai("gpt-4o-mini"));
	}
}
```

## Research Papers & Resources

### Key Papers

1. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** (Wei et al., 2022)

    - Original CoT paper showing 5-10x improvements on reasoning tasks
    - https://arxiv.org/abs/2201.11903

2. **STaR: Self-Taught Reasoner** (Zelikman et al., 2022)

    - How models learn reasoning from correct examples
    - https://arxiv.org/abs/2203.14465

3. **RL-STaR: Theoretical Analysis of Reinforcement Learning for Reasoning**

    - Convergence conditions for RL-based reasoning
    - https://arxiv.org/abs/2410.23912

4. **Process Supervision in Reinforcement Learning** (Lightman et al., 2023)
    - Why step-by-step rewards beat outcome-only rewards
    - OpenAI research

### Technical Deep Dives

-   **OpenAI o1 System Card**: Official technical report on architecture, safety, benchmarks
-   **François Chollet's ARC-AGI**: Understanding the visual reasoning benchmark
-   **DeepLearning.AI: Reasoning with o1**: Free course by Colin Jarvis (OpenAI)

## Future Directions

### 1. Open Source Reasoning Models

**DeepSeek R1** (January 2025):

-   Open-source reasoning model
-   27x cheaper than o1 ($0.01 vs $0.30 per query)
-   Competitive performance on math/code benchmarks
-   Shows reasoning models becoming commoditized

### 2. Efficiency Improvements

**Trends for 2025-2026**:

-   Adaptive thinking time (o3 approach becoming standard)
-   Smaller reasoning models (o1-mini → future "o1-nano")
-   Speculative reasoning (generate multiple chains in parallel, pick best)
-   Reasoning distillation (teach standard models to reason without RL)

### 3. ARC-AGI-2 Benchmark

**Released March 2025**:

-   Harder version of ARC-AGI
-   o3-high: 87.5% on ARC-AGI-1 → **4%** on ARC-AGI-2
-   Shows reasoning models still far from true AGI
-   Benchmarks evolving faster than models

### 4. Hybrid Architectures

**Emerging Pattern**:

```
Fast Model (GPT-4o-mini): Quick tasks, formatting
         ↓
Reasoning Model (o1): Complex planning, verification
         ↓
Specialized Model: Domain-specific (medical, legal)
```

## Key Takeaways

**Technical Summary**:

1. Reasoning models use hidden chain-of-thought to solve problems step-by-step
2. Trained with RL + process supervision to reward logical reasoning
3. o1: 6x better at math, o3: 2.7x better at visual reasoning
4. Thinking tokens are expensive (10-100x cost increase)
5. Best for verifiable domains (math, code, logic)

**Architectural Insights**:

-   Not a new architecture (still transformers)
-   Innovation is in training (RL at scale)
-   Process supervision > outcome supervision
-   Adaptive compute allocation (o3 innovation)

**Practical Guidance**:

-   Use selectively for complex tasks only
-   Implement cost controls (budget limits, caching)
-   Monitor reasoning effectiveness (tokens vs accuracy)
-   Hybrid approach: reasoning for planning, fast model for execution

**Your Agent**:

-   Currently optimized for speed/cost (GPT-4o-mini)
-   Could add o1-mini for complex planning phases
-   Keep standard model for tool execution (fast)
-   Estimated cost increase: $10/month → $30/month with selective reasoning

## Navigation

-   [← Previous: 0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)
-   [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
-   [→ Next: 0.2.3 When to Use Which Model](./0.2.3-when-to-use-which.md)

---

_Part of Layer 0: Foundations - Understanding how reasoning models work internally_
