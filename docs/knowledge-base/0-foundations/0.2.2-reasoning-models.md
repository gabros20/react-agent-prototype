# 0.2.2 Reasoning Models Deep Dive

## TL;DR

Reasoning models use reinforcement learning to generate hidden chains of thought before answering, achieving 6-8× improvements on math/science through internal self-verification—they're trained with process supervision (rewarding each reasoning step) rather than outcome-only rewards, enabling emergent behaviors like backtracking and alternative exploration.

-   **Status**: ✅ Complete
-   **Last Updated**: 2025-11-28
-   **Prerequisites**: [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)
-   **Grounded In**: OpenAI o3 architecture (2024-2025), DeepSeek-R1 paper (2025), Qwen3 Technical Report (2025), Process Supervision research (Lightman et al., 2023)

## Table of Contents

-   [Overview](#overview)
-   [The Problem: Why Standard Training Fails](#the-problem-why-standard-training-fails)
-   [Core Concept](#core-concept)
-   [Implementation Patterns](#implementation-patterns)
-   [Framework Integration](#framework-integration)
-   [Research & Benchmarks](#research--benchmarks)
-   [When to Use This Pattern](#when-to-use-this-pattern)
-   [Production Best Practices](#production-best-practices)
-   [Trade-offs & Considerations](#trade-offs--considerations)
-   [Key Takeaways](#key-takeaways)
-   [References](#references)

## Overview

Reasoning models represent a paradigm shift in AI: instead of answering immediately, they "think" through problems step-by-step, similar to human System 2 thinking. OpenAI's o1 and o3 models pioneered this approach using reinforcement learning to train models that generate hidden chains of thought before producing final answers.

**Key Innovation**: Models generate internal reasoning steps (thinking tokens) that are invisible to users but dramatically improve performance on complex tasks requiring logic, mathematics, and multi-step problem solving.

**As of November 2025**: Multiple reasoning model architectures exist:
- **OpenAI o3/o4-mini**: Proprietary, outcome-based RL, program synthesis capabilities
- **DeepSeek-R1**: Open-source, GRPO training, distillation to smaller models
- **Qwen3**: Hybrid thinking/non-thinking modes, four-stage training pipeline

### Key Research Findings (2024-2025)

-   **Training Compute**: o3 used 10× more training compute than o1 (OpenAI)
-   **RL Scaling**: Large-scale RL shows "more compute = better performance" trend
-   **Process Supervision**: Step-by-step rewards outperform outcome-only rewards (OpenAI)
-   **Distillation**: Reasoning patterns transfer to smaller models effectively (DeepSeek)
-   **Hybrid Modes**: Single model can switch between thinking/non-thinking (Qwen3)

**Date Verified**: 2025-11-28

## The Problem: Why Standard Training Fails

### The Classic Challenge

Standard language models are trained with next-token prediction—predicting the most likely next word given context. This works well for text generation but fails on tasks requiring multi-step reasoning.

**Why Next-Token Prediction Struggles**:

```
Standard Training:
Input: "What is 347 × 829?"
Target: "287,663"

Problem: Model learns to predict answer directly
         No intermediate reasoning steps encoded
         Errors compound without verification
```

**Problems**:

-   ❌ **No reasoning trace**: Model jumps to answer without showing work
-   ❌ **No self-correction**: Once committed to a path, model continues
-   ❌ **No verification**: No mechanism to check if answer makes sense
-   ❌ **Shallow pattern matching**: Relies on memorized patterns, not logic

### Why This Matters

**Training Limitation**: Standard fine-tuning can add knowledge but doesn't teach reasoning:

```
Fine-tuning on math problems:
- Model memorizes: "347 × 829 = 287,663"
- Doesn't learn: HOW to multiply any numbers
- Fails on novel: "348 × 830 = ?" (slightly different)
```

**Human Comparison**: Humans use "System 2" thinking for complex problems:

```
System 1: Fast, automatic ("What is 2+2?")
System 2: Slow, deliberate ("What is 347 × 829?")

Standard LLMs: Only System 1-like behavior
Reasoning LLMs: System 2-like deliberation
```

## Core Concept

### What is Reinforcement Learning for Reasoning?

Reasoning models are trained using **reinforcement learning (RL)** where the model learns to generate reasoning chains that lead to correct answers. Unlike standard training, RL provides feedback on the reasoning process itself, not just the final answer.

### Visual Representation

**Standard LLM Training**:

```
Input → Predict Next Token → Compare to Ground Truth → Update Weights
                ↓
         Single-step loss
         No reasoning feedback
```

**Reasoning Model Training (RL)**:

```
Input → Generate Reasoning Chain → Verify Each Step → Reward/Penalty
                    ↓                       ↓
           [Step 1] ──────────────────→ +1 (correct)
           [Step 2] ──────────────────→ +1 (correct)
           [Step 3] ──────────────────→ -1 (error detected)
           [Step 4] ──────────────────→ +1 (self-corrected)
           [Final]  ──────────────────→ +1 (correct answer)
                    ↓
           Update policy to favor this reasoning path
```

**Thinking Token Flow**:

```
User Input: "Solve this calculus problem..."
                    ↓
┌─────────────────────────────────────────────────────┐
│  THINKING TOKENS (Hidden, 5,000 tokens)             │
│  - "Let me try integration by parts..."             │
│  - "Wait, that doesn't work. Try substitution..."   │
│  - "u = x², du = 2x dx"                             │
│  - "Verify: take derivative to confirm..."          │
│  - "Yes, this checks out."                          │
└─────────────────────────────────────────────────────┘
                    ↓
Output: "The solution is: [detailed answer]"
```

### Key Principles

1. **Process Supervision**: Reward each reasoning step, not just final answer
2. **Exploration**: Model tries multiple reasoning paths before selecting best
3. **Self-Verification**: Model learns to check its own work
4. **Adaptive Depth**: Complex problems get more thinking tokens automatically

### Training Methodologies

**OpenAI o3 Approach**:
- Outcome-based RL with well-defined goals
- Model learns test-time strategies autonomously
- Example: Writes brute-force solution to verify optimized one
- 10× more training compute than o1

**DeepSeek-R1 Approach (GRPO)**:
1. **R1-Zero**: Pure RL without supervised fine-tuning
2. **Cold-start data**: Fix repetition/readability issues
3. **Rejection sampling**: Generate new SFT data from RL checkpoint
4. **Two-stage RL**: Reasoning-focused, then general capabilities

**Qwen3 Approach (Four-Stage Pipeline)**:
1. **CoT Cold Start**: Fine-tune on diverse long chain-of-thought data
2. **Reasoning RL**: Scale compute for reasoning strategy exploration
3. **Thinking Mode Fusion**: Integrate non-thinking capabilities
4. **General RL**: 20+ domain tasks for overall enhancement

## Implementation Patterns

### Pattern 1: Outcome-Based RL (OpenAI Style)

**Use Case**: When you have verifiable ground truth (math, code, logic)

**Approach**:

```
Problem → Generate reasoning chain → Check final answer → Reward
                                            ↓
                                    Correct: +1
                                    Wrong: -1
```

**Pros**:

-   ✅ Simple reward signal
-   ✅ Scales with compute
-   ✅ Model discovers own strategies

**Cons**:

-   ❌ Doesn't guide intermediate steps
-   ❌ May develop "motivated reasoning"
-   ❌ Requires verifiable domains

**When to Use**: Math, programming, formal logic with clear right/wrong answers

### Pattern 2: Process Supervision (Step-by-Step Rewards)

**Use Case**: When intermediate reasoning quality matters

**Approach**:

```
Problem → Generate reasoning chain → Score EACH step → Update policy
                    ↓
           Step 1: Parse problem    → +1 (good)
           Step 2: Identify approach → +1 (valid)
           Step 3: Make calculation  → -1 (arithmetic error)
           Step 4: Verify result     → +0.5 (attempted but flawed)
```

**Pros**:

-   ✅ Guides reasoning process
-   ✅ Catches errors earlier
-   ✅ More sample-efficient

**Cons**:

-   ❌ Requires step-level labels (expensive)
-   ❌ More complex reward model

**When to Use**: When you can afford human labeling of reasoning steps

### Pattern 3: Distillation (DeepSeek Style)

**Use Case**: Deploy reasoning capabilities in smaller models

**Approach**:

```
Large Reasoning Model (671B params)
            ↓
   Generate reasoning traces
            ↓
   Fine-tune smaller model on traces
            ↓
Small Distilled Model (7B-32B params)
```

**Pros**:

-   ✅ Cost-efficient deployment
-   ✅ Faster inference
-   ✅ Preserves reasoning patterns

**Cons**:

-   ❌ Performance ceiling below teacher
-   ❌ Requires large teacher model first

**When to Use**: Production deployment where cost matters

### Pattern 4: Hybrid Thinking/Non-Thinking (Qwen3 Style)

**Use Case**: Single model for all use cases

**Approach**:

```
User Query → Classify Complexity → Select Mode
                    ↓
           Simple: Non-thinking mode (fast)
           Complex: Thinking mode (thorough)
```

**Pros**:

-   ✅ Flexible deployment
-   ✅ Cost optimization built-in
-   ✅ Single model to maintain

**Cons**:

-   ❌ Complex training pipeline
-   ❌ Mode selection overhead

**When to Use**: Mixed workloads with varying complexity

## Framework Integration

### AI SDK 6 Reasoning Model Support

AI SDK 6 provides unified APIs for reasoning models across providers, with features for controlling reasoning effort, extracting reasoning traces, and streaming thinking tokens.

**OpenAI o3/o4-mini with Reasoning Effort Control**:

```typescript
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

const { text, usage, providerMetadata } = await generateText({
	model: openai("o3-mini"),
	prompt: "Prove that there are infinitely many prime numbers.",
	providerOptions: {
		openai: {
			reasoningEffort: "high", // low, medium, high
		},
	},
});

// Access reasoning token count
console.log("Reasoning tokens:", providerMetadata?.openai?.reasoningTokens);
console.log("Total tokens:", usage.totalTokens);
```

**DeepSeek Reasoner with Native Reasoning**:

```typescript
import { deepseek } from "@ai-sdk/deepseek";
import { generateText } from "ai";

const { text, reasoning } = await generateText({
	model: deepseek("deepseek-reasoner"),
	prompt: "Explain why P ≠ NP is believed to be true.",
});

// Reasoning trace is available directly
console.log("Internal reasoning:", reasoning);
console.log("Final answer:", text);
```

**Extracting Reasoning from Open-Source Models**:

```typescript
import { groq } from "@ai-sdk/groq";
import { fireworks } from "@ai-sdk/fireworks";
import { generateText, wrapLanguageModel, extractReasoningMiddleware } from "ai";

// Works with Groq, Fireworks, Together.ai, Azure
const enhancedModel = wrapLanguageModel({
	model: groq("deepseek-r1-distill-llama-70b"),
	middleware: extractReasoningMiddleware({ tagName: "think" }),
});

const { reasoning, text } = await generateText({
	model: enhancedModel,
	prompt: "What is the time complexity of quicksort?",
});
```

### Streaming Reasoning Tokens

For real-time UI showing model's thinking process:

```typescript
import { deepseek } from "@ai-sdk/deepseek";
import { streamText } from "ai";

const result = streamText({
	model: deepseek("deepseek-reasoner"),
	prompt: "Solve this differential equation...",
});

for await (const part of result.fullStream) {
	switch (part.type) {
		case "reasoning":
			// Display thinking in collapsed section
			console.log("[Thinking]", part.text);
			break;
		case "text":
			// Display final answer
			console.log("[Answer]", part.text);
			break;
	}
}
```

**Server Route with Reasoning Streaming**:

```typescript
// app/api/reason/route.ts
import { deepseek } from "@ai-sdk/deepseek";
import { streamText, convertToModelMessages, UIMessage } from "ai";

export const maxDuration = 300; // 5 minutes for complex reasoning

export async function POST(req: Request) {
	const { messages }: { messages: UIMessage[] } = await req.json();

	const result = streamText({
		model: deepseek("deepseek-reasoner"),
		messages: convertToModelMessages(messages),
	});

	return result.toDataStreamResponse({
		sendReasoning: true, // Stream thinking tokens to client
	});
}
```

### Key AI SDK 6 Features for Reasoning

| Feature | Purpose | Provider Support |
| ------- | ------- | ---------------- |
| `reasoningEffort` | Control thinking depth | OpenAI (o1, o3, o4-mini) |
| `extractReasoningMiddleware` | Parse `<think>` tags | Groq, Fireworks, Together, Azure |
| `sendReasoning: true` | Stream reasoning to client | All providers |
| `providerMetadata.reasoningTokens` | Token usage tracking | OpenAI |
| `reasoning` property | Access reasoning trace | DeepSeek native |

## Research & Benchmarks

### Process Supervision (OpenAI, 2023)

**Paper**: "Let's Verify Step by Step" (Lightman et al.)

-   **Key Finding**: Process rewards > outcome rewards for math reasoning
-   **Result**: 78% vs 72% accuracy on MATH benchmark
-   **Insight**: Rewarding each step prevents "motivated reasoning"

**Source**: [OpenAI Process Supervision](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision)

### OpenAI o3 Training (2024-2025)

**Key Findings** (from OpenAI livestream):

-   o3 used 10× more training compute than o1
-   RL scaling shows "more compute = better performance"
-   Model discovers own test-time strategies (e.g., brute-force verification)
-   Outcome-based RL, not process-based reward models

**Source**: [VentureBeat Analysis](https://venturebeat.com/ai/five-breakthroughs-that-make-openais-o3-a-turning-point-for-ai-and-one-big-challenge/)

### DeepSeek-R1 (January 2025)

**Paper**: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL"

-   **R1-Zero**: First to show pure RL can develop reasoning (no SFT)
-   **GRPO**: Group Relative Policy Optimization for efficiency
-   **Distillation**: 32B distilled model outperforms o1-mini
-   **Open-source**: Full weights and training details released

| Model | AIME 2024 | MATH-500 | Codeforces |
| ----- | --------- | -------- | ---------- |
| DeepSeek-R1 | 79.8% | 97.3% | 96.3 %ile |
| o1 | 83.3% | 94.8% | 93rd %ile |
| DeepSeek-R1-0528 | 91.4% | 97.3% | 96.3 %ile |

**Source**: [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948)

### Qwen3 (April 2025)

**Paper**: "Qwen3 Technical Report"

-   **Hybrid architecture**: Thinking + non-thinking in single model
-   **Four-stage training**: CoT cold start → Reasoning RL → Fusion → General RL
-   **Thinking budget**: Up to 38K tokens controllable via API
-   **36T tokens**: Double the training data of Qwen2.5

| Model | AIME 2024 | AIME 2025 | Mode |
| ----- | --------- | --------- | ---- |
| Qwen3-235B (Instruct) | 70.3% | N/A | Non-thinking |
| Qwen3-235B (Thinking) | 85.7% | 92.3% | Thinking |

**Source**: [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)

### ARC-AGI Benchmark Evolution

**Historical Progress**:

| Year | Model | ARC-AGI-1 | Method |
| ---- | ----- | --------- | ------ |
| 2019 | GPT-3 | 0% | Standard prompting |
| 2023 | GPT-4 | 5% | Few-shot learning |
| 2024 | o1 | 32% | Reasoning model |
| 2024 | o3-preview (high) | 87.5% | Program synthesis + RL |
| 2025 | o3 released (medium) | 53% | Production version |
| 2025 | o4-mini (medium) | 41% | Cost-efficient |
| Human | Average | 60% | Natural intelligence |

**ARC-AGI-2 (March 2025)**:
- Harder benchmark designed after o3's success
- o3: <3%, o4-mini: <3%, Humans: 60%
- Shows reasoning models still far from general intelligence

## When to Use This Pattern

### ✅ Use Reasoning Models When:

1. **Mathematical Proofs**
   - Requires step-by-step logical deduction
   - Self-verification catches errors

2. **Complex Algorithms**
   - Design requires considering multiple approaches
   - Correctness can be verified

3. **Scientific Analysis**
   - Multi-step reasoning about data
   - Hypothesis testing and verification

4. **Strategic Planning**
   - Requires exploring alternative paths
   - Benefits from backtracking

### ❌ Don't Use When:

1. **Simple Factual Questions**
   - "What is the capital of France?"
   - No reasoning needed, wastes tokens

2. **Creative Generation**
   - Poetry, stories, brainstorming
   - Reasoning adds latency without benefit

3. **Real-Time Applications**
   - Sub-second response required
   - 10-60s latency unacceptable

4. **High-Volume, Low-Value Tasks**
   - Cost per query matters
   - Standard models sufficient

### Decision Matrix

| Task Type | Reasoning Model | Standard Model |
| --------- | --------------- | -------------- |
| Multi-step math | ✅ Primary | ❌ Fallback only |
| Code generation | ✅ Complex algorithms | ✅ Simple CRUD |
| Scientific reasoning | ✅ Primary | ❌ Insufficient |
| Chat/conversation | ❌ Overkill | ✅ Primary |
| Content writing | ❌ Slow | ✅ Primary |
| Summarization | ❌ Unnecessary | ✅ Primary |

## Production Best Practices

### 1. Monitor Thinking Token Usage

**Principle**: Thinking tokens can explode unexpectedly. Track them separately.

```typescript
interface ReasoningMetrics {
	inputTokens: number;
	thinkingTokens: number;
	outputTokens: number;
	totalCost: number;
	latencyMs: number;
}

// Log after each reasoning call
await logMetrics({
	model: "o3-mini",
	inputTokens: 200,
	thinkingTokens: 4500, // Often largest component
	outputTokens: 800,
	totalCost: 0.32,
	latencyMs: 15000,
});
```

**Alert Thresholds**:
- Thinking tokens > 10,000: Investigate query
- Cost > $1/query: Review use case
- Latency > 60s: Consider timeout

### 2. Implement Cost Controls

**Strategy**: Set maximum reasoning budget per query.

```typescript
// Limit thinking tokens (where supported)
const result = await generateText({
	model: openai("o3-mini"),
	prompt: problem,
	providerOptions: {
		openai: {
			reasoningEffort: "low", // Cap thinking depth
		},
	},
});

// Fallback to standard model if reasoning fails/times out
const response = await Promise.race([
	reasoningCall,
	timeout(30000).then(() => fallbackToStandardModel()),
]);
```

### 3. Cache Reasoning Results

**Principle**: Identical problems should return cached reasoning.

```typescript
const problemHash = hashFunction(problem);
const cached = await cache.get(`reasoning:${problemHash}`);

if (cached) {
	return cached; // Skip $0.30 reasoning call
}

const result = await reasoningModel.generate(problem);
await cache.set(`reasoning:${problemHash}`, result, { ttl: 86400 });
```

### 4. Extract and Store Reasoning Traces

**Principle**: Reasoning traces are valuable for debugging and improvement.

```typescript
const { reasoning, text } = await generateText({
	model: enhancedModel,
	prompt: problem,
});

// Store for analysis
await db.insert({
	problem,
	reasoning, // Full thinking trace
	answer: text,
	timestamp: new Date(),
	success: await verifyAnswer(text),
});

// Use for debugging failed queries
if (!success) {
	console.log("Failed reasoning:", reasoning);
}
```

### 5. Use Hybrid Routing

**Principle**: Route based on task complexity to optimize cost.

```typescript
function routeQuery(query: string, context: Context) {
	const signals = analyzeComplexity(query);

	if (signals.requiresDeepReasoning) {
		return {
			model: openai("o3-mini"),
			reasoningEffort: signals.complexity > 0.8 ? "high" : "medium",
		};
	}

	return {
		model: openai("gpt-4o"),
		// No reasoning effort needed
	};
}
```

## Trade-offs & Considerations

### Advantages

1. **6-8× Better Accuracy**: On verifiable tasks (math, code, logic)
2. **Self-Verification**: Models catch and correct own errors
3. **Emergent Strategies**: Models discover novel problem-solving approaches
4. **Transparent Reasoning**: Traces help debug wrong answers
5. **Open-Source Options**: DeepSeek-R1, Qwen3 provide accessible alternatives

### Disadvantages

1. **10-30× Higher Cost**: Thinking tokens are expensive
2. **10-30× Higher Latency**: 10-60 seconds vs sub-second
3. **Domain Limitations**: Best for verifiable tasks
4. **Hidden Tokens**: Can't inspect OpenAI's thinking process
5. **Motivated Reasoning**: May justify wrong answers convincingly

### Cost Analysis

**Per-Query Costs (Typical)**:

| Model | Input | Thinking | Output | Total |
| ----- | ----- | -------- | ------ | ----- |
| GPT-4o | $0.002 | N/A | $0.006 | ~$0.01 |
| o3-mini (low) | $0.002 | ~$0.10 | $0.02 | ~$0.12 |
| o3-mini (high) | $0.002 | ~$0.50 | $0.02 | ~$0.52 |
| DeepSeek-R1 | Open-source | Open-source | Open-source | Compute only |

**Monthly Cost Scenarios** (10,000 queries):

| Strategy | Cost | Notes |
| -------- | ---- | ----- |
| All GPT-4o | $100 | Fast, good for simple tasks |
| All o3-mini (low) | $1,200 | 12× more, better accuracy |
| Hybrid (5% reasoning) | $155 | Best cost-accuracy balance |

## Key Takeaways

1. **RL enables reasoning** — Reinforcement learning teaches models to think step-by-step
2. **Process supervision > outcome supervision** — Rewarding each step beats final-answer-only
3. **Thinking tokens are hidden and expensive** — 2,000-10,000+ tokens per query
4. **Distillation preserves reasoning** — Smaller models can inherit reasoning abilities
5. **Hybrid modes optimize cost** — Single model can switch between thinking/non-thinking
6. **ARC-AGI-2 shows limits** — Even best models score <3% vs 60% for humans
7. **Open-source catching up** — DeepSeek-R1, Qwen3 match proprietary performance
8. **Production requires monitoring** — Track thinking tokens, costs, latency carefully

**Quick Implementation Checklist**:

-   [ ] Choose model based on accuracy vs cost requirements
-   [ ] Implement reasoning effort control (low/medium/high)
-   [ ] Set up thinking token monitoring and alerting
-   [ ] Add reasoning trace extraction for debugging
-   [ ] Implement caching for repeated problems
-   [ ] Use hybrid routing for mixed workloads
-   [ ] Set timeout fallbacks for long-running queries
-   [ ] Store reasoning traces for analysis and improvement

## References

### Foundational Papers

1. **Wei et al.** (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", NeurIPS. [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)

2. **Lightman et al.** (2023). "Let's Verify Step by Step", OpenAI. [https://arxiv.org/abs/2305.20050](https://arxiv.org/abs/2305.20050)

3. **Zelikman et al.** (2022). "STaR: Self-Taught Reasoner", NeurIPS. [https://arxiv.org/abs/2203.14465](https://arxiv.org/abs/2203.14465)

### Reasoning Model Papers

4. **DeepSeek AI** (2025). "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", ArXiv. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)

5. **Qwen Team** (2025). "Qwen3 Technical Report", ArXiv. [https://arxiv.org/abs/2505.09388](https://arxiv.org/abs/2505.09388)

### Technical Resources

6. **OpenAI** (2024). "Introducing OpenAI o3 and o4-mini". [https://openai.com/index/introducing-o3-and-o4-mini/](https://openai.com/index/introducing-o3-and-o4-mini/)

7. **Hugging Face** (2025). "From Zero to Reasoning Hero: How DeepSeek-R1 Leverages RL". [https://huggingface.co/blog/NormalUhr/deepseek-r1-explained](https://huggingface.co/blog/NormalUhr/deepseek-r1-explained)

8. **AI SDK Team** (2025). "AI SDK Reasoning Models Guide". [https://ai-sdk.dev/docs/guides/o1](https://ai-sdk.dev/docs/guides/o1)

### Analysis & Commentary

9. **VentureBeat** (2024). "Five breakthroughs that make OpenAI's o3 a turning point". [https://venturebeat.com/ai/five-breakthroughs-that-make-openais-o3-a-turning-point-for-ai-and-one-big-challenge/](https://venturebeat.com/ai/five-breakthroughs-that-make-openais-o3-a-turning-point-for-ai-and-one-big-challenge/)

10. **The Decoder** (2024). "OpenAI's o3 model shows major gains through RL scaling". [https://the-decoder.com/openais-o3-model-shows-major-gains-through-reinforcement-learning-scaling/](https://the-decoder.com/openais-o3-model-shows-major-gains-through-reinforcement-learning-scaling/)

11. **Sebastian Raschka** (2025). "The State of Reinforcement Learning for LLM Reasoning". [https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training](https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training)

12. **Adaline Labs** (2025). "Inside Reasoning Models: OpenAI o3 and DeepSeek R1". [https://labs.adaline.ai/p/inside-reasoning-models-openai-o3](https://labs.adaline.ai/p/inside-reasoning-models-openai-o3)

**Related Topics**:

-   [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)
-   [0.2.3 When to Use Which Model](./0.2.3-model-comparison.md)
-   [0.2.4 Trade-offs (Cost, Latency, Capabilities)](./0.2.4-tradeoffs.md)
-   [1.1.3 Chain-of-Thought Prompting](../1-prompts/1.1.3-chain-of-thought.md)

**Layer Index**: [Layer 0: Foundations](../../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations)
