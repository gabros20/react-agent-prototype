# 0.2.4 Cost-Latency-Quality Trade-offs

## Overview

Every AI model selection involves balancing three competing factors: **cost** (API pricing), **latency** (response speed), and **quality** (accuracy/capabilities). Understanding these trade-offs is critical for building production systems that are both performant and economically viable.

**The Iron Triangle**: You can optimize for any two, but not all three simultaneously.

```
         Quality
           △
          / \
         /   \
        /     \
       /       \
      /  PICK  \
     /   TWO   \
    /___________\
   Cost ←----→ Latency
```

## Understanding Latency Metrics

### Key Latency Measurements

**1. Time to First Token (TTFT)**
- Time from prompt submission to first token generated
- **Critical for**: Interactive applications (chatbots, voice assistants)
- **User perception**: <500ms feels instant, >2s feels slow
- **What impacts it**: Model size, server load, prompt length

**2. Tokens Per Second (TPS)** 
- Speed of generating subsequent tokens after first
- **Critical for**: Streaming responses, long-form generation
- **User perception**: >50 TPS feels smooth, <20 TPS feels sluggish
- **What impacts it**: Model architecture, batch size, GPU memory

**3. End-to-End Latency (E2E)**
- Total time from request to complete response
- **Formula**: `E2E = TTFT + (output_tokens / TPS)`
- **Critical for**: Batch processing, non-streaming apps
- **Example**: TTFT 1s + 500 tokens at 50 TPS = 11 seconds total

**4. Inter-Token Latency (ITL)**
- Time between consecutive tokens (inverse of TPS)
- **Critical for**: Streaming quality, perceived smoothness
- **Example**: 50 TPS = 20ms ITL (feels smooth), 10 TPS = 100ms ITL (feels choppy)

### Latency Benchmarks (2025 Data)

**Fast Models** (TTFT < 1s, TPS > 80):
| Model | TTFT | TPS | E2E (500 tokens) | Use Case |
|-------|------|-----|-----------------|----------|
| **Gemini 2.0 Flash** | 0.35s | 160 | 3.5s | Customer support |
| **GPT-4o-mini** | 0.62s | 100 | 5.6s | Agents, tools |
| **Claude Haiku** | 0.80s | 85 | 6.7s | High-throughput |

**Balanced Models** (TTFT 1-2s, TPS 40-80):
| Model | TTFT | TPS | E2E (500 tokens) | Use Case |
|-------|------|-----|-----------------|----------|
| **GPT-4o** | 0.85s | 75 | 7.5s | Production apps |
| **Gemini 1.5 Pro** | 1.2s | 60 | 9.5s | Long-context |
| **Claude Sonnet** | 1.4s | 55 | 10.5s | Coding, writing |

**Premium Models** (TTFT > 2s, TPS < 40):
| Model | TTFT | TPS | E2E (500 tokens) | Use Case |
|-------|------|-----|-----------------|----------|
| **Claude Opus** | 2.5s | 35 | 17s | Complex analysis |
| **GPT-4 Turbo** | 1.8s | 40 | 14.3s | Advanced tasks |

**Reasoning Models** (TTFT > 10s, variable TPS):
| Model | TTFT | Thinking Time | E2E (500 tokens) | Use Case |
|-------|------|---------------|-----------------|----------|
| **o1-preview** | 15-30s | 5-10k tokens | 40-60s | Math, logic |
| **o1-mini** | 8-15s | 2-5k tokens | 20-35s | Efficient reasoning |
| **o3-low** | 20-40s | Variable | 50-90s | Hard problems |

### Latency Requirements by Application

**Real-Time Interactive** (<1s TTFT required):
```
✓ Gemini Flash:     0.35s TTFT
✓ GPT-4o-mini:      0.62s TTFT
✓ Claude Haiku:     0.80s TTFT
✗ GPT-4o:           0.85s (marginal)
✗ Claude Sonnet:    1.40s (too slow)
✗ o1-preview:       15s+ (unusable)

Examples: Chatbots, voice assistants, customer support
```

**Streaming Long-Form** (>50 TPS required):
```
✓ Gemini Flash:     160 TPS
✓ GPT-4o-mini:      100 TPS
✓ GPT-4o:           75 TPS
✓ Gemini Pro:       60 TPS
✓ Claude Sonnet:    55 TPS
✗ Claude Opus:      35 TPS (feels sluggish)

Examples: Content generation, article writing, code completion
```

**Batch Processing** (E2E doesn't matter):
```
✓ All models acceptable
✓ Optimize for cost and quality instead
✗ Don't use reasoning models (expensive, overkill)

Examples: Data analysis, translation, summarization jobs
```

## Understanding Cost Structure

### Pricing Models (2025)

**Input vs Output Tokens**:
- Input tokens: 3-5x cheaper than output
- Output tokens: Where costs accumulate
- Thinking tokens (o1/o3): Charged as expensive output

**Cost Per 1M Tokens**:

| Model Tier | Input | Output | Ratio |
|------------|-------|--------|-------|
| **Ultra-Cheap** | $0.10 | $0.40 | 1:4 |
| **Cheap** | $0.15 | $0.60 | 1:4 |
| **Balanced** | $2.50 | $10.00 | 1:4 |
| **Premium** | $15.00 | $75.00 | 1:5 |
| **Reasoning** | $15.00 | $60.00* | 1:4 |

*Includes thinking tokens

### Cost Examples: Real Query Scenarios

**Simple Chatbot Query**:
```
Input:  50 tokens   ("How do I reset my password?")
Output: 100 tokens  (Step-by-step instructions)

Gemini Flash:  $0.10/$0.40 per 1M
  = (50 × $0.10 + 100 × $0.40) / 1M
  = $0.000045 per query
  = $0.05 per 1,000 queries

GPT-4o-mini:   $0.15/$0.60 per 1M
  = (50 × $0.15 + 100 × $0.60) / 1M
  = $0.000068 per query
  = $0.07 per 1,000 queries

Claude Opus:   $15.00/$75.00 per 1M
  = (50 × $15 + 100 × $75) / 1M
  = $0.00825 per query
  = $8.25 per 1,000 queries (121x more expensive!)
```

**Coding Assistant Session**:
```
Input:  500 tokens  (Existing code + instructions)
Output: 800 tokens  (Refactored code + explanation)

GPT-4o-mini:   $0.000555 per query → $0.56 per 1,000
Claude Sonnet: $0.0135 per query   → $13.50 per 1,000 (24x more)
o1-mini:       $0.0195 per query*  → $19.50 per 1,000 (35x more)

*Plus thinking tokens (variable 1000-3000)
```

**Reasoning Task** (Math Problem):
```
Input:  200 tokens  (Complex calculus problem)
Output: 500 tokens  (Solution + explanation)
Thinking: 5000 tokens (Hidden reasoning)

GPT-4o-mini (no reasoning):
  = (200 × $0.15 + 500 × $0.60) / 1M
  = $0.00033 per query

o1-preview (with reasoning):
  = (200 × $15 + 5500 × $60) / 1M
  = $0.333 per query
  
Cost increase: 1000x more expensive for reasoning!
```

### Monthly Cost Projections

**High-Volume Chatbot** (100k queries/month):
```
Scenario: 50 input + 100 output tokens average

Gemini Flash:   $4.50/month    (cheapest)
GPT-4o-mini:    $6.80/month    (+51%)
Claude Haiku:   $13.75/month   (+206%)
GPT-4o:         $275/month     (+6011%)
Claude Sonnet:  $412/month     (+9055%)
Claude Opus:    $825/month     (+18,233%)

Recommendation: Gemini Flash or GPT-4o-mini
```

**Coding Assistant** (1000 sessions/month):
```
Scenario: 500 input + 800 output tokens average

GPT-4o-mini:   $556/month     (baseline)
GPT-4o:        $8,250/month   (+1384%)
Claude Sonnet: $13,500/month  (+2329%)
o1-mini*:      $19,500/month  (+3406%)

*Includes 2000 thinking tokens per query

Recommendation: GPT-4o-mini for most tasks, 
                o1-mini for 5% hardest problems
Hybrid cost:   ~$1,500/month (73% savings)
```

**Autonomous Agent** (10k tool calls/month):
```
Scenario: 200 input + 150 output tokens per tool call

GPT-4o-mini:   $120/month     ← Your current setup
Gemini Flash:  $65/month      (46% cheaper)
GPT-4o:        $2,875/month   (2295% more expensive)

Recommendation: Stick with GPT-4o-mini
                (best tool calling + reasonable cost)
```

## Quality Metrics

### Capability Tiers

**Tier 1: Simple Tasks** (All models capable):
- Basic Q&A, formatting, classification
- Quality difference: 85% → 90% (marginal)
- **Decision driver**: Cost and latency

**Tier 2: Medium Tasks** (Premium models better):
- Content writing, code generation, analysis
- Quality difference: 70% → 85% (noticeable)
- **Decision driver**: Use case criticality

**Tier 3: Complex Tasks** (Premium/reasoning required):
- Advanced math, algorithms, legal analysis
- Quality difference: 40% → 85% (dramatic)
- **Decision driver**: Quality (cost justified)

### Quality Benchmarks (2025)

**Coding** (HumanEval, SWE-bench):
| Model | HumanEval | SWE-bench | Quality Tier |
|-------|-----------|-----------|--------------|
| **Gemini 2.5 Pro** | 99% | 85% | Excellent |
| **Claude Sonnet 3.5** | 86% | 75% | Excellent |
| **o1-mini** | 85% | 71% | Excellent |
| **GPT-4o** | 80% | 68% | Very Good |
| **GPT-4o-mini** | 65% | 48% | Good |
| **Gemini Flash** | 55% | 35% | Adequate |

**Math Reasoning** (AIME, MATH):
| Model | AIME 2024 | MATH | Quality Tier |
|-------|-----------|------|--------------|
| **o3-high** | 96.7% | 95% | Best |
| **o1-preview** | 83.3% | 85% | Excellent |
| **Claude Opus** | 65% | 75% | Very Good |
| **GPT-4o** | 55% | 70% | Good |
| **GPT-4o-mini** | 35% | 50% | Adequate |

**General Intelligence** (MMLU, GPQA):
| Model | MMLU | GPQA Diamond | Quality Tier |
|-------|------|--------------|--------------|
| **Claude Opus 4** | 90% | 85% | Best |
| **GPT-4o** | 88% | 78% | Excellent |
| **Gemini Pro** | 87% | 75% | Excellent |
| **Claude Sonnet** | 85% | 72% | Very Good |
| **GPT-4o-mini** | 75% | 60% | Good |

## The Trade-off Triangle

### Scenario 1: Cost-Optimized (Minimize Spend)

**Constraints**:
- Budget: $100/month max
- Quality: Acceptable (not perfect)
- Latency: Reasonable (not instant)

**Solution**:
```
Primary: Gemini Flash (95% of queries)
- $0.10/$0.40 per 1M tokens
- Very fast (160 TPS)
- Adequate quality for most tasks

Fallback: GPT-4o-mini (5% of queries)
- For tasks needing better reasoning
- Strong tool calling

Monthly cost: $85 for 150k queries
Average latency: 0.4s TTFT
Quality: 80% of premium models
```

**Your Codebase**:
```typescript
// Cost-optimized router
function selectModel(task: Task): Model {
  if (task.complexity < 0.5) {
    return google("gemini-1.5-flash");  // 95% of queries
  }
  return openai("gpt-4o-mini");  // 5% of queries
}

// Expected savings: 40% vs GPT-4o-mini only
```

### Scenario 2: Latency-Optimized (Minimize Wait Time)

**Constraints**:
- Latency: <500ms TTFT required
- Quality: Good enough
- Cost: Moderate budget ($500/month)

**Solution**:
```
Only option: Gemini 2.0 Flash
- 0.35s TTFT (best in class)
- 160 TPS (smoothest streaming)
- $0.10/$0.40 (affordable at scale)

Monthly cost: $450 for 1M queries
Average latency: 0.35s TTFT
Quality: 85% of premium models
```

**Trade-offs**:
- ✓ Fastest possible responses
- ✓ Affordable at high volume
- ✗ Lower quality than Claude/GPT-4
- ✗ Weaker at complex reasoning

### Scenario 3: Quality-Optimized (Maximize Accuracy)

**Constraints**:
- Quality: Must be best possible
- Cost: Secondary concern
- Latency: Can wait for accuracy

**Solution**:
```
Routing by task type:

Coding:           Claude Sonnet 3.5
Math/Logic:       o1-preview
General Analysis: Claude Opus 4
Research:         Gemini 2.5 Pro (2M context)

Monthly cost: $5,000 for 10k high-quality queries
Average latency: 5-15s (acceptable for quality)
Quality: 95%+ (best available)
```

**When to Use**:
- Legal document analysis
- Medical diagnosis assistance
- Financial modeling
- Scientific research
- Critical business decisions

### Scenario 4: Balanced (Production Default)

**Constraints**:
- Balance all three factors
- General-purpose application
- Most common scenario

**Solution**:
```
Primary: GPT-4o-mini (80% of queries)
- Good balance of cost/latency/quality
- Best tool calling for agents
- $0.15/$0.60 per 1M

Premium: Claude Sonnet (15% of queries)
- Complex coding, writing
- $3.00/$15.00 per 1M

Reasoning: o1-mini (5% of queries)
- Hard problems only
- $3.00/$12.00 per 1M (+ thinking)

Monthly cost: $800 for 100k queries
Average latency: 1.5s TTFT
Quality: 90% (excellent for most users)
```

**Your Current Approach**:
```typescript
// From orchestrator.ts
model: openai("gpt-4o-mini", { structuredOutputs: true })

// Analysis: ✓ Excellent balanced choice
// Cost: $10-15/month for typical usage
// Latency: 0.62s TTFT (good for agents)
// Quality: 85% (sufficient for tools)
```

## Advanced Trade-off Strategies

### 1. Cascade with Quality Checks

**Concept**: Try cheap/fast, validate, escalate if needed

```typescript
async function cascadeWithValidation(prompt: string) {
  // Try fast/cheap first
  const fastResult = await generateText({
    model: google("gemini-1.5-flash"),  // $0.10/$0.40
    prompt
  });
  
  // Validate with simple checks
  const quality = assessQuality(fastResult.text);
  
  if (quality.score > 0.8) {
    return fastResult;  // Good enough! Saved $0.01
  }
  
  // Escalate to premium
  const premiumResult = await generateText({
    model: anthropic("claude-3-5-sonnet"),  // $3.00/$15.00
    prompt
  });
  
  return premiumResult;
}

// Average cost: 90% succeed with Flash = $0.001
//               10% need Sonnet = $0.015
// Blended: $0.0024 per query (82% cheaper than always using Sonnet)
```

### 2. Speculative Generation

**Concept**: Generate with multiple models in parallel, pick best

```typescript
async function speculativeGeneration(prompt: string) {
  // Start multiple models simultaneously
  const [fast, medium, slow] = await Promise.allSettled([
    generateText({ model: google("gemini-1.5-flash"), prompt }),
    generateText({ model: openai("gpt-4o-mini"), prompt }),
    generateText({ model: anthropic("claude-3-5-sonnet"), prompt })
  ]);
  
  // Return first acceptable result
  if (fast.status === "fulfilled" && isGoodEnough(fast.value)) {
    return fast.value;  // Best case: fastest and cheapest
  }
  
  if (medium.status === "fulfilled" && isGoodEnough(medium.value)) {
    return medium.value;  // Fallback: balanced
  }
  
  return slow.value;  // Last resort: premium quality
}

// Latency: Min of all models (0.35s best case)
// Cost: Pay for all attempts (expensive but fast)
// Quality: Always get acceptable result
```

### 3. Adaptive Token Budgets

**Concept**: Dynamically adjust output length based on task

```typescript
async function adaptiveGeneration(task: Task) {
  const budget = estimateTokenBudget(task);
  
  return await generateText({
    model: openai("gpt-4o-mini"),
    prompt: task.prompt,
    maxTokens: budget,  // Limit output tokens
    temperature: task.requiresPrecision ? 0.3 : 0.7
  });
}

function estimateTokenBudget(task: Task): number {
  // Simple Q&A: 100 tokens max
  if (task.type === "qa") return 100;
  
  // Code generation: 500 tokens
  if (task.type === "code") return 500;
  
  // Long-form writing: 2000 tokens
  if (task.type === "writing") return 2000;
  
  return 500;  // Default
}

// Benefit: Reduce output tokens (expensive) by 60%
// Cost savings: $0.0006 → $0.0003 per query (50% reduction)
```

### 4. Prompt Caching (50-90% Savings)

**Concept**: Cache repeated context (docs, examples, system prompts)

```typescript
// Without caching: $0.10 per query
const result = await generateText({
  model: anthropic("claude-3-5-sonnet"),
  prompt: `
    ${LARGE_DOCUMENTATION}  // 10k tokens (repeated every query)
    
    User question: ${userQuestion}  // 50 tokens (unique)
  `
});

// With caching: $0.01 per query (90% savings)
const result = await generateText({
  model: anthropic("claude-3-5-sonnet"),
  system: {
    content: LARGE_DOCUMENTATION,
    cache_control: { type: "ephemeral" }  // Cache for 5 min
  },
  prompt: userQuestion
});

// First query: $0.10 (cache miss)
// Next 100 queries: $0.01 each (cache hit)
// Total for 101 queries: $1.10 (instead of $10.10)
// Savings: $9.00 (89%)
```

### 5. Batch Processing (50% Discount)

**Concept**: Delay non-urgent tasks, process in batches

```typescript
// Real-time: $0.60 per 1M output tokens
const results = await Promise.all(
  tasks.map(t => generateText({ model: openai("gpt-4o-mini"), prompt: t }))
);

// Batch (24-hour delay): $0.30 per 1M output tokens
const batch = await openai.batches.create({
  input_file: await uploadBatchFile(tasks),
  endpoint: "/v1/chat/completions",
  completion_window: "24h"
});

// Wait for results (check every hour)
const results = await pollBatchResults(batch.id);

// Cost savings: 50% for acceptable latency increase
```

## Decision Framework Summary

### Quick Decision Tree

```
                     START
                       │
           ┌───────────┴───────────┐
           │                       │
       Budget <$100?          Budget >$1000?
           │                       │
          YES                     YES
           │                       │
    Gemini Flash            Quality critical?
           │                       │
           │                   ┌───┴───┐
           │                  YES      NO
           │                   │       │
           │              Use best   Claude
           │              model for  Sonnet +
           │              each task  GPT-4o
           │                   │
           │                   │
           └───────────┬───────┘
                       │
               Latency critical?
                       │
              ┌────────┴────────┐
             YES               NO
              │                 │
       Gemini Flash      GPT-4o-mini
       (0.35s TTFT)     (balanced)
```

### Cost-Latency-Quality Matrix

```
                Quality →
             Low    Medium    High    Best
        ┌─────────────────────────────────┐
   Fast │ Flash   4o-mini   4o      Opus  │
Latency │                                 │
   Slow │ Flash   Sonnet    Opus    o3    │
        └─────────────────────────────────┘
         $0.10   $0.60     $15     $60
                  ← Cost →
```

### Your Codebase Recommendations

**Current Setup Analysis**:
```typescript
// orchestrator.ts
model: openai("gpt-4o-mini", { structuredOutputs: true })

Evaluation:
✓ Cost:    $10-15/month (excellent)
✓ Latency: 0.62s TTFT (good for agents)
✓ Quality: 85% (sufficient for tool calling)
✓ Balance: Optimal for autonomous agents

Recommendation: Keep as-is!
```

**Optional Enhancements**:
```typescript
// 1. Add reasoning for complex planning (5% of sessions)
if (task.complexity > 0.9) {
  return openai("o1-mini");  // +$5/month for 10x better planning
}

// 2. Add caching for system prompts
const cached = await redis.get(`prompt:${hash}`);
if (cached) return cached;  // Save $3/month

// 3. Implement cascade for simple queries
if (task.complexity < 0.3) {
  return google("gemini-1.5-flash");  // Save $2/month
}

Total enhancements: $10 → $10/month (net $0, but better quality)
```

## Common Pitfalls

### 1. Over-Provisioning

**Mistake**: Using Claude Opus for all tasks

```typescript
// Bad: $5000/month for 10k queries
const result = await generateText({
  model: anthropic("claude-opus-4"),  // Overkill
  prompt: "What's 2+2?"
});
```

**Solution**: Use cheap model for simple tasks

```typescript
// Good: $100/month for same queries
const model = task.complexity < 0.5 
  ? google("gemini-1.5-flash")  // $4/month for 90% of queries
  : anthropic("claude-3-5-sonnet");  // $96/month for 10%
```

### 2. Under-Provisioning

**Mistake**: Using Gemini Flash for complex reasoning

```typescript
// Bad: 40% accuracy on math problems
const result = await generateText({
  model: google("gemini-1.5-flash"),
  prompt: "Solve this differential equation..."
});
```

**Solution**: Use reasoning model for hard problems

```typescript
// Good: 85% accuracy
const result = await generateText({
  model: openai("o1-mini"),  // Worth the cost
  prompt: "Solve this differential equation..."
});
```

### 3. Ignoring Latency

**Mistake**: Using o1-preview for chatbot

```typescript
// Bad: 15-30s response time (users abandon)
const result = await generateText({
  model: openai("o1-preview"),
  prompt: chatMessage
});
```

**Solution**: Use fast model for interactive

```typescript
// Good: <1s response time
const result = await generateText({
  model: google("gemini-1.5-flash"),  // 0.35s TTFT
  prompt: chatMessage
});
```

## Key Takeaways

**The Iron Triangle**:
1. You cannot optimize all three (cost, latency, quality) simultaneously
2. Identify your primary constraint, then optimize the other two
3. Most production systems use multiple models (router pattern)

**Cost Optimization**:
- Gemini Flash: Cheapest ($0.10/$0.40)
- Prompt caching: 50-90% savings on repeated context
- Batch API: 50% discount for non-urgent tasks
- Smart routing: 80% savings vs always using premium

**Latency Optimization**:
- Gemini Flash: Fastest (0.35s TTFT, 160 TPS)
- GPT-4o-mini: Good balance (0.62s TTFT, 100 TPS)
- Avoid reasoning models for interactive (<1s required)
- Stream responses for perceived speed

**Quality Optimization**:
- Claude Sonnet: Best coding (86% HumanEval)
- o1-preview: Best reasoning (83% AIME)
- Claude Opus: Best analysis (90% MMLU)
- Use selectively (5-10% of queries)

**Your Agent Setup**:
- ✓ Current: GPT-4o-mini (optimal balanced choice)
- ✓ Cost: $10-15/month (affordable)
- ✓ Latency: 0.62s TTFT (acceptable)
- ✓ Quality: 85% (good for tools)
- → Enhancement: Add o1-mini for 5% hardest tasks (+$5/month)

## Navigation

- [← Previous: 0.2.3 When to Use Which Model](./0.2.3-when-to-use-which.md)
- [↑ Back to Knowledge Base TOC](../../AI_KNOWLEDGE_BASE_TOC.md)
- [→ Next: Layer 1 - Prompt Engineering](../1-prompt-engineering/)

---

*Part of Layer 0: Foundations - Understanding the cost-latency-quality trade-offs*
