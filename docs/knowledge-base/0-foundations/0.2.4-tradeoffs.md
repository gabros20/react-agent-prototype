# 0.2.4 Cost-Latency-Quality Trade-offs

## TL;DR

Every AI model selection involves balancing three competing factors—cost, latency, and quality—forming an "iron triangle" where optimizing two inevitably compromises the third; production systems use routing strategies to dynamically select models based on task requirements.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [0.1.5 Model Selection Guide](./0.1.5-model-selection.md), [0.2.3 When to Use Which Model](./0.2.3-when-to-use-which.md)
- **Grounded In**: OpenAI/Anthropic/Google pricing (2025), Vellum LLM Leaderboard, AIMultiple Latency Benchmarks

## Table of Contents

- [Overview](#overview)
- [The Problem: Competing Constraints](#the-problem-competing-constraints)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

The **Cost-Latency-Quality Triangle** is the fundamental constraint governing AI model selection. You can optimize for any two factors, but the third will always suffer. Understanding this trade-off is critical for building production systems that are both performant and economically viable.

```
           Quality
             △
            / \
           /   \
          /     \
         / PICK  \
        /   TWO   \
       /___________\
      Cost ←----→ Latency
```

**Key Research Findings (2025)**:

- **Gemini 2.0 Flash**: 0.34s TTFT, 169.5 TPS, $0.15/$0.60 per 1M tokens (best latency/cost)
- **GPT-4o-mini**: 0.35s TTFT, 100 TPS, $0.15/$0.60 per 1M tokens (best balance)
- **Claude 3.5 Sonnet**: 1.2s TTFT, 77 TPS, $3/$15 per 1M tokens (best coding quality)
- **Hybrid routing**: 80-90% cost savings vs always using premium models

**Date Verified**: 2025-12-03

## The Problem: Competing Constraints

### The Classic Challenge

Production teams face impossible trade-offs daily:

- **Startup**: "We need ChatGPT-level quality but can only afford $100/month"
- **Enterprise**: "Response time must be <500ms, but accuracy is critical"
- **Scale-up**: "Our costs grew 10x but quality barely improved"

**Problems**:

- ❌ **Budget overruns**: Premium models quickly exceed budgets at scale
- ❌ **User abandonment**: Slow responses (>3s) cause 40% drop-off
- ❌ **Quality failures**: Cheap models produce unacceptable outputs
- ❌ **Wrong trade-offs**: Teams optimize the wrong dimension for their use case

### Why This Matters

A 100k query/month chatbot costs:
- **Gemini Flash**: $4.50/month (baseline)
- **Claude Sonnet**: $412/month (91x more)
- **Claude Opus**: $825/month (183x more)

Choosing the wrong model can mean the difference between $50/month and $10,000/month for the same workload.

## Core Concept

### The Three Dimensions

**1. Cost (API Pricing)**

| Model Tier | Input/1M | Output/1M | Example Models |
|------------|----------|-----------|----------------|
| **Ultra-Cheap** | $0.15 | $0.60 | GPT-4o-mini, Gemini Flash |
| **Balanced** | $1.25-$5 | $5-$20 | GPT-4o, Gemini 1.5 Pro |
| **Premium** | $3-$15 | $15-$75 | Claude Sonnet, Claude Opus |
| **Reasoning** | $15 | $60+ | o1-preview, o3 |

**2. Latency (Response Speed)**

| Metric | Definition | User Perception |
|--------|------------|-----------------|
| **TTFT** | Time to First Token | <500ms = instant, >2s = slow |
| **TPS** | Tokens Per Second | >50 = smooth, <20 = choppy |
| **E2E** | Total response time | TTFT + (tokens/TPS) |

**3. Quality (Accuracy/Capability)**

| Task Complexity | Cheap Models | Premium Models | Gap |
|-----------------|--------------|----------------|-----|
| Simple Q&A | 85% | 90% | 5% (marginal) |
| Code generation | 55% | 86% | 31% (significant) |
| Math reasoning | 35% | 83% | 48% (dramatic) |

### Latency Benchmarks (2025)

```
Model                    TTFT      TPS      Best For
──────────────────────────────────────────────────────
Nova Micro               0.30s     —        Fastest TTFT
Gemini 2.0 Flash         0.34s     169      Real-time chat
GPT-4o-mini              0.35s     100      Balanced agents
GPT-4o                   0.53s     75       Production apps
Claude Sonnet            1.17s     77       Quality writing
o1-mini                  8-15s     —        Math/logic
o1-preview               15-30s    —        Hard reasoning
```

### Cost Examples: Real Scenarios

**Simple Chatbot Query** (50 input + 100 output tokens):

```
Gemini Flash:  $0.000045/query  →  $4.50/100k queries
GPT-4o-mini:   $0.000068/query  →  $6.80/100k queries
Claude Sonnet: $0.00165/query   →  $165/100k queries
Claude Opus:   $0.00825/query   →  $825/100k queries
```

**Coding Session** (500 input + 800 output tokens):

```
GPT-4o-mini:   $0.000555/query  →  $55.50/100k sessions
Claude Sonnet: $0.0135/query    →  $1,350/100k sessions (24x)
o1-mini*:      $0.0195/query    →  $1,950/100k sessions (35x)
* Plus 2000 thinking tokens
```

## Implementation Patterns

### Pattern 1: Static Tiering (Simplest)

**Use Case**: Fixed budget with predictable workloads

Assign models to task types statically:

```
Simple queries  → Gemini Flash ($0.10/$0.40)
Standard tasks  → GPT-4o-mini ($0.15/$0.60)
Complex tasks   → Claude Sonnet ($3/$15)
```

**Pros**:
- ✅ Simple to implement
- ✅ Predictable costs

**Cons**:
- ❌ Over-provisions for some queries
- ❌ Under-provisions for others

### Pattern 2: Complexity-Based Routing (Balanced)

**Use Case**: Variable workloads with mixed complexity

Route based on estimated task complexity:

```
Task Analysis → Complexity Score (0-1)
       ↓
  < 0.3: Flash   (80% of queries)
  0.3-0.7: Mini  (15% of queries)
  > 0.7: Sonnet  (5% of queries)
```

**Savings**: 70-80% vs always using premium

**When to Use**: General-purpose agents, production chatbots

### Pattern 3: Cascade with Validation (Quality-Focused)

**Use Case**: Quality-critical applications with budget constraints

Try cheap first, escalate if quality check fails:

```
Query → Flash → Quality Check
                    ↓
              Score > 0.8? → Return
                    ↓ No
              Sonnet → Return
```

**Pros**:
- ✅ 90% of queries use cheap model
- ✅ Quality guaranteed by validation

**Cons**:
- ❌ Added latency for validation
- ❌ Requires quality assessment logic

### Pattern 4: Parallel Speculation (Latency-Focused)

**Use Case**: Ultra-low latency requirements

Run multiple models in parallel, return first acceptable:

```
Query → [Flash, Mini, Sonnet] (parallel)
              ↓
       First acceptable → Return
```

**Pros**:
- ✅ Latency = min(all models)
- ✅ Always get acceptable quality

**Cons**:
- ❌ Pays for all attempts
- ❌ Expensive at scale

## When to Use This Pattern

### ✅ Use Cost-Optimization When:

1. **High volume, low margins**
   - Chatbots with 100k+ queries/month
   - Simple Q&A, classification, formatting

2. **Budget under $100/month**
   - Use Gemini Flash primarily (95%)
   - Fallback to Mini for edge cases (5%)

### ✅ Use Latency-Optimization When:

1. **Real-time interactive**
   - Voice assistants (TTFT <500ms required)
   - Gaming, live customer support

2. **Streaming long-form**
   - Content generation (TPS >50 required)
   - Code completion, article writing

### ✅ Use Quality-Optimization When:

1. **High-stakes decisions**
   - Legal, medical, financial analysis
   - Critical business decisions

2. **Complex reasoning**
   - Math, algorithms, scientific research
   - Multi-step planning, debugging

### Decision Matrix

| Your Situation | Primary Model | Fallback | Expected Cost |
|----------------|---------------|----------|---------------|
| Budget <$100/mo | Gemini Flash | Mini | $50/mo |
| Low latency required | Gemini Flash | — | $65/mo |
| Quality critical | Sonnet/Opus | o1 for hard | $1,000+/mo |
| Balanced production | GPT-4o-mini | Sonnet (5%) | $150/mo |
| Autonomous agents | GPT-4o-mini | o1-mini (5%) | $100/mo |

## Production Best Practices

### 1. Prompt Caching (50-90% Savings)

Cache repeated context (docs, examples, system prompts):

```
Without caching: 10k context tokens × $15/1M = $0.15/query
With caching:    10k cached + 50 new × $0.15/1M = $0.015/query

Savings: 90% for repeated context
```

**When**: Large system prompts, documentation injection, few-shot examples

### 2. Adaptive Token Budgets

Limit output tokens based on task type:

| Task Type | Token Budget | Cost Impact |
|-----------|--------------|-------------|
| Simple Q&A | 100 tokens | Baseline |
| Code snippet | 500 tokens | 5x |
| Full article | 2000 tokens | 20x |

**Benefit**: 50-60% reduction in output token costs

### 3. Batch Processing (50% Discount)

For non-urgent tasks, use batch APIs:

```
Real-time: $0.60/1M output tokens
Batch (24h): $0.30/1M output tokens

Use for: Analytics, translation, summarization jobs
```

### 4. Common Pitfalls

**❌ Over-Provisioning**:

Using Claude Opus for "What's 2+2?" wastes 100x the cost of Gemini Flash with identical results.

**❌ Under-Provisioning**:

Using Gemini Flash for differential equations yields 35% accuracy vs 85% with o1-mini. The cost savings aren't worth the quality loss.

**❌ Ignoring Latency**:

Using o1-preview (15-30s TTFT) for a chatbot causes 40%+ user abandonment. Use fast models for interactive, slow for batch.

## Trade-offs & Considerations

### Advantages of Multi-Model Routing

1. **Cost efficiency**: 80-90% reduction vs single premium model
2. **Appropriate quality**: Match model capability to task needs
3. **Latency flexibility**: Fast models for interactive, slow for complex

### Disadvantages

1. **Complexity**: Routing logic adds engineering overhead
2. **Inconsistency**: Different models may have different "personalities"
3. **Debugging**: Harder to trace issues across multiple models

### Cost Analysis Example

**100k queries/month, mixed complexity**:

| Approach | Monthly Cost | Quality |
|----------|-------------|---------|
| Always Opus | $8,250 | 95% |
| Always Sonnet | $1,350 | 90% |
| Always Mini | $68 | 80% |
| **Smart routing** | **$150** | **88%** |

Smart routing achieves 88% quality at 1.8% of the Opus cost.

## Key Takeaways

1. **Iron Triangle** - You cannot optimize cost, latency, and quality simultaneously; pick two
2. **Smart routing saves 80%+** - Use cheap models for simple tasks, premium for complex
3. **Latency has thresholds** - <500ms TTFT for interactive, >2s causes abandonment
4. **Prompt caching is free money** - 50-90% savings on repeated context
5. **Match model to task** - Over/under-provisioning both hurt business outcomes

**Quick Implementation Checklist**:

- [ ] Identify your primary constraint (cost, latency, or quality)
- [ ] Implement complexity-based routing
- [ ] Enable prompt caching for system prompts
- [ ] Set token budgets by task type
- [ ] Use batch API for non-urgent workloads

## References

1. **IntuitionLabs** (2025). "LLM API Pricing Comparison 2025". https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025
2. **Vellum** (2025). "LLM Leaderboard 2025". https://www.vellum.ai/llm-leaderboard
3. **AIMultiple** (2025). "LLM Latency Benchmark by Use Cases". https://research.aimultiple.com/llm-latency-benchmark/
4. **FutureAGI** (2025). "Compare 11 LLM API Providers 2025". https://futureagi.com/blogs/top-11-llm-api-providers-2025
5. **Softcery** (2025). "Choosing LLMs for AI Agents". https://softcery.com/lab/ai-agent-llm-selection
6. **OpenAI** (2025). Official Pricing. https://openai.com/pricing
7. **Anthropic** (2025). Claude Pricing. https://www.anthropic.com/pricing
8. **Google** (2025). Gemini API Pricing. https://ai.google.dev/pricing

**Related Topics**:

- [0.1.5 Model Selection Guide](./0.1.5-model-selection.md)
- [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)
- [0.2.3 When to Use Which Model](./0.2.3-when-to-use-which.md)

**Layer Index**: [Layer 0: Foundations](../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations-prerequisites)
