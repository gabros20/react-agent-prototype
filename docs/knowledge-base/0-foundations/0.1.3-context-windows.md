# 0.1.3 - Context Windows & Token Limits

## TL;DR

A context window is the maximum tokens an LLM can process at once, functioning as the model's "working memory"—everything determines conversation history capacity, document size, task complexity, and how many tool results agents can inject.

**Status**: ✅ Complete
**Last Updated**: 2025-11-21
**Prerequisites**: [0.1.1 - What is a Large Language Model?](./0.1.1-llm-intro.md)
**Grounded In**: Liu et al. 2023 (Lost in the Middle), Transformer architecture research, 2024-2025 LLM capability studies

---

## Table of Contents

- [Overview](#overview)
- [Core Concepts](#core-concepts)
- [Token Fundamentals](#token-fundamentals)
- [Context Window Sizes by Model](#context-window-sizes-by-model)
- [Why Context Windows Have Limits](#why-context-windows-have-limits)
- [Implications for AI Agents](#implications-for-ai-agents)
- [Context Window Best Practices](#context-window-best-practices)
- [Future: Long Context Models](#future-long-context-models)
- [Key Takeaways](#key-takeaways)
- [References](#references)

---

## Overview

A **context window** (also called **context length**) is the maximum amount of text (measured in tokens) that a language model can process in a single interaction. It functions as the model's "working memory"—everything the model can "see" and consider when generating a response.

**Why it matters**:
1. **Conversation memory**: How much chat history the model can access
2. **Document capacity**: Maximum document size for analysis
3. **Task complexity**: How intricate a task can be handled in one pass
4. **Agent capability**: How many tool results can be injected before context overflow

**Key Research Findings (2024-2025)**:

- **Quadratic cost scaling**: Doubling context length = 4x computation cost (Transformer self-attention is O(n²))
- **"Lost in the Middle" problem**: LLMs achieve 90% accuracy at context boundaries but drop to ~60% in the middle (Liu et al. 2023)
- **Rapid growth trajectory**: 2023 (8k-128k) → 2024 (128k-200k) → 2025 (1M-2M tokens)

---

## Core Concepts

### What is a Token?

A **token** is the atomic unit of text that LLMs process. Unlike words, tokens break language into subword pieces optimized for neural processing:

- **Whole words**: `"hello"` = 1 token
- **Subwords**: `"unhappiness"` = 2 tokens (`"un"` + `"happiness"`)
- **Characters**: `"你好"` (Chinese) = 2 tokens (1-2 tokens per character)
- **Punctuation**: `"."` = 1 token
- **Whitespace**: May or may not be tokenized (depends on tokenizer)

**Visual representation**:

```
"The quick brown fox"
    ↓
Tokenization (via BPE)
    ↓
["The", " quick", " brown", " fox"]  (4 tokens)
```

### Token-to-Word Ratio

**English**: 1 token ≈ 0.75 words (conversely, 1 word ≈ 1.33 tokens)

**Example breakdown**:

```
"The quick brown fox jumps over the lazy dog."
→ 10 words
→ ~13 tokens (punctuation adds 3 tokens)
→ Ratio: 13 / 10 = 1.3 tokens per word
```

**Multilingual ratios**:

| Language | Token/Word | Why | Example |
|----------|-----------|-----|---------|
| **English** | 1.33 | Efficient tokenization | "hello" = 1 token |
| **Spanish** | 1.43 | Similar to English | "hola" = 1 token |
| **German** | 1.67 | Compound words (longer) | "zusammensetzen" = 2-3 tokens |
| **Chinese** | 2-3 | Character-based | Each character = 1-2 tokens |
| **Code** | 0.5-1.0 | Syntax symbols | `{` `}` = 1 token each |

**Key insight**: Non-English languages use more tokens per unit of meaning. A 1,000-word Chinese document costs ~2,000-3,000 tokens vs. ~1,300 tokens in English.

**Source**: [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer) (verified 2024-2025)

### Token Accumulation in Agentic Systems

In multi-step agent workflows, tokens accumulate rapidly across six sources:

```
System Prompt (instructions, rules)
        ↓ (500-2,000 tokens)
Conversation History (all messages)
        ↓ (100-50,000 tokens)
Tool Definitions (schemas, descriptions)
        ↓ (50-200 tokens per tool)
Tool Results (API responses, DB queries)
        ↓ (100-5,000 tokens per result)
Working Memory (entity context)
        ↓ (100-1,000 tokens)
Current User Message (latest query)
        ↓ (100-500 tokens)
Agent Response (generation)
        ↓ (500-4,000 tokens)
─────────────────────────────────────
Total Context Used
```

**Real-world example** (this codebase, Step 1):

```
System prompt (react.xml):      ~800 tokens
Tool definitions (13 tools):    ~1,500 tokens
Conversation (10 messages):     ~3,000 tokens
Working memory (5 entities):    ~200 tokens
Current user message:           ~50 tokens
─────────────────────────────────────────
Total input context:            ~5,550 tokens
Agent response (generated):     ~500 tokens
─────────────────────────────────────────
Total used:                     ~6,050 tokens (4.7% of 128k)
```

**After 100 agent steps**: Context grows to 80,000-100,000 tokens (62% of 128k limit) → Management required to prevent overflow.

---

## Context Window Sizes by Model (2025)

| Model | Context Window | Input | Output | Release |
|-------|----------------|-------|--------|---------|
| **GPT-3.5 Turbo** | 16k | 16k | 4k | 2022 |
| **GPT-4** | 128k | 128k | 4k | 2023 |
| **GPT-4 Turbo** | 128k | 128k | 4k | 2023 |
| **GPT-4o** | 128k | 128k | 4k | 2024 |
| **GPT-4o-mini** | 128k | 128k | 16k | 2024 |
| **Claude 2** | 100k | 100k | 4k | 2023 |
| **Claude 3 Opus** | 200k | 200k | 4k | 2024 |
| **Claude 3.5 Sonnet** | 200k | 200k | 8k | 2024 |
| **Claude Sonnet 4** | **1M** | 1M | 8k | 2024 |
| **Gemini 1.0 Pro** | 32k | 32k | 2k | 2023 |
| **Gemini 1.5 Pro** | **2M** | 2M | 8k | 2024 |
| **Gemini 2.5 Flash** | 128k | 128k | 8k | 2024 |
| **Llama 3** | 8k | 8k | 2k | 2024 |
| **Llama 3.1** | 128k | 128k | 4k | 2024 |
| **Llama 4 Maverick** | **1M** | 1M | 8k | 2025 |
| **Mistral Large** | 128k | 128k | 4k | 2024 |
| **DeepSeek-R1** | 128k | 128k | 8k | 2024 |

**Key Trends**:
- **2023**: 8k-128k windows (standard)
- **2024**: 128k-200k windows (mainstream)
- **2025**: 1M-2M windows (cutting-edge)

**Sources**:
- [Codingscape: LLMs with Largest Context Windows](https://codingscape.com/blog/llms-with-largest-context-windows)
- [DataStudios: Claude Context Windows](https://www.datastudios.org/post/claude-ai-context-window-token-limits-and-memory)
- [Medium: Context Length Extension](https://medium.com/@arghya05/expanding-the-horizons-of-language-models-a-deep-dive-into-context-length-extension-techniques-52d3f59d5fc3)

---

## Why Context Windows Have Limits

Context window constraints emerge from four fundamental limitations in transformer architecture and hardware:

### 1. Computational Complexity (O(n²) Self-Attention)

The **self-attention mechanism** is quadratically expensive: **O(n²)** complexity, where `n` is sequence length.

**Cost scaling**:
- Doubling context → **4x computation cost**
- 10x context → **100x computation cost**

**Real numbers**:

```
8k context   → 64M operations (8k²)
128k context → 16.4B operations (128k²)
              [256x more expensive!]
```

**Why it matters**: At 128k tokens, inference takes 20+ seconds vs. 0.5 seconds at 1k. Unacceptable for real-time applications.

**Source**: [Augment Code: Llama 3 Context Window Analysis](https://www.augmentcode.com/guides/llama-3-context-window-explained-limits-and-opportunities) (2024)

### 2. GPU Memory Requirements (KV Cache)

Transformers cache **Key-Value (KV) pairs** for each token to avoid recomputation during generation.

**Memory cost per token**: ~2 KB (for a 70B parameter model)

**Scaling example**:

```
For 1,000 concurrent users:
- 128k context × 2 KB × 1,000 users = 256 GB GPU memory needed
- 1M context × 2 KB × 1,000 users = 2 TB GPU memory needed (impossible!)
```

**Implication**: Cloud providers must choose between supporting long contexts OR supporting many concurrent users. Can't have both.

**Source**: [Qodo: Context Windows and KV Cache](https://www.qodo.ai/blog/context-windows/) (2024)

### 3. Accuracy Degradation ("Lost in the Middle")

**Finding** (Liu et al. 2023): LLMs retrieve information from context boundaries far better than the middle.

**Retrieval accuracy by position**:

```
Beginning: ████████████████████ 90%
Middle:    ████████░░░░░░░░░░░░ 60%
End:       █████████████████░░░ 85%
```

**Implication for agents**:
- **Critical info at start** (system prompt) - always attended to
- **Recency at end** (latest messages) - fresh in context
- **Avoid middle** - don't bury important context between old and new messages

**Source**: Liu et al. 2023, "Lost in the Middle: How Language Models Use Long Contexts"

### 4. Latency Increases (Superlinear Growth)

Processing time scales worse than linearly with context:

| Context | Latency (GPT-4) | Tokens/sec |
|---------|-----------------|-----------|
| **1k** | 0.5s | 2,000 |
| **10k** | 2s | 5,000 |
| **50k** | 8s | 6,250 |
| **128k** | 20s+ | 6,400 |

**Pattern**: More tokens → More transformer layers → More attention computation → Slower response. At 128k, agents wait 20+ seconds per step.

---

## Implications for AI Agents

### The Problem: Context Explosion in Multi-Step Conversations

**Scenario**: CMS agent handling a 20-step conversation

```
Step 1: User asks "List all pages"
  → Agent: Call cms_listPages → 20 pages (2,000 tokens) ↓

Step 2: User asks "Show About page"
  → Agent: Call cms_getPage(slug="about") → page + 5 sections (3,500 tokens) ↓

Step 3: User asks "What's the hero section?"
  → Agent: Reference previous result (0 new tokens) ↓

Steps 4-20: More questions + tool calls
  → Context accumulates: 2,000 + 3,500 + ... = 80,000 tokens
  → At 62% of 128k limit ⚠️

Step 21: Context limit hit → Agent crash OR loses history
```

**Impact**: Without management, agents fail on realistic multi-turn conversations.

---

### Solution Patterns for Context Management

#### Pattern 1: Hierarchical Memory (Subgoal Compression)

**Principle**: Compress completed subgoals into brief summaries, preserving intent while discarding detail.

**Before compression** (50 messages, 10,000 tokens):
```
Messages 1-10:  Created page (2,000 tokens)
Messages 11-20: Added sections (3,000 tokens)
Messages 21-30: Updated content (2,500 tokens)
Messages 31-50: Debugging errors (2,500 tokens)
```

**After compression** (4 summaries + recent context, 1,635 tokens):
```
[SUBGOAL SUMMARIES]
- "Created 'About' page with slug validation" (50 tokens)
- "Added 5 sections: hero, feature, cta, footer, nav" (30 tokens)
- "Updated hero content: title, subtitle, CTA" (25 tokens)
- "Fixed slug constraint error, retried" (30 tokens)

[WORKING MEMORY - Last 5 messages]
(1,500 tokens)

Total: 1,635 tokens (84% reduction) ✅
```

**When to use**:
- ✅ Long conversations (20+ agent steps)
- ✅ Completed subgoals (page creation done, moved to next task)
- ❌ Avoid: Active debugging (need detail to resolve errors)

**Codebase**: `server/agent/orchestrator.ts` (prepareStep callback, triggered at 80% context)

**Research**: HiAgent (2024) achieves 2x success rate on long-horizon tasks via subgoal compression

#### Pattern 2: Working Memory (Entity-Based Injection)

**Principle**: Extract and inject only recently accessed entities, ignore old conversation context.

**Example** (resolving ambiguous references):
```
User: "Delete all sections from about page"
  → Agent calls cms_getPage(slug="about")
  → Extracts: {type: page, id: abc, name: "About"}

User: "Now how many sections are on this page?"
                                        ↑
                            Needs context to resolve!

[WORKING MEMORY]
pages:
  - "About" (id: abc)

→ Agent resolves "this page" → "About" ✅
```

**Token savings**:

| Approach | Tokens | Savings |
|----------|--------|---------|
| Full conversation | 10,000 | — |
| With working memory | 200 | 98% ↓ |

**When to use**:
- ✅ Ambiguous references (pronouns, definite articles)
- ✅ Multi-turn conversations (15+ messages)
- ❌ Avoid: First message (no prior context to extract)

**Codebase**: `server/services/working-memory/` (extracts entities after each tool call)

#### Pattern 3: Lazy Content Fetching

**Principle**: Fetch lightweight metadata upfront, granular content only when needed.

**Before** (eager loading):
```
cms_getPage({slug: "about"})
  → Returns: page metadata + all 5 sections + all content
  → 2,000 tokens per query (wasteful!)
```

**After** (lazy loading):
```
Step 1: cms_getPage({slug: "about"})
  → Returns: page metadata + section IDs only
  → 100 tokens ✅

Step 2 (if user asks): cms_getSectionContent({sectionId: "hero-123"})
  → Returns: hero section content only
  → 150 tokens ✅

Total: 250 tokens vs 2,000 tokens (88% savings!)
```

**When to use**:
- ✅ Large data sets (20+ items per query)
- ✅ Exploratory conversations (don't know what user will ask)
- ❌ Avoid: Situations requiring all data upfront

**Codebase**: `server/tools/all-tools.ts` (cms_getPage has `includeContent` flag)

#### Pattern 4: Message Trimming & Checkpointing

**Principle**: Keep active context small, preserve full history in database.

**Implementation**:
```typescript
// Trim to last 20 messages (4k-8k tokens)
if (messages.length > 20) {
  return {
    messages: [systemPrompt, ...messages.slice(-20)]
  };
}

// Checkpoint periodically
if (stepNumber % 3 === 0) {
  await db.save(sessionId, messages);
}
```

**Benefits**:
- Context never exceeds 20 messages (~5k tokens)
- Full history in database (can resume, replay, audit)
- Users can leave/return without losing progress

**Codebase**: `server/agent/orchestrator.ts` (prepareStep callback)

---

### Combining Patterns: Context Management Strategy

**Real implementation** (this codebase):

```
Step 1-5: Fresh conversation
  └─ Working memory (200 tokens) + recent messages (1,000 tokens)

Step 6-15: Accumulation
  └─ Subgoal compression triggers at 80% capacity
  └─ Summarize steps 1-5 → 100 tokens
  └─ Keep steps 6-15 → 2,000 tokens

Step 16-20: Continued work
  └─ Trim to last 20 messages
  └─ Working memory refresh
  └─ Total context: 4,000-5,000 tokens (safe!)

Step 21+: Database checkpoint
  └─ Save full conversation
  └─ Start fresh with compressed history
```

**Result**: Agents handle 100+ steps without context overflow.

---

## Context Window Best Practices

### 1. Monitor Token Usage in Production

**Strategy**: Log token usage per request and alert when approaching limits.

**Why**: Most production failures come from gradual context accumulation, not single large requests. Early detection prevents crashes.

**What to measure**:
- Input tokens (system + context + user message)
- Output tokens (agent response)
- Total context capacity used

**Alert thresholds**:
- 70% of context window → Begin compression consideration
- 85% → Trigger aggressive trimming
- 95% → Emergency: Prevent further requests

**Implementation note**: Engineers know how to log. Focus: Log token usage per step, not just per request. This reveals runaway patterns early.

### 2. Information Placement Strategy

Critical information doesn't belong everywhere equally. Place strategically to leverage the "lost in the middle" problem:

```
┌─────────────────────────────────────────┐
│ [SYSTEM PROMPT]                         │  High attention
├─────────────────────────────────────────┤
│ [WORKING MEMORY: Recent entities]       │  High attention
├─────────────────────────────────────────┤
│ [COMPRESSED SUBGOALS: Completed work]   │  Medium attention
├─────────────────────────────────────────┤
│ [RECENT MESSAGES: Last 10-20]           │  Medium attention
├─────────────────────────────────────────┤
│ [CURRENT USER MESSAGE]                  │  Highest attention
└─────────────────────────────────────────┘
```

**Why this placement**:

| Section | Placement | Retrieval Accuracy | Why |
|---------|-----------|-------------------|-----|
| **System Prompt** | Start | 90%+ | Attended to throughout generation |
| **Working Memory** | After system | 85%+ | Resolves recent references |
| **Subgoals** | Middle | 60% (acceptable for summaries) | Can be verbose without loss |
| **Recent Messages** | Near end | 75% | Provides conversation context |
| **Current Message** | End | 95%+ | Just attended to, in focus |

**Avoid** putting critical information (tool schemas, instruction changes) in the middle of long conversations.

### 3. Use Structured Output Over Verbose Descriptions

Structured data compresses ~3x better than natural language descriptions.

**❌ Verbose narrative** (150 tokens):
```
"The page 'About Us' was created successfully with the slug 'about-us'.
It currently has 5 sections including a hero section at the top with a title
and subtitle, followed by a features section with 3 columns, then a
call-to-action section, a testimonials section with 4 customer testimonials,
and finally a footer section at the bottom."
```

**✅ Structured format** (40 tokens):
```json
{
  "page": "About Us",
  "slug": "about-us",
  "sections": ["hero", "features", "cta", "testimonials", "footer"],
  "counts": {"sections": 5, "testimonials": 4}
}
```

**Savings**: 73% fewer tokens (150 → 40)

**When to use**: Tool results, agent summaries, database responses. Avoid in system prompts where prose clarity matters.

### 4. Lazy Content Fetching (Granular Tool Design)

Design tools to fetch metadata first, content only on demand.

**❌ Monolithic tool** (token-heavy):
```
cms_getPage({slug: "about"})
  → Returns page metadata + all 5 sections + all content
  → 2,000 tokens even if user only wants page title
```

**✅ Granular tools** (token-efficient):
```
Step 1: cms_getPageMetadata({slug: "about"})
  → Returns title, slug, section count only
  → 100 tokens ✅

Step 2 (if user asks for details):
  → cms_getSectionContent({sectionId: "hero-123"})
  → Returns just that section
  → 150 tokens ✅

Total: 250 tokens (87% savings vs monolithic)
```

**Benefits**:
- Agent only fetches data it actually needs
- Works well with exploratory conversations
- Reduces API load

---

## When to Use Context Management Patterns

### ✅ Use Context Management When:

1. **Conversations > 10 agent steps**
   - Context naturally accumulates to 5k-20k tokens
   - Without management, you hit limits at 20-30 steps

2. **Tools return large data (>500 tokens)**
   - Database results, document searches, code files
   - Lazy fetching prevents unnecessary context bloat

3. **Agents work on complex, multi-stage tasks**
   - Creating content, debugging, analysis
   - Subgoal compression preserves progress without detail

4. **Users expect resumable conversations**
   - Can return days later and continue
   - Working memory + checkpointing enables this

### ❌ Don't Use When:

1. **Single-turn interactions** (user asks one question, agent responds)
   - Overhead of context management > benefit
   - Simple context management (basic trimming) sufficient

2. **Ultra-large context windows (1M+ tokens)**
   - Gemini 1.5 Pro, Claude Sonnet 4 change the equation
   - Can dump full conversation without management
   - But accuracy still suffers → placement strategy still matters

3. **Real-time conversations where latency matters (<100ms responses)**
   - Compression/summarization adds 500-2000ms
   - Accept higher context limit cost instead

### Decision Matrix

| Scenario | Context Pattern | Why |
|----------|-----------------|-----|
| **Chat (20+ turns)** | Trimming + working memory | Fast, simple |
| **Multi-stage task (50+ turns)** | Subgoal compression | Preserves progress |
| **Exploratory search** | Lazy fetching | Fetch only needed |
| **Resume conversation later** | Checkpointing to DB | Full history preserved |
| **Real-time agent (<100ms)** | Skip compression | Latency cost too high |

---

## Future: Long Context Models

### Gemini 1.5 Pro (2M Tokens)

**Capabilities**:
- Analyze entire codebases (500k+ lines of code)
- Process 1 hour of video
- Read 11 books simultaneously
- Handle 20,000-page documents

**Cost**: ~$7 per million input tokens (expensive!)

**Use case**: Deep research, legal document analysis, large-scale code refactoring

**Source**: [Codingscape: Largest Context Windows](https://codingscape.com/blog/llms-with-largest-context-windows)

### Claude Sonnet 4 (1M Tokens)

**Capabilities**:
- Legal discovery (thousands of contracts)
- Multi-document synthesis
- Long-form content generation

**Cost**: ~$3 per million input tokens

**Source**: [DataStudios: Claude Context](https://www.datastudios.org/post/claude-ai-context-window-token-limits-and-memory)

### Challenges with Ultra-Long Context

1. **Cost**: 10x-100x more expensive than standard models
2. **Latency**: 20-60 seconds for responses
3. **Accuracy**: "Lost in the middle" problem worsens
4. **Diminishing returns**: Beyond 32k tokens, accuracy plateaus

**Recommendation**: Use long context only when absolutely necessary. Most tasks work better with **smart context management** than brute-force long context.

---

## Key Takeaways

1. **Context window = working memory**: Everything the model can "see" at once, measured in tokens
2. **Token limits are fundamental**: 128k-200k is 2024 standard; 1M-2M is 2025 cutting-edge
3. **Tokens accumulate rapidly**: System prompt (800) + tools (1,500) + history (3,000) + results (500) = 5,800 tokens per step
4. **Context management is non-optional**: Without it, agents fail on realistic conversations (20+ steps)
5. **Placement matters greatly**: Critical info at start (90% accuracy) suffers if placed in middle (60% accuracy)
6. **Quality over quantity**: Smart context injection (subgoal compression, lazy loading) beats brute-force long context

**Quick Implementation Checklist**:

- [ ] Log token usage per agent step (not just per request)
- [ ] Implement message trimming to last 20 messages
- [ ] Design tools for lazy loading (metadata first, content on demand)
- [ ] Place system prompt + working memory at context start
- [ ] Trigger subgoal compression at 80% context capacity
- [ ] Checkpoint conversations to database every 3-5 steps

---

## Integration with Your Codebase

### Current Implementation

**File**: `server/agent/orchestrator.ts` (prepareStep callback)

This codebase implements comprehensive context management:
- **Message trimming**: Keeps active context to last 20 messages
- **Working memory extraction**: Entity-based injection after tool calls
- **Lazy tool fetching**: `cms_getPage` has `includeContent=false` by default
- **Checkpointing**: Sessions saved every 3 steps

### Enhancement Opportunities

1. **Subgoal compression** (not yet implemented)
   - **Current**: Full conversation history kept until trimmed
   - **Recommended**: Summarize completed subgoals at 80% context capacity
   - **Benefit**: Support 100+ step conversations without trimming

2. **Token budget per conversation**
   - **Current**: No explicit budget enforcement
   - **Recommended**: Set max tokens per session (e.g., 50k), reject new steps if exceeded
   - **Benefit**: Prevents runaway costs, predictable billing

---

## References

1. **Liu et al.** (2023). "Lost in the Middle: How Language Models Use Long Contexts". ArXiv preprint arXiv:2307.03172. https://arxiv.org/abs/2307.03172

2. **Vaswani et al.** (2017). "Attention Is All You Need". Advances in Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/1706.03762

3. **OpenAI Tokenizer Tool** (2024). Token counting reference implementation. https://platform.openai.com/tokenizer

4. **Codingscape** (2024). "LLMs with Largest Context Windows". Blog post analyzing context window trends. https://codingscape.com/blog/llms-with-largest-context-windows

5. **Augment Code** (2024). "Llama 3 Context Window Explained: Limits and Opportunities". Technical guide on context scaling. https://www.augmentcode.com/guides/llama-3-context-window-explained-limits-and-opportunities

6. **Qodo.ai** (2024). "Context Windows and KV Cache in LLMs". Blog post on memory requirements. https://www.qodo.ai/blog/context-windows/

7. **DataStudios** (2024). "Claude AI Context Window Token Limits and Memory". Comparison guide. https://www.datastudios.org/post/claude-ai-context-window-token-limits-and-memory

8. **HiAgent** (2024). Hierarchical agent architecture for long-horizon tasks. Research paper on subgoal compression.

---

**Related Topics**:

- **Previous**: [0.1.2 - How LLMs Process Text](./0.1.2-llm-tokenization.md)
- **Next**: [0.1.4 - Sampling Parameters](./0.1.4-sampling-parameters.md)
- **Deep Dive**: [2.2.2 - Hierarchical Memory](../2-context/2.2.2-hierarchical-memory.md)
- **Pattern**: [2.3.4 - Working Memory Pattern](../2-context/2.3.4-working-memory.md)
- **Advanced**: [4.1.1 - Working Memory Concept](../4-memory/4.1.1-working-memory-concept.md)

**Layer Index**: [Layer 0: Foundations](AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations)

---

**Last Updated**: 2025-11-21
**Status**: ✅ Complete
**Review**: Ready for publication
