# 0.2.3 When to Use Which Model

## TL;DR

Use the smallest/cheapest model that meets your requirements: GPT-4.1 Mini ($0.40/$1.60) for agents and tool calling, Claude Sonnet 4.5 ($3/$15) for coding, Gemini 2.5 Pro ($1.25/$10) for long-context tasks, and o3/o4-mini for complex reasoning—implement hybrid routing to automatically select models based on task complexity for 80-90% cost savings.

-   **Status**: ✅ Complete
-   **Last Updated**: 2025-11-28
-   **Prerequisites**: [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md), [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)
-   **Grounded In**: OpenAI/Anthropic/Google API pricing (November 2025), production cost analysis, benchmark comparisons

## Table of Contents

-   [Overview](#overview)
-   [The Problem: Model Over/Under-Provisioning](#the-problem-model-overunder-provisioning)
-   [Core Concept](#core-concept)
-   [Implementation Patterns](#implementation-patterns)
-   [Framework Integration](#framework-integration)
-   [Research & Benchmarks](#research--benchmarks)
-   [When to Use This Pattern](#when-to-use-this-pattern)
-   [Production Best Practices](#production-best-practices)
-   [Trade-offs & Considerations](#trade-offs--considerations)
-   [Key Takeaways](#key-takeaways)
-   [References](#references)

## Overview

Selecting the right AI model is critical for balancing cost, performance, and capabilities. This guide provides decision frameworks, use case matrices, and practical routing patterns for choosing between GPT-4.1, Claude 4, Gemini 2.5, reasoning models (o3/o4-mini), and smaller models based on your specific needs.

**Key Principle**: Use the smallest/cheapest model that meets your requirements. Over-provisioning wastes resources; under-provisioning frustrates users.

**As of November 2025**: The model landscape has consolidated around three pricing tiers:
- **Fast & Cheap**: GPT-4.1 Mini, Gemini 2.5 Flash, Claude Haiku 4.5 ($0.10-1.00/M input)
- **Balanced**: GPT-4.1, Claude Sonnet 4.5, Gemini 2.5 Pro ($1.25-3.00/M input)
- **Premium/Reasoning**: Claude Opus 4.5, o3, o4-mini ($5-10/M input)

### Key Research Findings (2024-2025)

-   **Cost Reduction**: GPT-4.1 is 26% cheaper than GPT-4o; Claude Opus 4.5 is 67% cheaper than Opus 4
-   **Reasoning Models**: o3 pricing reduced 80% to $0.40/$1.60—matching GPT-4.1 Mini
-   **Context Windows**: Gemini 2.5 Pro offers 2M tokens; Claude offers 200k
-   **Coding Excellence**: Claude Sonnet 4 leads SWE-bench (72.7%), Opus 4 (72.5%)
-   **Hybrid Routing**: Saves 80-90% on costs by using cheap models for simple tasks

**Date Verified**: 2025-11-28

## The Problem: Model Over/Under-Provisioning

### The Classic Challenges

**Over-Provisioning** (Using too expensive a model):

```
Scenario: Customer support chatbot using Claude Opus 4.5
- Cost: $5/M input, $25/M output
- 90% of queries are simple: "What's my order status?"
- Actual need: GPT-4.1 Mini at $0.40/$1.60
- Waste: 12× more expensive than necessary
- Monthly cost: $2,500 vs $200 (correct choice)
```

**Under-Provisioning** (Using too weak a model):

```
Scenario: Code review assistant using GPT-4.1 Mini
- Misses subtle bugs in complex algorithms
- Users re-ask questions frequently
- 30% of reviews require human override
- Lost productivity: $5k/month in developer time
- Correct choice: Claude Sonnet 4.5 ($450/month)
```

**Problems**:

-   ❌ **Budget waste**: Premium models on simple tasks
-   ❌ **User frustration**: Weak models on complex tasks
-   ❌ **No adaptivity**: Same model for all query types
-   ❌ **Latency issues**: Slow models where speed matters

### Why This Matters

**Cost Impact** (10,000 queries/month):

| Strategy | Monthly Cost | Notes |
| -------- | ------------ | ----- |
| All Claude Opus 4.5 | $2,500 | Overkill for most queries |
| All GPT-4.1 Mini | $80 | Fails on complex tasks |
| Hybrid (5% premium) | $200 | Best cost-quality balance |

**Quality Impact**:

| Model | Simple Tasks | Complex Tasks | Reasoning Tasks |
| ----- | ------------ | ------------- | --------------- |
| GPT-4.1 Mini | ✅ Excellent | ⚠️ Adequate | ❌ Poor |
| Claude Sonnet 4.5 | ✅ Excellent | ✅ Excellent | ⚠️ Adequate |
| o3/o4-mini | ✅ Excellent | ✅ Excellent | ✅ Excellent |

## Core Concept

### Model Categories (November 2025 Pricing)

**1. Fast & Cheap Models** — High-volume, straightforward tasks

| Model | Input ($/M) | Output ($/M) | Context | Best For |
| ----- | ----------- | ------------ | ------- | -------- |
| **GPT-4.1 Mini** | $0.40 | $1.60 | 128k | Agents, tool calling |
| **GPT-4.1 Nano** | $0.10 | $0.40 | 128k | High-volume simple tasks |
| **Gemini 2.5 Flash** | $0.10 | $0.40 | 1M | Customer support, parsing |
| **Claude Haiku 4.5** | $1.00 | $5.00 | 200k | Chat, simple analysis |

**2. Balanced Models** — General-purpose applications

| Model | Input ($/M) | Output ($/M) | Context | Best For |
| ----- | ----------- | ------------ | ------- | -------- |
| **GPT-4.1** | $2.00 | $8.00 | 128k | Production apps |
| **Claude Sonnet 4.5** | $3.00 | $15.00 | 200k | Coding, writing |
| **Gemini 2.5 Pro** | $1.25 | $10.00 | 2M | Long-context, research |

**3. Premium/Reasoning Models** — Complex reasoning, highest quality

| Model | Input ($/M) | Output ($/M) | Context | Best For |
| ----- | ----------- | ------------ | ------- | -------- |
| **Claude Opus 4.5** | $5.00 | $25.00 | 200k | Complex decisions |
| **o3** | $10.00 | $40.00 | 200k | Deep reasoning |
| **o4-mini** | $1.10 | $4.40 | 200k | Cost-efficient reasoning |

*Note: o3 pricing reduced to $0.40/$1.60 with new flex pricing tier*

### Visual Representation

**Decision Flow**:

```
START
    │
    ├─ Is it simple? (single-turn, factual)
    │   └─ YES → Gemini Flash / GPT-4.1 Nano
    │
    ├─ Does it need reasoning? (math, logic, proofs)
    │   └─ YES → o4-mini (cost) / o3 (quality)
    │
    ├─ Is it coding?
    │   ├─ Simple → GPT-4.1 Mini
    │   ├─ Standard → Claude Sonnet 4.5
    │   └─ Complex → Claude Opus 4.5
    │
    ├─ Is it writing?
    │   ├─ Short-form → GPT-4.1 Mini
    │   └─ Long-form → Claude Sonnet 4.5
    │
    ├─ Does it need long context? (>100k tokens)
    │   ├─ 200k → Claude
    │   └─ 2M → Gemini 2.5 Pro
    │
    ├─ Is speed critical? (<2 seconds)
    │   └─ YES → Gemini Flash > GPT-4.1 Mini
    │
    └─ Is accuracy critical? (legal, medical)
        └─ YES → Claude Opus 4.5 / o3

Default: GPT-4.1 Mini (best all-around value)
```

### Key Principles

1. **Start cheap, upgrade when needed**: Begin with GPT-4.1 Mini, escalate on failure
2. **Match model to task complexity**: Simple tasks don't need premium models
3. **Use specialists**: Claude for coding, Gemini for long-context, o3 for reasoning
4. **Monitor and adapt**: Track success rates and costs per model

## Implementation Patterns

### Pattern 1: Complexity-Based Router

**Use Case**: Automatically route queries to appropriate model tier

```typescript
import { openai } from "@ai-sdk/openai";
import { anthropic } from "@ai-sdk/anthropic";
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

interface ModelRouter {
	selectModel(task: Task): { model: LanguageModel; reasoning?: string };
}

class CostOptimizedRouter implements ModelRouter {
	selectModel(task: Task) {
		const complexity = this.analyzeComplexity(task);

		// Tier 1: Simple tasks → Cheapest models
		if (complexity < 0.3) {
			return {
				model: google("gemini-2.5-flash"),
				reasoning: "Simple task, using cheapest model",
			};
		}

		// Tier 2: Medium tasks → Balanced models
		if (complexity < 0.7) {
			if (task.domain === "code") {
				return {
					model: anthropic("claude-sonnet-4-5-20250929"),
					reasoning: "Coding task, using Claude Sonnet",
				};
			}
			return {
				model: openai("gpt-4.1-mini"),
				reasoning: "Medium complexity, using balanced model",
			};
		}

		// Tier 3: Complex tasks → Premium/Reasoning
		if (task.requiresReasoning) {
			return {
				model: openai("o4-mini"),
				reasoning: "Requires reasoning, using o4-mini",
			};
		}

		return {
			model: anthropic("claude-opus-4-5-20251124"),
			reasoning: "Complex task, using premium model",
		};
	}

	analyzeComplexity(task: Task): number {
		let score = 0;

		if (task.steps > 5) score += 0.3;
		if (task.requiresMath) score += 0.2;
		if (task.requiresCode) score += 0.2;
		if (task.contextLength > 10000) score += 0.1;
		if (task.previousAttemptsFailed) score += 0.2;

		return Math.min(score, 1.0);
	}
}
```

**Pros**:

-   ✅ Automatic cost optimization
-   ✅ Task-appropriate model selection
-   ✅ Easy to tune thresholds

**Cons**:

-   ❌ Requires complexity classification
-   ❌ Potential misclassification

**When to Use**: Production systems with mixed workloads

### Pattern 2: Cascade (Try Cheap First)

**Use Case**: Try cheap model first, escalate if quality insufficient

```typescript
import { openai } from "@ai-sdk/openai";
import { anthropic } from "@ai-sdk/anthropic";
import { generateText } from "ai";

async function cascadeGeneration(prompt: string, requirements: Requirements) {
	// Step 1: Try cheapest model
	const fastResult = await generateText({
		model: openai("gpt-4.1-nano"),
		prompt,
	});

	// Step 2: Validate quality
	const quality = await assessQuality(fastResult.text, requirements);

	if (quality > 0.8) {
		return { result: fastResult, model: "gpt-4.1-nano", escalated: false };
	}

	// Step 3: Escalate to better model
	const betterResult = await generateText({
		model: anthropic("claude-sonnet-4-5-20250929"),
		prompt: `Improve this response:\n\n${fastResult.text}\n\nOriginal prompt: ${prompt}`,
	});

	return { result: betterResult, model: "claude-sonnet-4.5", escalated: true };
}
```

**Pros**:

-   ✅ Maximizes cost savings (try cheap first)
-   ✅ Quality validation ensures good output
-   ✅ Learns which queries need escalation

**Cons**:

-   ❌ Higher latency on escalated requests
-   ❌ Requires quality assessment logic

**When to Use**: When you can tolerate occasional delays for cost savings

### Pattern 3: Specialist Routing

**Use Case**: Different models for different subtasks in a workflow

```typescript
import { openai } from "@ai-sdk/openai";
import { anthropic } from "@ai-sdk/anthropic";
import { generateText } from "ai";

async function generateBlogPost(topic: string) {
	// Step 1: o4-mini for outline (reasoning task)
	const outline = await generateText({
		model: openai("o4-mini"),
		prompt: `Create comprehensive blog outline: ${topic}`,
		providerOptions: {
			openai: { reasoningEffort: "medium" },
		},
	});

	// Step 2: Claude Sonnet for content (best writing)
	const content = await generateText({
		model: anthropic("claude-sonnet-4-5-20250929"),
		prompt: `Write blog post from outline:\n\n${outline.text}`,
	});

	// Step 3: GPT-4.1 Mini for SEO metadata (simple task)
	const metadata = await generateText({
		model: openai("gpt-4.1-mini"),
		prompt: `Generate SEO title/description for:\n\n${content.text.slice(0, 500)}`,
	});

	return { content: content.text, metadata: metadata.text };
}
```

**Pros**:

-   ✅ Best model for each subtask
-   ✅ Cost-optimized workflow
-   ✅ Quality maximized where it matters

**Cons**:

-   ❌ More complex orchestration
-   ❌ Multiple API calls

**When to Use**: Multi-step workflows with diverse subtask requirements

### Pattern 4: Custom Provider Abstraction

**Use Case**: Abstract model selection behind semantic aliases

```typescript
import { anthropic } from "@ai-sdk/anthropic";
import { openai } from "@ai-sdk/openai";
import { google } from "@ai-sdk/google";
import { customProvider, wrapLanguageModel, defaultSettingsMiddleware } from "ai";

export const myProvider = customProvider({
	languageModels: {
		// Semantic aliases hide implementation details
		"fast": google("gemini-2.5-flash"),
		"balanced": openai("gpt-4.1-mini"),
		"coding": anthropic("claude-sonnet-4-5-20250929"),
		"premium": anthropic("claude-opus-4-5-20251124"),
		"reasoning": wrapLanguageModel({
			model: openai("o4-mini"),
			middleware: defaultSettingsMiddleware({
				settings: {
					providerOptions: {
						openai: { reasoningEffort: "medium" },
					},
				},
			}),
		}),
		"reasoning-deep": wrapLanguageModel({
			model: openai("o3"),
			middleware: defaultSettingsMiddleware({
				settings: {
					providerOptions: {
						openai: { reasoningEffort: "high" },
					},
				},
			}),
		}),
	},
	embeddingModels: {
		"embedding": openai.textEmbeddingModel("text-embedding-3-small"),
	},
});

// Usage: Clean semantic interface
const { text } = await generateText({
	model: myProvider.languageModel("coding"),
	prompt: "Write a binary search implementation",
});
```

**Pros**:

-   ✅ Clean API (semantic model names)
-   ✅ Easy to swap implementations
-   ✅ Centralized model management

**Cons**:

-   ❌ Upfront setup required
-   ❌ Less flexibility per-call

**When to Use**: Teams with multiple developers, need consistent model usage

## Framework Integration

### AI SDK 6 Multi-Provider Setup

AI SDK 6 provides unified APIs for multiple providers. Install provider packages:

```bash
pnpm add @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google ai
```

**Basic Multi-Provider Usage**:

```typescript
import { openai } from "@ai-sdk/openai";
import { anthropic } from "@ai-sdk/anthropic";
import { google } from "@ai-sdk/google";
import { generateText } from "ai";

// Same API, different providers
const openaiResult = await generateText({
	model: openai("gpt-4.1-mini"),
	prompt: "Hello from OpenAI",
});

const claudeResult = await generateText({
	model: anthropic("claude-sonnet-4-5-20250929"),
	prompt: "Hello from Anthropic",
});

const geminiResult = await generateText({
	model: google("gemini-2.5-flash"),
	prompt: "Hello from Google",
});
```

**Provider-Specific Options**:

```typescript
// OpenAI with reasoning effort
const { text } = await generateText({
	model: openai("o3"),
	prompt: "Complex reasoning task",
	providerOptions: {
		openai: {
			reasoningEffort: "high",
		},
	},
});

// Anthropic with prompt caching
const { text } = await generateText({
	model: anthropic("claude-sonnet-4-5-20250929"),
	messages: [
		{
			role: "user",
			content: largeDocumentation,
			providerOptions: {
				anthropic: {
					cacheControl: { type: "ephemeral" },
				},
			},
		},
		{ role: "user", content: "Summarize this document" },
	],
});

// Google with long context
const { text } = await generateText({
	model: google("gemini-2.5-pro"),
	prompt: `Analyze this 500-page document:\n\n${veryLongDocument}`,
});
```

### Provider Strengths Summary

| Provider | Best For | Avoid For |
| -------- | -------- | --------- |
| **OpenAI** | Tool calling, agents, structured outputs, reasoning (o3) | Long-form creative writing |
| **Anthropic** | Coding, writing, safety-critical, large codebases | Speed-critical apps, high-volume |
| **Google** | Long-context (2M), cost efficiency, multimodal | Complex reasoning, critical applications |

## Research & Benchmarks

### Coding Benchmarks (November 2025)

| Model | SWE-bench | Terminal-bench | Notes |
| ----- | --------- | -------------- | ----- |
| Claude Opus 4 | 72.5% | 43.2% | World's best coding model |
| Claude Sonnet 4 | 72.7% | N/A | State-of-the-art, 30% faster than Opus |
| GPT-4.1 | 65% | N/A | Strong general coding |
| o4-mini | 68% | N/A | Good for algorithmic problems |

**Source**: [Anthropic Claude 4 Announcement](https://www.anthropic.com/news/claude-4)

### Reasoning Benchmarks

| Model | AIME 2024 | ARC-AGI-1 | GPQA Diamond |
| ----- | --------- | --------- | ------------ |
| o3 (high) | 96.7% | 53% | ~83% |
| o4-mini | N/A | 41% | N/A |
| DeepSeek-R1-0528 | 91.4% | N/A | 81% |
| Claude Opus 4.5 | N/A | N/A | N/A |

### Cost Comparison (November 2025)

| Model | Input ($/M) | Output ($/M) | Relative Cost |
| ----- | ----------- | ------------ | ------------- |
| GPT-4.1 Nano | $0.10 | $0.40 | 1× (baseline) |
| Gemini 2.5 Flash | $0.10 | $0.40 | 1× |
| GPT-4.1 Mini | $0.40 | $1.60 | 4× |
| o4-mini | $1.10 | $4.40 | 11× |
| Gemini 2.5 Pro | $1.25 | $10.00 | 12.5× |
| GPT-4.1 | $2.00 | $8.00 | 20× |
| Claude Sonnet 4.5 | $3.00 | $15.00 | 30× |
| Claude Opus 4.5 | $5.00 | $25.00 | 50× |
| o3 | $10.00 | $40.00 | 100× |

**Sources**: [OpenAI Pricing](https://openai.com/api/pricing/), [Anthropic Pricing](https://docs.claude.com/en/docs/about-claude/pricing), [LLM API Pricing Comparison](https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025)

## When to Use This Pattern

### Use Case Recommendations

**1. Customer Support Chatbot**

```
Primary: Gemini 2.5 Flash ($0.10/$0.40)
- 95% of queries (simple Q&A)
- $50/month for 100k queries

Fallback: GPT-4.1 Mini ($0.40/$1.60)
- Complex queries with tool use
- $30/month for 5k queries

Total: $80/month for 105k queries
```

**2. Coding Assistant**

```
Primary: Claude Sonnet 4.5 ($3.00/$15.00)
- Standard coding tasks
- $200/month for 1k sessions

For Algorithms: o4-mini ($1.10/$4.40)
- Complex algorithm design
- Only when needed

Monthly Cost: $250 for 1k coding sessions
```

**3. Content Writing**

```
Blog Posts: Claude Sonnet 4.5
- Best writing quality
- $0.50 per 2000-word article

Social Media: GPT-4.1 Mini
- Short-form content
- $0.05 per 10 posts

Long-Form: Claude Opus 4.5
- Reports, whitepapers
- $2.50 per 10k-word document
```

**4. Research & Analysis**

```
Document Analysis: Gemini 2.5 Pro
- 2M token context
- $2.50 to analyze 500-page PDF

Deep Research: Claude Opus 4.5
- Nuanced synthesis
- $5.00 per research session

Mathematical: o3
- Complex calculations
- Use sparingly ($10+/query)
```

**5. Autonomous Agents**

```
Orchestration: GPT-4.1 Mini
- Best tool calling support
- ~$10/month for 10k interactions

Complex Planning: o4-mini
- 1-2 calls per session
- ~$5/month additional

Total: ~$15/month for agent workloads
```

### Decision Matrix

| Scenario | Recommended Model | Monthly Cost (10k queries) |
| -------- | ----------------- | -------------------------- |
| High-volume simple | Gemini Flash | $50 |
| General-purpose | GPT-4.1 Mini | $80 |
| Coding-heavy | Claude Sonnet 4.5 | $450 |
| Long documents | Gemini 2.5 Pro | $200 |
| Complex reasoning | o4-mini | $550 |
| Critical accuracy | Claude Opus 4.5 | $2,500 |
| Deep reasoning | o3 | $5,000+ |

## Production Best Practices

### 1. Implement Cost Monitoring

**Principle**: Track costs per model, per use case, per user.

```typescript
interface UsageMetrics {
	model: string;
	inputTokens: number;
	outputTokens: number;
	reasoningTokens?: number;
	cost: number;
	useCase: string;
	latencyMs: number;
}

async function trackedGenerate(params: GenerateParams) {
	const start = Date.now();
	const result = await generateText(params);
	const latency = Date.now() - start;

	await logUsage({
		model: params.model.modelId,
		inputTokens: result.usage.promptTokens,
		outputTokens: result.usage.completionTokens,
		reasoningTokens: result.providerMetadata?.openai?.reasoningTokens,
		cost: calculateCost(result.usage, params.model),
		useCase: params.metadata?.useCase,
		latencyMs: latency,
	});

	return result;
}
```

### 2. Use Prompt Caching

**Principle**: Cache repeated context to save 50-90% on tokens.

```typescript
// Anthropic: Cache large system prompts
const result = await generateText({
	model: anthropic("claude-sonnet-4-5-20250929"),
	messages: [
		{
			role: "user",
			content: largeDocumentation,
			providerOptions: {
				anthropic: { cacheControl: { type: "ephemeral" } },
			},
		},
		{ role: "user", content: userQuestion },
	],
});

// Savings: 90% on repeated queries with same documentation
```

### 3. Batch Non-Urgent Requests

**Principle**: Use batch APIs for 50% discount on non-time-sensitive tasks.

```typescript
// OpenAI Batch API: 50% off, results in 24 hours
const batch = await openai.batches.create({
	requests: tasks.map((task) => ({ prompt: task })),
	model: "gpt-4.1-mini",
});
```

### 4. Set Budget Alerts

**Principle**: Alert before costs exceed thresholds.

```typescript
const DAILY_BUDGET = 100; // $100/day

async function checkBudget() {
	const todayCost = await getTodayCost();
	if (todayCost > DAILY_BUDGET * 0.8) {
		await alert(`Warning: Daily budget at ${(todayCost / DAILY_BUDGET) * 100}%`);
	}
	if (todayCost > DAILY_BUDGET) {
		await alert("CRITICAL: Daily budget exceeded");
		// Optionally: switch to cheaper models
	}
}
```

### 5. A/B Test Model Selection

**Principle**: Validate that cheaper models maintain quality.

```typescript
async function abTestModels(prompt: string) {
	const variant = Math.random() < 0.1 ? "expensive" : "cheap";

	const model =
		variant === "expensive"
			? anthropic("claude-sonnet-4-5-20250929")
			: openai("gpt-4.1-mini");

	const result = await generateText({ model, prompt });

	await logExperiment({
		variant,
		prompt,
		response: result.text,
		// Collect user feedback later
	});

	return result;
}
```

## Trade-offs & Considerations

### Advantages of Model Routing

1. **80-90% Cost Savings**: Cheap models handle most queries
2. **Quality Where Needed**: Premium models for complex tasks
3. **Latency Optimization**: Fast models for simple tasks
4. **Scalability**: Handle high volume economically

### Disadvantages

1. **Implementation Complexity**: Need routing logic
2. **Misclassification Risk**: Wrong model for task
3. **Debugging Difficulty**: Multiple models to monitor
4. **Provider Dependency**: Lock-in to multiple providers

### Provider Comparison

| Factor | OpenAI | Anthropic | Google |
| ------ | ------ | --------- | ------ |
| **Tool Calling** | ✅ Best | ⚠️ Good | ⚠️ Good |
| **Coding** | ⚠️ Good | ✅ Best | ⚠️ Good |
| **Writing** | ⚠️ Good | ✅ Best | ⚠️ Good |
| **Long Context** | 128k | 200k | ✅ 2M |
| **Speed** | Fast | Medium | ✅ Fastest |
| **Cost** | Medium | Expensive | ✅ Cheapest |
| **Reasoning** | ✅ o3/o4 | ⚠️ Hybrid | ❌ Limited |

## Key Takeaways

1. **Start with GPT-4.1 Mini** — Best all-around value for most tasks
2. **Use Claude for coding** — Sonnet 4.5 leads SWE-bench at 72.7%
3. **Use Gemini for long context** — 2M tokens at competitive prices
4. **Use o4-mini for reasoning** — 10× cheaper than o3, still capable
5. **Implement hybrid routing** — 80-90% cost savings on mixed workloads
6. **Cache aggressively** — 50-90% savings on repeated context
7. **Monitor costs per model** — Track ROI of model choices
8. **A/B test downgrades** — Validate cheap models maintain quality

**Quick Implementation Checklist**:

-   [ ] Choose default model (GPT-4.1 Mini recommended)
-   [ ] Identify tasks requiring premium models
-   [ ] Implement complexity-based routing
-   [ ] Set up cost monitoring and alerts
-   [ ] Enable prompt caching where supported
-   [ ] Use batch API for non-urgent tasks
-   [ ] A/B test model downgrades
-   [ ] Review costs weekly, adjust routing

## References

### Pricing & Capabilities

1. **OpenAI** (2025). "API Pricing". [https://openai.com/api/pricing/](https://openai.com/api/pricing/)

2. **Anthropic** (2025). "Claude Pricing". [https://docs.claude.com/en/docs/about-claude/pricing](https://docs.claude.com/en/docs/about-claude/pricing)

3. **IntuitionLabs** (2025). "LLM API Pricing Comparison 2025". [https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025](https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025)

### Model Announcements

4. **Anthropic** (2025). "Introducing Claude 4". [https://www.anthropic.com/news/claude-4](https://www.anthropic.com/news/claude-4)

5. **Anthropic** (2025). "Introducing Claude Opus 4.5". [https://www.anthropic.com/news/claude-opus-4-5](https://www.anthropic.com/news/claude-opus-4-5)

6. **OpenAI** (2025). "Introducing o3 and o4-mini". [https://openai.com/index/introducing-o3-and-o4-mini/](https://openai.com/index/introducing-o3-and-o4-mini/)

### Technical Resources

7. **AI SDK Team** (2025). "Provider Management". [https://ai-sdk.dev/docs/ai-sdk-core/provider-management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management)

8. **DataCamp** (2025). "o4-mini: Tests, Features, o3 Comparison". [https://www.datacamp.com/blog/o4-mini](https://www.datacamp.com/blog/o4-mini)

### Analysis & Comparisons

9. **Simon Willison** (2025). "Claude Opus 4.5, and why evaluating new LLMs is increasingly difficult". [https://simonwillison.net/2025/Nov/24/claude-opus/](https://simonwillison.net/2025/Nov/24/claude-opus/)

10. **Analytics Vidhya** (2025). "Claude 4 vs GPT-4o vs Gemini 2.5 Pro: Which AI Codes Best?". [https://www.analyticsvidhya.com/blog/2025/05/best-ai-for-coding/](https://www.analyticsvidhya.com/blog/2025/05/best-ai-for-coding/)

**Related Topics**:

-   [0.2.1 Standard vs Thinking Models](./0.2.1-standard-models.md)
-   [0.2.2 Reasoning Models Deep Dive](./0.2.2-reasoning-models.md)
-   [0.2.4 Trade-offs (Cost, Latency, Capabilities)](./0.2.4-tradeoffs.md)

**Layer Index**: [Layer 0: Foundations](../../AI_KNOWLEDGE_BASE_TOC.md#layer-0-foundations)
