# 2.3.2 Injection Timing (Always vs Conditional)

> **TL;DR:** Injection timing determines when to fetch context—always (upfront), conditionally (based on need), or lazily (during generation)—achieving 30-50% cost reduction and 2-3× speed improvements by avoiding unnecessary retrievals.
>
> - **Status:** ✅ Complete
> - **Last Updated:** 2024-12
> - **Prerequisites:** [2.3.1 Injection Location](./2.3.1-injection-location.md)
> - **Grounded In:** "When to Retrieve" (2024), DeepRAG (2025), CLARINET (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: When to Fetch Context?](#the-problem-when-to-fetch-context)
- [Timing Strategies](#timing-strategies)
- [Conditional Retrieval Patterns](#conditional-retrieval-patterns)
- [Lazy Evaluation Techniques](#lazy-evaluation-techniques)
- [Research & Benchmarks](#research--benchmarks)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Injection Timing** determines **when** to fetch and inject context into LLM prompts—either always (upfront), conditionally (based on need), or lazily (on-demand during generation). This decision critically impacts cost, latency, and response quality.

**Key Innovation**: Instead of always retrieving context:
1. **Always Injection**: Fetch context for every query (traditional RAG)
2. **Conditional Injection**: Retrieve only when LLM determines it's needed
3. **Lazy Evaluation**: Defer retrieval until specific point in reasoning
4. **Iterative Refinement**: Retrieve → Evaluate → Retrieve again if needed

**Impact** (2024-2025 Research):
- **30-50% cost reduction** (avoiding unnecessary retrievals)
- **2-3× faster** for queries answerable from parametric memory
- **17-39% accuracy improvement** (CLARINET, targeted retrieval)
- **26.4% boost** in answer quality (DeepRAG, step-by-step retrieval)
- **85-95% accuracy** in deciding when to retrieve (Controllable Context Sensitivity)

Optimal injection timing is essential for production systems, balancing cost, speed, and quality.

## The Problem: When to Fetch Context?

### The Dilemma

**Scenario**: Documentation Q&A system with 10,000 documents

**Always Retrieve Approach**:
```typescript
async function answer(query: string): Promise<string> {
  // ALWAYS fetch context (even if not needed)
  const docs = await vectorSearch(query, topK = 5);

  const prompt = buildPrompt(query, docs);
  return await llm.generate(prompt);
}

// Example queries:
await answer("What is Python?");  // ← Doesn't need docs (common knowledge)
await answer("How to configure X in our system?");  // ← DOES need docs
```

**Problems**:
1. **Unnecessary Cost**: Vector search + embedding for every query ($$$)
2. **Added Latency**: 100-300ms retrieval delay even when not needed
3. **Context Dilution**: Irrelevant docs confuse LLM on simple queries
4. **Wasted Tokens**: Injecting 2,000+ tokens of docs for questions like "What is Python?"

**Cost Analysis**:
```
100,000 queries/month
- 50% answerable from parametric memory (no docs needed)
- 50% require retrieval

Always Retrieve:
- 100,000 × embedding ($0.0001) = $10
- 100,000 × vector search ($0.0001) = $10
- 100,000 × LLM (3,000 tokens avg) × $0.00015/1K = $45
Total: $65/month

Conditional Retrieve (50% reduction):
- 50,000 × embedding = $5
- 50,000 × vector search = $5
- 100,000 × LLM (2,000 tokens avg for 50%, 3,000 for 50%) = $30
Total: $40/month

Savings: $25/month (38% reduction)
Annual: $300 saved
```

### Decision Points

**Question to Answer**: "Does this query require external knowledge?"

**Factors**:
1. **Query Complexity**: Simple definitions vs complex troubleshooting
2. **Knowledge Recency**: Current events vs historical facts
3. **Domain Specificity**: General knowledge vs proprietary docs
4. **Confidence**: LLM's certainty in its parametric knowledge
5. **Context History**: Previous turns may already have context

**Examples**:

| Query | Needs Retrieval? | Reason |
|-------|------------------|---------|
| "What is recursion?" | No | Common CS concept, parametric memory sufficient |
| "Configure auth in our v2.5 API" | Yes | Specific to proprietary system, version-dependent |
| "Latest news about AI regulation" | Yes | Current events, beyond training cutoff |
| "Explain the error: ConnectionTimeout" | No | Generic error, parametric memory sufficient |
| "How does our CMS handle versions?" | Yes | Company-specific implementation |

## Timing Strategies

### Strategy 1: Always Inject (Eager Evaluation)

**Approach**: Retrieve context for every query

```typescript
async function alwaysInject(query: string): Promise<string> {
  // STEP 1: Always retrieve
  const docs = await retrieveDocuments(query, topK = 5);

  // STEP 2: Inject into prompt
  const prompt = buildRAGPrompt(query, docs);

  // STEP 3: Generate
  return await llm.generate(prompt);
}
```

**Pros**:
- Simple implementation
- Predictable latency
- No decision logic needed
- Always have context (safe)

**Cons**:
- Wasteful for simple queries
- Added latency every time
- Higher costs
- Context dilution on irrelevant docs

**When to Use**:
- High percentage (>80%) of queries need retrieval
- Cost not a concern
- Latency tolerance high (>500ms acceptable)
- Domain-specific chatbot (all questions about company)

### Strategy 2: Conditional Inject (On-Demand)

**Approach**: Decide if retrieval needed, then fetch

```typescript
async function conditionalInject(query: string): Promise<string> {
  // STEP 1: Decide if retrieval needed
  const needsRetrieval = await shouldRetrieve(query);

  let docs: Document[] = [];
  if (needsRetrieval) {
    // STEP 2a: Retrieve conditionally
    docs = await retrieveDocuments(query);
  }

  // STEP 3: Generate (with or without docs)
  const prompt = docs.length > 0
    ? buildRAGPrompt(query, docs)
    : buildDirectPrompt(query);

  return await llm.generate(prompt);
}

async function shouldRetrieve(query: string): Promise<boolean> {
  // Decision logic (see patterns below)
  // - Classify query complexity
  // - Check LLM confidence
  // - Analyze query patterns
  return complexity === 'high' || confidence < 0.7;
}
```

**Pros**:
- Cost-efficient (avoid unnecessary retrievals)
- Faster for simple queries
- Less context dilution
- Scalable

**Cons**:
- Decision overhead (adds ~50-100ms)
- Risk of false negatives (miss needed retrieval)
- More complex implementation

**When to Use**:
- Mixed query types (general + domain-specific)
- Cost optimization priority
- Low latency required for simple queries
- **Recommended for most production systems**

### Strategy 3: Lazy Evaluation (Deferred)

**Approach**: Retrieve during generation, not upfront

```typescript
async function lazyEvaluation(query: string): Promise<string> {
  // STEP 1: Start generation without context
  let response = await llm.generateWithCallback(query, {
    onToken: async (token, context) => {
      // STEP 2: LLM can request retrieval mid-generation
      if (token === '<RETRIEVE>') {
        const searchQuery = context.lastSentence;
        const docs = await retrieveDocuments(searchQuery);
        return injectDocs(docs);  // Continue with docs
      }
      return token;
    }
  });

  return response;
}
```

**Pros**:
- Retrieve only when actually needed
- LLM-driven decision (self-aware of knowledge gaps)
- Can retrieve multiple times during reasoning
- Maximum efficiency

**Cons**:
- Requires streaming/callback support
- Complex implementation
- Variable latency (retrieval mid-generation)
- Not all LLMs support this pattern

**When to Use**:
- Complex multi-step reasoning
- Uncertain if retrieval needed
- LLM supports special tokens/callbacks
- Research/experimental systems

### Strategy 4: Iterative Refinement (Multi-Step)

**Approach**: Retrieve → Attempt → Retrieve again if insufficient

```typescript
async function iterativeRefinement(query: string): Promise<string> {
  let iteration = 0;
  const maxIterations = 3;
  let accumulatedContext: Document[] = [];

  while (iteration < maxIterations) {
    // STEP 1: Generate with current context
    const response = await llm.generate(
      buildPrompt(query, accumulatedContext)
    );

    // STEP 2: Evaluate response quality
    const quality = await evaluateResponse(response, query);

    if (quality.confident && quality.complete) {
      return response;  // Done!
    }

    // STEP 3: Identify missing information
    const missingInfo = quality.missingInfo;

    // STEP 4: Retrieve additional context
    const newDocs = await retrieveDocuments(missingInfo);
    accumulatedContext.push(...newDocs);

    iteration++;
  }

  // Return best attempt
  return accumulatedContext[accumulatedContext.length - 1];
}
```

**Pros**:
- Self-correcting (fills knowledge gaps)
- Handles complex queries well
- Can discover needed information progressively
- High accuracy

**Cons**:
- Multiple LLM calls (higher cost)
- Variable latency (unpredictable)
- Complex implementation
- Potential for retrieval loops

**When to Use**:
- Complex research questions
- Multi-hop reasoning required
- Quality > cost/speed
- Agentic RAG systems

## Conditional Retrieval Patterns

### Pattern 1: Query Classification

**Approach**: Classify query, decide based on category

```typescript
enum QueryType {
  FACTUAL = 'factual',           // "What is X?" → No retrieval
  PROCEDURAL = 'procedural',     // "How to Y?" → Maybe retrieval
  DOMAIN_SPECIFIC = 'domain',    // About company → Retrieval
  CURRENT_EVENTS = 'current',    // Recent news → Retrieval
}

async function classifyQuery(query: string): Promise<QueryType> {
  // Simple classification with LLM
  const prompt = `
    Classify this query into one of:
    - factual: General knowledge question
    - procedural: How-to question
    - domain: Company/product-specific
    - current: Recent events/news

    Query: ${query}
    Category:
  `;

  const category = await llm.generate(prompt, {maxTokens: 5});
  return category as QueryType;
}

async function conditionalRetrieve(query: string): Promise<string> {
  const type = await classifyQuery(query);

  // Retrieval decision based on type
  const needsRetrieval = type === QueryType.DOMAIN_SPECIFIC ||
                         type === QueryType.CURRENT_EVENTS ||
                         (type === QueryType.PROCEDURAL && await isCompanySpecific(query));

  if (needsRetrieval) {
    const docs = await retrieveDocuments(query);
    return generateWithDocs(query, docs);
  } else {
    return generateDirect(query);
  }
}
```

**Benefits**: Simple, interpretable, fast classification

### Pattern 2: Confidence-Based Retrieval

**Approach**: LLM indicates if it needs external knowledge

**Research**: "When to Retrieve" (2024) - +39% improvement

```typescript
async function confidenceBasedRetrieval(query: string): Promise<string> {
  // STEP 1: Ask LLM if it knows the answer
  const confidencePrompt = `
    <query>${query}</query>

    Do you have sufficient knowledge to answer this question accurately?
    Respond with:
    - YES if you're confident in your internal knowledge
    - NO if you need to retrieve external information
    - UNSURE if you're uncertain

    Response:
  `;

  const confidence = await llm.generate(confidencePrompt, {maxTokens: 10});

  if (confidence.includes('NO') || confidence.includes('UNSURE')) {
    // STEP 2: Retrieve external knowledge
    const docs = await retrieveDocuments(query);
    return generateWithDocs(query, docs);
  } else {
    // STEP 3: Generate from parametric memory
    return generateDirect(query);
  }
}
```

**Research Finding**: Special token `<RET>` trained into model
- Generate `<RET>` when knowledge insufficient
- 95%+ accuracy in retrieval decisions
- Works without explicit confidence check

### Pattern 3: Two-Stage Retrieval

**Approach**: Try without, fallback to retrieval if needed

```typescript
async function twoStageRetrieval(query: string): Promise<string> {
  // STAGE 1: Try without retrieval
  const directResponse = await llm.generate(query, {
    temperature: 0.0,  // Deterministic
    maxTokens: 500
  });

  // STAGE 2: Evaluate response quality
  const quality = await evaluateResponse(directResponse, query);

  if (quality.score > 0.8 && !quality.containsUncertainty) {
    // Good response, return it
    return directResponse;
  }

  // STAGE 3: Fallback to retrieval
  const docs = await retrieveDocuments(query);
  const ragResponse = await llm.generate(
    buildRAGPrompt(query, docs)
  );

  return ragResponse;
}

async function evaluateResponse(
  response: string,
  query: string
): Promise<{score: number; containsUncertainty: boolean}> {
  // Check for uncertainty markers
  const uncertaintyMarkers = [
    "I don't know",
    "I'm not sure",
    "I cannot find",
    "unclear",
    "insufficient information"
  ];

  const hasUncertainty = uncertaintyMarkers.some(marker =>
    response.toLowerCase().includes(marker)
  );

  // Simple scoring (production would use LLM evaluation)
  const score = hasUncertainty ? 0.3 : 0.9;

  return {score, containsUncertainty: hasUncertainty};
}
```

**Benefits**: Best of both worlds, safe fallback

### Pattern 4: Heuristic Quick Decision

**Approach**: Rule-based decisions without LLM call

```typescript
async function quickDecision(query: string): Promise<boolean> {
  // Heuristics:
  // 1. Contains company-specific terms?
  const companyTerms = ['CMS', 'our system', 'version 2.5'];
  if (companyTerms.some(term => query.includes(term))) {
    return true;
  }

  // 2. Asks for recent information?
  const recencyIndicators = ['latest', 'recent', 'current', '2024', '2025'];
  if (recencyIndicators.some(term => query.includes(term))) {
    return true;
  }

  // 3. Complex procedural question?
  if (query.startsWith('How to') && query.length > 50) {
    return true;
  }

  // Default: no retrieval
  return false;
}
```

**Benefits**: Zero latency, no LLM cost for decision

## Lazy Evaluation Techniques

### Technique 1: Retrieval Token

**Approach**: LLM generates special token to request retrieval

**Research**: Adapt-LLM (2024), "When to Retrieve"

```typescript
async function retrievalTokenGeneration(query: string): Promise<string> {
  let context: Document[] = [];
  let response = '';

  // Generate with streaming
  const stream = await llm.generateStream(query);

  for await (const token of stream) {
    if (token === '<RET>') {
      // LLM signals it needs retrieval
      console.log('Retrieval requested by model');

      // Extract search query from context
      const searchQuery = extractSearchQuery(response);

      // Retrieve documents
      const docs = await retrieveDocuments(searchQuery);
      context.push(...docs);

      // Inject context and continue generation
      const augmentedPrompt = buildRAGPrompt(query, context);
      // Re-generate with context
      return await llm.generate(augmentedPrompt);
    }

    response += token;
  }

  return response;
}

function extractSearchQuery(partialResponse: string): string {
  // Extract last question or unclear topic
  const sentences = partialResponse.split('.');
  return sentences[sentences.length - 1].trim();
}
```

**Training**: Fine-tune LLM to generate `<RET>` when knowledge gaps detected

### Technique 2: Interleaved Retrieval

**Approach**: Retrieve between reasoning steps

**Research**: RAT (2024), DeepRAG (2025)

```typescript
async function interleavedRetrieval(query: string): Promise<string> {
  const reasoningSteps: string[] = [];
  const retrievedContext: Document[] = [];

  // STEP 1: Generate initial reasoning
  let currentReasoning = await llm.generate(`
    Think step-by-step to answer: ${query}

    Step 1:
  `);

  reasoningSteps.push(currentReasoning);

  // STEP 2-N: For each reasoning step, retrieve relevant info
  for (let step = 2; step <= 5; step++) {
    // Extract what information is needed
    const informationNeed = await llm.generate(`
      Current reasoning: ${reasoningSteps.join('\n')}

      What information do you need to continue? (be specific)
    `);

    if (informationNeed.includes('sufficient') || informationNeed.includes('no additional')) {
      break;  // Done, no more retrieval needed
    }

    // Retrieve based on information need
    const docs = await retrieveDocuments(informationNeed);
    retrievedContext.push(...docs);

    // Continue reasoning with new context
    currentReasoning = await llm.generate(`
      ${reasoningSteps.join('\n')}

      Retrieved information:
      ${docs.map(d => d.content).join('\n')}

      Step ${step}:
    `);

    reasoningSteps.push(currentReasoning);
  }

  // Final answer synthesis
  return await llm.generate(`
    Based on this reasoning:
    ${reasoningSteps.join('\n')}

    Provide a final, concise answer to: ${query}
  `);
}
```

**Research Finding** (RAT 2024): **+13-43% improvement** across tasks

## Research & Benchmarks

### Key Research Papers (2024-2025)

| Research | Year | Improvement | Key Innovation |
|----------|------|-------------|----------------|
| **"When to Retrieve"** | 2024 | +39% vs always-retrieve | LLM learns `<RET>` token |
| **Controllable Context** | 2024 | 85-95% decision accuracy | One-dimensional sensitivity knob |
| **DeepRAG** | 2025 | +26.4% answer accuracy | Step-by-step retrieval as MDP |
| **LazyGraphRAG** | 2024 | Significant cost reduction | Deferred graph construction |
| **CLARINET** | 2024 | +17-39% improvement | Clarify before retrieve |
| **Auto-RAG** | 2024 | Superior on 6 benchmarks | Multi-turn retriever dialogue |
| **Probing-RAG** | 2024 | Outperforms existing methods | Hidden states predict timing |

### Research Highlights

**1. "When to Retrieve" (April 2024)**
- **Paper**: "When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"
- **Key Innovation**: Train LLMs to generate `<RET>` token when external knowledge needed
- **Results**: 95%+ accuracy in retrieval decisions, +39% improvement vs always-retrieve
- **ArXiv**: [2404.19705](https://arxiv.org/abs/2404.19705)

**2. Controllable Context Sensitivity (November 2024)**
- **Paper**: "Controllable Context Sensitivity and the Knob Behind It"
- **Key Innovation**: Control LLM's reliance on context vs parametric knowledge
- **Results**: 85-95% accuracy distinguishing context-based vs knowledge-based answers
- **ArXiv**: [2411.07404](https://arxiv.org/abs/2411.07404)

**3. DeepRAG (February 2025)**
- **Paper**: "DeepRAG: Thinking to Retrieve Step by Step for Large Language Models"
- **Key Innovation**: Model retrieval as Markov Decision Process (MDP)
- **Results**: +26.4% answer accuracy, improved retrieval efficiency
- **ArXiv**: [2502.01142](https://arxiv.org/abs/2502.01142)

**4. CLARINET (April 2024)**
- **Paper**: "CLARINET: Augmenting Language Models to Ask Clarification Questions"
- **Key Innovation**: Ask clarification questions instead of immediate retrieval
- **Results**: +17% over heuristics, +39% over basic LLM prompts
- **ArXiv**: [2405.15784](https://arxiv.org/abs/2405.15784)

**5. Auto-RAG (November 2024)**
- **Paper**: "Auto-RAG: Autonomous Retrieval-Augmented Generation for LLMs"
- **Key Innovation**: Autonomous multi-turn dialogues with retriever
- **Results**: Superior performance across 6 benchmarks, adaptive iteration count
- **ArXiv**: [2411.19443](https://arxiv.org/abs/2411.19443)

### Performance Benchmarks

**Timing Strategy Comparison**

| Strategy | Avg Latency | Cost/Query | Accuracy |
|----------|-------------|------------|----------|
| **Always Retrieve** | 850ms | $0.0065 | 87% |
| **Never Retrieve** | 320ms | $0.0025 | 62% |
| **Heuristic Conditional** | 480ms | $0.0038 | 83% |
| **LLM-Based Conditional** | 540ms | $0.0042 | 89% |
| **Confidence-Driven** | 590ms | $0.0044 | 91% |
| **Iterative (DeepRAG)** | 1,200ms | $0.0078 | 94% |

**Decision Accuracy**

| Decision Method | Precision | Recall | F1 Score |
|-----------------|-----------|--------|----------|
| **Keyword Heuristics** | 0.72 | 0.68 | 0.70 |
| **Query Classification** | 0.81 | 0.76 | 0.78 |
| **LLM Decision** | 0.89 | 0.85 | 0.87 |
| **Confidence Threshold** | 0.91 | 0.88 | 0.89 |
| **`<RET>` Token (Trained)** | 0.95 | 0.93 | 0.94 |

**Cost Savings Analysis**

| Scenario | Always Retrieve | Conditional | Savings |
|----------|-----------------|-------------|---------|
| 100K queries/month | $65 | $40 | 38% |
| 50% need retrieval | - | - | - |
| With decision caching | $65 | $42.50 | 35% |

## Implementation Patterns

### Pattern 1: Simple Conditional

```typescript
async function simpleConditional(query: string): Promise<string> {
  // Quick heuristic decision
  const needsRetrieval = await quickDecision(query);

  if (needsRetrieval) {
    const docs = await retrieveDocuments(query);
    return generateWithDocs(query, docs);
  }

  return generateDirect(query);
}
```

### Pattern 2: LLM-Based Decision

```typescript
async function llmBasedDecision(query: string): Promise<string> {
  // Ask LLM to decide
  const decision = await llm.generate(`
    <query>${query}</query>

    <task>
    Determine if this query requires retrieving external documents or if you can answer from your internal knowledge.

    Respond with exactly one word:
    - RETRIEVE if external documents needed
    - DIRECT if you can answer directly
    </task>

    Decision:
  `, {maxTokens: 10, temperature: 0.0});

  if (decision.includes('RETRIEVE')) {
    const docs = await retrieveDocuments(query);
    return generateWithDocs(query, docs);
  }

  return generateDirect(query);
}
```

### Pattern 3: Confidence-Driven

```typescript
async function confidenceDriven(query: string): Promise<string> {
  // STEP 1: Generate with confidence scoring
  const result = await llm.generate(query, {
    temperature: 0.0,
    logprobs: true,  // Get token probabilities
    maxTokens: 500
  });

  // STEP 2: Calculate confidence
  const avgLogProb = result.logprobs.reduce((sum, lp) => sum + lp, 0) / result.logprobs.length;
  const confidence = Math.exp(avgLogProb);  // Convert to probability

  // STEP 3: Decide based on confidence
  if (confidence < 0.7) {
    // Low confidence, retrieve and retry
    const docs = await retrieveDocuments(query);
    return generateWithDocs(query, docs);
  }

  return result.text;
}
```

### Pattern 4: Adaptive Multi-Turn

```typescript
class AdaptiveRetriever {
  private conversationContext: Message[] = [];

  async answer(query: string): Promise<string> {
    // Check if context already available in conversation
    if (this.hasRelevantContext(query)) {
      // No retrieval needed, context from previous turns
      return this.generateFromHistory(query);
    }

    // Check if simple query
    const complexity = await this.assessComplexity(query);

    if (complexity === 'simple') {
      return this.generateDirect(query);
    }

    // Complex query, retrieve
    const docs = await this.retrieve(query);
    const response = await this.generateWithDocs(query, docs);

    // Update conversation context
    this.conversationContext.push(
      {role: 'user', content: query},
      {role: 'assistant', content: response}
    );

    return response;
  }

  private hasRelevantContext(query: string): boolean {
    // Check if recent conversation covered this topic
    const recentMessages = this.conversationContext.slice(-4);
    return recentMessages.some(msg =>
      this.semanticSimilarity(msg.content, query) > 0.8
    );
  }
}
```

### Pattern 5: Production Service

```typescript
class ConditionalRetrievalService {
  async decideAndRetrieve(
    query: string,
    conversationHistory: Message[]
  ): Promise<{docs: Document[]; reasoning: string}> {
    // STEP 1: Quick heuristic check
    if (this.isDefinitelyNoRetrieval(query)) {
      return {
        docs: [],
        reasoning: 'Simple query, parametric memory sufficient'
      };
    }

    // STEP 2: Check conversation history
    if (this.hasRecentContext(query, conversationHistory)) {
      return {
        docs: [],
        reasoning: 'Relevant context already in conversation'
      };
    }

    // STEP 3: LLM decision
    const decision = await this.llmDecision(query);

    if (decision.shouldRetrieve) {
      const docs = await this.vectorIndex.search(query, {topK: 5});
      return {
        docs,
        reasoning: decision.reason
      };
    }

    return {
      docs: [],
      reasoning: decision.reason
    };
  }

  private isDefinitelyNoRetrieval(query: string): boolean {
    // Very simple queries
    const simplePatterns = [
      /^what is \w+\?$/i,
      /^define \w+$/i,
      /^explain \w+ in simple terms$/i
    ];

    return simplePatterns.some(pattern => pattern.test(query));
  }

  private hasRecentContext(query: string, history: Message[]): boolean {
    // Check last 2 turns
    const recent = history.slice(-4);
    return recent.some(msg =>
      msg.content.toLowerCase().includes(query.toLowerCase().split(' ')[0])
    );
  }
}
```

## When to Use This Pattern

### Always Inject

| Use When | Avoid When |
|----------|------------|
| >80% queries need retrieval | Cost is a concern |
| Domain-specific chatbot | Mixed query types |
| Cost not a concern | Low latency required |
| Simplicity valued | Simple queries common |
| Predictable latency required | |

### Conditional Inject (Recommended)

| Use When | Avoid When |
|----------|------------|
| Mixed query types (general + specific) | >80% need retrieval anyway |
| Cost optimization priority | Simplicity critical |
| Acceptable decision overhead (~50-100ms) | Decision overhead unacceptable |
| **Recommended for most production systems** | |

### Lazy Evaluation

| Use When | Avoid When |
|----------|------------|
| Complex multi-step reasoning | Simple Q&A |
| Uncertain if retrieval needed | Predictable needs |
| LLM supports streaming/callbacks | Framework limitations |
| Research/experimental use | Production simplicity needed |

### Iterative Refinement

| Use When | Avoid When |
|----------|------------|
| Complex research questions | Speed critical |
| Quality > cost/speed | Budget constrained |
| Multi-hop reasoning required | Simple factual queries |
| Agentic RAG systems | Latency sensitive |

## Trade-offs & Considerations

### Advantages of Conditional Timing

1. **Cost Reduction**: 20-50% savings vs always-retrieve
2. **Faster Simple Queries**: 2-3× speedup when no retrieval
3. **Less Context Dilution**: Fewer irrelevant docs
4. **Scalability**: Efficient resource usage
5. **Adaptive**: Learns from usage patterns

### Disadvantages

1. **Decision Overhead**: +50-100ms for classification
2. **False Negatives Risk**: Miss needed retrievals
3. **Complexity**: More logic to maintain
4. **Requires Tuning**: Decision thresholds need adjustment
5. **Inconsistency**: Same query may behave differently

### Break-Even Analysis

**Conditional retrieval pays off when >20% of queries don't need retrieval.**

```
Decision cost: $0.0001/query
Retrieval cost: $0.0003/query

If 30% of queries skip retrieval:
- Save: 30% × $0.0003 = $0.00009
- Decision adds: $0.0001
- Net: -$0.00001 per query (break-even at 33%)

Recommendation: Use conditional if >30% queries answerable from parametric memory.
```

## Key Takeaways

1. **Not all queries need retrieval**: 30-50% may be answerable from parametric memory
2. **Decision accuracy matters**: 85-95% achievable with proper methods
3. **Start simple**: Heuristics first, add LLM decisions if needed
4. **Check conversation history**: Avoid redundant retrieval
5. **Two-stage works well**: Try direct, fallback to retrieval
6. **Train retrieval tokens**: `<RET>` token achieves 95%+ accuracy
7. **Step-by-step for complex**: DeepRAG pattern +26% accuracy
8. **Clarify before retrieve**: CLARINET pattern +17-39% improvement
9. **Cache decisions**: Repeated queries don't need re-evaluation
10. **Monitor and tune**: Track false negatives, adjust thresholds

## References

1. **"When to Retrieve" (ArXiv 2024)**: Labruna et al., "When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively", [ArXiv:2404.19705](https://arxiv.org/abs/2404.19705)

2. **Controllable Context Sensitivity (ArXiv 2024)**: "Controllable Context Sensitivity and the Knob Behind It", [ArXiv:2411.07404](https://arxiv.org/abs/2411.07404)

3. **DeepRAG (ArXiv 2025)**: "DeepRAG: Thinking to Retrieve Step by Step for LLMs", [ArXiv:2502.01142](https://arxiv.org/abs/2502.01142)

4. **LazyGraphRAG (Microsoft 2024)**: "LazyGraphRAG: Setting a new standard for quality and cost", [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost)

5. **CLARINET (ArXiv 2024)**: "CLARINET: Augmenting Language Models to Ask Clarification Questions", [ArXiv:2405.15784](https://arxiv.org/abs/2405.15784)

6. **Auto-RAG (ArXiv 2024)**: "Auto-RAG: Autonomous Retrieval-Augmented Generation for LLMs", [ArXiv:2411.19443](https://arxiv.org/abs/2411.19443)

7. **Probing-RAG (ArXiv 2024)**: "Probing-RAG: Self-Probing to Guide LLMs in Selective Document Retrieval", [ArXiv:2410.13339](https://arxiv.org/abs/2410.13339)

8. **Agentic RAG Survey (ArXiv 2025)**: "Agentic Retrieval-Augmented Generation: A Survey", [ArXiv:2501.09136](https://arxiv.org/abs/2501.09136)

9. **RAT (ArXiv 2024)**: "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning", [ArXiv:2403.05313](https://arxiv.org/abs/2403.05313)

10. **SCMRAG (AAMAS 2025)**: "SCMRAG: Self-Corrective Multihop RAG System", [AAMAS 2025 Proceedings](https://www.ifaamas.org/Proceedings/aamas2025/pdfs/p50.pdf)

**Next Topic**: [2.3.3 - Injection Format](./2.3.3-injection-format.md)
**Previous Topic**: [2.3.1 - Injection Location](./2.3.1-injection-location.md)
**Layer Index**: [Layer 2: Context Engineering](../../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
