# 2.2.4 KV-Cache Optimization (Memory Efficiency)

> **TL;DR:** KV-Cache optimization reduces memory bottlenecks in LLM inference by compressing, quantizing, or selectively evicting stored key-value attention states—achieving 3.5-400× memory reduction with 2-5× speedup while maintaining quality.
>
> - **Status:** ✅ Complete
> - **Last Updated:** 2024-12
> - **Prerequisites:** [2.2.3 Context Pruning](./2.2.3-context-pruning.md)
> - **Grounded In:** RocketKV (2025), MiniCache (2024), SnapKV (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: KV-Cache Memory Bottleneck](#the-problem-kv-cache-memory-bottleneck)
- [Core Concept: How KV-Cache Works](#core-concept-how-kv-cache-works)
- [Optimization Strategies](#optimization-strategies)
- [Research & Benchmarks](#research--benchmarks)
- [Implementation Patterns](#implementation-patterns)
- [Production Frameworks](#production-frameworks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**KV-Cache Optimization** is an infrastructure-level technique for improving **memory efficiency** and **inference speed** in LLM applications. The Key-Value (KV) cache stores intermediate attention computations, allowing autoregressive generation without recomputing past tokens. However, this cache can grow enormous—often exceeding the model's parameter memory—creating a major bottleneck.

**Key Innovation**: Instead of recomputing attention for all previous tokens during each generation step, KV-Cache:
1. **Stores** key/value projections from past tokens
2. **Reuses** them for subsequent tokens
3. **Optimizes** storage via compression, eviction, and quantization

**Impact** (2024-2025 Research):
- **3.5-5.02× compression** (MiniCache, RocketKV)
- **1.82-3.7× speedup** (FastKV, RocketKV)
- **4-8.2× memory efficiency** (EpiCache, SnapKV)
- **61.6% memory reduction** (XKV)
- **400× compression ratio** (RocketKV extreme case)

KV-Cache optimization is essential for production deployments, especially with long contexts (10K+ tokens) where the cache dominates memory usage.

## The Problem: KV-Cache Memory Bottleneck

### Understanding the KV-Cache Growth

**Attention Mechanism**:
```
For each token, compute:
Q = query_projection(token)
K = key_projection(token)
V = value_projection(token)

Attention(Q, K, V) = softmax(Q @ K.T / √d) @ V
```

**Without KV-Cache** (naive approach):
```python
# Generate token-by-token
generated_tokens = []
for step in range(max_length):
    # Recompute ALL previous keys/values
    K_all = [key_proj(t) for t in context + generated_tokens]  # O(n²) operations!
    V_all = [value_proj(t) for t in context + generated_tokens]

    # Compute attention
    attention = compute_attention(Q_current, K_all, V_all)
    next_token = generate(attention)
    generated_tokens.append(next_token)
```

**Problem**: **O(n²) complexity** — recomputing K/V for all previous tokens at every step.

**With KV-Cache** (standard approach):
```python
# Initialize cache
kv_cache = {'keys': [], 'values': []}

generated_tokens = []
for step in range(max_length):
    # Only compute K/V for NEW token
    K_new = key_proj(current_token)
    V_new = value_proj(current_token)

    # Append to cache
    kv_cache['keys'].append(K_new)
    kv_cache['values'].append(V_new)

    # Use entire cached K/V
    attention = compute_attention(Q_current, kv_cache['keys'], kv_cache['values'])
    next_token = generate(attention)
    generated_tokens.append(next_token)
```

**Benefit**: **O(n) complexity** — only compute new token's K/V, reuse cached ones.

### The Memory Problem

**KV-Cache Size Calculation**:

```
Per-token KV size = 2 × num_layers × hidden_dim × precision

Example (LLaMA-2 13B):
- Layers: 40
- Hidden dim: 5120
- Precision: FP16 (2 bytes)

Per-token = 2 × 40 × 5120 × 2 bytes = 819,200 bytes ≈ 0.8 MB

For 10,000-token context:
Total KV cache = 10,000 × 0.8 MB = 8 GB!

For 100,000-token context:
Total KV cache = 100,000 × 0.8 MB = 80 GB!! (exceeds single GPU)
```

**Comparison**:
- **Model parameters**: LLaMA-2 13B = ~26 GB (FP16)
- **KV cache** (100K tokens): ~80 GB
- **KV cache > model size by 3×!**

### Multi-Turn Memory Growth

**Scenario**: Multi-turn conversation agent (10 turns, 500 tokens each)

```
Turn 1: 500 tokens → KV cache: 400 MB
Turn 5: 2,500 tokens → KV cache: 2 GB
Turn 10: 5,000 tokens → KV cache: 4 GB

With 4 concurrent users:
Total KV cache: 4 × 4 GB = 16 GB (half of A100's 80GB capacity!)
```

**Problem**: KV-cache limits:
1. **Maximum batch size** (fewer concurrent requests)
2. **Maximum context length** (cannot fit long contexts)
3. **Inference cost** (GPU memory is expensive)

## Core Concept: How KV-Cache Works

### Detailed Mechanism

**Transformer Attention**:

```python
class MultiHeadAttention:
    def forward(self, x, kv_cache=None):
        # Query, Key, Value projections
        Q = self.query_proj(x)  # [batch, seq_len, hidden]
        K = self.key_proj(x)    # [batch, seq_len, hidden]
        V = self.value_proj(x)  # [batch, seq_len, hidden]

        # If using cache, concatenate with cached K/V
        if kv_cache is not None:
            K = torch.cat([kv_cache['keys'], K], dim=1)
            V = torch.cat([kv_cache['values'], V], dim=1)

            # Update cache for next iteration
            kv_cache['keys'] = K
            kv_cache['values'] = V

        # Scaled dot-product attention
        attention_scores = (Q @ K.transpose(-2, -1)) / sqrt(d_k)
        attention_probs = softmax(attention_scores, dim=-1)
        output = attention_probs @ V

        return output, kv_cache
```

**Cache Structure**:

```
┌─────────────────────────────────────────────────────────┐
│                    KV-Cache Structure                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Layer 0:  ┌──────────┐  ┌──────────┐                  │
│            │ Head 0   │  │ Head 1   │  ... Head 31     │
│            │ keys: [] │  │ keys: [] │                  │
│            │ vals: [] │  │ vals: [] │                  │
│            └──────────┘  └──────────┘                  │
│                                                         │
│  Layer 1:  ┌──────────┐  ┌──────────┐                  │
│            │ Head 0   │  │ Head 1   │  ... Head 31     │
│            └──────────┘  └──────────┘                  │
│                                                         │
│    ...                                                  │
│                                                         │
│  Layer 39: ┌──────────┐  ┌──────────┐                  │
│            │ Head 0   │  │ Head 1   │  ... Head 31     │
│            └──────────┘  └──────────┘                  │
│                                                         │
│  Size: layers × heads × sequence_length × hidden_dim   │
└─────────────────────────────────────────────────────────┘
```

### Why KV-Cache is Essential

**Without Cache**:
```
Time to generate 100 tokens:
Token 1:  Compute attention for 1 token
Token 2:  Recompute attention for 1+1 = 2 tokens
Token 3:  Recompute attention for 2+1 = 3 tokens
...
Token 100: Recompute attention for 99+1 = 100 tokens

Total computations: 1 + 2 + 3 + ... + 100 = 5,050 attention operations
Complexity: O(n²)
```

**With Cache**:
```
Time to generate 100 tokens:
Token 1:  Compute attention for 1 token, cache K/V
Token 2:  Compute attention for 1 new token, reuse cached K/V (2 total)
Token 3:  Compute attention for 1 new token, reuse cached K/V (3 total)
...
Token 100: Compute attention for 1 new token, reuse cached K/V (100 total)

Total new computations: 100 (one per token)
Complexity: O(n)
```

**Speedup**: **~50× faster** for 100-token generation

## Optimization Strategies

### Strategy 1: Quantization

**Concept**: Reduce precision of K/V tensors

**Standard**: FP16 (2 bytes per value)
**Optimized**: INT8 (1 byte), INT4 (0.5 bytes), or mixed precision

```python
class QuantizedKVCache:
    def __init__(self, quantization_bits=8):
        self.bits = quantization_bits
        self.scale = {}  # Per-tensor scaling factors
        self.cache = {}  # Quantized tensors

    def quantize(self, tensor: torch.Tensor) -> torch.Tensor:
        # Quantize to INT8
        min_val = tensor.min()
        max_val = tensor.max()
        scale = (max_val - min_val) / (2 ** self.bits - 1)

        quantized = ((tensor - min_val) / scale).round().to(torch.int8)
        return quantized, scale

    def dequantize(self, quantized: torch.Tensor, scale: float) -> torch.Tensor:
        return quantized.float() * scale

    def store(self, layer_id: int, keys: torch.Tensor, values: torch.Tensor):
        k_quant, k_scale = self.quantize(keys)
        v_quant, v_scale = self.quantize(values)

        self.cache[layer_id] = {
            'keys': k_quant,
            'values': v_quant
        }
        self.scale[layer_id] = {
            'key_scale': k_scale,
            'value_scale': v_scale
        }
```

**Memory Savings**:
- FP16 → INT8: **2× reduction**
- FP16 → INT4: **4× reduction**
- FP16 → INT2: **8× reduction** (extreme, quality loss)

**Research Finding** (2024): **INT4 quantization maintains 95%+ accuracy** in most tasks.

### Strategy 2: Eviction (Selective Dropping)

**Concept**: Remove less-important K/V pairs from cache

#### Method A: Recency-Based Eviction

```python
class RecencyBasedEviction:
    def __init__(self, max_cache_size=2048):
        self.max_size = max_cache_size

    def evict(self, kv_cache: dict) -> dict:
        current_size = len(kv_cache['keys'])

        if current_size > self.max_size:
            # Keep most recent tokens
            keep_size = self.max_size
            kv_cache['keys'] = kv_cache['keys'][-keep_size:]
            kv_cache['values'] = kv_cache['values'][-keep_size:]

        return kv_cache
```

**Problem**: May discard important early context (e.g., system prompt, critical facts)

#### Method B: Attention-Based Eviction

**Concept**: Evict tokens with low attention scores

```python
class AttentionBasedEviction:
    def __init__(self, keep_ratio=0.5):
        self.keep_ratio = keep_ratio

    def evict(self, kv_cache: dict, attention_scores: torch.Tensor) -> dict:
        # Average attention received by each token
        avg_attention = attention_scores.mean(dim=(0, 1, 2))  # [seq_len]

        # Keep top-K most-attended tokens
        num_keep = int(len(avg_attention) * self.keep_ratio)
        top_k_indices = avg_attention.topk(num_keep).indices

        # Reorder to maintain chronological order
        top_k_indices = top_k_indices.sort().values

        # Evict low-attention K/V pairs
        kv_cache['keys'] = kv_cache['keys'][top_k_indices]
        kv_cache['values'] = kv_cache['values'][top_k_indices]

        return kv_cache
```

**Research Finding** (SnapKV 2024): **Can retain only 10-20% of cache** with minimal accuracy loss by keeping high-attention tokens.

#### Method C: Attention Sink + Sliding Window

**Concept**: Keep initial tokens (attention sinks) + recent window

**Research**: StreamingLLM (MIT 2024) — discovered that **first 4 tokens** act as "attention sinks" that stabilize model behavior.

```python
class AttentionSinkEviction:
    def __init__(self, sink_size=4, window_size=1024):
        self.sink_size = sink_size
        self.window_size = window_size

    def evict(self, kv_cache: dict) -> dict:
        current_size = len(kv_cache['keys'])

        if current_size > self.sink_size + self.window_size:
            # Keep: [first 4 tokens] + [recent 1024 tokens]
            sink_keys = kv_cache['keys'][:self.sink_size]
            sink_values = kv_cache['values'][:self.sink_size]

            window_keys = kv_cache['keys'][-self.window_size:]
            window_values = kv_cache['values'][-self.window_size:]

            kv_cache['keys'] = torch.cat([sink_keys, window_keys])
            kv_cache['values'] = torch.cat([sink_values, window_values])

        return kv_cache
```

**Result**: **Constant memory** O(sink + window), **infinite generation** capability.

### Strategy 3: Compression (Layer-wise Merging)

**Concept**: Merge similar K/V entries across layers

**Research**: MiniCache (2024) — K/V states in adjacent layers are highly similar, especially in deep layers.

```python
class LayerwiseCompression:
    def __init__(self, merge_every=2):
        self.merge_every = merge_every

    def compress(self, kv_cache: dict) -> dict:
        compressed = {}

        # Merge every N layers
        for layer_id in range(0, len(kv_cache), self.merge_every):
            # Get adjacent layers
            layers_to_merge = [
                kv_cache[i] for i in range(layer_id, min(layer_id + self.merge_every, len(kv_cache)))
            ]

            # Average K/V across layers
            merged_keys = torch.stack([l['keys'] for l in layers_to_merge]).mean(dim=0)
            merged_values = torch.stack([l['values'] for l in layers_to_merge]).mean(dim=0)

            # Store merged
            compressed[layer_id] = {'keys': merged_keys, 'values': merged_values}

        return compressed
```

**Memory Savings**: **2-5× reduction** by merging similar layers.

### Strategy 4: Token-Selective Propagation

**Concept**: Only propagate important tokens to deeper layers

**Research**: FastKV (2025)

```python
class TokenSelectivePropagation:
    def __init__(self, importance_threshold=0.3):
        self.threshold = importance_threshold

    def forward(self, x, layer_id):
        # Compute importance scores for each token
        importance = self.compute_importance(x, layer_id)  # [batch, seq_len]

        # Select important tokens
        important_mask = importance > self.threshold

        if layer_id > 0:
            # Only propagate important tokens to next layer
            x = x[:, important_mask]

        return x, important_mask
```

**Result**: **1.82× prefill speedup, 2.87× decoding speedup** (FastKV).

### Strategy 5: Budget Allocation (Dynamic Per-Layer)

**Concept**: Allocate different cache sizes to different layers based on importance

**Research**: Ada-KV (2024), LAVa (2025)

```python
class DynamicBudgetAllocation:
    def __init__(self, total_budget=2048):
        self.total_budget = total_budget
        self.layer_budgets = {}  # Per-layer cache size limits

    def allocate_budgets(self, attention_patterns):
        # Analyze which layers have sparse vs dense attention
        for layer_id, attention in enumerate(attention_patterns):
            sparsity = self.measure_sparsity(attention)

            # Sparse layers get smaller budget
            if sparsity > 0.9:
                self.layer_budgets[layer_id] = 128
            elif sparsity > 0.7:
                self.layer_budgets[layer_id] = 512
            else:
                self.layer_budgets[layer_id] = 1024

    def evict_per_layer(self, kv_cache, layer_id):
        budget = self.layer_budgets.get(layer_id, 512)
        if len(kv_cache['keys']) > budget:
            # Evict to meet budget
            kv_cache = self.evict_to_budget(kv_cache, budget)
        return kv_cache
```

**Result**: **Optimal memory allocation** — more cache where needed, less where not.

## Research & Benchmarks

### Key Research Papers (2024-2025)

| System | Year | Memory Reduction | Speed Improvement | Key Technique |
|--------|------|------------------|-------------------|---------------|
| **RocketKV** | 2025 | Up to 400× | 3.7× end-to-end | Two-stage compression |
| **FastKV** | 2025 | N/A | 1.82× prefill, 2.87× decode | Token-selective propagation |
| **EpiCache** | 2025 | 4-6× | 2.4× latency reduction | Episodic management |
| **MiniCache** | 2024 | 5.02× | 5× throughput | Layer-wise compression |
| **SnapKV** | 2024 | 8.2× | 3.6× generation | Attention-based eviction |
| **Ada-KV** | 2024 | Variable | Significant | Adaptive budget allocation |
| **XKV** | 2024 | 61.6% | 2.1× efficiency, 5.5× throughput | Personalized layer allocation |
| **CacheGen** | 2024 | 3.5-4.3× | 3.2-3.7× total delay | Compression + streaming |
| **TreeKV** | 2025 | 16× (94% reduction) | N/A | Tree-based structure |

### Research Highlights

**1. RocketKV (February 2025)**
- **Paper**: "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression"
- **Key Innovation**: Two-stage compression (coarse eviction + fine-grained sparse attention)
- **Results**: Up to 400× compression ratio, 3.7× end-to-end speedup, 32.6% peak memory reduction
- **ArXiv**: [2502.14051](https://arxiv.org/abs/2502.14051)

**2. MiniCache (May 2024)**
- **Paper**: "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"
- **Key Innovation**: Compress across layers (depth dimension) instead of sequence dimension
- **Results**: 5.02× compression ratio, 5× inference throughput increase, 41% memory footprint reduction
- **ArXiv**: [2405.14366](https://arxiv.org/abs/2405.14366)

**3. SnapKV (April 2024)**
- **Paper**: "SnapKV: LLM Knows What You are Looking for Before Generation"
- **Key Innovation**: LLMs consistently attend to specific features at end of prompts
- **Results**: 3.6× generation speed increase, 8.2× memory efficiency, handles up to 380K tokens
- **ArXiv**: [2404.14469](https://arxiv.org/abs/2404.14469)

**4. EpiCache (September 2025)**
- **Paper**: "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering"
- **Key Innovation**: Episodic memory management—cluster conversation into coherent episodes
- **Results**: Up to 40% accuracy improvement, 4-6× compression, 2.4× latency reduction
- **ArXiv**: [2509.17396](https://arxiv.org/abs/2509.17396)

**5. FastKV (February 2025)**
- **Paper**: "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"
- **Key Innovation**: Token-Selective Propagation (TSP) — only propagate informative tokens
- **Results**: 1.82× prefill speedup, 2.87× decoding speedup
- **ArXiv**: [2502.01068](https://arxiv.org/abs/2502.01068)

### Memory Reduction Comparison

| Method | Memory Reduction | Quality Impact | Speedup |
|--------|------------------|----------------|---------|
| **No Optimization** | 0% | Baseline | 1.0× |
| **Quantization (INT8)** | 50% | <1% loss | 1.1× |
| **Quantization (INT4)** | 75% | 2-5% loss | 1.3× |
| **Eviction (SnapKV)** | 87.5% (keep 12.5%) | <2% loss | 3.6× |
| **Compression (MiniCache)** | 80% (5×) | <1% loss | 5.0× |
| **Hybrid (RocketKV)** | 99.75% (400×) | 5-10% loss | 3.7× |

### Production Metrics Example

**Scenario**: Multi-turn conversational agent (LLaMA-2 13B)

**Configuration**:
- Context: 10K tokens average
- Batch size: 8 concurrent users
- GPU: A100-80GB

**Without Optimization**:
```
Memory per request: 8 GB (KV cache)
Max batch size: 10 (80 GB / 8 GB)
Throughput: ~10 requests concurrently
Cost per hour: ~$3 (cloud GPU pricing)
```

**With Optimization** (SnapKV + Quantization):
```
Memory per request: 1 GB (8× reduction)
Max batch size: 80 (80 GB / 1 GB)
Throughput: ~80 requests concurrently
Cost per hour: ~$0.38 (8× improvement in cost-efficiency)

Annual savings: ($3 - $0.38) × 24 × 365 = $22,963
```

## Implementation Patterns

### Pattern 1: Simple Quantization

**Use Case**: Reduce memory with minimal code changes

```python
class SimpleQuantizedCache:
    """
    INT8 quantization for KV cache.
    Memory savings: 2× (FP16 → INT8)
    """
    def __init__(self):
        self.cache = {}  # {layer_id: {keys, values, scales}}

    def quantize(self, tensor: torch.Tensor) -> tuple:
        # Find min/max
        min_val = tensor.min()
        max_val = tensor.max()

        # Scale to INT8 range [-128, 127]
        scale = (max_val - min_val) / 255
        quantized = ((tensor - min_val) / scale).round().to(torch.int8) - 128

        return quantized, scale, min_val

    def dequantize(self, quantized: torch.Tensor, scale: float, min_val: float):
        return (quantized.float() + 128) * scale + min_val

    def store(self, layer_id: int, keys: torch.Tensor, values: torch.Tensor):
        k_q, k_scale, k_min = self.quantize(keys)
        v_q, v_scale, v_min = self.quantize(values)

        self.cache[layer_id] = {
            'keys': k_q, 'values': v_q,
            'k_scale': k_scale, 'v_scale': v_scale,
            'k_min': k_min, 'v_min': v_min
        }
```

### Pattern 2: Attention-Based Eviction

**Use Case**: Keep only high-attention tokens

```python
class AttentionBasedEviction:
    """
    Evict low-attention tokens from cache.
    Memory savings: 70-90% with keep_ratio=0.1-0.3
    """
    def __init__(self, keep_ratio: float = 0.3):
        self.keep_ratio = keep_ratio

    def evict(
        self,
        kv_cache: dict,
        attention_scores: torch.Tensor
    ) -> dict:
        """
        Args:
            kv_cache: {'keys': [batch, num_heads, seq_len, head_dim],
                       'values': [batch, num_heads, seq_len, head_dim]}
            attention_scores: [batch, num_heads, seq_len, seq_len]
        """
        # Average attention received by each position
        avg_attention = attention_scores.sum(dim=2).mean(dim=(0, 1))  # [seq_len]

        # Keep top-K positions
        seq_len = avg_attention.size(0)
        num_keep = int(seq_len * self.keep_ratio)
        _, top_indices = torch.topk(avg_attention, num_keep)

        # Sort to maintain order
        top_indices = top_indices.sort().values

        # Select from cache
        kv_cache['keys'] = kv_cache['keys'][:, :, top_indices, :]
        kv_cache['values'] = kv_cache['values'][:, :, top_indices, :]

        return kv_cache
```

### Pattern 3: Attention Sink + Sliding Window

**Use Case**: Infinite generation with constant memory

```python
class AttentionSinkCache:
    """
    StreamingLLM-style cache: attention sinks + sliding window.
    Memory: O(sink_size + window_size) constant
    """
    def __init__(self, sink_size: int = 4, window_size: int = 2048):
        self.sink_size = sink_size
        self.window_size = window_size
        self.cache = {'keys': [], 'values': []}

    def update(self, new_keys: torch.Tensor, new_values: torch.Tensor):
        """Add new K/V and maintain size limits."""
        self.cache['keys'].append(new_keys)
        self.cache['values'].append(new_values)

        current_size = len(self.cache['keys'])
        max_size = self.sink_size + self.window_size

        if current_size > max_size:
            # Keep: [sink tokens] + [recent window]
            sink_keys = self.cache['keys'][:self.sink_size]
            sink_values = self.cache['values'][:self.sink_size]

            window_keys = self.cache['keys'][-self.window_size:]
            window_values = self.cache['values'][-self.window_size:]

            self.cache['keys'] = sink_keys + window_keys
            self.cache['values'] = sink_values + window_values

    def get_cache(self) -> dict:
        return {
            'keys': torch.cat(self.cache['keys'], dim=2),
            'values': torch.cat(self.cache['values'], dim=2)
        }
```

### Pattern 4: Dynamic Budget Allocation

**Use Case**: Optimize memory across layers based on attention patterns

```python
class DynamicBudgetCache:
    """
    Per-layer adaptive cache sizes based on attention sparsity.
    Layers with sparse attention get smaller budgets.
    """
    def __init__(self, total_budget: int = 8192):
        self.total_budget = total_budget
        self.layer_budgets = {}
        self.layer_caches = {}

    def analyze_and_allocate(self, attention_patterns: dict):
        """Allocate budgets based on layer sparsity."""
        layer_importances = {}

        for layer_id, attn in attention_patterns.items():
            sparsity = (attn < 0.01).float().mean().item()
            layer_importances[layer_id] = 1 - sparsity

        # Normalize and allocate
        total = sum(layer_importances.values())
        for layer_id, importance in layer_importances.items():
            self.layer_budgets[layer_id] = int(
                self.total_budget * (importance / total)
            )

    def update_layer(self, layer_id: int, keys: torch.Tensor, values: torch.Tensor):
        """Update cache for layer, respecting budget."""
        if layer_id not in self.layer_caches:
            self.layer_caches[layer_id] = {'keys': [], 'values': []}

        self.layer_caches[layer_id]['keys'].append(keys)
        self.layer_caches[layer_id]['values'].append(values)

        # Enforce budget
        budget = self.layer_budgets.get(layer_id, self.total_budget // 40)
        current_size = len(self.layer_caches[layer_id]['keys'])

        if current_size > budget:
            self.layer_caches[layer_id]['keys'] = \
                self.layer_caches[layer_id]['keys'][-budget:]
            self.layer_caches[layer_id]['values'] = \
                self.layer_caches[layer_id]['values'][-budget:]
```

## Production Frameworks

### vLLM with PagedAttention

**vLLM**: State-of-the-art LLM serving framework

**PagedAttention**: Memory management technique inspired by OS virtual memory

**Key Idea**: Store KV cache in non-contiguous "pages" instead of contiguous blocks

```
┌─────────────────────────────────────────────────────────┐
│                 PagedAttention Architecture              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Traditional (Contiguous):                              │
│  ┌──────────────────────────────────────────────┐      │
│  │ Request 1 KV Cache                           │      │
│  └──────────────────────────────────────────────┘      │
│  ┌─────────────────────┐  ← Wasted (fragmentation)     │
│  │ Request 2 KV Cache  │                               │
│  └─────────────────────┘                               │
│                                                         │
│  PagedAttention (Non-Contiguous):                       │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐            │
│  │ R1 │ │ R2 │ │ R1 │ │ R2 │ │ R1 │ │ R2 │            │
│  └────┘ └────┘ └────┘ └────┘ └────┘ └────┘            │
│  ↑ Pages can be allocated anywhere, no fragmentation   │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**Benefits**:
- **Eliminates fragmentation**
- **Flexible memory allocation**
- **Up to 24× throughput improvement** vs naive approach

**Usage**:
```python
from vllm import LLM, SamplingParams

# Initialize with PagedAttention (automatic)
llm = LLM(model="meta-llama/Llama-2-7b-hf")

prompts = ["Tell me about AI"]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# vLLM automatically manages KV cache with PagedAttention
outputs = llm.generate(prompts, sampling_params)
```

### FlashAttention

**FlashAttention**: Optimized attention algorithm that reduces memory IO

**Key Innovation**: Fuse attention operations, minimize memory reads/writes

```python
import torch
from flash_attn import flash_attn_func

# Replace standard attention with FlashAttention
def optimized_attention(q, k, v):
    # q, k, v: [batch, seq_len, num_heads, head_dim]
    return flash_attn_func(q, k, v, causal=True)

# Automatic speedup + memory reduction
```

**Memory Savings**: **10-20× less memory** for attention computation.

### LMCache

**LMCache**: Layerwise KV transfer framework

**Key Innovation**: Load/store KV cache layer-by-layer (not all at once)

**Benefits**:
- **Overlap computation with memory operations**
- **Reduce latency** for cache loading
- **Efficient for multi-tenant serving**

## When to Use This Pattern

### Use KV-Cache Optimization When

| Scenario | Why It Helps |
|----------|--------------|
| **Long Contexts (>4K tokens)** | KV cache grows large, memory becomes bottleneck |
| **High Batch Sizes** | Multiple concurrent requests share GPU memory |
| **Memory-Constrained Deployment** | Limited GPU memory (e.g., T4 16GB), edge devices |
| **Cost Optimization** | Maximize throughput per GPU, reduce cloud costs |
| **Streaming Generation** | Long-running tasks need constant memory |

### Don't Use When

| Scenario | Why |
|----------|-----|
| **Short Contexts (<2K tokens)** | Overhead not justified, memory not constrained |
| **Low Batch Sizes (1-2 concurrent)** | Memory abundant, complexity not worth it |
| **Maximum Quality Priority** | Any quality loss unacceptable (high-stakes applications) |
| **Development/Testing** | Simplicity preferred, debugging easier |

## Trade-offs & Considerations

### Quality-Efficiency Frontier

```
                High Quality
                     │
                     │  Standard (No Optimization)
                     │       ●
                     │
                     │   Conservative Quantization (INT8)
                     │             ●
                     │
                     │       SnapKV (keep 20%)
                     │                 ●
                     │
                     │           MiniCache (5× compression)
                     │                       ●
                     │
                     │               RocketKV (extreme)
                     │                           ●
                     │
              Low Quality  ←───────────────────────────→  High Efficiency
```

### Advantages

1. **Massive Memory Savings**: 50-95% reduction typical
2. **Increased Throughput**: 2-8× more requests per GPU
3. **Cost Reduction**: Directly proportional to throughput increase
4. **Longer Context Support**: Fit 100K+ tokens in memory
5. **Faster Inference**: Less memory → faster transfers
6. **Batch Size Increase**: 5-10× larger batches possible

### Disadvantages

1. **Quality Trade-off**: 1-10% accuracy loss (depending on aggressiveness)
2. **Implementation Complexity**: Non-trivial engineering
3. **Framework Dependency**: May require specific libraries (vLLM, etc.)
4. **Debugging Difficulty**: Compressed cache harder to inspect
5. **Tuning Required**: Optimal settings task-dependent
6. **Hardware Specific**: Some techniques need specific GPU features

**Recommendation**: Start conservative (INT8 quantization), progressively optimize.

## Key Takeaways

1. **KV-Cache is the memory bottleneck** — often exceeds model size for long contexts
2. **O(n²) → O(n)** — caching reduces complexity but creates storage challenge
3. **Five optimization strategies**: quantization, eviction, compression, selective propagation, budget allocation
4. **Attention sinks matter** — keep first 4 tokens for stability (StreamingLLM)
5. **Layer similarity enables compression** — adjacent layers share K/V states (MiniCache)
6. **Attention reveals importance** — use attention scores for eviction decisions (SnapKV)
7. **3.5-400× compression possible** — from conservative to extreme optimization
8. **vLLM + FlashAttention** — production-ready frameworks with automatic optimization
9. **Start conservative** — INT8 quantization, then progressively optimize
10. **Trade-off aware** — quality vs efficiency depends on use case

## References

1. **RocketKV (ArXiv 2025)**: "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", [ArXiv:2502.14051](https://arxiv.org/abs/2502.14051)

2. **FastKV (ArXiv 2025)**: "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation", [ArXiv:2502.01068](https://arxiv.org/abs/2502.01068)

3. **EpiCache (ArXiv 2025)**: Kim et al., "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering", [ArXiv:2509.17396](https://arxiv.org/abs/2509.17396)

4. **MiniCache (ArXiv 2024)**: Liu et al., "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models", [ArXiv:2405.14366](https://arxiv.org/abs/2405.14366)

5. **SnapKV (ArXiv 2024)**: Li et al., "SnapKV: LLM Knows What You are Looking for Before Generation", [ArXiv:2404.14469](https://arxiv.org/abs/2404.14469)

6. **Ada-KV (ArXiv 2024)**: "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation", [ArXiv:2407.11550](https://arxiv.org/abs/2407.11550)

7. **LAVa (ArXiv 2025)**: Nguyen et al., "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation", [ArXiv:2509.09754](https://arxiv.org/abs/2509.09754)

8. **XKV (ArXiv 2024)**: "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference", [ArXiv:2412.05896](https://arxiv.org/abs/2412.05896)

9. **CacheGen (SIGCOMM 2024)**: Liu et al., "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving", [SIGCOMM 2024](https://cs.stanford.edu/~keithw/sigcomm2024/sigcomm24-final1571-acmpaginated.pdf)

10. **TreeKV (ArXiv 2025)**: "TreeKV: Smooth Key-Value Cache Compression with Tree Structures", [ArXiv:2501.04987](https://arxiv.org/abs/2501.04987)

11. **vLLM & PagedAttention**: Kwon et al., "Efficient Memory Management for Large Language Model Serving with PagedAttention", [vLLM Docs](https://docs.vllm.ai/)

12. **FlashAttention**: Dao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention", [GitHub](https://github.com/Dao-AILab/flash-attention)

13. **LMCache**: "Layerwise KV Transfer", [LMCache Docs](https://docs.lmcache.ai/)

14. **DynamicKV (EMNLP 2025)**: "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs", [ACL Anthology](https://aclanthology.org/2025.findings-emnlp.426.pdf)

15. **NACL (ArXiv 2024)**: "NACL: A General and Effective KV Cache Eviction Framework", [ArXiv:2408.03675](https://arxiv.org/abs/2408.03675)

**Next Topic**: [2.3.1 - Injection Location](./2.3.1-injection-location.md)
**Previous Topic**: [2.2.3 - Context Pruning](./2.2.3-context-pruning.md)
**Layer Index**: [Layer 2: Context Engineering](../../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
