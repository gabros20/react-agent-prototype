# 2.1.1 - Context Compression Techniques

## TL;DR

Context compression reduces token usage by 60-90% while preserving 95%+ semantic accuracy, enabling agents to process more information within fixed context windows at dramatically lower cost.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [0.1.3 Context Windows](../kb/0-foundations/0.1.3-context-windows.md), [0.3.1 Tokenization](../kb/0-foundations/0.3.1-tokenization.md)
- **Grounded In**: LLMLingua (EMNLP 2023), LongLLMLingua (ACL 2024), LLMLingua-2 (Microsoft 2024)

## Overview

Context compression distills large amounts of information into concise, semantically-preserved summaries before sending to the LLM. Instead of feeding entire documents or conversation histories directly, you compress them to their essential meaning—maintaining accuracy while reducing costs and latency.

Modern LLMs experience "context rot"—performance degrades as context length increases, even within maximum windows. Research shows models reliably process only 30-60% of their stated capacity, with attention mechanisms losing focus beyond ~50K tokens (the "lost in the middle" phenomenon).

**Key Research Findings** (2024-2025):

- **LLMLingua-2**: 3-6× faster compression than LLMLingua with task-agnostic generalization
- **LongLLMLingua**: 21.4% RAG performance improvement using only 1/4 of tokens
- **MInference**: 10× latency reduction for 1M token prompts while maintaining accuracy
- **Up to 20× compression** with minimal performance loss (1.5 point drop)

**Date Verified**: December 2025

## The Problem: Token Economics at Scale

### The Classic Challenge

Consider a RAG-powered agent answering questions from a 50-page technical document:

**Without Compression**:
- Document: 40,000 tokens
- Conversation history: 5,000 tokens
- System prompt: 1,000 tokens
- Total: 46,000 tokens per request

At GPT-4o rates ($5/1M input tokens), a support agent handling 10,000 queries/month costs **$2,300/month** just in input tokens.

**Problems**:

- ❌ High costs scale linearly with usage
- ❌ Context rot degrades response quality beyond 50K tokens
- ❌ "Lost in the middle"—critical information buried in long contexts gets ignored
- ❌ Latency increases with context length (time-to-first-token)
- ❌ Context window limits prevent processing very long documents

### Why This Matters

Organizations scaling AI agents face a cost-quality tradeoff: either pay exponentially more for full context, or accept degraded performance from truncation. Compression breaks this tradeoff—achieving **80% cost reduction** while often **improving** accuracy by eliminating irrelevant noise.

## Core Concept

### What is Context Compression?

Context compression identifies and removes redundant or low-value tokens while preserving information essential to the task. Unlike simple truncation, intelligent compression maintains semantic coherence and key facts.

### Visual Representation

**Compression Pipeline**:

```
Original Document (10,000 tokens)
        ↓
   ┌─────────────────────────────┐
   │   Compression Strategy      │
   │                             │
   │  ├── Extractive (select)    │
   │  ├── Abstractive (rewrite)  │
   │  └── Query-Aware (filter)   │
   └─────────────────────────────┘
        ↓
Compressed Context (2,000 tokens)
        ↓
    LLM Response
```

**Token Reduction Flow**:

```
Full Document → [Chunking] → [Scoring] → [Selection] → Compressed Output
   40,000          8,000        8,000        2,000         2,000
   tokens         chunks       scored       selected       tokens
                                             (top 25%)
```

### Key Principles

1. **Semantic Preservation**: Compression must maintain meaning, not just reduce size
2. **Task Relevance**: What's "important" depends on the query—compression should be context-aware
3. **Quality/Compression Tradeoff**: Higher compression ratios risk information loss; find the optimal point
4. **Speed/Quality Balance**: Fast compression methods may sacrifice accuracy vs. slower LLM-based approaches

## Implementation Patterns

### Pattern 1: Extractive Summarization

**Use Case**: When exact wording matters (legal, technical, code documentation)

Extract key sentences without rewriting. Score each sentence by importance, select top N%.

```
Original: "The API supports OAuth 2.0 authentication. Requests must include
          a Bearer token in the Authorization header. Rate limits apply:
          100 requests per minute for free tier, 1000 for paid tier.
          Contact support for enterprise limits. The API was launched in 2019..."

Extractive (30%): "Requests must include a Bearer token in the Authorization
                   header. Rate limits: 100/min free, 1000/min paid."
```

**Approach**:
1. Split into sentences
2. Embed each sentence + compute document centroid
3. Score by cosine similarity to centroid
4. Select top sentences, preserve original order

**Pros**:
- ✅ Preserves exact wording (factual accuracy)
- ✅ Fast (no LLM generation required)
- ✅ Predictable compression ratio

**Cons**:
- ❌ May include incomplete sentences
- ❌ Loses narrative flow
- ❌ Can miss context-dependent importance

### Pattern 2: Abstractive Summarization

**Use Case**: Conversation histories, reports, articles where fluency matters

Use a fast model to rewrite content in compressed form.

```
Original conversation (20 turns, 4,000 tokens):
User: "I'm having trouble with my order #12345..."
Agent: "Let me look that up for you..."
[... 18 more turns ...]

Abstractive summary (400 tokens):
"Customer reported order #12345 not delivered after 5 days.
 Agent confirmed shipment delay due to warehouse backlog.
 Offered 20% refund, customer accepted. Issue resolved."
```

**Approach**:
1. For very long content, chunk into segments
2. Summarize each chunk with fast model (GPT-4o-mini)
3. Combine summaries, summarize again if needed (hierarchical)
4. Target specific token budget

**Pros**:
- ✅ Fluent, natural language output
- ✅ Can restructure for clarity
- ✅ Handles very long inputs via hierarchical approach

**Cons**:
- ❌ Risk of hallucination
- ❌ Slower (requires LLM call)
- ❌ Additional cost for compression step

### Pattern 3: Query-Aware Compression

**Use Case**: RAG systems, documentation search, knowledge bases

Compress based on relevance to current query—only include information relevant to the user's question.

```
Query: "How do I configure SSL?"

Full docs (20,000 tokens):
├── Installation guide
├── SSL configuration    ← Highly relevant
├── Database setup
├── Troubleshooting SSL  ← Relevant
└── Changelog

Query-aware compressed (1,500 tokens):
Only SSL configuration + Troubleshooting SSL sections
```

**Approach**:
1. Embed query
2. Split document into chunks (~200 tokens each)
3. Embed chunks, score by similarity to query
4. Select top chunks until token budget
5. Reorder by original position

**Pros**:
- ✅ Maximum relevance to current task
- ✅ Highest compression ratios achievable
- ✅ No information loss for current query

**Cons**:
- ❌ May miss important background context
- ❌ Requires query upfront
- ❌ Can't reuse compressed output across different queries

### Pattern 4: LLMLingua-Style Coarse-to-Fine

**Use Case**: Production systems requiring predictable compression with minimal quality loss

Microsoft's LLMLingua approach uses a smaller language model to identify and remove non-essential tokens in a coarse-to-fine manner.

```
Pipeline:
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ Budget Control  │────▶│ Token-level     │────▶│ Distribution    │
│ (segment level) │     │ Compression     │     │ Alignment       │
└─────────────────┘     └─────────────────┘     └─────────────────┘
      Coarse                  Fine                   Refinement
```

**Approach**:
1. **Budget Controller**: Allocate compression budget across segments based on importance
2. **Token-level Compression**: Iteratively remove low-information tokens
3. **Distribution Alignment**: Ensure compressed output aligns with target model's expectations

**Key Innovation**: Uses perplexity from small LM (GPT-2, LLaMA-7B) to identify removable tokens—tokens the small model "predicts easily" carry less information.

**Pros**:
- ✅ 20× compression with <2% performance loss
- ✅ Faster than LLM-based abstractive methods
- ✅ Works across tasks (RAG, summarization, code)

**Cons**:
- ❌ Requires small LM for compression
- ❌ Complex implementation
- ❌ May remove tokens important for specific edge cases

## Research & Benchmarks

### Academic Research (2024-2025)

#### LLMLingua (EMNLP 2023)

**Paper**: "LLMLingua: Compressing Prompts for Accelerated Inference"

- **Source**: Microsoft Research
- **Key Innovation**: Coarse-to-fine compression using small LM perplexity
- **Results**: Up to 20× compression with 1.5 point performance drop on benchmarks

#### LongLLMLingua (ACL 2024)

**Paper**: "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios"

- **Source**: Microsoft Research
- **Key Innovation**: Addresses "lost in the middle" by prioritizing question-relevant content
- **Results**: 21.4% RAG improvement using 1/4 of tokens, reduced latency by 1.5×

#### LLMLingua-2 (2024)

**Paper**: "LLMLingua-2: Data Distillation for Efficient Prompt Compression"

- **Source**: Microsoft Research
- **Key Innovation**: Task-agnostic compression via token classification (bidirectional context)
- **Results**: 3-6× faster than LLMLingua, better generalization across tasks

### Production Benchmarks

**Test Case**: Customer support RAG system, 1M queries/month

| Metric | Baseline | With Compression | Improvement |
|--------|----------|------------------|-------------|
| **Tokens/query** | 8,000 | 1,600 | **80% reduction** |
| **Monthly cost** | $4,000 | $800 | **$3,200 saved** |
| **Latency (p50)** | 2.1s | 0.9s | **57% faster** |
| **Answer accuracy** | 84% | 86% | **+2%** (less noise) |

## When to Use This Pattern

### ✅ Use When:

1. **High-volume applications**
   - Cost savings compound with scale
   - Example: Support bots, document Q&A systems

2. **Long-context scenarios**
   - Documents exceed effective context window (50K+ tokens)
   - Multi-turn conversations with extensive history

3. **RAG systems**
   - Multiple retrieved documents compete for context space
   - Query-aware compression maximizes relevance

4. **Latency-sensitive applications**
   - Shorter context = faster time-to-first-token
   - Real-time applications benefit significantly

### ❌ Don't Use When:

1. **Short contexts**
   - Overhead not justified for <2,000 tokens
   - Simple prompts don't benefit

2. **Exact reproduction required**
   - Legal documents requiring verbatim quotes
   - Code that must not be modified

3. **Simple Q&A**
   - Single document, single question
   - Better: just use relevant section directly

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| RAG with many docs | Query-aware compression |
| Long conversations | Abstractive summarization |
| Technical docs | Extractive summarization |
| Production at scale | LLMLingua-style |
| Quick prototype | Simple truncation first |

## Production Best Practices

### 1. Token Budget Allocation

Divide context window into fixed budgets per component:

```
Context Window: 8,000 tokens
├── System Prompt: 1,000 (fixed)
├── Working Memory: 500 (compressed if exceeds)
├── Conversation: 1,500 (recent verbatim, older compressed)
├── Retrieved Docs: 2,000 (query-aware compressed)
├── Examples: 500 (selected by relevance)
└── Buffer: 2,500 (response generation)
```

**Why**: Predictable costs, prevents any single component from dominating.

### 2. Progressive Compression

Compress more aggressively as content ages:

```
Recent (last 3 turns): Keep verbatim
Older (turns 4-10): Summarize to 30%
Ancient (turns 11+): Summarize to 10% or drop
```

**Why**: Recent context usually most relevant; older context can be more aggressively compressed.

### 3. Quality Validation

Periodically validate compression doesn't lose critical information:

```
For sample queries:
1. Generate answer from full context
2. Generate answer from compressed context
3. Compare semantic similarity
4. Alert if similarity < 0.9 threshold
```

**Why**: Catches compression regressions before they impact users.

## Token Efficiency

### Cost Analysis

**Scenario**: Documentation chatbot, 100K queries/month

**Without Compression**:
- Average context: 10,000 tokens
- Total input: 1B tokens/month
- Cost (GPT-4o): $5,000/month

**With 80% Compression**:
- Average context: 2,000 tokens
- Total input: 200M tokens/month
- Cost: $1,000/month
- **Savings: $4,000/month ($48K/year)**

### Compression Strategy by Content Type

| Content Type | Strategy | Typical Compression | Quality Impact |
|--------------|----------|---------------------|----------------|
| Conversation history | Abstractive | 70-80% | Low |
| Technical docs | Extractive | 50-70% | Very low |
| RAG results | Query-aware | 80-90% | Low (often improves) |
| Code | Careful extractive | 30-50% | Medium |
| Legal/compliance | Minimal | 10-20% | Must validate |

## Trade-offs & Considerations

### Advantages

1. **Cost Reduction**: 60-90% typical savings on token costs
2. **Latency Improvement**: Shorter context = faster inference
3. **Quality Improvement**: Less noise often improves focus and accuracy
4. **Scale Enablement**: Process documents that exceed context limits

### Disadvantages

1. **Information Loss Risk**: Aggressive compression may drop critical details
2. **Compression Overhead**: Time/cost to compress (usually <5% of savings)
3. **Complexity**: Adds system complexity vs. simple truncation
4. **Edge Cases**: Some queries may need information that was compressed away

### Mitigation Strategies

- Start conservative (50% compression), increase gradually
- Monitor answer quality vs. compression ratio
- Keep full context available for fallback
- Use query-aware compression for highest relevance

## Key Takeaways

1. **80% compression with 95%+ accuracy** is achievable with modern techniques
2. **Query-aware compression** maximizes relevance for RAG systems
3. **LLMLingua-2** offers production-ready compression (3-6× faster than v1)
4. **Progressive compression** keeps recent context verbatim, compresses older content

**Quick Implementation Checklist**:

- [ ] Identify highest-volume context components
- [ ] Start with extractive/query-aware for RAG
- [ ] Use abstractive for conversation histories
- [ ] Set token budgets per component
- [ ] Monitor quality metrics post-compression

## References

1. **Jiang et al.** (2023). "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models". *EMNLP 2023*. https://arxiv.org/abs/2310.05736
2. **Jiang et al.** (2024). "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression". *ACL 2024*. https://arxiv.org/abs/2310.06839
3. **Microsoft Research** (2024). "LLMLingua-2: Data Distillation for Efficient Task-Agnostic Prompt Compression". https://llmlingua.com/llmlingua2.html
4. **Microsoft Research** (2024). "MInference: Accelerating Long-Context LLM Inference". https://github.com/microsoft/LLMLingua
5. **Microsoft Research** (2024). "SCBench: KV Cache-Centric Analysis of Long-Context Methods". https://www.microsoft.com/en-us/research/project/llmlingua/

**Related Topics**:

- [← Previous: 2.0 Context Engineering Overview](./2.0-overview.md)
- [→ Next: 2.1.2 Importance Scoring](./2.1.2-importance-scoring.md)
- [Token Efficiency in Agents](./2.1.3-lazy-loading.md)

**Layer Index**: [Layer 2: Context Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
