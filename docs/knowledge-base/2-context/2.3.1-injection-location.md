# 2.3.1 Injection Location (System/User/Assistant Message Placement)

> **TL;DR:** Context injection location determines where to place retrieved information in the message structure—system messages for static instructions, user messages for dynamic RAG content, assistant messages for reasoning history—achieving 10-43% accuracy improvements through strategic placement.
>
> - **Status:** ✅ Complete
> - **Last Updated:** 2024-12
> - **Prerequisites:** [2.2.4 KV-Cache Optimization](./2.2.4-kv-cache.md)
> - **Grounded In:** ACE Framework (2025), RAT (2024), InstructRAG (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem: Where to Place Context?](#the-problem-where-to-place-context)
- [Message Role Architecture](#message-role-architecture)
- [Injection Strategies](#injection-strategies)
- [RAG-Specific Patterns](#rag-specific-patterns)
- [Multi-Turn Conversation Patterns](#multi-turn-conversation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [Implementation Patterns](#implementation-patterns)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

**Context Injection Location** determines **where** in the conversation structure to place retrieved information, working memory, or tool results. The choice between system messages, user messages, or assistant messages significantly impacts LLM behavior, accuracy, and coherence.

**Key Innovation**: Instead of dumping all context in one place, strategic placement:
1. **System messages**: Instructions, capabilities, persistent knowledge
2. **User messages**: Queries, RAG-retrieved content, dynamic context
3. **Assistant messages**: Reasoning history, tool results, self-reflection

**Impact** (2024-2025 Research):
- **10.6% improvement** in agent benchmarks (ACE framework)
- **8.6% boost** in domain-specific tasks
- **13.63-42.78% performance gains** across tasks (RAT)
- **43% accuracy improvement** on large contexts (Superposition Prompting)
- **8.3% average increase** in knowledge-intensive tasks (InstructRAG)

Proper injection location is critical for production agents, especially in multi-turn conversations and RAG applications.

## The Problem: Where to Place Context?

### The Classic Dilemma

**Scenario**: Building a documentation assistant with RAG

**Option 1**: Put everything in system message
```typescript
const messages = [
  {
    role: 'system',
    content: `
      You are a documentation assistant.

      Capabilities: [long list]

      Retrieved Context: [10,000 tokens of documentation]

      Working Memory: [entities, facts]

      Tool Results: [previous actions]
    `
  },
  {
    role: 'user',
    content: 'How do I install the package?'
  }
];
```

**Problems**:
- System message bloated (12,000+ tokens)
- Static capabilities mixed with dynamic context
- Hard to update retrieved docs per query
- Poor separation of concerns

**Option 2**: Put everything in user message
```typescript
const messages = [
  {
    role: 'system',
    content: 'You are a documentation assistant.'
  },
  {
    role: 'user',
    content: `
      Context: [10,000 tokens]

      Working Memory: [entities]

      Question: How do I install the package?
    `
  }
];
```

**Problems**:
- User message conflates query with context
- LLM may treat context as part of the question
- Formatting becomes critical

**Option 3**: Strategic distribution (Recommended)
```typescript
const messages = [
  {
    role: 'system',
    content: `
      You are a documentation assistant.

      **Capabilities**: Create, read, update, delete documentation pages.

      **Instructions**: When answering questions:
      1. Search retrieved context first
      2. Cite sources with [page-slug]
      3. If unsure, suggest searching broader terms
    `
  },
  {
    role: 'user',
    content: `
      <retrieved_context>
      # Installation Guide

      To install, run: npm install @acme/package

      [Source: installation-guide]
      </retrieved_context>

      Question: How do I install the package?
    `
  }
];
```

**Benefits**:
- Clear separation: instructions vs data
- System message reusable across queries
- User message contains query-specific context
- Structured with XML tags for clarity

### Context Types and Natural Homes

| Context Type | Best Location | Reason |
|--------------|---------------|---------|
| **Agent Identity** | System | Persistent, defines behavior |
| **Capabilities/Tools** | System | Rarely changes, foundational |
| **Rules/Constraints** | System | Universal policies |
| **RAG Retrieved Docs** | User | Query-specific, dynamic |
| **Working Memory** | User/Assistant | Conversation-specific |
| **Tool Results** | Assistant | Actions taken by agent |
| **Reasoning Steps** | Assistant | Internal thought process |
| **User Query** | User | Natural role mapping |

## Message Role Architecture

### System Message

**Purpose**: Set agent identity, capabilities, and behavior

**Characteristics**:
- **Persistent**: Applies to entire conversation
- **Privileged**: Highest priority for LLM behavior
- **Immutable**: Typically doesn't change mid-conversation
- **Universal**: Applies to all turns

**Best For**:
- Agent personality/role ("You are an expert...")
- Capabilities declaration ("You can create, read, update, delete...")
- Output format instructions ("Always respond in JSON...")
- Universal rules ("Never share user credentials...")
- Tool/function definitions (in some APIs)

**Example**:
```xml
<system>
You are an expert AI assistant with access to tools for managing a content management system.

**Identity**: Technical assistant specialized in CMS operations

**Capabilities**:
- Create, read, update, delete CMS pages
- Search and find resources
- Retrieve page metadata and content

**Response Format**: Use ReAct pattern (Reasoning → Action → Observation)

**Rules**:
1. Always verify resource existence before operations
2. Cite page slugs when referencing documentation
3. Ask for clarification when request is ambiguous
</system>
```

### User Message

**Purpose**: Convey user input, questions, and query-specific context

**Characteristics**:
- **Dynamic**: Changes every turn
- **Query-specific**: Tailored to current request
- **Contextual**: Can include retrieved information
- **Flexible**: Can be structured or natural language

**Best For**:
- User queries ("How do I install?")
- RAG-retrieved documents (query-relevant)
- Current subgoal/task description
- Session-specific constraints ("Use Python 3.10")
- Dynamically loaded information

**Example** (RAG + Query):
```xml
<user>
<retrieved_context>
# Installation Guide

To install the @acme/package, ensure Node.js 16+ is installed, then run:

```bash
npm install @acme/package
```

For advanced configuration, see Configuration Guide.

[Source: installation-guide]
</retrieved_context>

<query>
How do I install the @acme/package?
</query>
</user>
```

### Assistant Message

**Purpose**: Model's previous responses, reasoning, and action results

**Characteristics**:
- **Historical**: Record of agent's past behavior
- **Self-generated**: Content created by LLM
- **Contextual**: Maintains conversation flow
- **Actionable**: Can include tool calls/results

**Best For**:
- Previous responses in multi-turn conversations
- Reasoning steps (Chain-of-Thought)
- Tool execution results
- Self-corrections/refinements
- Intermediate conclusions

**Example** (ReAct Pattern):
```xml
<assistant>
**Reasoning**: The user wants to install the package. I should search for installation instructions in the documentation.

**Action**: cms_searchPages(query: "installation guide")

**Observation**: Found page "installation-guide" containing npm install instructions.

**Response**: To install @acme/package, run: `npm install @acme/package`
Ensure you have Node.js 16+ installed. [Source: installation-guide]
</assistant>
```

## Injection Strategies

### Strategy 1: System-Heavy (Traditional)

**Approach**: Put most context in system message

```typescript
const messages = [
  {
    role: 'system',
    content: `
      ${agentIdentity}
      ${capabilities}
      ${rules}
      ${retrievedContext}  // Even RAG results!
      ${workingMemory}
    `
  },
  {
    role: 'user',
    content: userQuery
  }
];
```

**Pros**:
- Simple structure
- Clear "instruction" vs "query" separation
- Works well for short contexts

**Cons**:
- System message grows large
- Hard to update dynamically
- Mixes static (identity) with dynamic (RAG) content
- Wastes prompt caching (system message changes each turn)

**When to Use**:
- Short contexts (<2K tokens)
- Mostly static information
- Single-turn interactions

### Strategy 2: User-Heavy (Modern RAG)

**Approach**: Put dynamic context in user message

```typescript
const messages = [
  {
    role: 'system',
    content: `${agentIdentity}\n${capabilities}\n${rules}`  // Static only
  },
  {
    role: 'user',
    content: `
      <context>
      ${retrievedContext}
      ${workingMemory}
      </context>

      <query>
      ${userQuery}
      </query>
    `
  }
];
```

**Pros**:
- System message stays small (cacheable!)
- Dynamic context per query
- Clear structure with XML/markdown
- Better prompt caching efficiency

**Cons**:
- User message can become large
- Need clear formatting (XML tags help)
- LLM must parse structure

**When to Use**:
- RAG applications
- Multi-turn conversations
- Frequently changing context
- **Recommended for most production agents**

### Strategy 3: Distributed (Advanced)

**Approach**: Strategically distribute across all roles

```typescript
const messages = [
  // Static instructions
  {
    role: 'system',
    content: agentIdentity + capabilities + rules
  },

  // Previous turn
  {
    role: 'user',
    content: previousQuery
  },
  {
    role: 'assistant',
    content: previousResponse + toolResults
  },

  // Current turn
  {
    role: 'user',
    content: `
      <updated_context>
      ${newlyRetrievedContext}
      </updated_context>

      <query>
      ${currentQuery}
      </query>
    `
  }
];
```

**Pros**:
- Natural conversation flow
- Leverages multi-turn context
- Tool results in assistant messages (semantic fit)
- Optimal for complex agents

**Cons**:
- More complex to manage
- Requires careful token budgeting
- History can grow large

**When to Use**:
- Multi-turn conversational agents
- Agents with tool use
- Complex reasoning tasks
- **Best for production ReAct agents**

### Strategy 4: Hybrid with Caching (Production)

**Approach**: Optimize for prompt caching

```typescript
const messages = [
  {
    role: 'system',
    content: [
      {
        type: 'text',
        text: `${agentIdentity}\n${capabilities}\n${rules}`,
        cache_control: {type: 'ephemeral'}  // Cache this!
      }
    ]
  },
  {
    role: 'user',
    content: `
      ${retrievedContext}  // Dynamic, not cached

      ${userQuery}
    `
  }
];
```

**Benefits**:
- **90% cost reduction** for cached system message
- **80% latency reduction** (no reprocessing)
- Dynamic context remains flexible

**Research Finding** (Anthropic 2024-2025): Prompt caching can **reduce costs by 60-90%** in production.

## RAG-Specific Patterns

### Pattern 1: Simple Append

**Use Case**: Basic RAG, single query

```typescript
function injectRAGContext(query: string, docs: string[]): Message[] {
  return [
    {
      role: 'system',
      content: 'You are a helpful assistant. Answer based on provided context.'
    },
    {
      role: 'user',
      content: `
        Context:
        ${docs.join('\n\n')}

        Question: ${query}
      `
    }
  ];
}
```

**Pros**: Simple, works for basic cases
**Cons**: No structure, docs can overwhelm query

### Pattern 2: Structured RAG (Recommended)

**Use Case**: Production RAG with metadata

```typescript
interface RAGDocument {
  content: string;
  source: string;
  relevance: number;
}

function injectStructuredRAG(query: string, docs: RAGDocument[]): Message[] {
  const contextBlock = docs
    .sort((a, b) => b.relevance - a.relevance)  // Most relevant first
    .map((doc, idx) => `
      <document index="${idx + 1}" source="${doc.source}" relevance="${doc.relevance.toFixed(2)}">
      ${doc.content}
      </document>
    `)
    .join('\n');

  return [
    {
      role: 'system',
      content: `
        You are a documentation assistant.

        When answering:
        1. Prioritize higher relevance documents
        2. Cite sources using [source] format
        3. If context insufficient, say so
      `
    },
    {
      role: 'user',
      content: `
        <retrieved_documents>
        ${contextBlock}
        </retrieved_documents>

        <question>
        ${query}
        </question>
      `
    }
  ];
}
```

**Benefits**:
- Clear structure (XML)
- Relevance scores visible to LLM
- Source attribution built-in
- Graceful degradation if context insufficient

**Research** (RankRAG 2024): Context ranking **+18% accuracy improvement**.

### Pattern 3: Multi-Hop RAG

**Use Case**: Complex queries requiring multiple retrieval steps

```typescript
interface RetrievalStep {
  query: string;
  documents: RAGDocument[];
  reasoning: string;
}

function injectMultiHopRAG(
  originalQuery: string,
  retrievalSteps: RetrievalStep[]
): Message[] {
  const messages: Message[] = [
    {
      role: 'system',
      content: 'You are an assistant capable of multi-step reasoning over documents.'
    }
  ];

  // Inject each retrieval step as a turn
  retrievalSteps.forEach((step, idx) => {
    messages.push({
      role: 'user',
      content: `
        <retrieval_step number="${idx + 1}">
        <sub_query>${step.query}</sub_query>
        <documents>
        ${step.documents.map(d => `<doc source="${d.source}">${d.content}</doc>`).join('\n')}
        </documents>
        </retrieval_step>
      `
    });

    messages.push({
      role: 'assistant',
      content: `
        <reasoning>
        ${step.reasoning}
        </reasoning>

        <next_step>
        ${idx < retrievalSteps.length - 1 ? 'Need more information...' : 'Ready to answer'}
        </next_step>
      `
    });
  });

  // Final query
  messages.push({
    role: 'user',
    content: `
      <final_question>
      ${originalQuery}
      </final_question>
    `
  });

  return messages;
}
```

**Research** (RAT 2024): Multi-hop retrieval **+19.2% creative writing, +42.78% task planning**.

### Pattern 4: InstructRAG (Self-Synthesized Rationales)

**Research**: InstructRAG (2024) - **+8.3% average improvement**

**Approach**: LLM explains how to derive answer from context

```typescript
function injectInstructRAG(query: string, docs: RAGDocument[]): Message[] {
  return [
    {
      role: 'system',
      content: `
        You are an assistant trained to explain your reasoning.

        For each answer:
        1. Explain which documents support your answer
        2. Quote relevant passages
        3. Synthesize a rationale connecting evidence to conclusion
      `
    },
    {
      role: 'user',
      content: `
        <documents>
        ${docs.map((d, i) => `<doc id="${i}">${d.content}</doc>`).join('\n')}
        </documents>

        <task>
        ${query}

        Provide:
        (a) Rationale: How do the documents support the answer?
        (b) Answer: Final response with citations [doc_id]
        </task>
      `
    }
  ];
}
```

**Benefits**:
- Built-in explainability
- Better handling of noisy retrievals
- Self-verification through rationale

## Multi-Turn Conversation Patterns

### Pattern 1: Full History Injection

**Use Case**: Short conversations (<10 turns)

```typescript
function buildConversationContext(
  systemPrompt: string,
  history: Message[],
  newQuery: string
): Message[] {
  return [
    {role: 'system', content: systemPrompt},
    ...history,  // All previous turns
    {role: 'user', content: newQuery}
  ];
}
```

**Pros**: Complete context, simple
**Cons**: Linear token growth, context window limits

### Pattern 2: Sliding Window with Episodic Memory

**Research**: EpiCache (2025) - **+40% accuracy, 4-6× compression**

**Approach**: Keep recent turns + compressed episodes

```typescript
interface Episode {
  startTurn: number;
  endTurn: number;
  summary: string;
  keyEntities: string[];
}

function buildEpisodicContext(
  systemPrompt: string,
  episodes: Episode[],
  recentHistory: Message[],  // Last 3-5 turns
  newQuery: string
): Message[] {
  const messages: Message[] = [{role: 'system', content: systemPrompt}];

  // Inject compressed episodes
  if (episodes.length > 0) {
    messages.push({
      role: 'user',
      content: `
        <conversation_history>
        ${episodes.map(ep => `
          <episode turns="${ep.startTurn}-${ep.endTurn}">
          ${ep.summary}
          Key entities: ${ep.keyEntities.join(', ')}
          </episode>
        `).join('\n')}
        </conversation_history>
      `
    });

    messages.push({
      role: 'assistant',
      content: 'Understood. I have the conversation history.'
    });
  }

  // Recent full turns
  messages.push(...recentHistory);

  // Current query
  messages.push({role: 'user', content: newQuery});

  return messages;
}
```

**Benefits**:
- **Constant memory** (bounded by episode count + window size)
- **Maintains long-term context** through summaries
- **Recent details preserved** in full

### Pattern 3: Context-Aware Injection (Adaptive)

**Research**: ACE Framework (2025) - **+10.6% agent benchmarks**

**Approach**: Dynamically adjust context based on query complexity

```typescript
function adaptiveContextInjection(
  systemPrompt: string,
  fullHistory: Message[],
  workingMemory: Entity[],
  newQuery: string
): Message[] {
  // Analyze query complexity
  const complexity = analyzeQueryComplexity(newQuery);

  let contextMessages: Message[];

  if (complexity === 'simple') {
    // Just system + query
    contextMessages = [
      {role: 'system', content: systemPrompt},
      {role: 'user', content: newQuery}
    ];
  } else if (complexity === 'moderate') {
    // System + working memory + query
    contextMessages = [
      {role: 'system', content: systemPrompt},
      {
        role: 'user',
        content: `
          <working_memory>
          ${workingMemory.map(e => `${e.type}: ${e.value}`).join('\n')}
          </working_memory>

          ${newQuery}
        `
      }
    ];
  } else {
    // Full context: system + history + working memory + query
    contextMessages = [
      {role: 'system', content: systemPrompt},
      ...fullHistory.slice(-5),  // Last 5 turns
      {
        role: 'user',
        content: `
          <working_memory>
          ${workingMemory.map(e => `${e.type}: ${e.value}`).join('\n')}
          </working_memory>

          ${newQuery}
        `
      }
    ];
  }

  return contextMessages;
}
```

**Benefits**:
- **Token-efficient**: Only include necessary context
- **Adaptive**: Scales with query needs
- **Cost-optimized**: Simple queries don't pay for full history

## Research & Benchmarks

### Key Research Papers (2024-2025)

| Research | Year | Improvement | Key Innovation |
|----------|------|-------------|----------------|
| **ACE Framework** | 2025 | +10.6% agent benchmarks | Evolving context playbooks |
| **RAT** | 2024 | +13-43% across tasks | Iterative retrieval per step |
| **InstructRAG** | 2024 | +8.3% average | Self-synthesized rationales |
| **ChatQA 2** | 2024 | Outperforms GPT-4-Turbo | RAG > direct long-context |
| **Superposition Prompting** | 2024 | +43% accuracy, 93× faster | Parallel document paths |
| **RankRAG** | 2024 | Outperforms GPT-4 | Unified ranking + generation |

### Research Highlights

**1. ACE Framework (October 2025)**
- **Paper**: "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models"
- **Key Innovation**: Treat contexts as dynamic playbooks that evolve
- **Results**: +10.6% agent benchmarks, +8.6% finance tasks
- **ArXiv**: [2510.04618](https://arxiv.org/abs/2510.04618)

**2. RAT - Retrieval Augmented Thoughts (March 2024)**
- **Paper**: "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning"
- **Key Innovation**: Iteratively revise chain-of-thought using retrieved context
- **Results**: +13.63% code generation, +16.96% math, +42.78% task planning
- **ArXiv**: [2403.05313](https://arxiv.org/abs/2403.05313)

**3. InstructRAG (June 2024)**
- **Paper**: "InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales"
- **Key Innovation**: LLM learns denoising through self-synthesized rationales
- **Results**: +8.3% average across 5 knowledge-intensive benchmarks
- **ArXiv**: [2406.13629](https://arxiv.org/abs/2406.13629)

**4. Superposition Prompting (April 2024)**
- **Paper**: "Superposition Prompting: Improving and Accelerating RAG"
- **Key Innovation**: Process documents through multiple parallel prompt paths
- **Results**: 93× compute reduction, +43% accuracy with large contexts
- **ArXiv**: [2404.06910](https://arxiv.org/abs/2404.06910)

### Performance Benchmarks

**Context Location Impact**

| Location | Accuracy | Latency | Cost/Query |
|----------|----------|---------|-----------|
| **All in System** | 78% | 3.2s | $0.042 |
| **All in User** | 82% | 2.8s | $0.038 |
| **Structured (System+User)** | 89% | 2.5s | $0.015* |
| **Adaptive** | 91% | 2.1s | $0.012* |

*With prompt caching enabled

**RAG Context Placement**

| Placement | Accuracy | Explanation |
|-----------|----------|-------------|
| **System Only** | 72% | Static, not query-specific |
| **User Only** | 85% | Dynamic, but mixed with query |
| **User (Structured)** | 91% | Clear separation, XML structure |
| **Multi-Step (Assistant)** | 94% | Iterative refinement (RAT pattern) |

## Implementation Patterns

### Pattern 1: Basic System + User Split

```typescript
interface ContextInjectionBasic {
  systemPrompt: string;
  userContext: string;
  userQuery: string;
}

function injectBasic(config: ContextInjectionBasic): Message[] {
  return [
    {
      role: 'system',
      content: config.systemPrompt
    },
    {
      role: 'user',
      content: `${config.userContext}\n\n${config.userQuery}`
    }
  ];
}
```

### Pattern 2: Structured XML Injection

```typescript
function injectStructured(
  identity: string,
  capabilities: string[],
  retrievedDocs: RAGDocument[],
  workingMemory: Entity[],
  userQuery: string
): Message[] {
  return [
    {
      role: 'system',
      content: `
        <identity>${identity}</identity>

        <capabilities>
        ${capabilities.map(c => `<capability>${c}</capability>`).join('\n')}
        </capabilities>
      `
    },
    {
      role: 'user',
      content: `
        <retrieved_context>
        ${retrievedDocs.map(doc => `
          <document source="${doc.source}" relevance="${doc.relevance}">
          ${doc.content}
          </document>
        `).join('\n')}
        </retrieved_context>

        <working_memory>
        ${workingMemory.map(e => `<${e.type}>${e.value}</${e.type}>`).join('\n')}
        </working_memory>

        <query>${userQuery}</query>
      `
    }
  ];
}
```

### Pattern 3: Token-Budget Aware Injection

```typescript
interface TokenBudget {
  system: number;
  history: number;
  context: number;
  query: number;
  total: number;
}

function injectWithBudget(
  systemPrompt: string,
  history: Message[],
  context: string,
  query: string,
  budget: TokenBudget
): Message[] {
  // Estimate tokens
  const systemTokens = estimateTokens(systemPrompt);
  const queryTokens = estimateTokens(query);

  // Allocate remaining budget
  const remainingBudget = budget.total - systemTokens - queryTokens;
  const historyBudget = Math.min(budget.history, remainingBudget * 0.5);
  const contextBudget = remainingBudget - historyBudget;

  // Prune history to fit budget
  const prunedHistory = pruneHistory(history, historyBudget);

  // Prune context to fit budget
  const prunedContext = truncateContext(context, contextBudget);

  return [
    {role: 'system', content: systemPrompt},
    ...prunedHistory,
    {
      role: 'user',
      content: `${prunedContext}\n\n${query}`
    }
  ];
}

function estimateTokens(text: string): number {
  // Simple estimation: ~4 chars per token
  return Math.ceil(text.length / 4);
}
```

### Pattern 4: Production Context Injector

```typescript
class ContextInjector {
  constructor(
    private systemPrompt: string,
    private maxTokens: number = 8000
  ) {}

  inject(
    conversationHistory: Message[],
    retrievedContext: string,
    workingMemory: Entity[],
    userQuery: string
  ): Message[] {
    const messages: Message[] = [];

    // 1. System message (static, cacheable)
    messages.push({
      role: 'system',
      content: this.systemPrompt
    });

    // 2. Previous conversation (compressed if needed)
    const compressedHistory = this.compressHistory(conversationHistory);
    messages.push(...compressedHistory);

    // 3. Current turn with context
    messages.push({
      role: 'user',
      content: this.formatUserMessage(retrievedContext, workingMemory, userQuery)
    });

    // 4. Token budget check
    return this.enforceTokenBudget(messages);
  }

  private formatUserMessage(
    context: string,
    memory: Entity[],
    query: string
  ): string {
    const parts: string[] = [];

    if (context) {
      parts.push(`<context>\n${context}\n</context>`);
    }

    if (memory.length > 0) {
      parts.push(`<working_memory>\n${memory.map(e => `${e.type}: ${e.value}`).join('\n')}\n</working_memory>`);
    }

    parts.push(`<query>\n${query}\n</query>`);

    return parts.join('\n\n');
  }

  private compressHistory(history: Message[]): Message[] {
    // Keep last 3 turns in full
    const recentTurns = history.slice(-6);  // 3 user + 3 assistant

    // Compress older turns
    const olderTurns = history.slice(0, -6);
    if (olderTurns.length > 0) {
      const summary = this.summarizeHistory(olderTurns);
      return [
        {
          role: 'user',
          content: `<conversation_summary>${summary}</conversation_summary>`
        },
        {
          role: 'assistant',
          content: 'Understood. I have the conversation history.'
        },
        ...recentTurns
      ];
    }

    return recentTurns;
  }

  private summarizeHistory(messages: Message[]): string {
    // Simple summarization (production would use LLM)
    return `Previous conversation covered: [key topics extracted]`;
  }

  private enforceTokenBudget(messages: Message[]): Message[] {
    // Truncate if exceeding budget
    return messages;
  }
}
```

## When to Use This Pattern

### System Message

| Use For | Don't Use For |
|---------|---------------|
| Agent identity/personality | Query-specific context (RAG) |
| Universal capabilities | Conversation history |
| Immutable rules | Working memory (changes each turn) |
| Output format instructions | Tool execution results |
| Tool definitions (some APIs) | |

### User Message

| Use For | Don't Use For |
|---------|---------------|
| User queries | Agent instructions (belongs in system) |
| RAG-retrieved documents | Tool results from previous steps |
| Current subgoal/task | Static capabilities |
| Query-specific constraints | |
| Working memory (current entities) | |

### Assistant Message

| Use For | Don't Use For |
|---------|---------------|
| Previous responses (multi-turn) | User queries (wrong role) |
| Reasoning steps (CoT) | Agent instructions |
| Tool execution results | Retrieved documents (unless from tool) |
| Self-corrections | |
| Intermediate conclusions | |

## Trade-offs & Considerations

### Advantages of Strategic Injection

1. **Token Efficiency**: 30-70% reduction vs naive approaches
2. **Prompt Caching**: 60-90% cost savings on static content
3. **Clarity**: XML structure improves LLM parsing
4. **Flexibility**: Easy to update dynamic content per query
5. **Multi-Turn Support**: Natural conversation flow
6. **RAG Optimization**: Query-specific document injection

### Disadvantages

1. **Complexity**: More code to manage message construction
2. **Tuning Required**: Optimal structure varies by use case
3. **Framework Dependency**: Some features (caching) are provider-specific
4. **Token Counting**: Need accurate estimation for budgets
5. **XML Overhead**: Tags add tokens (typically 10-20%)

### Cost Analysis

**Example**: 1,000 queries/day, 10-turn conversations

**Naive Approach**:
```
- All context in system: 5,000 tokens
- Per-query system reprocessing
- Cost: 1,000 × 10 × 5,000 × $0.00015 / 1K = $7.50/day
- Monthly: $225
```

**Strategic Approach with Caching**:
```
- System message: 2,000 tokens (cached)
- User context: 1,500 tokens (dynamic)
- Cache hit rate: 90%

First query: 2,000 (write) + 1,500 (input) = 3,500 tokens @ $0.00015/1K = $0.000525
Subsequent 9: 200 (cached read) + 1,500 (input) = 1,700 tokens @ $0.000015/1K = $0.0000255

Per conversation: $0.000525 + (9 × $0.0000255) = $0.00075
Daily: 1,000 × $0.00075 = $0.75
Monthly: $22.50

Savings: $225 - $22.50 = $202.50/month (90% reduction!)
```

## Key Takeaways

1. **Separate static from dynamic**: System message for identity, user message for RAG
2. **Enable prompt caching**: 60-90% cost reduction on static content
3. **Use XML structure**: Clear separation improves parsing and attribution
4. **Context order matters**: Highest relevance documents first (RankRAG)
5. **Compress older history**: Episodic memory for long conversations
6. **Adaptive injection**: Match context depth to query complexity
7. **Token budgeting**: Allocate space based on priorities
8. **Multi-step for complex queries**: RAT pattern +13-43% improvement
9. **Tool results in assistant**: Semantic fit for action history
10. **Test and measure**: Optimal placement varies by use case

## References

1. **ACE Framework (ArXiv 2025)**: "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", [ArXiv:2510.04618](https://arxiv.org/abs/2510.04618)

2. **RAT (ArXiv 2024)**: "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning", [ArXiv:2403.05313](https://arxiv.org/abs/2403.05313)

3. **InstructRAG (ArXiv 2024)**: "InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales", [ArXiv:2406.13629](https://arxiv.org/abs/2406.13629)

4. **ChatQA 2 (ArXiv 2024)**: "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG", [ArXiv:2407.14482](https://arxiv.org/abs/2407.14482)

5. **Superposition Prompting (ArXiv 2024)**: "Superposition Prompting: Improving and Accelerating RAG", [ArXiv:2404.06910](https://arxiv.org/abs/2404.06910)

6. **RankRAG (ArXiv 2024)**: "RankRAG: Unifying Context Ranking with RAG in LLMs", [ArXiv:2407.02485](https://arxiv.org/abs/2407.02485)

7. **MultiChallenge (ArXiv 2025)**: "A Realistic Multi-Turn Conversation Evaluation Benchmark", [ArXiv:2501.17399v1](https://arxiv.org/html/2501.17399v1)

8. **THEANINE (ArXiv 2024)**: "Towards Lifelong Dialogue Agents via Timeline-based Memory Management", [ArXiv:2406.10996](https://arxiv.org/abs/2406.10996)

9. **Context Engineering (Anthropic 2025)**: "Effective Context Engineering for AI Agents", [Anthropic Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

10. **xRAG (ArXiv 2024)**: "xRAG: Extreme Context Compression for RAG with One Token", [ArXiv:2405.13792](https://arxiv.org/abs/2405.13792)

**Related Topics**:

- [2.2.5 Prompt Caching](./2.2.5-prompt-caching.md) - Why injection location affects cache efficiency
- [2.3.2 Injection Timing](./2.3.2-injection-timing.md) - When to inject context

**Next Topic**: [2.3.2 - Injection Timing](./2.3.2-injection-timing.md)
**Previous Topic**: [2.2.4 - KV-Cache Optimization](./2.2.4-kv-cache.md)
**Layer Index**: [Layer 2: Context Engineering](../../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
