# 2.2.3 Context Management Patterns: Context Pruning

## TL;DR

Context pruning selectively removes low-value content from the context window using importance scoring—achieving 40-70% token reduction with maintained or improved task performance by eliminating redundant, expired, or irrelevant information.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [2.2.2 Hierarchical Memory](./2.2.2-hierarchical-memory.md)
- **Grounded In**: AgentDiet (2024) - 39.9-59.7% token reduction; LazyLLM (2024) - 2.34× speedup

## Table of Contents

- [Overview](#overview)
- [The Problem: Context Bloat](#the-problem-context-bloat)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Token Efficiency](#token-efficiency)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Context pruning selectively removes low-value content from the context window while preserving essential information. Unlike compression (which summarizes) or lazy loading (which delays fetching), pruning **actively eliminates** irrelevant, redundant, or expired content to keep context lean and focused.

The key insight: instead of feeding all available information to the LLM, use **importance scoring** to identify and remove low-value content while retaining high-signal information.

**Key Research Findings** (2024-2025):

- **39.9-59.7% token reduction**: AgentDiet trajectory pruning with no performance degradation
- **80% context prunable**: ETH 2024 research shows most context unnecessary
- **2.34× speedup**: LazyLLM dynamic token pruning in prefilling
- **23.84× faster attention**: TokenSelect via KV cache selection

**Date Verified**: 2025-12-03

## The Problem: Context Bloat

### The Classic Challenge

Agent context accumulates low-value content from multiple sources:

**Verbose Tool Results**:
```
Tool returns 500+ tokens:
{
  "id": "123",
  "title": "Getting Started",
  "content": "[3000 words]",
  "metadata": { /* 15 fields */ },
  "relatedPages": [ /* 10 objects */ ]
}

Agent only needs: "Page '123' retrieved: 'Getting Started'"
```

**Waste**: 95% of tokens unnecessary for decision-making.

**Problems**:

- ❌ Verbose tool results (full JSON objects when IDs suffice)
- ❌ Expired entities (deleted resources still in context)
- ❌ Redundant information (same data repeated across observations)
- ❌ Irrelevant context (40+ tools listed when only 1 needed)

### Why This Matters

**30-step agent task without pruning**:
```
Step 1:  5,000 tokens
Step 10: 30,000 tokens
Step 30: 90,000 tokens (approaching limit!)

Average: 40,000 tokens/step
Cost: 1.2M tokens × $0.15/1M = $0.18/task
```

**With 60% pruning**:
```
Step 1:  5,000 tokens
Step 10: 15,000 tokens
Step 30: 35,000 tokens

Average: 18,000 tokens/step
Cost: 540K tokens × $0.15/1M = $0.08/task
Savings: 56%
```

At 1,000 tasks/day: **$36,000 annual savings**.

## Core Concept

### What is Context Pruning?

Context pruning removes content based on importance scoring, keeping only high-value information:

```
┌──────────────────────────────────────────┐
│     Original Context (100 tokens)        │
│  [High] [Low] [High] [Low] [Low] [High]  │
└──────────────────────────────────────────┘
                    ↓ Pruning
┌──────────────────────────────────────────┐
│      Pruned Context (40 tokens)          │
│  [High] ______ [High] ______ ______ [High]│
└──────────────────────────────────────────┘
```

### Key Differences from Related Techniques

| Technique | Mechanism | Result |
|-----------|-----------|--------|
| **Compression** | Summarize content | Shorter, but all content represented |
| **Lazy Loading** | Delay fetching | Selective inclusion |
| **Pruning** | Remove content | Permanent exclusion from context |

### What Gets Pruned vs Protected

**Pruned**:
- Redundant observations (same info across steps)
- Expired entities (deleted/superseded resources)
- Low-relevance tool results
- Error messages from recovered failures

**Protected**:
- Task objective (always in context)
- Current subgoal (essential for decisions)
- Recent high-importance actions (last 2-3 steps)
- Critical entity references

## Implementation Patterns

### Pattern 1: Hybrid Importance Scoring

**Use Case**: Production systems needing robust pruning with multiple signals.

```typescript
interface HybridPruning {
  recencyWeight: number;
  relevanceWeight: number;
  attentionWeight: number;
  threshold: number;
}

async function hybridPruning(
  history: Action[],
  currentGoal: string,
  config: HybridPruning
): Promise<Action[]> {
  const currentStep = history.length;

  const scored = await Promise.all(
    history.map(async (action, idx) => {
      // Recency score (0-1, newer = higher)
      const recencyScore = 1 - (currentStep - idx) / currentStep;

      // Relevance score (0-1, semantic similarity)
      const relevanceScore = await scoreRelevance(action.observation, currentGoal);

      // Weighted combination
      const finalScore =
        (recencyScore * config.recencyWeight) +
        (relevanceScore * config.relevanceWeight);

      return { action, score: finalScore };
    })
  );

  return scored
    .filter(s => s.score >= config.threshold)
    .map(s => s.action);
}

// Usage
const pruned = await hybridPruning(history, currentGoal, {
  recencyWeight: 0.3,
  relevanceWeight: 0.5,
  attentionWeight: 0.2,
  threshold: 0.4
});
```

**Pros**:
- ✅ Robust to edge cases (multiple signals)
- ✅ Tunable per use case
- ✅ Graceful degradation

**Cons**:
- ❌ Embedding costs for relevance scoring
- ❌ More complex implementation

**When to Use**: Production agents where quality matters.

### Pattern 2: Observation Pruning (AgentDiet Style)

**Use Case**: Prune verbose tool observations at the source.

```typescript
class ObservationPruner {
  async pruneObservation(
    observation: string,
    toolName: string
  ): Promise<string> {
    // Strategy 1: Extract only essential information
    if (this.isStructuredTool(toolName)) {
      return this.extractEssentials(observation);
    }

    // Strategy 2: Summarize long observations
    if (observation.length > 500) {
      return this.summarizeObservation(observation);
    }

    // Strategy 3: Keep short observations as-is
    return observation;
  }

  private extractEssentials(observation: string): string {
    try {
      const data = JSON.parse(observation);
      // Keep only essential fields
      return JSON.stringify({
        id: data.id,
        title: data.title,
        status: data.status
        // Omit: content, metadata, relatedPages
      });
    } catch {
      return observation;
    }
  }
}
```

**Pros**:
- ✅ Prunes at source (prevents bloat)
- ✅ Simple rules for common tools
- ✅ No LLM calls needed

**Cons**:
- ❌ Tool-specific rules required
- ❌ May miss nuanced importance

**When to Use**: Systems with verbose tool outputs.

### Pattern 3: Token Budget Allocation

**Use Case**: Enforce strict token limits with priority-based allocation.

```typescript
interface TokenBudget {
  system: number;
  task: number;
  history: number;
  entities: number;
  total: number;
}

class TokenBudgetPruner {
  constructor(private budget: TokenBudget) {}

  async prune(context: AgentContext): Promise<AgentContext> {
    // Score and sort history by importance
    const scoredHistory = await this.scoreHistory(context);

    // Greedily include until budget exhausted
    const includedHistory: Action[] = [];
    let tokenCount = 0;

    for (const item of scoredHistory) {
      if (tokenCount + item.tokens <= this.budget.history) {
        includedHistory.push(item.action);
        tokenCount += item.tokens;
      }
    }

    // Restore chronological order
    includedHistory.sort((a, b) => a.step - b.step);

    return { ...context, history: includedHistory };
  }
}

// Usage
const pruner = new TokenBudgetPruner({
  system: 500,
  task: 200,
  history: 3000,
  entities: 300,
  total: 4000
});
```

**Pros**:
- ✅ Predictable token usage
- ✅ Prioritizes most important content
- ✅ Guaranteed budget compliance

**Cons**:
- ❌ Requires accurate token estimation
- ❌ May cut important content at boundary

**When to Use**: Cost-critical applications with strict budgets.

### Pattern 4: Sliding Window with Importance Protection

**Use Case**: Combine recency with importance to protect valuable old content.

```typescript
class ImportanceProtectedWindow {
  constructor(
    private windowSize: number = 10,
    private importanceThreshold: number = 0.7
  ) {}

  async prune(history: Action[], currentGoal: string): Promise<Action[]> {
    // Always keep recent actions
    const recentActions = history.slice(-this.windowSize);

    // From older actions, keep only high-importance ones
    const olderActions = history.slice(0, -this.windowSize);
    const importantOld = await this.filterImportant(olderActions, currentGoal);

    // Combine: important old + recent
    return [...importantOld, ...recentActions];
  }

  private async filterImportant(
    actions: Action[],
    goal: string
  ): Promise<Action[]> {
    const scored = await Promise.all(
      actions.map(async (action) => ({
        action,
        score: await this.scoreImportance(action, goal)
      }))
    );

    return scored
      .filter(s => s.score >= this.importanceThreshold)
      .map(s => s.action);
  }
}
```

**Pros**:
- ✅ Protects important old content
- ✅ Maintains recency bias
- ✅ Best of both worlds

**Cons**:
- ❌ Importance scoring overhead
- ❌ Threshold tuning required

**When to Use**: Long-running tasks where old context occasionally matters.

## Research & Benchmarks

### Academic Research (2024-2025)

#### LazyLLM (July 2024)

**Paper**: "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"

- **Source**: ArXiv 2407.14057
- **Key Innovation**: Dynamic token selection during generation—compute KV cache only for important tokens
- **Results**: 2.34× speedup in prefilling, no accuracy loss, training-free

**Why It Works**: Attention is inherently sparse—most tokens receive minimal attention. Skip processing unattended tokens.

#### TokenSelect (November 2024)

**Paper**: "TokenSelect: Efficient Long-Context Inference via Dynamic Token-Level KV Cache Selection"

- **Authors**: Wei Wu et al.
- **Source**: ArXiv 2411.02886
- **Key Innovation**: Per-head soft voting for token selection
- **Results**: 23.84× attention speedup, 2.28× end-to-end latency reduction

#### AgentDiet (2024)

**Paper**: "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction"

- **Authors**: Yuan-An Xiao et al.
- **Source**: ArXiv 2509.23586
- **Key Innovation**: Trajectory-level pruning identifying redundant, useless, and expired information
- **Results**: 39.9-59.7% token reduction, 21.1-35.9% cost savings, no performance degradation

### Production Benchmarks

**Test Case**: 30-step documentation agent task

| Pruning Strategy | Avg Tokens | Reduction | Cost/Task | Speed |
|------------------|------------|-----------|-----------|-------|
| **No Pruning** | 42,000 | 0% | $0.63 | Baseline |
| **Recency-Based** | 28,000 | 33% | $0.42 | 1.2× |
| **Relevance-Based** | 18,000 | 57% | $0.27 | 1.8× |
| **Hybrid** | 16,000 | 62% | $0.24 | 2.0× |

**Quality Impact** (threshold tuning):

| Pruning Threshold | Token Reduction | Success Rate |
|-------------------|-----------------|--------------|
| No Pruning | 0% | 87% |
| Aggressive (0.2) | 75% | 82% |
| Moderate (0.3) | 62% | 87% |
| Conservative (0.4) | 48% | 88% |

**Observation**: Moderate pruning maintains quality while achieving significant savings.

## When to Use This Pattern

### ✅ Use When:

1. **Long-Running Tasks** (>15 steps)
   - History accumulates quickly
   - Risk of context overflow
   - Example: Multi-page documentation generation

2. **Verbose Tool Results**
   - Tools return large JSON objects
   - Most metadata unnecessary
   - Example: CMS tools with full page content

3. **Token Budget Constraints**
   - Cost optimization priority
   - Strict latency requirements
   - Example: High-volume production systems

### ❌ Don't Use When:

1. **Short Tasks** (<10 steps)
   - Overhead not justified
   - Context naturally small

2. **Every Detail Matters**
   - Forensic analysis
   - Debugging failed runs
   - Example: Error diagnosis

3. **Development/Testing**
   - Need full context for debugging
   - Verifying agent behavior

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| Simple cost reduction | Recency-based (simplest) |
| Quality-sensitive | Hybrid scoring |
| Verbose tools | Observation pruning |
| Strict budget | Token budget allocation |

## Production Best Practices

### 1. Static vs Dynamic Pruning

**Static Pruning**: Prune before LLM call
- One-time operation
- Reduces input tokens
- Cannot recover pruned content

**Dynamic Pruning**: Prune during generation
- Uses attention scores in real-time
- Adapts per token
- Requires framework support (LazyLLM, TokenSelect)

**Recommendation**: Start with static pruning, upgrade to dynamic for maximum efficiency.

### 2. Heuristic Importance Scoring

Fast scoring without LLM calls:

```typescript
function heuristicScore(action: Action): number {
  let score = 0.0;

  // Entity creation/modification (important)
  if (action.action.includes('create') || action.action.includes('update')) {
    score += 0.5;
  }

  // Contains IDs/references (might be needed)
  if (/\b(id|slug):\s*\S+/i.test(action.observation)) {
    score += 0.3;
  }

  // Success indicators
  if (action.observation.includes('success')) {
    score += 0.2;
  }

  return Math.min(score, 1.0);
}
```

**Why**: Fast, cheap, no external calls—use as first pass before expensive scoring.

### 3. Common Pitfalls

#### ❌ Pitfall 1: Over-Pruning Important References

**Problem**: Pruning an ID needed later.

#### ✅ Solution: Protect Entity References

Always keep actions containing IDs/references that might be needed for future operations.

#### ❌ Pitfall 2: Ignoring Recency

**Problem**: Purely relevance-based pruning removes recent context.

#### ✅ Solution: Always Protect Recent Window

Keep last 3-5 actions regardless of relevance score—they establish current context.

## Token Efficiency

### Context Size Impact

```
Without pruning (30-step task):
- Per step average: 42,000 tokens
- Total: 42,000 × 30 = 1.26M tokens

With 60% pruning:
- Per step average: 16,800 tokens
- Total: 16,800 × 30 = 504K tokens
- Reduction: 756K tokens (60%)
```

### Cost at Scale

**Scenario**: 100 tasks/day, 30 steps each

**Without Pruning**:
```
100 × 30 × 40,000 tokens × $0.15/1M = $180/day
Monthly: $5,400
Annual: $64,800
```

**With 60% Pruning**:
```
100 × 30 × 16,000 tokens × $0.15/1M = $72/day
Monthly: $2,160
Annual: $25,920
Savings: $38,880/year
```

**Pruning Overhead**: ~$9/month for embedding API calls—negligible vs savings.

## Trade-offs & Considerations

### Advantages

1. **40-70% token reduction**: Typical for production systems
2. **Direct cost savings**: Proportional to token reduction
3. **Speed improvement**: Shorter contexts = 1.5-2.5× faster
4. **Quality improvement**: Focused context reduces "lost in the middle" effect

### Disadvantages

1. **Risk of over-pruning**: May remove important content
2. **Scoring overhead**: Embedding calls add latency/cost
3. **Tuning required**: Threshold selection non-trivial
4. **Irreversible**: Once pruned, cannot recover in same call

### Cost Analysis

**Break-even**: Pruning implementation ~1 week. ROI positive after first month at 100+ tasks/day scale.

## Key Takeaways

1. **Hybrid scoring** (recency + relevance) provides best quality/efficiency balance
2. **Protect recent window** always—last 3-5 actions should never be pruned
3. **Prune at source** for verbose tools using observation pruning
4. **Start conservative** (0.4 threshold), decrease gradually while monitoring quality

**Quick Implementation Checklist**:

- [ ] Implement heuristic scoring for fast first-pass
- [ ] Add semantic relevance for important tasks
- [ ] Protect recent window (last 3-5 steps)
- [ ] Set up token budget enforcement
- [ ] Monitor pruning effectiveness with metrics

## References

1. **LazyLLM** (2024). "Dynamic Token Pruning for Efficient Long Context LLM Inference". _ArXiv_. https://arxiv.org/abs/2407.14057
2. **TokenSelect** (2024). "Efficient Long-Context Inference via Dynamic Token-Level KV Cache Selection". _ArXiv_. https://arxiv.org/abs/2411.02886
3. **AgentDiet** (2024). "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction". _ArXiv_. https://arxiv.org/abs/2509.23586
4. **Provence** (2025). "Efficient and robust context pruning for retrieval-augmented generation". _ArXiv_. https://arxiv.org/abs/2501.16214
5. **ThinkPrune** (2025). "Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning". _ArXiv_. https://arxiv.org/abs/2504.01296
6. **GemFilter** (2024). "Discovering the Gems in Early Layers". _ArXiv_. https://arxiv.org/abs/2409.17422
7. **Selective Attention** (2024). "Selective Attention Improves Transformer". _ArXiv_. https://arxiv.org/abs/2410.02703

**Related Topics**:

- [2.2.2 Hierarchical Memory](./2.2.2-hierarchical-memory.md) - Alternative memory organization
- [2.2.4 KV-Cache Optimization](./2.2.4-kv-cache.md) - Infrastructure-level optimization
- [2.3.1 Injection Location](./2.3.1-injection-location.md) - Context placement strategies

**Layer Index**: [Layer 2: Context Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
