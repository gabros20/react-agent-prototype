# 2.2.5 - Prompt Caching & Context Compaction

## TL;DR

Prompt caching saves 50-90% on LLM costs by reusing computed prefixes, but requires careful architecture - modifying system prompts or injecting summaries in the wrong location invalidates the entire cache, costing more than not caching at all.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [2.2.1 Sliding Window](./2.2.1-sliding-window.md), [4.2.4 Summarization Strategies](../4-memory/4.2.4-summarization-strategies.md)
- **Grounded In**: Anthropic Prompt Caching (2024), OpenAI Automatic Caching (2024), DeepSeek Context Caching (2025), OpenCode/Aider production implementations

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-cache-invalidation-destroys-savings)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Multi-Provider Compatibility](#multi-provider-compatibility)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Token Efficiency](#token-efficiency)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Prompt caching is a cost optimization technique where LLM providers store and reuse the computed key-value (KV) cache from previous requests. When your new request shares a common prefix with a cached request, the provider skips recomputing those tokens - delivering 50-90% cost savings on cached portions.

However, **all major providers use prefix-based caching** - the cache key is computed from the sequence of tokens from the start of the prompt. Any modification to early content (system prompt, tools, or injected summaries) invalidates everything that follows.

**Key Research Findings** (2024-2025):

- **Anthropic**: 90% cost reduction on cache reads, 25% write premium, 5-minute TTL
- **OpenAI**: 50% discount on cached tokens, automatic, no configuration needed
- **DeepSeek**: Up to 90% savings with automatic prefix caching
- **Production Impact**: OpenCode limits system prompts to "max 2 system prompt messages for caching purposes"

**Date Verified**: 2025-12-12

## The Problem: Cache Invalidation Destroys Savings

### The Classic Challenge

When agents run long conversations or need to inject context summaries (compaction), developers often add the summary to the system prompt - this seems clean but destroys caching:

```
Request 1:
┌────────────────────────────────────────────────────┐
│ System Prompt (1000 tokens) │ Tools │ Messages     │
│         [CACHED]            │[CACHED]│  [NEW]      │
└────────────────────────────────────────────────────┘
Cache hit: 90% savings on system + tools

Request 2 (with injected summary):
┌────────────────────────────────────────────────────┐
│ System Prompt + Summary (1200 tokens) │ Tools │... │
│              [CACHE MISS]             │[MISS] │    │
└────────────────────────────────────────────────────┘
Cache miss: Full recomputation + 25% cache write premium
```

**Problems**:

- ❌ Every summary injection invalidates the entire cache
- ❌ 25% premium for writing new cache entries (Anthropic)
- ❌ Tools definition (often 3000+ tokens) recomputed every request
- ❌ Net result: Paying MORE than without caching

### Why This Matters

For agentic workloads with 10-50 tool calls per session:
- System prompt + tools: ~4000 tokens
- Without caching: 4000 tokens × 10 calls = 40,000 input tokens
- With proper caching: 4000 cached + 500 new × 10 = 9,000 effective tokens (78% savings)
- With broken caching: 40,000 tokens + 25% write premium = 50,000 effective tokens (25% WORSE)

## Core Concept

### How Prefix Caching Works

All major LLM providers use **prefix-based cache keys**. The cache is indexed by the exact sequence of tokens from the beginning of the prompt:

```
Cache Key = hash(token[0] + token[1] + ... + token[N])
```

This means:
1. **Cumulative hashing**: Changing any early token invalidates all subsequent cache
2. **Prefix matching**: Only content at the START can be cached
3. **No partial matching**: You can't cache just the tools if the system prompt changed

### Visual Representation

**Correct Architecture (cache preserved):**

```
┌─────────────────────────────────────────────────────────────┐
│                     PROMPT STRUCTURE                         │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  [System Prompt]  →  [Tools]  →  [Messages]  →  [New Turn]  │
│       STATIC           STATIC       DYNAMIC        NEW       │
│       [CACHED]        [CACHED]    conversation    content    │
│                                                              │
│  ◄────── PREFIX (stable, cached) ──────►◄─── SUFFIX ───►    │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

**Broken Architecture (cache invalidated):**

```
┌─────────────────────────────────────────────────────────────┐
│                     PROMPT STRUCTURE                         │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  [System + Summary]  →  [Tools]  →  [Messages]              │
│      MODIFIED            NOW          NOW                    │
│    [CACHE MISS]      [CACHE MISS]  [CACHE MISS]             │
│                                                              │
│  ◄────────────── ALL RECOMPUTED ──────────────────►         │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Prefix Stability**: Keep system prompt and tools IDENTICAL across requests
2. **Summary as Conversation**: Inject compaction summaries as user-assistant message pairs, not system prompt modifications
3. **Cache-Aware Ordering**: Most stable content first, most dynamic content last

## Implementation Patterns

### Pattern 1: User-Assistant Compaction Pair (Industry Standard)

**Use Case**: Injecting context summaries during conversation compaction

When context needs compacting, inject the summary as a conversation turn, not a system prompt modification:

```typescript
// ✅ CORRECT: Summary as conversation pair
const compactedMessages = [
  // System prompt unchanged (cached)
  { role: "system", content: STATIC_SYSTEM_PROMPT },

  // Compaction summary as user-assistant exchange
  {
    role: "user",
    content: "[Previous conversation summary request]"
  },
  {
    role: "assistant",
    content: `Summary of conversation so far:
- User is building a landing page for product X
- Created hero section with headline "Transform Your Business"
- Added 3 feature cards: Speed, Security, Scale
- Current focus: CTA button styling`
  },

  // Continue with new messages
  { role: "user", content: "Now add a pricing section" }
];
```

**Why User-Assistant Pair?**

| Approach | Description | Problem |
|----------|-------------|---------|
| System prompt injection | Add summary to system prompt | ❌ Invalidates entire cache |
| Assistant message only | Just put summary in assistant role | ❌ Puts words in LLM's mouth, confusing |
| User-assistant pair | User asks for summary, assistant provides | ✅ Natural flow, cache preserved |

**Pros**:
- ✅ System prompt stays stable (cached)
- ✅ Tools definition stays stable (cached)
- ✅ Natural conversation flow
- ✅ Works across all providers

**Cons**:
- ❌ Adds ~50 tokens for the "summary request" wrapper
- ❌ Slightly less elegant than system prompt injection

**When to Use**: Always use this pattern for context compaction in production systems.

### Pattern 2: Explicit Cache Breakpoints (Anthropic)

**Use Case**: Anthropic Claude with manual cache control

Anthropic requires explicit `cache_control` markers. Place breakpoints strategically:

```typescript
const messages = [
  {
    role: "system",
    content: [
      {
        type: "text",
        text: SYSTEM_PROMPT,
        cache_control: { type: "ephemeral" } // Cache breakpoint 1
      }
    ]
  },
  {
    role: "user",
    content: [
      {
        type: "text",
        text: TOOL_DEFINITIONS_AS_TEXT,
        cache_control: { type: "ephemeral" } // Cache breakpoint 2
      }
    ]
  },
  // Dynamic conversation follows...
];
```

**AI SDK Integration**:

```typescript
import { anthropic } from "@ai-sdk/anthropic";

const result = await generateText({
  model: anthropic("claude-sonnet-4-20250514"),
  messages,
  providerOptions: {
    anthropic: {
      cacheControl: true // Enable cache_control in AI SDK
    }
  }
});
```

**Pros**:
- ✅ Fine-grained control over cache points
- ✅ Can cache large context documents
- ✅ 90% savings on cached portions

**Cons**:
- ❌ Anthropic-specific
- ❌ 25% premium on cache writes
- ❌ 5-minute TTL requires active conversations

**When to Use**: Anthropic workloads with stable large contexts (docs, code files).

### Pattern 3: Automatic Prefix Caching (OpenAI/DeepSeek)

**Use Case**: OpenAI and DeepSeek with automatic caching

These providers cache automatically - no configuration needed, but you must maintain prefix stability:

```typescript
// Keep this IDENTICAL across all requests in a session
const stablePrefix = {
  model: "gpt-4o",
  messages: [
    { role: "system", content: STATIC_SYSTEM_PROMPT },
    // Tools are automatically part of the prefix
  ],
  tools: STATIC_TOOL_DEFINITIONS
};

// Only the suffix changes
async function sendMessage(userMessage: string, history: Message[]) {
  return openai.chat.completions.create({
    ...stablePrefix,
    messages: [
      ...stablePrefix.messages,
      ...history,
      { role: "user", content: userMessage }
    ]
  });
}
```

**Pros**:
- ✅ Zero configuration
- ✅ 50% automatic discount on cached tokens
- ✅ Works transparently

**Cons**:
- ❌ Less control than Anthropic
- ❌ Cache eviction not documented
- ❌ Can't force cache writes

**When to Use**: OpenAI/DeepSeek workloads where you want simplicity.

### Pattern 4: OpenRouter Gateway Caching

**Use Case**: Multi-provider routing through OpenRouter

OpenRouter supports Anthropic caching and routes requests to maintain cache affinity:

```typescript
import { createOpenRouter } from "@openrouter/ai-sdk-provider";

const openrouter = createOpenRouter({ apiKey: process.env.OPENROUTER_API_KEY });

const result = await generateText({
  model: openrouter.chat("anthropic/claude-sonnet-4"),
  messages: messagesWithCacheControl,
  // OpenRouter passes cache_control to Anthropic
});
```

**Key Insight**: OpenRouter maintains provider affinity for caching. Switching between Claude and GPT-4 in the same session will break caches for both.

**Pros**:
- ✅ Multi-provider flexibility
- ✅ Anthropic caching works through gateway
- ✅ Fallback routing available

**Cons**:
- ❌ Additional latency (~50-100ms)
- ❌ Cache affinity requires consistent provider selection
- ❌ Not all providers support caching through gateway

**When to Use**: Production systems needing provider redundancy.

## Multi-Provider Compatibility

All major providers benefit from prefix stability, even with different caching mechanisms:

| Provider | Caching Type | Prefix Stability Impact | Savings |
|----------|--------------|------------------------|---------|
| **OpenAI** (GPT-4o, 4o-mini) | Automatic | ✅ Prefix matching - system prompt changes = cache miss | 50% |
| **DeepSeek** (V3, R1) | Automatic | ✅ Prefix matching - system prompt changes = cache miss | Up to 90% |
| **Anthropic** (Claude) | Manual (`cache_control`) | ✅ Cumulative hash - system prompt changes = full invalidation | 90% read, 25% write premium |
| **Groq** (Kimi K2) | Automatic | ✅ Prefix matching | Varies |
| **Google Gemini** | Implicit | ✅ Prefix matching | Varies |

**Universal Rule**: Regardless of provider, keep your system prompt and tool definitions stable. The user-assistant compaction pair pattern works across ALL providers.

## Research & Benchmarks

### Academic Research (2024-2025)

#### Anthropic Prompt Caching (August 2024)

**Source**: Anthropic Blog - "Prompt Caching"

- **Key Innovation**: Explicit cache_control markers for fine-grained caching
- **Results**:
  - 90% cost reduction on cache reads
  - 25% premium on cache writes
  - 5-minute TTL for ephemeral caches
- **Critical Finding**: "Cache keys are cumulative across cache-control-designated blocks"

#### OpenAI Automatic Caching (October 2024)

**Source**: OpenAI API Documentation

- **Key Innovation**: Zero-configuration prefix caching
- **Results**:
  - 50% discount on cached input tokens
  - Automatic across all models
  - No TTL documented (provider-managed)

#### OpenCode Production Implementation

**Source**: OpenCode GitHub Repository (2025)

- **Key Pattern**: "Max 2 system prompt messages for caching purposes"
- **Compaction Strategy**: User-assistant pairs for summary injection
- **Production Validation**: Used in daily coding assistant with millions of requests

### Production Benchmarks

**Test Case**: CMS Agent with 41 tools, 10-turn conversations

| Metric | No Caching | Broken Caching | Correct Caching |
|--------|------------|----------------|-----------------|
| **System + Tools** | 4,000 tokens/request | 4,000 + 25% write | 400 effective |
| **10-Turn Session** | 40,000 tokens | 50,000 tokens | 8,000 tokens |
| **Cost (GPT-4o)** | $0.40 | $0.50 | $0.08 |
| **Savings** | Baseline | -25% (worse!) | **80%** |

## When to Use This Pattern

### ✅ Use When:

1. **Long Conversations with Compaction**
   - Sessions exceed context window limits
   - Need to summarize and continue

2. **Agentic Workloads**
   - Many tool calls per session
   - Large tool definitions (10+ tools)

3. **Multi-Turn Sessions**
   - 5+ turns per conversation
   - Same user returning frequently

### ❌ Don't Use When:

1. **Single-Turn Requests**
   - No conversation history
   - Cache overhead not worth it

2. **Highly Dynamic System Prompts**
   - System prompt changes every request
   - Per-user customization in system prompt

### Decision Matrix

| Your Situation | Recommended Approach |
|----------------|---------------------|
| Claude with large docs | Explicit cache_control breakpoints |
| OpenAI/DeepSeek agent | Automatic caching, stable prefix |
| Multi-provider with OpenRouter | User-assistant pairs, provider affinity |
| Compaction needed | Always use user-assistant pairs |

## Production Best Practices

### 1. Stable Prefix Architecture

Structure your prompts for maximum cache reuse:

```
STABLE ZONE (cached):
├── System Prompt (static)
├── Tool Definitions (static)
└── Base Instructions (static)

DYNAMIC ZONE (not cached):
├── User-Assistant History
├── Compaction Summaries (as conversation)
└── Current Turn
```

### 2. Compaction Timing

Trigger compaction BEFORE hitting limits, not after:

```
80% capacity → Trigger compaction
     ↓
Summarize oldest messages
     ↓
Inject as user-assistant pair
     ↓
Continue with headroom
```

**Why 80%?** Leaves room for:
- LLM response tokens
- Emergency context if compaction fails
- Gradual degradation vs hard cutoff

### 3. Common Pitfalls

#### ❌ Pitfall 1: System Prompt Injection

```typescript
// BAD: This destroys cache every compaction
const systemPrompt = BASE_PROMPT + "\n\nContext Summary:\n" + summary;
```

**Problem**: Every summary change invalidates entire cache chain.

#### ✅ Solution: Conversation Injection

```typescript
// GOOD: System prompt stays stable
const messages = [
  { role: "system", content: BASE_PROMPT }, // Cached
  { role: "user", content: "Summarize our progress" },
  { role: "assistant", content: summary },
  ...newMessages
];
```

#### ❌ Pitfall 2: Per-Request Tool Filtering

```typescript
// BAD: Different tools = different cache
const tools = filterToolsByUserPermissions(allTools, user);
```

**Problem**: Each user gets different cache, no sharing.

#### ✅ Solution: Runtime Permission Checks

```typescript
// GOOD: Same tools for everyone, check in execute()
const tools = ALL_TOOLS; // Cached across all users

async function execute(input, context) {
  if (!context.user.hasPermission(this.requiredPermission)) {
    return { error: "Permission denied" };
  }
  // ... proceed
}
```

## Token Efficiency

### Cache Impact Analysis

**Scenario**: Agent with 40 tools (4,000 tokens system+tools)

```
Without Caching:
- Request 1: 4,000 (system) + 100 (message) = 4,100 tokens
- Request 2: 4,000 (system) + 200 (messages) = 4,200 tokens
- Request 10: 4,000 (system) + 1,000 (messages) = 5,000 tokens
- Total: 45,500 tokens

With Correct Caching:
- Request 1: 4,000 (cache write) + 100 = 4,100 tokens @ 1.25x
- Request 2: 400 (cache read) + 200 = 600 tokens @ 0.5x
- Request 10: 400 (cache read) + 1,000 = 1,400 tokens @ 0.5x
- Total: ~12,000 effective tokens

Savings: 73%
```

### Optimization Strategies

#### 1. Cache-Aware Tool Ordering

Put most stable, largest tools first:

```typescript
const tools = [
  // Large, never-changing tools (maximizes cache value)
  complexSearchTool,      // 500 tokens
  databaseQueryTool,      // 400 tokens

  // Smaller, stable tools
  formatTool,             // 100 tokens
  validateTool,           // 100 tokens

  // If any tools change, put them last
  experimentalTool,       // 50 tokens (might change)
];
```

#### 2. Summary Compression

Keep compaction summaries concise:

```
❌ Verbose (500 tokens):
"In our conversation, we discussed many topics including..."

✅ Concise (100 tokens):
"Progress: Landing page with hero, 3 features, CTA. Current: pricing section."
```

## Trade-offs & Considerations

### Advantages

1. **50-90% Cost Reduction**: Significant savings at scale
2. **Lower Latency**: Cached prefixes compute faster
3. **No Accuracy Impact**: Same model, same quality

### Disadvantages

1. **Architectural Constraints**: Must keep prefix stable
2. **TTL Management**: Caches expire (5 min Anthropic)
3. **Provider Lock-in**: Cache strategies differ per provider

### Cost Analysis

**Scenario**: 100,000 agent sessions/month, 10 turns each

**Without Optimization**:
```
- 1M requests × 4,000 cached tokens = 4B input tokens
- Cost: $40,000/month (GPT-4o @ $10/M)
```

**With Correct Caching**:
```
- Cache hits: 90% of requests
- Effective tokens: 400M (90% reduction)
- Cost: $4,000/month
- Savings: $36,000/month ($432K/year)
```

## Key Takeaways

1. **Prefix Stability is Universal** - All providers use prefix-based caching; keep system prompt and tools unchanged
2. **User-Assistant Pairs for Compaction** - Never inject summaries into system prompt; use conversation turns
3. **Cache-Aware Architecture** - Structure prompts with stable content first, dynamic content last
4. **Provider Differences Matter** - Anthropic needs manual markers, OpenAI/DeepSeek are automatic

**Quick Implementation Checklist**:

- [ ] System prompt is identical across all requests in session
- [ ] Tool definitions are identical across all requests
- [ ] Compaction summaries injected as user-assistant pairs
- [ ] Cache breakpoints set correctly (Anthropic)
- [ ] Monitoring cache hit rates in production

## References

1. **Anthropic** (2024). "Prompt Caching". Anthropic Blog. https://www.anthropic.com/news/prompt-caching
2. **OpenAI** (2024). "Prompt Caching". OpenAI API Documentation. https://platform.openai.com/docs/guides/prompt-caching
3. **DeepSeek** (2025). "Context Caching". DeepSeek API Documentation. https://api-docs.deepseek.com/
4. **OpenCode** (2025). "Context Management". GitHub Repository. https://github.com/opencode-ai/opencode
5. **Anthropic** (2024). "Context Engineering Guide". Anthropic Documentation.
6. **Aider** (2024). "Prompt Caching with Claude". Aider Blog. https://aider.chat/

**Related Topics**:

- [2.2.1 Sliding Window](./2.2.1-sliding-window.md) - Basic context management
- [2.2.3 Context Pruning](./2.2.3-context-pruning.md) - Reducing context size
- [4.2.4 Summarization Strategies](../4-memory/4.2.4-summarization-strategies.md) - What to include in compaction summaries

**Layer Index**: [Layer 2: Context Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
