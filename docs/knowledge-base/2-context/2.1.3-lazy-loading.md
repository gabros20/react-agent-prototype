# 2.1.3 - Lazy Loading (Fetch on Demand)

## TL;DR

Lazy loading defers content fetching until actually needed, starting with lightweight metadata and retrieving full content only when the agent determines it's necessary—achieving 90%+ token reduction for browsing operations while maintaining complete data access.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [2.1.1 Compression](./2.1.1-compression.md), [2.1.2 Importance Scoring](./2.1.2-importance-scoring.md)
- **Grounded In**: Context Engineering (Anthropic 2025), MCP Tool Design (2025), Agent Memory Patterns (2024)

## Overview

Lazy loading applies the classic software engineering principle of deferred execution to AI agent context management. Instead of prefetching all possible context upfront (expensive, wasteful), agents start with minimal context and fetch additional details only when needed.

This mirrors how modern applications work: load a list of items with titles first, fetch full details when a user clicks. For agents, this means providing tools that expose lightweight metadata by default, with full content available on-demand.

**Key Research Findings** (2024-2025):

- **60% cost reduction** achievable with lazy loading combined with RAG and smart windowing
- **Progressive disclosure** enables agents to discover context through exploration
- **MCP scaling**: With 100s-1000s of tools, loading all definitions upfront slows agents significantly
- **Context window growth**: From 4K (2022) to 1M+ (2025) tokens hasn't eliminated need for lazy loading—effective processing capacity still limited

**Date Verified**: December 2025

## The Problem: Eager Loading Waste

### The Classic Challenge

Consider an agent helping manage a content repository:

**Without Lazy Loading** (eager approach):

```
Agent starts task: "Help user find articles about React"

Eager loading:
├── Load ALL 500 articles with full content
├── Each article: ~2,000 tokens
├── Total context: 1,000,000 tokens
├── Most content never referenced
└── Context window: EXCEEDED
```

**What actually happens**:
1. User asks about React articles
2. Agent needs ~10 relevant titles to show
3. User picks 2 to read in detail
4. Only 2 articles needed full content

**Problems**:

- ❌ Massive token waste (99% of loaded content unused)
- ❌ High database/API load on every request
- ❌ Slow initial response (loading everything)
- ❌ Context window overflow for large datasets
- ❌ Costs scale with dataset size, not task complexity

### Why This Matters

Without lazy loading, agent costs scale with data size regardless of task complexity. A simple "list all pages" query costs the same as complex multi-page analysis because everything loads upfront.

Real-world metrics from production agents:
- **92% of loaded context never referenced** in typical sessions
- **10× latency improvement** with lazy loading for listing operations
- **Per-query costs drop from $1.50 to $0.12** (based on typical content repository)

## Core Concept

### What is Lazy Loading?

Lazy loading is a design pattern where resources are loaded only when explicitly requested, rather than pre-emptively. For AI agents, this means:

1. Start with lightweight identifiers (titles, slugs, summaries)
2. Agent decides what needs detail based on task
3. Full content fetched only for items being actively processed

### Visual Representation

**Eager vs. Lazy Loading**:

```
EAGER LOADING:
┌──────────────────────────────────────────────┐
│ Query: "List React articles"                  │
│                                              │
│ Load: ALL 500 articles × 2,000 tokens each   │
│ Total: 1,000,000 tokens                      │
│ Used: 10 titles (500 tokens)                 │
│ Waste: 99.95%                                │
└──────────────────────────────────────────────┘

LAZY LOADING:
┌──────────────────────────────────────────────┐
│ Query: "List React articles"                  │
│                                              │
│ Step 1: Load titles only (500 tokens)        │
│ Step 2: User selects 2 articles              │
│ Step 3: Load 2 articles × 2,000 = 4,000      │
│ Total: 4,500 tokens                          │
│ Savings: 99.55%                              │
└──────────────────────────────────────────────┘
```

**Lazy Loading Flow**:

```
User Request
     ↓
┌─────────────────┐
│ Load Metadata   │ ← Lightweight (titles, IDs)
│ (~50 tokens)    │
└─────────────────┘
     ↓
Agent Decides: Need details?
  ├── No  → Respond with metadata
  └── Yes → Fetch specific items
              ↓
        ┌─────────────────┐
        │ Load Full Item  │ ← Heavy (content, sections)
        │ (~1,000 tokens) │
        └─────────────────┘
              ↓
         Respond with details
```

### Key Principles

1. **Minimal upfront**: Load only identifiers initially
2. **Agent-driven**: Let the agent decide what needs detail
3. **Incremental disclosure**: Reveal information progressively
4. **Cost proportional to task**: Simple tasks = low cost, complex tasks = proportional cost

## Implementation Patterns

### Pattern 1: Two-Tier Fetching

**Use Case**: Most common pattern—metadata vs. full content

Provide tools with explicit fetch mode parameter.

```
Tool: getPage(slug, fetchMode)

fetchMode: 'lightweight'
├── id, title, slug
├── summary (1-2 sentences)
├── tags, author, date
└── ~50 tokens

fetchMode: 'full'
├── Everything from lightweight
├── Full content (HTML/markdown)
├── All sections with content
└── ~1,000+ tokens
```

**Agent workflow**:

```
User: "What pages do we have about authentication?"

Agent thinks: "Need to list pages, not read them yet"

Tool call: listPages(query: "authentication", fetchMode: "lightweight")
Result: 8 pages found:
- Authentication Guide (slug: auth-guide)
- OAuth Setup (slug: oauth-setup)
- Session Management (slug: sessions)
[...]

Agent responds: "Found 8 pages about authentication: [list]"

User: "Show me the OAuth Setup page"

Agent thinks: "Now I need full content for one specific page"

Tool call: getPage(slug: "oauth-setup", fetchMode: "full")
Result: [Full 1,200 token content]

Agent responds: [Detailed OAuth content]
```

**Token savings**:
- Without lazy loading: 8 pages × 1,000 = 8,000 tokens
- With lazy loading: 8 × 50 + 1 × 1,000 = 1,400 tokens
- **82% reduction**

### Pattern 2: Three-Tier Fetching

**Use Case**: Large content with frequently-accessed summaries

Add intermediate tier for common use cases.

```
Tier 1: Metadata (~10 tokens)
├── id, title, slug
└── updatedAt

Tier 2: Summary (~50 tokens)
├── Everything from Tier 1
├── summary, tags
└── author, sectionCount

Tier 3: Full (~1,000+ tokens)
├── Everything from Tier 2
├── Full content
└── All sections, metadata
```

**When to use each tier**:

| Operation | Tier | Tokens |
|-----------|------|--------|
| List all pages | Metadata | 10 per item |
| Search results | Summary | 50 per item |
| Read/edit page | Full | 1,000+ per item |

### Pattern 3: Paginated Loading

**Use Case**: Large datasets that can't fit even as metadata

Load results in batches as agent needs more.

```
Initial request: "Show all articles"

Batch 1: Load first 20 titles
├── Has more: true
├── Total: 500
└── Tokens: 200

Agent: "Found 500 articles. Here are the first 20: [list]"

User: "Show me more"

Batch 2: Load next 20 titles
├── Has more: true
├── Total: 500
└── Tokens: 200

[Continue until user satisfied or all loaded]
```

**Benefits**:
- Never loads more than needed
- Works with any dataset size
- User controls pagination

### Pattern 4: Hierarchical Loading

**Use Case**: Nested content structures (categories → pages → sections)

Load progressively from broad to specific.

```
Level 1: Categories
├── Category: "Frontend" (15 pages)
├── Category: "Backend" (12 pages)
└── Category: "DevOps" (8 pages)
    Tokens: 30

Level 2: Pages in "Frontend"
├── React Basics
├── Vue Guide
└── Angular Tutorial
    Tokens: 150

Level 3: React Basics content
├── Full article
├── All sections
└── Related pages
    Tokens: 1,200
```

**Agent workflow**:

```
User: "Tell me about our frontend documentation"

Step 1: listCategories() → 30 tokens
Agent: "We have Frontend (15 pages), Backend (12), DevOps (8)"

User: "What's in Frontend?"

Step 2: listPagesInCategory("Frontend") → 150 tokens
Agent: "React Basics, Vue Guide, Angular Tutorial, ..."

User: "Explain React Basics"

Step 3: getPage("react-basics", "full") → 1,200 tokens
Agent: [Detailed React content]

Total: 1,380 tokens vs 35,000 (all pages eager loaded)
```

### Pattern 5: Progressive Tool Enhancement

**Use Case**: Agentic systems with many available tools

Load tool definitions lazily based on task context.

```
Initial tool loading:
├── Core tools always loaded (5 tools, 500 tokens)
├── Category metadata loaded (list of available categories)
└── Full tool schemas loaded on-demand

Agent receives task: "Help with image processing"

Agent: "I need image tools"
System: Loads image tool definitions (800 tokens)

Agent: "Now I need database tools"
System: Loads database tool definitions (600 tokens)

vs. eager loading all 100 tools: 10,000 tokens
```

**MCP consideration**: With modern agents connecting to dozens of MCP servers with hundreds of tools, lazy tool loading becomes essential.

## Research & Benchmarks

### Production Patterns (2024-2025)

#### Anthropic Claude Code Architecture

**Source**: Anthropic Engineering Blog (2025)

Rather than loading entire codebases, Claude Code maintains lightweight identifiers (file paths) and uses tools like grep and glob to dynamically retrieve relevant files as needed.

**Key insight**: "RAG dramatically reduces the initial context burden—the agent starts with a lean context window and only pulls in information as needed."

#### MCP Tool Scaling

**Source**: Anthropic MCP Documentation (2025)

"As the number of connected tools grows, loading all tool definitions upfront and passing intermediate results through the context window slows down agents and increases costs."

**Solution**: Lazy tool loading with tool discovery patterns.

### Production Benchmarks

**Test Case**: Content management system with 1,000 articles

| Operation | Eager Loading | Lazy Loading | Savings |
|-----------|--------------|--------------|---------|
| List articles | 2M tokens | 10K tokens | **99.5%** |
| Search results (20) | 40K tokens | 1K tokens | **97.5%** |
| Read 3 articles | 2M tokens | 6K tokens | **99.7%** |
| Full browsing session | 2M tokens | 15K tokens | **99.25%** |

## When to Use This Pattern

### ✅ Use When:

1. **Large datasets**
   - Hundreds or thousands of items
   - Full content would exceed context window

2. **Browsing/exploration tasks**
   - User doesn't know what they want yet
   - Need to show options before details

3. **Variable detail requirements**
   - Some tasks need titles only
   - Others need full content

4. **Cost-sensitive applications**
   - High query volume
   - Per-token pricing matters

### ❌ Don't Use When:

1. **Small, fixed datasets**
   - <50 items that always fit
   - Overhead not justified

2. **Always-need-full-content tasks**
   - Every request needs all details
   - Two-round-trip adds latency

3. **Real-time requirements**
   - Extra round-trips unacceptable
   - Better: pre-warm cache

### Decision Matrix

| Situation | Recommendation |
|-----------|----------------|
| >100 items possible | Always use lazy loading |
| User browsing first | Two-tier (metadata → full) |
| Deep hierarchies | Hierarchical loading |
| Many tools | Progressive tool loading |
| <20 items, always need full | Eager loading acceptable |

## Production Best Practices

### 1. Cache Frequently Accessed Content

Lazy loading shouldn't mean repeated fetching:

```
Cache strategy:
├── Metadata: Cache 5+ minutes (rarely changes)
├── Summary: Cache 2-3 minutes
├── Full content: Cache 1 minute (may be edited)
└── Invalidate on known writes
```

**Why**: Same content fetched multiple times in session shouldn't hit database repeatedly.

### 2. Smart Prefetching

Predict likely next requests:

```
Prefetch heuristics:
├── If user viewed list → Prefetch top 3 items in background
├── If summary requested → Likely wants full next
├── If browsing category → Prefetch category index
└── If asked about "X" → Prefetch related items
```

**Why**: Eliminates perceived latency when predictions are correct.

### 3. Clear Tool Descriptions

Guide agent on when to use each fetch mode:

```
Tool: getPage
Description: Get page information.

Parameters:
- slug: Page identifier
- fetchMode:
  - 'metadata': Just title and ID (~10 tokens)
    USE FOR: Listing, counting, existence checks
  - 'summary': Add description, tags (~50 tokens)
    USE FOR: Search results, quick overview
  - 'full': Complete content (~1,000+ tokens)
    USE FOR: Reading, editing, detailed questions

IMPORTANT: Default to 'summary'. Only use 'full' when you need
to read/analyze/modify the actual content.
```

### 4. Track Fetch Mode Distribution

Monitor to ensure lazy loading is working:

```
Metrics to track:
├── Requests by fetch mode (metadata/summary/full)
├── Upgrade rate (summary → full in same session)
├── Token savings vs. theoretical eager load
└── Cache hit rate by tier
```

**Target distribution**: ~60% metadata, ~30% summary, ~10% full

## Token Efficiency

### Savings by Operation Type

| Operation Type | Eager Tokens | Lazy Tokens | Savings |
|---------------|--------------|-------------|---------|
| List 50 items | 100,000 | 500 | 99.5% |
| Browse + read 3 | 100,000 | 3,500 | 96.5% |
| Full analysis of 10 | 100,000 | 10,500 | 89.5% |
| Search + read 1 | 100,000 | 1,500 | 98.5% |

### Cost Impact

**Scenario**: Support agent handling 50K sessions/month, average 10 content lookups per session

**Without lazy loading**:
- Per session: 50 items × 2,000 tokens = 100,000 tokens
- Monthly: 5B tokens = $25,000 (GPT-4o)

**With lazy loading** (typical pattern):
- Per session: 10 summaries + 2 full = 2,500 tokens
- Monthly: 125M tokens = $625
- **Savings: $24,375/month ($292K/year)**

## Trade-offs & Considerations

### Advantages

1. **Massive token savings**: 90%+ reduction typical
2. **Scales to any dataset size**: Never loads everything
3. **Faster initial response**: Metadata loads instantly
4. **Cost proportional to task complexity**: Simple = cheap

### Disadvantages

1. **Multiple round-trips**: Full content requires second call
2. **Implementation complexity**: More tool parameters
3. **Agent must learn pattern**: Tool descriptions must be clear
4. **Caching required**: Without cache, repeated fetches slow

### Mitigation Strategies

- Implement intelligent prefetching
- Use aggressive caching for metadata
- Provide clear tool descriptions
- Monitor and tune fetch mode distribution

## Key Takeaways

1. **Default to lightweight**: Metadata first, full content only when needed
2. **90%+ savings typical** for browsing and search operations
3. **Agent-driven decisions**: Let agent choose fetch level based on task
4. **Cache aggressively**: Metadata changes rarely, cache it

**Quick Implementation Checklist**:

- [ ] Add fetchMode parameter to content tools
- [ ] Define clear token budgets per tier
- [ ] Write explicit tool descriptions for each mode
- [ ] Implement caching layer
- [ ] Monitor fetch mode distribution
- [ ] Add prefetching for common patterns

## References

1. **Anthropic** (2025). "Code Execution with MCP: Building More Efficient Agents". https://www.anthropic.com/engineering/code-execution-with-mcp
2. **FlowHunt** (2025). "Context Engineering for AI Agents: Token Optimization". https://www.flowhunt.io/blog/context-engineering-ai-agents-token-optimization/
3. **Zilliz** (2025). "Context Engineering Strategies for AI Agents". https://medium.com/@zilliz_learn/context-engineering-strategies-for-ai-agents-a-developers-guide
4. **Elementor Engineering** (2024). "Optimizing Token Usage in Agent-Based Assistants". https://medium.com/elementor-engineers/optimizing-token-usage-in-agent-based-assistants

**Related Topics**:

- [← Previous: 2.1.2 Importance Scoring](./2.1.2-importance-scoring.md)
- [→ Next: 2.1.4 Hybrid Content Fetching](./2.1.4-hybrid-fetching.md)
- [Working Memory Patterns](./2.3.4-working-memory.md)

**Layer Index**: [Layer 2: Context Engineering](../AI_KNOWLEDGE_BASE_TOC.md#layer-2-context-engineering)
