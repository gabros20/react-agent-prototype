# 5.3.1 Vector Search (Semantic)

## TL;DR

Vector search finds semantically similar content by comparing embedding distances, enabling RAG systems to retrieve conceptually related documents even without keyword matches—achieving 85-95% recall when properly configured with appropriate indices.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md), [5.1.2 Similarity Metrics](./5.1.2-similarity-metrics.md)
- **Grounded In**: MTEB Benchmarks (2024), Pinecone Production Patterns, Weaviate Research (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-keyword-limitations)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Vector search (semantic search) transforms the retrieval problem from string matching to geometric similarity. Documents and queries are converted to high-dimensional vectors (embeddings), and retrieval becomes finding the nearest vectors in embedding space.

The core insight is that embeddings capture meaning, not just words. A search for "feline companion" returns documents about "cats" because their vectors are close in semantic space, even without shared vocabulary.

**Key Research Findings** (2024-2025):

- **Market Growth**: Vector database market grew from $1.73B (2024) to projected $10.6B by 2032
- **Sub-100ms Latency**: Modern vector DBs (Weaviate, Qdrant) achieve sub-100ms queries on millions of vectors
- **85-95% Recall**: HNSW indices achieve 95%+ recall with proper configuration

**Date Verified**: 2025-12-12

## The Problem: Keyword Limitations

### The Classic Challenge

Traditional keyword search fails when users and documents use different vocabulary:

```
Document: "The cardiovascular system pumps blood through arteries and veins."

Query: "How does the heart work?"

Keyword Search: ❌ No match
- "heart" ≠ "cardiovascular"
- "work" ≠ "pumps"

Vector Search: ✅ High similarity
- Embeddings capture: heart ≈ cardiovascular
- Semantic relation: work ≈ function ≈ pumps
```

**Problems with Keyword Search**:

- ❌ Vocabulary mismatch (synonyms, paraphrases)
- ❌ Concept queries fail ("things related to X")
- ❌ Cross-lingual retrieval impossible
- ❌ Cannot handle typos or variations

### Why This Matters

RAG systems require semantic understanding:

- Users express needs in their own words
- Documents use domain-specific terminology
- Concepts span multiple phrasings
- Multilingual knowledge bases need unified search

## Core Concept

### What is Vector Search?

Vector search compares query and document embeddings to find semantic neighbors:

```
Query: "How do I fix authentication errors?"
                    ↓
            Embedding Model
                    ↓
    Query Vector: [0.12, -0.34, 0.56, ...]
                    ↓
         ┌──────────────────────┐
         │    Vector Database   │
         │                      │
         │  Doc1: [0.11, -0.33, 0.55, ...] → Similarity: 0.94
         │  Doc2: [0.45, 0.12, -0.22, ...] → Similarity: 0.31
         │  Doc3: [0.10, -0.35, 0.57, ...] → Similarity: 0.96
         │                      │
         └──────────────────────┘
                    ↓
        Top-K Results: [Doc3, Doc1]
```

### The Search Process

```
1. EMBED QUERY
   "authentication error" → [0.12, -0.34, 0.56, ...]

2. APPROXIMATE NEAREST NEIGHBOR (ANN)
   Search index for closest vectors
   (HNSW, IVF, or Flat scan)

3. COMPUTE SIMILARITY
   Cosine/Dot Product/Euclidean distance

4. RANK AND RETURN
   Top-K most similar documents
```

### Key Principles

1. **Semantic Similarity**: Meaning proximity, not string matching
2. **Approximate Search**: Trade perfect recall for speed (ANN algorithms)
3. **Dense Retrieval**: All documents get vectors (vs sparse keyword indices)

## Implementation Patterns

### Pattern 1: Basic Vector Search with AI SDK

**Use Case**: Standard semantic retrieval pipeline

```typescript
import { embed } from 'ai';
import { openai } from '@ai-sdk/openai';

interface VectorSearchResult {
  id: string;
  content: string;
  similarity: number;
  metadata: Record<string, any>;
}

async function vectorSearch(
  query: string,
  vectorDb: VectorDatabase,
  options = { topK: 5, minSimilarity: 0.7 }
): Promise<VectorSearchResult[]> {
  // 1. Embed the query
  const { embedding: queryVector } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  // 2. Search vector database
  const results = await vectorDb.search({
    vector: queryVector,
    topK: options.topK,
    includeMetadata: true,
  });

  // 3. Filter by minimum similarity
  return results
    .filter((r) => r.similarity >= options.minSimilarity)
    .map((r) => ({
      id: r.id,
      content: r.content,
      similarity: r.similarity,
      metadata: r.metadata,
    }));
}
```

### Pattern 2: pgvector Integration

**Use Case**: PostgreSQL-native vector search

```typescript
import { drizzle } from 'drizzle-orm/postgres-js';
import { pgTable, text, vector } from 'drizzle-orm/pg-core';
import { cosineDistance, desc, sql } from 'drizzle-orm';

// Schema definition
const documents = pgTable('documents', {
  id: text('id').primaryKey(),
  content: text('content').notNull(),
  embedding: vector('embedding', { dimensions: 1536 }),
  metadata: text('metadata'),
});

async function pgvectorSearch(
  db: PostgresDB,
  queryEmbedding: number[],
  topK: number = 5
): Promise<SearchResult[]> {
  const results = await db
    .select({
      id: documents.id,
      content: documents.content,
      metadata: documents.metadata,
      similarity: sql<number>`1 - (${cosineDistance(
        documents.embedding,
        queryEmbedding
      )})`,
    })
    .from(documents)
    .orderBy(desc(sql`1 - ${cosineDistance(documents.embedding, queryEmbedding)}`))
    .limit(topK);

  return results;
}

// Create HNSW index for performance
// CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
// WITH (m = 16, ef_construction = 64);
```

### Pattern 3: Filtered Vector Search

**Use Case**: Combining semantic search with metadata filters

```typescript
interface FilteredSearchOptions {
  query: string;
  filters: {
    docType?: string[];
    dateAfter?: Date;
    tags?: string[];
  };
  topK: number;
}

async function filteredVectorSearch(
  vectorDb: VectorDatabase,
  options: FilteredSearchOptions
): Promise<SearchResult[]> {
  // Embed query
  const queryVector = await embedQuery(options.query);

  // Build metadata filter
  const filter: Record<string, any> = {};

  if (options.filters.docType?.length) {
    filter.docType = { $in: options.filters.docType };
  }

  if (options.filters.dateAfter) {
    filter.createdAt = { $gte: options.filters.dateAfter.toISOString() };
  }

  if (options.filters.tags?.length) {
    filter.tags = { $containsAny: options.filters.tags };
  }

  // Search with filter applied BEFORE similarity calculation
  return vectorDb.search({
    vector: queryVector,
    topK: options.topK,
    filter, // Reduces search space, improves relevance
  });
}
```

### Pattern 4: Multi-Vector Search

**Use Case**: Searching across multiple embedding spaces

```typescript
interface MultiVectorDocument {
  id: string;
  titleEmbedding: number[];    // Title-optimized
  contentEmbedding: number[];  // Content-optimized
  summaryEmbedding: number[];  // Summary-optimized
}

async function multiVectorSearch(
  query: string,
  vectorDb: VectorDatabase,
  weights = { title: 0.3, content: 0.5, summary: 0.2 }
): Promise<SearchResult[]> {
  const queryVector = await embedQuery(query);

  // Search each embedding space
  const [titleResults, contentResults, summaryResults] = await Promise.all([
    vectorDb.search({ vector: queryVector, field: 'titleEmbedding', topK: 20 }),
    vectorDb.search({ vector: queryVector, field: 'contentEmbedding', topK: 20 }),
    vectorDb.search({ vector: queryVector, field: 'summaryEmbedding', topK: 20 }),
  ]);

  // Combine scores with weights
  const combinedScores = new Map<string, number>();

  for (const r of titleResults) {
    combinedScores.set(r.id, (combinedScores.get(r.id) || 0) + r.similarity * weights.title);
  }
  for (const r of contentResults) {
    combinedScores.set(r.id, (combinedScores.get(r.id) || 0) + r.similarity * weights.content);
  }
  for (const r of summaryResults) {
    combinedScores.set(r.id, (combinedScores.get(r.id) || 0) + r.similarity * weights.summary);
  }

  // Sort by combined score and return top-K
  return Array.from(combinedScores.entries())
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10)
    .map(([id, score]) => ({ id, combinedScore: score }));
}
```

## Research & Benchmarks

### Vector Database Performance (2024)

| Database | Query Latency (1M vectors) | Recall@10 | Key Feature |
|----------|---------------------------|-----------|-------------|
| **Qdrant** | 5-15ms | 95%+ | Rust-based, filtering |
| **Weaviate** | <100ms | 95%+ | Hybrid search built-in |
| **Pinecone** | 10-50ms | 95%+ | Serverless, managed |
| **pgvector** | 20-100ms | 90-95% | PostgreSQL native |
| **Milvus** | 10-30ms | 95%+ | Distributed scale |

### Embedding Model Impact

| Model | MTEB Score | Dimensions | Best For |
|-------|------------|------------|----------|
| **Voyage-3-large** | #1 MTEB | 1024 | Maximum quality |
| **text-embedding-3-small** | 80.5% | 1536 | Cost-effective |
| **text-embedding-3-large** | 83.1% | 3072 | High quality |
| **SBERT** | 78-82% | 768 | Open source |

### Recall vs Latency Trade-offs

| Index Type | Recall@10 | Latency (1M vectors) | Memory |
|------------|-----------|----------------------|--------|
| **Flat** | 100% | 500-2000ms | Low |
| **IVF** | 85-95% | 10-50ms | Medium |
| **HNSW** | 95-99% | 5-20ms | High |

## When to Use This Pattern

### ✅ Use When:

1. **Semantic Matching Needed**
   - Synonyms, paraphrases, concepts
   - User queries don't match document vocabulary

2. **Exploratory Search**
   - "Find related content"
   - Discovery-oriented retrieval

3. **Cross-Lingual Retrieval**
   - Multilingual knowledge bases
   - Query and documents in different languages

### ❌ Don't Use When:

1. **Exact Match Required**
   - Product SKUs, IDs, codes
   - Better: Keyword/BM25 search

2. **Boolean Queries**
   - Must include/exclude specific terms
   - Better: Keyword search + filters

3. **Very Short Queries**
   - Single words may embed poorly
   - Better: Combine with keyword search

### Decision Matrix

| Query Type | Vector Search | BM25 | Hybrid |
|------------|---------------|------|--------|
| Conceptual ("related to X") | ✅ | ❌ | ✅ |
| Exact phrase | ❌ | ✅ | ✅ |
| Natural language | ✅ | ⚠️ | ✅ |
| Technical terms | ⚠️ | ✅ | ✅ |

## Production Best Practices

### 1. Use HNSW Indices for Production

HNSW provides best speed/recall balance:

```sql
-- pgvector HNSW index
CREATE INDEX ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Set ef_search at query time for recall/speed trade-off
SET hnsw.ef_search = 100;
```

### 2. Pre-filter When Possible

Filter before vector search to reduce computation:

```typescript
// Good: Filter first, then vector search on subset
const filteredSearch = vectorDb.search({
  vector: queryVector,
  filter: { docType: 'policy' }, // Applied first
  topK: 10,
});

// Bad: Vector search all, then filter
const allResults = await vectorDb.search({ vector: queryVector, topK: 1000 });
const filtered = allResults.filter((r) => r.metadata.docType === 'policy');
```

### 3. Set Minimum Similarity Thresholds

Avoid returning irrelevant results:

```typescript
const MIN_SIMILARITY = 0.7; // Tune for your domain

const results = await vectorDb.search({ vector: queryVector, topK: 10 });
const relevant = results.filter((r) => r.similarity >= MIN_SIMILARITY);

if (relevant.length === 0) {
  return { message: 'No relevant documents found', fallback: true };
}
```

### 4. Cache Frequent Query Embeddings

Reduce embedding API calls:

```typescript
const queryEmbeddingCache = new LRUCache<string, number[]>({ max: 1000 });

async function getCachedQueryEmbedding(query: string): Promise<number[]> {
  const normalized = query.toLowerCase().trim();

  if (queryEmbeddingCache.has(normalized)) {
    return queryEmbeddingCache.get(normalized)!;
  }

  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  queryEmbeddingCache.set(normalized, embedding);
  return embedding;
}
```

## Key Takeaways

1. **Semantic > Lexical**: Vector search finds meaning, not just words
2. **HNSW Default**: 95%+ recall with sub-20ms latency
3. **Filter First**: Metadata filtering improves both speed and relevance
4. **Set Thresholds**: Don't return low-similarity results
5. **Hybrid Is Better**: Combine with BM25 for best results

**Quick Implementation Checklist**:

- [ ] Choose embedding model (text-embedding-3-small for most cases)
- [ ] Set up vector database with HNSW index
- [ ] Implement query embedding with caching
- [ ] Add metadata filtering capability
- [ ] Set minimum similarity threshold
- [ ] Consider hybrid search for production

## References

1. **Pinecone** (2024). "Understanding Vector Databases". https://www.pinecone.io/learn/vector-database/
2. **Weaviate** (2024). "Vector Search Explained". https://weaviate.io/developers/weaviate
3. **Vercel** (2024). "Understanding Vector Databases for AI Apps". https://vercel.com/guides/understanding-vector-databases-for-ai-apps
4. **ZenML** (2024). "Vector Databases for RAG Pipelines". https://www.zenml.io/blog/vector-databases-for-rag
5. **DEV.to** (2025). "Vector Databases Guide: RAG Applications 2025". https://dev.to/klement_gunndu_e16216829c/vector-databases-guide-rag-applications-2025

**Related Topics**:

- [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- [5.1.2 Similarity Metrics](./5.1.2-similarity-metrics.md)
- [5.3.3 BM25 Keyword Search](./5.3.3-bm25.md)
- [5.3.5 Fusion Strategies](./5.3.5-fusion.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
