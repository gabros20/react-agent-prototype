# 5.1.2 - Similarity Metrics for Vector Search

## TL;DR

**Similarity metrics determine how "close" two vectors are in semantic space—cosine similarity (angle-based) is the default for 95% of RAG use cases, while dot product is 2-3× faster when embeddings are normalized.** Wrong metric choice can degrade recall by 30-40%.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- **Grounded In**: Weaviate (2024), Pinecone (2024), Zilliz (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-choosing-the-right-measure)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Once documents are embedded as vectors, similarity metrics determine how "close" two vectors are in semantic space. The choice of metric impacts retrieval accuracy, query speed, and system behavior. Three metrics dominate production RAG systems: cosine similarity, dot product, and Euclidean distance.

Cosine similarity measures the angle between vectors (direction), making it magnitude-invariant—a short document and a long document on the same topic have high similarity. Dot product measures both direction and magnitude. Euclidean distance measures straight-line distance between points.

**Key Research Findings (2024-2025)**:

- **Cosine dominance**: Used in **85%** of production RAG systems ([Weaviate, 2024](https://weaviate.io/blog/distance-metrics-in-vector-search))
- **Dot product speed**: **2-3× faster** than cosine for normalized vectors with equivalent results ([Pinecone, 2024](https://www.pinecone.io/learn/vector-similarity/))
- **Euclidean limitations**: **15-20% worse** retrieval accuracy for text embeddings ([Chroma, 2024](https://research.trychroma.com/evaluating-chunking))
- **Metric mismatch**: Wrong metric can degrade recall by **30-40%** ([VIBE Benchmark, 2025](https://arxiv.org/abs/2505.17810))

## The Problem: Choosing the Right Measure

### The Classic Challenge

Different metrics produce different "nearest neighbors" for the same query:

```
Query Vector: [0.8, 0.6]

Document A: [0.16, 0.12]  (same direction, smaller magnitude)
Document B: [0.9, 0.1]    (different direction, similar magnitude)

Cosine Similarity:
- Doc A: 1.0  (identical direction) ✓ Winner
- Doc B: 0.78

Euclidean Distance:
- Doc A: 0.80 (far due to magnitude)
- Doc B: 0.50 (closer) ✓ Winner

Different metrics → Different results!
```

**Problems**:

- ❌ **Metric mismatch**: Using Euclidean for text embeddings loses 15-20% accuracy
- ❌ **Magnitude sensitivity**: Long documents penalized unfairly with Euclidean
- ❌ **Training-deployment gap**: Model trained with cosine, deployed with Euclidean = poor results
- ❌ **Speed tradeoffs**: Cosine 1.5× slower than dot product without normalization

### Why This Matters

The embedding model's training objective determines which metric works best. OpenAI and most embedding models optimize for cosine similarity. Using a different metric at deployment means the "nearest" documents found aren't actually the most semantically similar.

## Core Concept

### The Three Core Metrics

```
┌─────────────────────────────────────────────────────────────────┐
│                    SIMILARITY METRICS                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  COSINE SIMILARITY (angle-based)                                │
│  ────────────────────────────────                               │
│       Doc A                                                      │
│       /  θ = 15° → similarity = 0.97                           │
│      /                                                           │
│  Query────────────                                              │
│      \                                                           │
│       \  θ = 60° → similarity = 0.50                           │
│        Doc B                                                     │
│                                                                  │
│  Range: [-1, 1]  |  1 = identical  |  0 = orthogonal           │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  DOT PRODUCT (direction + magnitude)                            │
│  ───────────────────────────────────                            │
│  Result = sum of element-wise products                          │
│  [0.8, 0.6] · [0.9, 0.1] = 0.72 + 0.06 = 0.78                  │
│                                                                  │
│  Range: [-∞, +∞]  |  Higher = more similar                     │
│  Note: Equals cosine when vectors are normalized                │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  EUCLIDEAN DISTANCE (straight-line)                             │
│  ──────────────────────────────────                             │
│  Query ═══════15 units═══════ Doc A (close)                    │
│         \                                                        │
│          ═══════45 units═══════ Doc B (far)                    │
│                                                                  │
│  Range: [0, ∞]  |  0 = identical  |  Higher = more different   │
│  Problem: Magnitude affects distance (short vs long docs)       │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Match to training**: Use the same metric the embedding model was trained with
2. **Normalize for speed**: Normalized vectors make dot product equivalent to cosine but 2-3× faster
3. **Cosine for text**: Default choice for NLP, handles document length variations
4. **Euclidean for spatial**: Reserve for GPS, physical measurements, pixel data

## Implementation Patterns

### Pattern 1: Cosine Similarity (Default)

**Use Case**: Text embeddings, NLP, most RAG applications

```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (magA * magB);
}

// Example
const query = [0.8, 0.6, 0.0];
const docA = [0.16, 0.12, 0.0]; // Same direction, smaller magnitude
const docB = [0.9, 0.1, 0.0]; // Different direction

console.log(cosineSimilarity(query, docA)); // 1.0 (identical direction)
console.log(cosineSimilarity(query, docB)); // 0.78 (different angle)
```

**Pros**:

- ✅ Magnitude-invariant (long vs short documents treated equally)
- ✅ Intuitive interpretation (angle-based)
- ✅ Works with 95% of embedding models

**Cons**:

- ❌ Requires normalization step (slower than dot product)
- ❌ Slightly more computation per comparison

**When to Use**: Default choice for text embeddings (OpenAI, Cohere, SBERT)

### Pattern 2: Dot Product (Fast Alternative)

**Use Case**: Normalized embeddings, high-throughput systems, hybrid search

```typescript
function dotProduct(a: number[], b: number[]): number {
  return a.reduce((sum, val, i) => sum + val * b[i], 0);
}

// Normalize once during indexing
function normalize(vector: number[]): number[] {
  const magnitude = Math.sqrt(vector.reduce((sum, v) => sum + v * v, 0));
  return vector.map((v) => v / magnitude);
}

// Pre-normalize embeddings
const normalizedDoc = normalize(documentEmbedding);
const normalizedQuery = normalize(queryEmbedding);

// Now dot product equals cosine similarity, but 2-3× faster
const similarity = dotProduct(normalizedDoc, normalizedQuery);
```

**Pros**:

- ✅ 2-3× faster than cosine (no per-query normalization)
- ✅ Equivalent results when vectors pre-normalized
- ✅ Easy to combine with BM25 scores (linear combination)

**Cons**:

- ❌ Requires pre-normalization (extra index-time work)
- ❌ Misleading results if vectors aren't normalized

**When to Use**: High-volume production (>1M queries/day), hybrid search pipelines

### Pattern 3: Euclidean Distance (Spatial Data)

**Use Case**: GPS coordinates, image pixels, physical measurements

```typescript
function euclideanDistance(a: number[], b: number[]): number {
  return Math.sqrt(a.reduce((sum, val, i) => sum + Math.pow(val - b[i], 2), 0));
}

// Example: GPS coordinates
const location1 = [40.7128, -74.006]; // NYC
const location2 = [34.0522, -118.2437]; // LA
const distance = euclideanDistance(location1, location2); // ~50 units
```

**Pros**:

- ✅ Intuitive for physical/spatial data
- ✅ Natural choice when magnitude matters (counts, measurements)

**Cons**:

- ❌ **15-20% worse for text** embeddings
- ❌ Penalizes longer documents unfairly
- ❌ Sensitive to feature scaling

**When to Use**: Spatial queries only (GPS, image retrieval by pixel distance)

## Framework Integration

### AI SDK 6 with Vector Databases

```typescript
import { cosineSimilarity, embed, embedMany } from 'ai';
import { openai } from '@ai-sdk/openai';

// Built-in cosine similarity from AI SDK
const { embeddings } = await embedMany({
  model: openai.textEmbeddingModel('text-embedding-3-small'),
  values: ['sunny day at beach', 'rainy afternoon in city'],
});

const similarity = cosineSimilarity(embeddings[0], embeddings[1]);
console.log(`Similarity: ${similarity.toFixed(3)}`); // ~0.65
```

### LanceDB Integration

```typescript
import { connect } from 'vectordb';

const db = await connect('./data/lancedb');
const table = await db.openTable('documents');

// Cosine (default, safest)
const cosineResults = await table
  .search(queryEmbedding)
  .distanceType('cosine')
  .limit(10)
  .execute();

// Dot product (faster if embeddings normalized)
const dotResults = await table
  .search(normalize(queryEmbedding))
  .distanceType('dot')
  .limit(10)
  .execute();

// Euclidean (rarely used for text)
const l2Results = await table.search(queryEmbedding).distanceType('l2').limit(10).execute();
```

### Pinecone Integration

```typescript
import { Pinecone } from '@pinecone-database/pinecone';

const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });

// Metric is set at index creation time (immutable)
await pinecone.createIndex({
  name: 'my-index',
  dimension: 1536,
  metric: 'cosine', // or 'dotproduct', 'euclidean'
});

const index = pinecone.index('my-index');
const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
});
```

## Research & Benchmarks

### Performance Comparison

| Metric | Speed (normalized) | BEIR Accuracy | Best For |
|--------|-------------------|---------------|----------|
| **Dot Product** | 1× (fastest) | 95.2% | Normalized embeddings |
| **Cosine** | 1.5× | 95.3% | General purpose |
| **Euclidean** | 1.2× | 88.7% | Spatial data |

### When Metrics Diverge

| Scenario | Cosine | Euclidean | Impact |
|----------|--------|-----------|--------|
| Short vs long doc (same topic) | High (0.95) | Low (0.60) | Euclidean penalizes length |
| Normalized embeddings | = Dot product | Different | Use dot product for speed |
| Mixed magnitude data | Robust | Sensitive | Cosine handles variance |

## When to Use This Pattern

### ✅ Use Cosine When:

1. **Text embeddings (default)**
   - OpenAI, Cohere, SBERT all optimize for cosine
   - Document length shouldn't affect similarity

2. **Unknown embedding normalization**
   - Cosine handles both normalized and unnormalized
   - Safe default when unsure

### ✅ Use Dot Product When:

1. **Pre-normalized embeddings**
   - OpenAI embeddings are already L2-normalized
   - 2-3× speed improvement over cosine

2. **Hybrid search pipelines**
   - Easy to combine with BM25 scores: `final = α × vector + (1-α) × bm25`

### ✅ Use Euclidean When:

1. **Spatial/physical data**
   - GPS coordinates, sensor readings
   - Pixel-level image comparison

2. **Magnitude carries meaning**
   - Counts, measurements, activity levels

### Decision Matrix

| Your Situation | Recommended Metric |
|----------------|-------------------|
| RAG with text documents | Cosine (default) |
| High-throughput search (>1M queries/day) | Dot product (pre-normalize) |
| Hybrid search (BM25 + vector) | Dot product |
| GPS/location search | Euclidean |
| Image similarity (CLIP) | Cosine |
| Code search | Cosine |

## Production Best Practices

### 1. Match Training Metric

```typescript
// Check embedding model documentation for recommended metric
// OpenAI: Cosine (embeddings pre-normalized, so dot product works too)
// SBERT: Cosine
// BGE: Cosine

// BAD: Train with cosine, deploy with Euclidean
const results = await search({ metric: 'euclidean' }); // 30-40% accuracy drop

// GOOD: Match training metric
const results = await search({ metric: 'cosine' });
```

### 2. Pre-Normalize for Dot Product Speed

```typescript
// Index time: Normalize once
const normalized = normalize(embedding);
await db.insert({ vector: normalized });

// Query time: Fast dot product
const queryNorm = normalize(queryEmbedding);
const results = await db.search(queryNorm).distanceType('dot');
// 2-3× faster than cosine with identical results
```

### 3. Verify OpenAI Normalization

```typescript
// OpenAI embeddings should be pre-normalized (magnitude ≈ 1)
const magnitude = Math.sqrt(embedding.reduce((sum, v) => sum + v * v, 0));
console.log(`Magnitude: ${magnitude}`); // Should be ~1.0

// If not 1.0, normalize explicitly
if (Math.abs(magnitude - 1.0) > 0.01) {
  embedding = normalize(embedding);
}
```

### Common Pitfalls

#### ❌ Pitfall: Using Euclidean for Text

**Problem**: Euclidean penalizes document length, not just semantic difference.

```typescript
const shortDoc = [0.1, 0.1, 0.1]; // 3 sentences
const longDoc = [0.3, 0.3, 0.3]; // Same topic, 9 sentences

euclideanDistance(shortDoc, longDoc); // 0.35 (seems "far")
cosineSimilarity(shortDoc, longDoc); // 1.0 (identical direction!)
```

**Solution**: Use cosine for text embeddings.

#### ❌ Pitfall: Forgetting to Normalize for Dot Product

**Problem**: Dot product on unnormalized vectors gives misleading results.

```typescript
// BAD: Raw embeddings with dot product
const score = dotProduct(rawA, rawB); // Magnitude affects score

// GOOD: Normalize first
const score = dotProduct(normalize(rawA), normalize(rawB));
```

#### ❌ Pitfall: Mixing Metrics Between Train and Deploy

**Problem**: 30-40% accuracy drop from metric mismatch.

**Solution**: Always check embedding model documentation for recommended metric.

## Key Takeaways

1. **Cosine similarity is the default** - Used by 85% of RAG systems, handles document length variations
2. **Dot product = cosine when normalized** - 2-3× faster, use for high-throughput
3. **Euclidean is wrong for text** - 15-20% accuracy loss, reserve for spatial data
4. **Match training metric** - Wrong metric → 30-40% accuracy drop
5. **OpenAI embeddings are pre-normalized** - Dot product safe to use directly

**Quick Implementation Checklist**:

- [ ] Check embedding model documentation for recommended metric
- [ ] Default to cosine for text embeddings
- [ ] Pre-normalize if using dot product for speed
- [ ] Verify embedding magnitudes (~1.0 for normalized)
- [ ] Test retrieval quality with your chosen metric

## References

1. **Pinecone** (2024). "Vector Similarity Explained". https://www.pinecone.io/learn/vector-similarity/
2. **Weaviate** (2024). "Distance Metrics in Vector Search". https://weaviate.io/blog/distance-metrics-in-vector-search
3. **Zilliz** (2024). "Similarity Metrics for Vector Search". https://zilliz.com/blog/similarity-metrics-for-vector-search
4. **Google** (2024). "Measuring Similarity from Embeddings". https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity
5. **VIBE Benchmark** (2025). "Vector Index Benchmark for Embeddings". *arXiv*. https://arxiv.org/abs/2505.17810

**Related Topics**:

- [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- [5.1.3 Index Types](./5.1.3-index-types.md)
- [5.3.3 BM25](./5.3.3-bm25.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-rag--retrieval)
