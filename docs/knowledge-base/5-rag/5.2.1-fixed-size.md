# 5.2.1 - Fixed-Size Chunking

## Overview

**Fixed-size chunking** is the simplest and most common document splitting strategy, dividing text into segments of predetermined length (character count or token count). While straightforward to implement, this approach has significant trade-offs that impact retrieval quality. This guide covers when to use fixed-size chunks, optimal configurations, and production patterns.

**Key Research Findings (2024-2025)**:

- **Chunk size matters**: 512-token chunks achieve **best balance** of context vs precision (NVIDIA, 2024)
- **Character vs token**: Token-based chunking is **15-20% more accurate** than character-based (Firecrawl, 2025)
- **Recall gaps**: Wrong chunk size causes up to **9% recall loss** (Chroma Research, 2024)
- **Overlap critical**: 10-20% overlap improves retrieval by **30-50%** (Adnan Masood, 2025)
- **Model dependency**: Optimal size varies by embedding model - 384 tokens for SBERT, 512 for OpenAI (LangChain, 2024)

**Date Verified**: November 20, 2025

---

## How Fixed-Size Chunking Works

### Basic Algorithm

```typescript
// Simple character-based chunking
function chunkByCharacters(text: string, chunkSize: number): string[] {
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize) {
    chunks.push(text.slice(i, i + chunkSize));
  }
  return chunks;
}

// Example
const document = "Next.js 15 introduces Server Components..."; // 5000 chars
const chunks = chunkByCharacters(document, 1000);
// Result: [chunk1, chunk2, chunk3, chunk4, chunk5]
```

### The Problem with Naive Splitting

```typescript
// Problem: Splits mid-sentence, mid-word
const text = "React hooks like useState and useEffect are essential. They enable...";
const chunks = chunkByCharacters(text, 50);
// chunk1: "React hooks like useState and useEffect are es"
// chunk2: "sential. They enable..."
//          ^^^^^^^^ Word split! Context lost!

// Problem: Semantic boundaries ignored
const chunk = "...end of function}\n\n# New Section\nThis section discusses...";
// Chunks can split sections, breaking semantic coherence
```

---

## Production Implementation

### Pattern 1: Token-Based Chunking (Recommended)

**Why tokens?** Embedding models have token limits, not character limits.

```typescript
// server/services/token-chunker.ts
import { encoding_for_model } from 'tiktoken';

class TokenChunker {
  private encoding = encoding_for_model('gpt-4o-mini'); // Or your model
  
  /**
   * Chunk by token count (respects model limits)
   * 15-20% more accurate than character-based (Firecrawl, 2025)
   */
  chunkByTokens(
    text: string,
    options: {
      chunkSize?: number;
      overlap?: number;
    } = {}
  ): Array<{ text: string; tokens: number; start: number; end: number }> {
    const { chunkSize = 512, overlap = 50 } = options;
    
    // Step 1: Encode text to tokens
    const tokens = this.encoding.encode(text);
    
    // Step 2: Split into chunks with overlap
    const chunks: Array<{ text: string; tokens: number; start: number; end: number }> = [];
    const stride = chunkSize - overlap; // Sliding window stride
    
    for (let i = 0; i < tokens.length; i += stride) {
      const chunkTokens = tokens.slice(i, i + chunkSize);
      const chunkText = this.encoding.decode(chunkTokens);
      
      chunks.push({
        text: chunkText,
        tokens: chunkTokens.length,
        start: i,
        end: i + chunkTokens.length,
      });
      
      // Stop if this chunk reaches the end
      if (i + chunkSize >= tokens.length) break;
    }
    
    return chunks;
  }
  
  /**
   * Estimate token count (fast approximation)
   * Rule of thumb: 1 token ≈ 4 characters for English
   */
  estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
  
  /**
   * Validate chunk fits in embedding model limit
   */
  validateChunk(text: string, maxTokens = 8191): boolean {
    const tokens = this.encoding.encode(text);
    return tokens.length <= maxTokens;
  }
}

// Usage
const chunker = new TokenChunker();

const document = `
Next.js 15 introduces React Server Components, a paradigm shift...
[5000 tokens of content]
`;

const chunks = chunker.chunkByTokens(document, {
  chunkSize: 512,  // Tokens per chunk
  overlap: 50,     // 10% overlap (50/512)
});

console.log(`Split into ${chunks.length} chunks`);
// chunk[0]: tokens=512, text="Next.js 15 introduces..."
// chunk[1]: tokens=512, text="...paradigm shift... [overlap] ...new features..."
```

**Why 512 tokens?**

```typescript
// Benchmark: Retrieval accuracy by chunk size (NVIDIA, 2024)

256 tokens: Recall@10 = 82%, Precision@5 = 76%
            ✅ Fast, compact
            ❌ Loses context

512 tokens: Recall@10 = 91%, Precision@5 = 88%
            ✅ Best balance ⭐
            ⚠️ Standard choice

1024 tokens: Recall@10 = 89%, Precision@5 = 84%
             ⚠️ Too broad, dilutes relevance
             ❌ Slower retrieval

2048 tokens: Recall@10 = 85%, Precision@5 = 79%
             ❌ Query matches multiple topics in chunk
             ❌ Wastes LLM context

// Recommendation: 512 tokens (200-250 words)
```

---

### Pattern 2: Sentence-Aware Fixed-Size

**Improvement**: Respect sentence boundaries to avoid mid-sentence splits.

```typescript
// server/services/sentence-aware-chunker.ts
class SentenceAwareChunker {
  /**
   * Fixed-size chunking that respects sentence boundaries
   * Reduces context loss by 15-25% (LangChain, 2024)
   */
  chunkBySentences(
    text: string,
    options: {
      chunkSize?: number;     // Target tokens per chunk
      tolerance?: number;     // Allow ±tolerance deviation
      overlap?: number;       // Overlap in sentences
    } = {}
  ): string[] {
    const { chunkSize = 512, tolerance = 0.2, overlap = 1 } = options;
    
    // Step 1: Split into sentences
    const sentences = this.splitIntoSentences(text);
    
    // Step 2: Group sentences into chunks ~chunkSize
    const chunks: string[] = [];
    let currentChunk: string[] = [];
    let currentTokens = 0;
    
    const minSize = chunkSize * (1 - tolerance); // e.g., 410
    const maxSize = chunkSize * (1 + tolerance); // e.g., 614
    
    for (let i = 0; i < sentences.length; i++) {
      const sentence = sentences[i];
      const sentenceTokens = this.estimateTokens(sentence);
      
      // Check if adding sentence exceeds max
      if (currentTokens + sentenceTokens > maxSize && currentChunk.length > 0) {
        // Finalize current chunk
        chunks.push(currentChunk.join(' '));
        
        // Start new chunk with overlap
        const overlapStart = Math.max(0, currentChunk.length - overlap);
        currentChunk = currentChunk.slice(overlapStart);
        currentTokens = this.estimateTokens(currentChunk.join(' '));
      }
      
      // Add sentence to current chunk
      currentChunk.push(sentence);
      currentTokens += sentenceTokens;
      
      // If chunk is at least minSize, can finalize
      if (currentTokens >= minSize && i < sentences.length - 1) {
        const nextSentenceTokens = this.estimateTokens(sentences[i + 1]);
        if (currentTokens + nextSentenceTokens > maxSize) {
          // Next sentence would exceed max, finalize now
          chunks.push(currentChunk.join(' '));
          currentChunk = currentChunk.slice(-overlap);
          currentTokens = this.estimateTokens(currentChunk.join(' '));
        }
      }
    }
    
    // Add remaining chunk
    if (currentChunk.length > 0) {
      chunks.push(currentChunk.join(' '));
    }
    
    return chunks;
  }
  
  /**
   * Split text into sentences (naive but fast)
   */
  private splitIntoSentences(text: string): string[] {
    // Simple regex: Split on .!? followed by space and capital letter
    return text
      .split(/(?<=[.!?])\s+(?=[A-Z])/)
      .filter(s => s.trim().length > 0);
  }
  
  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
}

// Usage
const sentenceChunker = new SentenceAwareChunker();

const chunks = sentenceChunker.chunkBySentences(document, {
  chunkSize: 512,
  tolerance: 0.2,  // Allow 410-614 tokens
  overlap: 1,      // 1 sentence overlap
});

// Result: Each chunk ends at sentence boundary
// chunk1: "...Server Components enable better performance."
// chunk2: "Server Components enable better performance. Additionally, they..."
//         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Overlap (1 sentence)
```

---

### Pattern 3: LangChain Integration

```typescript
// server/services/langchain-chunker.ts
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

class LangChainChunker {
  /**
   * Use LangChain's recursive splitter (production-ready)
   */
  async chunkDocument(
    document: string,
    options: {
      chunkSize?: number;
      chunkOverlap?: number;
      separators?: string[];
    } = {}
  ) {
    const {
      chunkSize = 512,
      chunkOverlap = 50,
      separators = ['\n\n', '\n', '. ', ' ', ''],
    } = options;
    
    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize,
      chunkOverlap,
      separators, // Try these separators in order
      lengthFunction: (text) => Math.ceil(text.length / 4), // Token estimate
    });
    
    const docs = await splitter.createDocuments([document]);
    
    return docs.map((doc, i) => ({
      text: doc.pageContent,
      metadata: {
        chunkIndex: i,
        ...doc.metadata,
      },
    }));
  }
}

// Usage
const langchainChunker = new LangChainChunker();

const chunks = await langchainChunker.chunkDocument(document, {
  chunkSize: 512,
  chunkOverlap: 50,
});

// LangChain benefits:
// ✅ Tries paragraph breaks first (\n\n)
// ✅ Falls back to sentence breaks (. )
// ✅ Only splits on spaces/characters as last resort
// ✅ Respects semantic structure when possible
```

---

## Overlap Strategies

### Why Overlap Matters

```typescript
// Without overlap: Context lost at boundaries
chunk1: "...React hooks enable state management."
chunk2: "Server components run on the server..." // No connection!

// With overlap: Smooth transitions
chunk1: "...React hooks enable state management."
chunk2: "...state management. Server components..." // Maintains context!

// Performance impact (Adnan Masood, 2025)
No overlap:  Recall@10 = 78%
10% overlap: Recall@10 = 91% (+13%)
20% overlap: Recall@10 = 94% (+16%)
30% overlap: Recall@10 = 94% (diminishing returns)

// Recommendation: 10-20% overlap (50-100 tokens for 512-token chunks)
```

### Smart Overlap Implementation

```typescript
// server/services/smart-overlap.ts
class SmartOverlapChunker {
  /**
   * Dynamic overlap based on chunk content
   */
  chunkWithSmartOverlap(
    text: string,
    options: {
      chunkSize?: number;
      minOverlap?: number;
      maxOverlap?: number;
    } = {}
  ): string[] {
    const { chunkSize = 512, minOverlap = 25, maxOverlap = 100 } = options;
    
    const sentences = this.splitIntoSentences(text);
    const chunks: string[] = [];
    let current: string[] = [];
    let tokens = 0;
    
    for (const sentence of sentences) {
      const sentenceTokens = this.estimateTokens(sentence);
      
      if (tokens + sentenceTokens > chunkSize && current.length > 0) {
        // Determine overlap: More for important boundaries
        const overlap = this.determineOverlap(current, minOverlap, maxOverlap);
        
        chunks.push(current.join(' '));
        
        // Keep last N sentences for overlap
        const overlapSentences = Math.ceil(overlap / (tokens / current.length));
        current = current.slice(-overlapSentences);
        tokens = this.estimateTokens(current.join(' '));
      }
      
      current.push(sentence);
      tokens += sentenceTokens;
    }
    
    if (current.length > 0) {
      chunks.push(current.join(' '));
    }
    
    return chunks;
  }
  
  /**
   * Increase overlap at section/topic boundaries
   */
  private determineOverlap(
    currentChunk: string[],
    minOverlap: number,
    maxOverlap: number
  ): number {
    const lastSentence = currentChunk[currentChunk.length - 1];
    
    // More overlap if chunk ends with section marker
    if (/^#{1,6}\s/.test(lastSentence)) {
      return maxOverlap; // Section heading → max overlap
    }
    
    // More overlap if chunk ends with transition words
    if (/\b(however|therefore|additionally|furthermore)\b/i.test(lastSentence)) {
      return (minOverlap + maxOverlap) / 2; // Medium overlap
    }
    
    return minOverlap; // Default: min overlap
  }
  
  private splitIntoSentences(text: string): string[] {
    return text.split(/(?<=[.!?])\s+(?=[A-Z])/).filter(s => s.trim());
  }
  
  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
}
```

---

## Document-Type Specific Strategies

### Code Documents

```typescript
// Code: Chunk by function/class boundaries
class CodeChunker {
  chunkCode(
    code: string,
    language: 'typescript' | 'python' | 'javascript'
  ): string[] {
    if (language === 'typescript' || language === 'javascript') {
      return this.chunkJavaScript(code);
    }
    // ... other languages
    return [];
  }
  
  private chunkJavaScript(code: string): string[] {
    // Split on top-level function/class declarations
    const functionRegex = /^(export\s+)?(async\s+)?function\s+\w+/gm;
    const classRegex = /^(export\s+)?class\s+\w+/gm;
    
    // Find boundaries
    const boundaries: number[] = [0];
    let match: RegExpExecArray | null;
    
    while ((match = functionRegex.exec(code)) !== null) {
      boundaries.push(match.index);
    }
    while ((match = classRegex.exec(code)) !== null) {
      boundaries.push(match.index);
    }
    
    boundaries.sort((a, b) => a - b);
    boundaries.push(code.length);
    
    // Create chunks
    return boundaries
      .slice(0, -1)
      .map((start, i) => code.slice(start, boundaries[i + 1]))
      .filter(chunk => chunk.trim().length > 0);
  }
}
```

### Markdown Documents

```typescript
// Markdown: Chunk by heading hierarchy
class MarkdownChunker {
  chunkMarkdown(markdown: string): Array<{
    text: string;
    heading: string;
    level: number;
  }> {
    const lines = markdown.split('\n');
    const chunks: Array<{ text: string; heading: string; level: number }> = [];
    
    let currentHeading = '';
    let currentLevel = 0;
    let currentContent: string[] = [];
    
    for (const line of lines) {
      const headingMatch = line.match(/^(#{1,6})\s+(.+)$/);
      
      if (headingMatch) {
        // Save previous chunk
        if (currentContent.length > 0) {
          chunks.push({
            text: currentContent.join('\n'),
            heading: currentHeading,
            level: currentLevel,
          });
        }
        
        // Start new chunk
        currentLevel = headingMatch[1].length;
        currentHeading = headingMatch[2];
        currentContent = [line];
      } else {
        currentContent.push(line);
      }
    }
    
    // Save last chunk
    if (currentContent.length > 0) {
      chunks.push({
        text: currentContent.join('\n'),
        heading: currentHeading,
        level: currentLevel,
      });
    }
    
    return chunks;
  }
}
```

---

## Common Pitfalls

### 1. ❌ Character-Based Instead of Token-Based

```typescript
// BAD: Character-based chunking
const chunks = text.match(/.{1,2000}/g); // 2000 chars
// Problem: May be 500 tokens or 800 tokens (unpredictable!)

// GOOD: Token-based chunking
const chunks = chunker.chunkByTokens(text, { chunkSize: 512 });
// Predictable: Each chunk ≤ 512 tokens
```

### 2. ❌ No Overlap

```typescript
// BAD: No overlap
const chunks = chunker.chunkByTokens(text, { chunkSize: 512, overlap: 0 });
// Lost recall: 13-16% (queries matching chunk boundaries fail)

// GOOD: 10-20% overlap
const chunks = chunker.chunkByTokens(text, { chunkSize: 512, overlap: 50 });
```

### 3. ❌ Ignoring Document Structure

```typescript
// BAD: Split markdown mid-heading
const chunks = naiveChunk(markdown, 1000);
// chunk1: "...end of paragraph\n\n### New Se"
// chunk2: "ction\nThis section discusses..." ❌

// GOOD: Respect markdown structure
const chunks = markdownChunker.chunkMarkdown(markdown);
// Each chunk = complete section with heading
```

---

## Research Citations

1. **NVIDIA** (2024). "Best Chunking Strategies for RAG". Retrieved from: https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
2. **Firecrawl** (2025). "Token-Based vs Character-Based Chunking". Retrieved from: https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
3. **Chroma Research** (2024). "Evaluating Chunking Strategies for Retrieval". Retrieved from: https://research.trychroma.com/evaluating-chunking
4. **Adnan Masood** (2025). "Optimizing Chunking, Embedding, and Vectorization for RAG". Retrieved from: https://medium.com/@adnanmasood/optimizing-chunking-embedding-and-vectorization

---

## Next Steps

- **[5.2.2 Semantic Chunking](./5.2.2-semantic-chunks.md)**: Learn meaning-based chunking
- **[5.2.3 Overlapping Windows](./5.2.3-overlapping-windows.md)**: Advanced overlap strategies
- **[5.2.5 Chunk Size Trade-offs](./5.2.5-tradeoffs.md)**: Optimize chunk size for your use case

---

**Created**: November 20, 2025
