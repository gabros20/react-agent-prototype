# 5.2.1 - Fixed-Size Chunking: Token-Based Document Splitting

## TL;DR

**Fixed-size chunking splits documents into uniform segments by token count (not characters)—512 tokens with 10-20% overlap is the production default, achieving the best balance of retrieval precision and context preservation.** Token-based is 15-20% more accurate than character-based because embedding models have token limits, not character limits.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- **Grounded In**: Chroma Research (2024), NVIDIA (2024), Firecrawl (2025)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-semantic-boundary-breaks)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Fixed-size chunking divides documents into segments of predetermined length. While straightforward to implement, proper configuration—**token-based sizing, 512-token chunks, 10-20% overlap**—is critical for retrieval quality.

The approach is the most common chunking strategy because it's predictable, efficient, and works well for diverse document types. However, naive implementations (character-based, no overlap) can lose up to 9% recall.

**Key Research Findings (2024-2025)**:

- **Chunk size matters**: 512-token chunks achieve **best balance** of context vs precision ([NVIDIA, 2024](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025))
- **Token vs character**: Token-based chunking is **15-20% more accurate** than character-based ([Firecrawl, 2025](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025))
- **Overlap critical**: 10-20% overlap improves retrieval by **13-16%** ([Adnan Masood, 2025](https://medium.com/@adnanmasood/optimizing-chunking-embedding-and-vectorization))
- **Wrong size impact**: Suboptimal chunk size causes up to **9% recall loss** ([Chroma, 2024](https://research.trychroma.com/evaluating-chunking))

## The Problem: Semantic Boundary Breaks

### The Classic Challenge

Naive fixed-size chunking splits at arbitrary positions, breaking semantic coherence:

```
Original Text:
"React hooks like useState and useEffect are essential. They enable..."

❌ Character-based split at 50 characters:
chunk1: "React hooks like useState and useEffect are es"
chunk2: "sential. They enable..."
                                ^^^^^^^^
                                Word split! Context lost!

❌ No overlap:
chunk1: "...React hooks enable state management."
chunk2: "Server components run on the server..."
         No connection between chunks!

✅ Token-based with overlap:
chunk1: "...React hooks enable state management."
chunk2: "...state management. Server components..."
         ^^^^^^^^^^^^^^^^^^^^^
         Overlap maintains context
```

**Problems**:

- ❌ **Word splits**: Mid-word breaks create nonsense tokens
- ❌ **Sentence breaks**: Lost context when splitting mid-sentence
- ❌ **Section breaks**: Chunks can split headers from their content
- ❌ **Boundary queries**: Queries matching chunk boundaries fail to retrieve

### Why This Matters

Chunking quality directly impacts RAG performance:
- **Embedding quality**: Broken text produces poor embeddings
- **Retrieval accuracy**: Queries can't match split concepts
- **LLM comprehension**: Incomplete context confuses the model
- **Cost efficiency**: More chunks = more embeddings = higher costs

## Core Concept

### How Fixed-Size Chunking Works

```
┌─────────────────────────────────────────────────────────────────┐
│                    FIXED-SIZE CHUNKING                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Document (2000 tokens)                                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Token 0                                       Token 1999 │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│  Chunk Size: 512 tokens, Overlap: 50 tokens                     │
│                                                                  │
│  Chunk 1: [0────────────────512]                               │
│  Chunk 2:        [462────────────────974]                      │
│  Chunk 3:               [924────────────────1436]              │
│  Chunk 4:                      [1386────────────────1898]      │
│  Chunk 5:                             [1848───────────1999]    │
│                                                                  │
│  Overlap zones (50 tokens each):                                │
│           ↑──────↑       ↑──────↑       ↑──────↑               │
│          462-512       924-974      1386-1436                   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Token-based sizing**: Match embedding model input limits (not characters)
2. **Consistent chunk sizes**: Predictable token counts for cost estimation
3. **Overlap for continuity**: 10-20% overlap preserves cross-boundary context
4. **Sentence boundaries**: Prefer splitting at sentence ends when possible

## Implementation Patterns

### Pattern 1: Token-Based Chunking (Production Default)

**Use Case**: Default for all RAG applications

```typescript
import { encoding_for_model } from 'tiktoken';

class TokenChunker {
  private encoding = encoding_for_model('gpt-4o-mini');

  /**
   * Chunk by token count with overlap
   * 15-20% more accurate than character-based
   */
  chunkByTokens(
    text: string,
    options: { chunkSize?: number; overlap?: number } = {}
  ): Array<{ text: string; tokens: number; start: number; end: number }> {
    const { chunkSize = 512, overlap = 50 } = options;

    // Encode text to tokens
    const tokens = this.encoding.encode(text);
    const stride = chunkSize - overlap;

    const chunks: Array<{ text: string; tokens: number; start: number; end: number }> = [];

    for (let i = 0; i < tokens.length; i += stride) {
      const chunkTokens = tokens.slice(i, i + chunkSize);
      const chunkText = this.encoding.decode(chunkTokens);

      chunks.push({
        text: chunkText,
        tokens: chunkTokens.length,
        start: i,
        end: i + chunkTokens.length,
      });

      // Stop if this chunk reaches the end
      if (i + chunkSize >= tokens.length) break;
    }

    return chunks;
  }

  /**
   * Estimate token count (4 chars ≈ 1 token for English)
   */
  estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
}

// Usage
const chunker = new TokenChunker();
const chunks = chunker.chunkByTokens(document, {
  chunkSize: 512, // Tokens per chunk
  overlap: 50, // 10% overlap
});
```

**Pros**:

- ✅ Predictable token counts (matches embedding limits)
- ✅ 15-20% more accurate than character-based
- ✅ Simple to implement and maintain

**Cons**:

- ❌ May split mid-sentence
- ❌ Requires tokenizer library
- ❌ Ignores document structure

**When to Use**: Default for all document types

### Pattern 2: Sentence-Aware Fixed-Size

**Use Case**: Prioritizing semantic coherence over exact sizing

```typescript
class SentenceAwareChunker {
  /**
   * Fixed-size chunking that respects sentence boundaries
   * Reduces context loss by 15-25%
   */
  chunkBySentences(
    text: string,
    options: {
      chunkSize?: number;
      tolerance?: number; // Allow ±tolerance deviation
      overlapSentences?: number;
    } = {}
  ): string[] {
    const { chunkSize = 512, tolerance = 0.2, overlapSentences = 1 } = options;

    const sentences = this.splitIntoSentences(text);
    const chunks: string[] = [];
    let currentChunk: string[] = [];
    let currentTokens = 0;

    const minSize = chunkSize * (1 - tolerance); // ~410 tokens
    const maxSize = chunkSize * (1 + tolerance); // ~614 tokens

    for (let i = 0; i < sentences.length; i++) {
      const sentence = sentences[i];
      const sentenceTokens = this.estimateTokens(sentence);

      // Check if adding sentence exceeds max
      if (currentTokens + sentenceTokens > maxSize && currentChunk.length > 0) {
        chunks.push(currentChunk.join(' '));

        // Start new chunk with overlap
        const overlapStart = Math.max(0, currentChunk.length - overlapSentences);
        currentChunk = currentChunk.slice(overlapStart);
        currentTokens = this.estimateTokens(currentChunk.join(' '));
      }

      currentChunk.push(sentence);
      currentTokens += sentenceTokens;
    }

    // Add remaining
    if (currentChunk.length > 0) {
      chunks.push(currentChunk.join(' '));
    }

    return chunks;
  }

  private splitIntoSentences(text: string): string[] {
    // Split on .!? followed by space and capital letter
    return text.split(/(?<=[.!?])\s+(?=[A-Z])/).filter((s) => s.trim().length > 0);
  }

  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
}

// Usage
const chunker = new SentenceAwareChunker();
const chunks = chunker.chunkBySentences(document, {
  chunkSize: 512,
  tolerance: 0.2, // Allow 410-614 tokens
  overlapSentences: 1,
});
```

**Pros**:

- ✅ No mid-sentence splits
- ✅ Better semantic coherence
- ✅ 15-25% less context loss

**Cons**:

- ❌ Variable chunk sizes (±20%)
- ❌ Slightly more complex
- ❌ Sentence detection can be imperfect

**When to Use**: Prose documents, articles, documentation

### Pattern 3: Recursive Character Text Splitter (LangChain)

**Use Case**: Production-ready with fallback hierarchy

```typescript
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

async function chunkWithLangChain(
  document: string,
  options: {
    chunkSize?: number;
    chunkOverlap?: number;
  } = {}
) {
  const { chunkSize = 512, chunkOverlap = 50 } = options;

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize,
    chunkOverlap,
    separators: ['\n\n', '\n', '. ', ' ', ''], // Try in order
    lengthFunction: (text) => Math.ceil(text.length / 4), // Token estimate
  });

  const docs = await splitter.createDocuments([document]);

  return docs.map((doc, i) => ({
    text: doc.pageContent,
    metadata: { chunkIndex: i, ...doc.metadata },
  }));
}

// Benefits:
// ✅ Tries paragraph breaks first (\n\n)
// ✅ Falls back to sentence breaks (. )
// ✅ Only splits on spaces/characters as last resort
// ✅ Handles edge cases automatically
```

**Pros**:

- ✅ Battle-tested, production-ready
- ✅ Smart separator hierarchy
- ✅ Automatic edge case handling

**Cons**:

- ❌ Character-based by default (configure for tokens)
- ❌ Additional dependency

**When to Use**: Quick implementation, mixed document types

## Framework Integration

### AI SDK 6 with Chunking

```typescript
import { embed, embedMany } from 'ai';
import { openai } from '@ai-sdk/openai';

async function indexDocument(document: string) {
  // Step 1: Chunk
  const chunker = new TokenChunker();
  const chunks = chunker.chunkByTokens(document, {
    chunkSize: 512,
    overlap: 50,
  });

  // Step 2: Embed all chunks
  const { embeddings } = await embedMany({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    values: chunks.map((c) => c.text),
    maxParallelCalls: 5,
  });

  // Step 3: Store in vector DB
  const records = chunks.map((chunk, i) => ({
    id: `doc-chunk-${i}`,
    text: chunk.text,
    vector: embeddings[i],
    metadata: {
      tokenStart: chunk.start,
      tokenEnd: chunk.end,
      chunkIndex: i,
    },
  }));

  await vectorDb.insert(records);

  return { chunksCreated: chunks.length };
}
```

### Batch Processing Pipeline

```typescript
async function processDocumentBatch(documents: string[]) {
  const chunker = new TokenChunker();
  const allChunks: { text: string; docId: number; chunkIndex: number }[] = [];

  // Chunk all documents
  for (let docId = 0; docId < documents.length; docId++) {
    const chunks = chunker.chunkByTokens(documents[docId], {
      chunkSize: 512,
      overlap: 50,
    });

    chunks.forEach((chunk, chunkIndex) => {
      allChunks.push({
        text: chunk.text,
        docId,
        chunkIndex,
      });
    });
  }

  // Batch embed (efficient)
  const batchSize = 100;
  for (let i = 0; i < allChunks.length; i += batchSize) {
    const batch = allChunks.slice(i, i + batchSize);

    const { embeddings } = await embedMany({
      model: openai.textEmbeddingModel('text-embedding-3-small'),
      values: batch.map((c) => c.text),
    });

    await vectorDb.insert(
      batch.map((chunk, j) => ({
        ...chunk,
        vector: embeddings[j],
      }))
    );
  }

  return { totalChunks: allChunks.length };
}
```

## Research & Benchmarks

### Chunk Size Comparison

| Chunk Size | Recall@10 | Precision@5 | Tokens/Doc | Use Case |
|------------|-----------|-------------|------------|----------|
| **256** | 82% | 76% | 2× | Short docs, Q&A |
| **512** | 91% | 88% | 1× | **Default (best balance)** |
| **1024** | 89% | 84% | 0.5× | Long-form content |
| **2048** | 85% | 79% | 0.25× | Full sections |

### Overlap Impact

| Overlap | Recall@10 | Storage Overhead |
|---------|-----------|------------------|
| **0%** | 78% | 1× |
| **10%** | 91% | 1.1× |
| **20%** | 94% | 1.25× |
| **30%** | 94% | 1.4× (diminishing returns) |

**Source**: [Chroma Research (2024)](https://research.trychroma.com/evaluating-chunking)

### Character vs Token Comparison

| Method | Recall@10 | Predictability | Speed |
|--------|-----------|----------------|-------|
| **Character-based** | 76% | Low | Fast |
| **Token-based** | 91% | High | Medium |
| **Sentence-aware** | 89% | Medium | Medium |

## When to Use This Pattern

### ✅ Use Fixed-Size When:

1. **General-purpose RAG**
   - Unknown document types
   - Need predictable behavior

2. **Cost estimation matters**
   - Budget planning requires predictable chunk counts
   - Consistent embedding costs

3. **Simple implementation**
   - Quick MVP
   - Don't need semantic chunking

### ❌ Consider Alternatives When:

1. **Code documents**
   - Use AST-based chunking
   - Respect function/class boundaries

2. **Structured documents**
   - Markdown: chunk by heading
   - HTML: chunk by section tags

3. **Variable-length content**
   - Mix of short and long documents
   - Semantic chunking may be better

### Decision Matrix

| Document Type | Recommended Approach |
|---------------|---------------------|
| **General text** | Token-based, 512 tokens, 10% overlap |
| **Technical docs** | Sentence-aware, 512 tokens, 20% overlap |
| **Code files** | AST-based or function boundaries |
| **Markdown** | Heading-based + fixed-size fallback |
| **PDF/scanned** | Character-based + OCR cleanup |

## Production Best Practices

### 1. Always Use Token-Based Sizing

```typescript
// BAD: Character-based (unpredictable tokens)
const chunks = text.match(/.{1,2000}/g);
// Could be 400-800 tokens!

// GOOD: Token-based
const chunks = tokenChunker.chunkByTokens(text, { chunkSize: 512 });
// Exactly 512 tokens (or less for last chunk)
```

### 2. Always Add Overlap

```typescript
// BAD: No overlap (13-16% recall loss)
const chunks = chunker.chunkByTokens(text, { overlap: 0 });

// GOOD: 10-20% overlap
const chunks = chunker.chunkByTokens(text, { chunkSize: 512, overlap: 50 });
```

### 3. Store Chunk Metadata

```typescript
// Store position info for context reconstruction
const record = {
  text: chunk.text,
  vector: embedding,
  metadata: {
    documentId: doc.id,
    chunkIndex: i,
    tokenStart: chunk.start,
    tokenEnd: chunk.end,
    totalChunks: chunks.length,
  },
};
```

### Common Pitfalls

#### ❌ Pitfall: Character-Based Chunking

**Problem**: Unpredictable token counts, exceeds embedding limits.

```typescript
// BAD: 2000 characters could be 300-800 tokens
const chunks = text.match(/.{1,2000}/g);

// GOOD: Exact token control
const chunks = chunker.chunkByTokens(text, { chunkSize: 512 });
```

#### ❌ Pitfall: Zero Overlap

**Problem**: 13-16% recall loss on boundary queries.

```typescript
// BAD: Information lost at boundaries
chunkByTokens(text, { chunkSize: 512, overlap: 0 });

// GOOD: Overlap preserves context
chunkByTokens(text, { chunkSize: 512, overlap: 50 });
```

#### ❌ Pitfall: Ignoring Model Limits

**Problem**: Chunks exceeding 8K tokens fail silently.

```typescript
// BAD: No validation
const embedding = await embed(longText);
// May truncate silently!

// GOOD: Validate before embedding
if (tokenCount(text) > 8000) {
  throw new Error('Chunk exceeds model limit');
}
```

## Key Takeaways

1. **Token-based is mandatory** - 15-20% more accurate than character-based
2. **512 tokens is the sweet spot** - Best balance of precision and context
3. **10-20% overlap is required** - Prevents 13-16% recall loss at boundaries
4. **Sentence-awareness helps** - 15-25% less context loss
5. **Store metadata** - Enable context reconstruction later

**Quick Implementation Checklist**:

- [ ] Use token-based chunking (not characters)
- [ ] Set chunk size to 512 tokens
- [ ] Add 10% overlap (50 tokens)
- [ ] Consider sentence boundaries
- [ ] Store chunk position metadata
- [ ] Validate against model token limits

## References

1. **Chroma Research** (2024). "Evaluating Chunking Strategies for Retrieval". https://research.trychroma.com/evaluating-chunking
2. **NVIDIA** (2024). "Best Chunking Strategies for RAG". https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
3. **Firecrawl** (2025). "Token-Based vs Character-Based Chunking". https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
4. **Adnan Masood** (2025). "Optimizing Chunking, Embedding, and Vectorization for RAG". https://medium.com/@adnanmasood/optimizing-chunking-embedding-and-vectorization
5. **LangChain** (2024). "Text Splitters Documentation". https://python.langchain.com/docs/modules/data_connection/document_transformers/

**Related Topics**:

- [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- [5.2.2 Semantic Chunking](./5.2.2-semantic-chunks.md)
- [5.2.3 Overlapping Windows](./5.2.3-overlapping-windows.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-rag--retrieval)
