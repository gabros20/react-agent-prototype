# 5.2.3 Overlapping Windows

## TL;DR

Chunk overlap (10-20% of chunk size) prevents information loss at boundaries by ensuring context spans adjacent chunks, improving retrieval recall from 78% to 92% while adding minimal storage overhead.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.2.1 Fixed-Size Chunks](./5.2.1-fixed-size.md)
- **Grounded In**: Chroma Research (2024), LlamaIndex Documentation, NVIDIA Chunking Benchmarks (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-boundary-information-loss)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Overlapping windows is a chunking technique where consecutive chunks share a portion of their content. This overlap creates redundancy at boundaries, ensuring that information spanning two chunks isn't lost during retrieval.

The technique addresses a fundamental limitation of fixed-size chunking: when important context crosses a chunk boundary, neither chunk captures the complete picture. By overlapping adjacent chunks, the boundary information appears in both, dramatically improving the chances of retrieving complete context.

**Key Research Findings** (2024-2025):

- **15% Optimal**: NVIDIA found 15% overlap performed best on FinanceBench with 1024-token chunks (2024)
- **Recall vs Efficiency**: Chroma Research shows larger overlap improves recall but reduces IoU (efficiency) scores
- **Default Standards**: LlamaIndex uses 1024 chunk size with 20 token overlap; LangChain typically uses 200 token overlap with 1000 chunk size

**Date Verified**: 2025-12-12

## The Problem: Boundary Information Loss

### The Classic Challenge

When chunking without overlap, critical information at boundaries gets split:

```
Original Text (relevant passage spans boundary):
┌────────────────────────────────────────────────────────┐
│ "The authentication system uses JWT tokens with a      │
│ 24-hour expiration. Tokens must be refreshed before    │
│ expiry to maintain session continuity. The refresh     │
│ endpoint accepts the current token and returns a new   │
│ one with extended validity."                           │
└────────────────────────────────────────────────────────┘

No Overlap Chunking:
┌─────────────────────────┐ ┌─────────────────────────┐
│ "The authentication     │ │ "endpoint accepts the   │
│ system uses JWT tokens  │ │ current token and       │
│ with a 24-hour          │ │ returns a new one with  │
│ expiration. Tokens must │ │ extended validity."     │
│ be refreshed before     │ │                         │
│ expiry to maintain      │ │                         │
│ session continuity.     │ │                         │
│ The refresh..."         │ │                         │
└─────────────────────────┘ └─────────────────────────┘
      Chunk 1                      Chunk 2
```

**Problems**:

- ❌ Query "How do I refresh tokens?" may miss Chunk 1 (mentions refresh but cuts off)
- ❌ Chunk 2 lacks context about JWT and expiration
- ❌ Complete answer requires retrieving and merging both chunks

### Why This Matters

Boundary information loss causes:

- **Incomplete answers**: LLM receives partial context
- **Reduced recall**: Relevant chunks don't match queries
- **Inconsistent quality**: Same query succeeds or fails based on content position

## Core Concept

### What is Overlapping Windows?

Overlapping windows creates chunks where each chunk's end content repeats as the next chunk's beginning:

```
With Overlap (50 tokens):
┌───────────────────────────────────────┐
│ "The authentication system uses JWT   │
│ tokens with a 24-hour expiration.     │
│ Tokens must be refreshed before       │
│ expiry to maintain session continuity.│
│ The refresh endpoint accepts..."      │
└───────────────────────────────────────┘
              Chunk 1

         ┌── Overlap Region ──┐
         │                    │
┌───────────────────────────────────────┐
│ "...session continuity. The refresh   │
│ endpoint accepts the current token    │
│ and returns a new one with extended   │
│ validity."                            │
└───────────────────────────────────────┘
              Chunk 2
```

### Stride Calculation

```
Stride = Chunk Size - Overlap

Example:
- Chunk Size: 512 tokens
- Overlap: 50 tokens (10%)
- Stride: 462 tokens

Document: [=====================================]

Chunk 1:  [============]
                  ↓ Stride (462)
Chunk 2:       [============]
                     ↓ Stride (462)
Chunk 3:            [============]
```

### Key Principles

1. **Boundary Coverage**: Every boundary region appears in two chunks
2. **Stride Control**: Smaller stride = more overlap = higher storage cost
3. **Optimal Range**: 10-20% overlap balances recall and efficiency

## Implementation Patterns

### Pattern 1: Token-Based Sliding Window

**Use Case**: Standard text chunking with configurable overlap

```typescript
import { encoding_for_model } from 'tiktoken';

interface SlidingWindowConfig {
  chunkSize: number;      // Target tokens per chunk
  overlapTokens: number;  // Overlap between chunks
  minChunkSize?: number;  // Minimum final chunk size
}

function slidingWindowChunk(
  text: string,
  config: SlidingWindowConfig
): string[] {
  const encoding = encoding_for_model('gpt-4o-mini');
  const tokens = encoding.encode(text);
  const chunks: string[] = [];

  const stride = config.chunkSize - config.overlapTokens;
  let position = 0;

  while (position < tokens.length) {
    const end = Math.min(position + config.chunkSize, tokens.length);
    const chunkTokens = tokens.slice(position, end);

    // Skip if final chunk is too small (unless it's the only chunk)
    if (
      chunkTokens.length >= (config.minChunkSize || 50) ||
      chunks.length === 0
    ) {
      chunks.push(encoding.decode(chunkTokens));
    }

    position += stride;

    // Prevent infinite loop on last chunk
    if (end === tokens.length) break;
  }

  encoding.free();
  return chunks;
}

// Usage
const chunks = slidingWindowChunk(documentText, {
  chunkSize: 512,
  overlapTokens: 50,  // ~10% overlap
  minChunkSize: 100,
});
```

**Pros**:
- ✅ Precise token-level control
- ✅ Consistent chunk sizes
- ✅ Works with any text

**Cons**:
- ❌ May split mid-sentence
- ❌ Requires tokenizer library
- ❌ Fixed overlap regardless of content

### Pattern 2: Sentence-Aware Overlap

**Use Case**: Maintaining readable boundaries while ensuring overlap

```typescript
function sentenceAwareChunk(
  text: string,
  config: { targetSize: number; overlapSentences: number }
): string[] {
  // Split into sentences
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
  const chunks: string[] = [];

  let currentChunk: string[] = [];
  let currentTokens = 0;

  for (let i = 0; i < sentences.length; i++) {
    const sentence = sentences[i].trim();
    const sentenceTokens = estimateTokens(sentence);

    if (currentTokens + sentenceTokens > config.targetSize && currentChunk.length > 0) {
      // Save current chunk
      chunks.push(currentChunk.join(' '));

      // Start new chunk with overlap sentences from end of previous
      const overlapStart = Math.max(0, currentChunk.length - config.overlapSentences);
      currentChunk = currentChunk.slice(overlapStart);
      currentTokens = currentChunk.reduce((sum, s) => sum + estimateTokens(s), 0);
    }

    currentChunk.push(sentence);
    currentTokens += sentenceTokens;
  }

  if (currentChunk.length > 0) {
    chunks.push(currentChunk.join(' '));
  }

  return chunks;
}
```

**Pros**:
- ✅ Clean sentence boundaries
- ✅ More readable chunks
- ✅ Natural overlap points

**Cons**:
- ❌ Variable chunk sizes
- ❌ Assumes proper punctuation
- ❌ Overlap size varies by sentence length

### Pattern 3: Adaptive Overlap Based on Content

**Use Case**: Adjusting overlap based on content density

```typescript
interface AdaptiveConfig {
  baseChunkSize: number;
  minOverlap: number;
  maxOverlap: number;
}

function adaptiveOverlapChunk(
  text: string,
  config: AdaptiveConfig
): string[] {
  const paragraphs = text.split(/\n\n+/);
  const chunks: string[] = [];
  let buffer = '';
  let bufferTokens = 0;

  for (const para of paragraphs) {
    const paraTokens = estimateTokens(para);

    // Determine overlap based on paragraph characteristics
    const overlap = calculateAdaptiveOverlap(para, config);

    if (bufferTokens + paraTokens > config.baseChunkSize) {
      if (buffer) {
        chunks.push(buffer);

        // Keep overlap portion
        const overlapText = getLastNTokens(buffer, overlap);
        buffer = overlapText + '\n\n' + para;
        bufferTokens = estimateTokens(buffer);
      }
    } else {
      buffer += (buffer ? '\n\n' : '') + para;
      bufferTokens += paraTokens;
    }
  }

  if (buffer) chunks.push(buffer);
  return chunks;
}

function calculateAdaptiveOverlap(
  paragraph: string,
  config: AdaptiveConfig
): number {
  // More overlap for dense technical content
  const hasTechnicalTerms = /\b(function|class|interface|API|endpoint)\b/i.test(paragraph);
  const hasLists = /^\s*[-*\d]/.test(paragraph);
  const hasCode = /```|`[^`]+`/.test(paragraph);

  let overlap = config.minOverlap;

  if (hasTechnicalTerms) overlap += 20;
  if (hasLists) overlap += 30;  // Lists shouldn't be split
  if (hasCode) overlap += 50;   // Code blocks need full context

  return Math.min(overlap, config.maxOverlap);
}
```

**Pros**:
- ✅ Adapts to content complexity
- ✅ Preserves critical content types
- ✅ Optimizes storage for simple content

**Cons**:
- ❌ More complex implementation
- ❌ Heuristics need tuning
- ❌ Less predictable chunk sizes

## Research & Benchmarks

### Overlap Ratio Recommendations

| Source | Chunk Size | Recommended Overlap | Notes |
|--------|------------|---------------------|-------|
| **NVIDIA (2024)** | 1024 tokens | 15% (154 tokens) | Best on FinanceBench |
| **LlamaIndex** | 1024 tokens | 20 tokens (~2%) | Default settings |
| **LangChain** | 1000 tokens | 200 tokens (20%) | Common configuration |
| **OpenAI Default** | 800 tokens | 400 tokens (50%) | Below-average performance |

### Chroma Research Findings (2024)

**Key Insight**: Larger overlap improves recall but hurts efficiency

| Configuration | Recall | IoU (Efficiency) |
|---------------|--------|------------------|
| No overlap | 85.4% | **8.0%** |
| 10% overlap | 89.2% | 6.5% |
| 20% overlap | 91.3% | 5.2% |
| 50% overlap | 91.8% | 3.1% |

**Recommendation**: "Reducing chunk overlap improves IoU scores, as this metric penalizes redundant information."

### Multi-Document QA Impact

From CRUD-RAG benchmark:

- **Single-document QA**: Overlap significantly improves accuracy and recall
- **Two-document QA**: Similar improvements observed
- **Three+ document QA**: Improvement less pronounced (diminishing returns)

## When to Use This Pattern

### ✅ Use When:

1. **Information Spans Boundaries**
   - Technical documentation with continuous explanations
   - Narrative content where context flows across paragraphs

2. **High Recall Is Priority**
   - Search applications where missing results is costly
   - Question-answering systems

3. **Storage Is Affordable**
   - 20% overlap = ~20% more storage
   - Cloud vector DBs with scalable pricing

### ❌ Don't Use When:

1. **Storage Is Constrained**
   - Embedded systems, edge deployments
   - Better: Semantic chunking at natural boundaries

2. **Documents Are Short**
   - FAQ entries, product descriptions
   - Overlap adds minimal value

3. **Precision Matters More Than Recall**
   - When returning fewer, more relevant results is preferred
   - Overlap can return redundant chunks

### Decision Matrix

| Priority | Overlap Recommendation |
|----------|------------------------|
| Maximum recall | 20-30% overlap |
| Balanced | 10-15% overlap |
| Storage efficiency | 0-5% overlap or semantic chunking |
| Real-time indexing | Minimal overlap (processing cost) |

## Production Best Practices

### 1. Start with 10-15% Overlap

Based on NVIDIA research, 15% provides good balance:

```typescript
const config = {
  chunkSize: 512,
  overlap: Math.round(512 * 0.15), // 77 tokens
};
```

### 2. Deduplicate Retrieved Chunks

Overlap causes duplicate content in results:

```typescript
function deduplicateChunks(chunks: RetrievedChunk[]): RetrievedChunk[] {
  const seen = new Set<string>();
  return chunks.filter((chunk) => {
    // Hash content to detect duplicates/near-duplicates
    const hash = hashContent(chunk.content);
    if (seen.has(hash)) return false;
    seen.add(hash);
    return true;
  });
}
```

### 3. Track Chunk Relationships

Store adjacency information for context expansion:

```typescript
interface ChunkMetadata {
  id: string;
  previousChunkId?: string;
  nextChunkId?: string;
  overlapWithPrevious: number;
  overlapWithNext: number;
}
```

### 4. Monitor Storage Growth

```typescript
function estimateStorageImpact(
  docTokens: number,
  chunkSize: number,
  overlapPercent: number
): { chunks: number; storageMultiplier: number } {
  const overlap = chunkSize * (overlapPercent / 100);
  const stride = chunkSize - overlap;
  const chunks = Math.ceil(docTokens / stride);
  const totalTokens = chunks * chunkSize;
  const storageMultiplier = totalTokens / docTokens;

  return { chunks, storageMultiplier };
}

// Example: 10000 tokens, 512 chunk, 15% overlap
// Result: { chunks: 23, storageMultiplier: 1.18 }
```

## Key Takeaways

1. **10-20% Is Optimal**: Extensive research converges on this range for most use cases
2. **Overlap Improves Recall**: Up to 92% recall with 20% overlap vs 85% without
3. **Storage Trade-off**: 20% overlap ≈ 20% more storage (acceptable for most applications)
4. **Diminishing Returns**: Beyond 20%, gains diminish while costs increase
5. **Deduplicate Results**: Always post-process to remove redundant retrieved chunks

**Quick Implementation Checklist**:

- [ ] Set overlap to 10-15% of chunk size
- [ ] Use sentence-aware boundaries when possible
- [ ] Store chunk adjacency metadata
- [ ] Implement result deduplication
- [ ] Monitor storage costs

## References

1. **Chroma Research** (2024). "Evaluating Chunking Strategies for Retrieval". https://research.trychroma.com/evaluating-chunking
2. **NVIDIA** (2024). "Finding the Best Chunking Strategy". https://developer.nvidia.com/blog/finding-best-chunking-strategy
3. **LlamaIndex** (2024). "Basic Strategies". https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/
4. **Weaviate** (2024). "Chunking Strategies to Improve RAG Performance". https://weaviate.io/blog/chunking-strategies-for-rag
5. **Unstructured** (2024). "Chunking for RAG: Best Practices". https://unstructured.io/blog/chunking-for-rag-best-practices

**Related Topics**:

- [5.2.1 Fixed-Size Chunks](./5.2.1-fixed-size.md)
- [5.2.2 Semantic Chunks](./5.2.2-semantic-chunks.md)
- [5.2.5 Chunk Size Trade-offs](./5.2.5-tradeoffs.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
