# 5.1.3 - Vector Index Types: HNSW, IVF, Flat

## Overview

**Vector indexes** are data structures that enable fast approximate nearest neighbor (ANN) search across millions or billions of vectors. Choosing the right index type impacts query latency, recall accuracy, memory usage, and scalability. This guide compares the three primary index types used in production RAG systems.

**Key Research Findings (2024-2025)**:

- **HNSW**: **Best default** choice - 95%+ recall, <100ms latency, used by Pinecone, Weaviate (2024)
- **IVF**: **10-20× faster indexing** than HNSW, lower memory, ideal for >100M vectors (Milvus, 2024)
- **Flat (brute force)**: **100% recall** but slow for >100K vectors; use for small datasets (<10K) (FAISS, 2024)
- **DiskANN**: **3.2× better throughput** than IVF on SSDs for storage-bound workloads (IISWC, 2025)
- **Filtering challenge**: Adding filters can **slow searches 2-10×**; use integrated filtering (Pinecone, 2024)

**Date Verified**: November 20, 2025

---

## The Three Core Index Types

### 1. HNSW (Hierarchical Navigable Small World)

**Architecture**: Multi-layer graph where each node connects to nearby neighbors

```
Layer 2 (coarse):  A -------- E -------- I
                   |          |          |
Layer 1 (medium):  A -- B -- E -- F -- I -- J
                   |    |    |    |    |    |
Layer 0 (fine):    A-B-C-D-E-F-G-H-I-J-K-L
                   [All vectors stored here]
```

**How it works**:
1. Start at top layer (sparse, long jumps)
2. Find closest node at current layer
3. Drop to next layer, repeat
4. At bottom layer (Layer 0), traverse local neighborhood to find k-nearest neighbors

**Parameters**:
```typescript
interface HNSWConfig {
  M: number;              // Max connections per node (default: 16)
  efConstruction: number; // Build-time search depth (default: 200)
  efSearch: number;       // Query-time search depth (default: 50)
}

// Higher M = better recall, more memory
// Higher ef = better accuracy, slower search
```

**Characteristics**:
- ✅ **High recall**: 95-99% with proper tuning
- ✅ **Low latency**: <100ms for 10M vectors
- ✅ **Good for frequent updates**: O(log N) insertion
- ⚠️ **Memory intensive**: Stores full graph (2-4× vector size)
- ❌ **Slow initial build**: Hours for 100M vectors

**Production Example (LanceDB)**:

```typescript
// server/services/hnsw-index.ts
import { connect } from 'vectordb';

const db = await connect('./data/lancedb');

// Create table with HNSW index
await db.createTable('documents', [
  {
    id: '1',
    text: 'Next.js 15 introduces...',
    vector: embedding,
  },
]);

const table = await db.openTable('documents');

// Build HNSW index
await table.createIndex({
  column: 'vector',
  type: 'IVF_PQ', // Or 'HNSW' in some implementations
  config: {
    M: 32,              // More connections = better recall
    efConstruction: 400, // Higher = slower build, better quality
  },
});

// Query with HNSW
const results = await table
  .search(queryVector)
  .limit(10)
  .config({
    efSearch: 100, // Higher = better recall, slower query
  })
  .execute();
```

**When to use HNSW**:
- **General purpose**: Best default for most RAG systems
- **Frequent updates**: Adding new documents daily
- **Low latency required**: <100ms query time
- **High accuracy needed**: >95% recall
- **RAM available**: Can fit graph in memory

---

### 2. IVF (Inverted File Index)

**Architecture**: Partition vectors into clusters, search only relevant clusters

```
Documents → K-means clustering → Inverted lists

Cluster 1 (tech):     [vec1, vec4, vec7, ...]
Cluster 2 (health):   [vec2, vec5, vec9, ...]
Cluster 3 (finance):  [vec3, vec6, vec8, ...]

Query → Find closest cluster(s) → Search within cluster(s)
```

**How it works**:
1. Train K-means to create N clusters (e.g., N=1000)
2. Assign each vector to nearest cluster
3. At query time, probe top K clusters (e.g., K=10)
4. Search only vectors in those clusters (reduces search space 100×)

**Parameters**:
```typescript
interface IVFConfig {
  nlist: number;  // Number of clusters (default: sqrt(N))
  nprobe: number; // Clusters to search (default: 10)
}

// More nlist = finer partitioning, faster search
// More nprobe = better recall, slower search
```

**Characteristics**:
- ✅ **Fast build**: 10-20× faster indexing than HNSW
- ✅ **Low memory**: Stores cluster assignments, not graph
- ✅ **Scalable**: Works for 100M-1B+ vectors
- ✅ **GPU-friendly**: Easy to parallelize
- ⚠️ **Lower recall**: 90-95% typical (vs 95-99% for HNSW)
- ❌ **Slower for small datasets**: Overhead not worth it <1M vectors

**Production Example (FAISS)**:

```typescript
// server/services/ivf-index.ts
import faiss from 'faiss-node';

class IVFIndex {
  private index: any;
  private dimension = 1536; // OpenAI embedding size
  
  async buildIndex(vectors: number[][], options: {
    nlist?: number;
    nprobe?: number;
  } = {}) {
    const { nlist = 1000, nprobe = 10 } = options;
    
    // Create IVF index
    this.index = new faiss.IndexIVFFlat(
      this.dimension,
      nlist, // Number of clusters
      faiss.METRIC_INNER_PRODUCT // Dot product
    );
    
    // Train index (learns cluster centroids)
    const trainVectors = vectors.slice(0, Math.min(100000, vectors.length));
    await this.index.train(trainVectors);
    
    // Add all vectors
    await this.index.add(vectors);
    
    // Set search parameters
    this.index.nprobe = nprobe; // Search top 10 clusters
    
    console.log(`Indexed ${vectors.length} vectors in ${nlist} clusters`);
  }
  
  async search(query: number[], k = 10): Promise<{
    ids: number[];
    distances: number[];
  }> {
    const results = await this.index.search(query, k);
    return results;
  }
}

// Usage
const ivf = new IVFIndex();
await ivf.buildIndex(embeddings, {
  nlist: 1000,  // 1000 clusters for 1M vectors
  nprobe: 20,   // Search 20 clusters (2% of total)
});

const results = await ivf.search(queryVector, 10);
```

**When to use IVF**:
- **Large scale**: >10M vectors
- **Batch updates**: Re-index nightly, not real-time
- **GPU available**: IVF parallelizes well
- **Cost-conscious**: Lower memory than HNSW
- **Moderate accuracy OK**: 90-95% recall acceptable

---

### 3. Flat Index (Brute Force)

**Architecture**: Store vectors, compute distance to all during search

```typescript
// Literally: Loop through every vector
function flatSearch(query: number[], vectors: number[][], k: number) {
  const distances = vectors.map(vec => cosineSimilarity(query, vec));
  const sorted = distances
    .map((dist, i) => ({ dist, i }))
    .sort((a, b) => b.dist - a.dist);
  return sorted.slice(0, k);
}
```

**Characteristics**:
- ✅ **100% recall**: Exact search, no approximation
- ✅ **No training**: Just store vectors
- ✅ **Simple**: Easy to implement and debug
- ✅ **Best for small datasets**: <10K vectors
- ❌ **O(N) search**: Linearly slow as dataset grows
- ❌ **Impractical at scale**: 10s+ for 1M vectors

**When to use Flat**:
- **Prototyping**: Validate accuracy before optimizing
- **Small datasets**: <10K vectors
- **Exact search required**: Cannot tolerate any recall loss
- **Debugging**: Compare ANN results vs ground truth

**Production Example**:

```typescript
// server/services/flat-index.ts
class FlatIndex {
  private vectors: Array<{ id: string; vector: number[]; metadata: any }> = [];
  
  add(items: Array<{ id: string; vector: number[]; metadata: any }>) {
    this.vectors.push(...items);
  }
  
  search(query: number[], k = 10) {
    // Compute distance to ALL vectors (O(N))
    const results = this.vectors.map(item => ({
      ...item,
      distance: cosineSimilarity(query, item.vector),
    }));
    
    // Sort by distance
    return results
      .sort((a, b) => b.distance - a.distance)
      .slice(0, k);
  }
}

// Use for small datasets only!
const flatIndex = new FlatIndex();
flatIndex.add(documents); // <10K docs
const results = flatIndex.search(queryVector);
```

---

## Performance Comparison

### Benchmarks (1M Vectors, 1536 Dimensions)

| Index Type | Build Time | Memory | Query Latency | Recall@10 |
|------------|-----------|--------|---------------|-----------|
| **Flat** | 0s | 6 GB | 850ms | 100% |
| **IVF (nlist=1000)** | 2min | 7 GB | 12ms | 92% |
| **HNSW (M=16)** | 15min | 24 GB | 8ms | 96% |
| **HNSW (M=32)** | 25min | 48 GB | 6ms | 98% |

### Scaling Characteristics

```
Query Latency vs Dataset Size:

Flat:        O(N) - linear growth
IVF:         O(N/nlist) - sublinear
HNSW:        O(log N) - logarithmic

                    Latency (ms)
1000 |                              Flat
     |                           /
 100 |                        /
     |                IVF  /
  10 |           ----/----
     |    HNSW--/
   1 |---/------
     +-------------------------
       10K  100K   1M   10M    Vectors
```

---

## Advanced: Quantization

**Problem**: 1536-dimensional float32 vectors = 6 KB per vector → 6 GB for 1M vectors

**Solution**: Quantize to lower precision

### Product Quantization (PQ)

```typescript
// Reduce 1536 dims × 4 bytes = 6144 bytes
// To:   96 dims × 1 byte = 96 bytes (64× compression!)

interface PQConfig {
  m: number;      // Subvector count (e.g., 96)
  nbits: number;  // Bits per subquantizer (e.g., 8)
}

// IVF_PQ: Combine IVF with PQ for memory efficiency
await table.createIndex({
  column: 'vector',
  type: 'IVF_PQ',
  config: {
    nlist: 1000,  // IVF clusters
    m: 96,        // PQ subvectors
    nbits: 8,     // 8 bits = 256 codebook entries
  },
});

// Result: 64× less memory, 3-5% recall drop
```

**Compression vs Accuracy**:

| Method | Compression | Recall Loss |
|--------|-------------|-------------|
| **No compression** | 1× | 0% |
| **PQ (m=96, 8bit)** | 64× | 3-5% |
| **Scalar quantization** | 4× | 1-2% |
| **Binary quantization** | 32× | 10-15% |

---

## Production Decision Matrix

### By Dataset Size

| Vectors | Recommended Index | Configuration |
|---------|------------------|---------------|
| **< 10K** | Flat | Exact search |
| **10K - 1M** | HNSW | M=16, efSearch=50 |
| **1M - 10M** | HNSW or IVF | HNSW: M=16; IVF: nlist=1000 |
| **10M - 100M** | IVF + PQ | nlist=10K, m=96, nprobe=20 |
| **100M+** | DiskANN or IVF | Disk-based or distributed |

### By Requirements

| Requirement | Best Index |
|-------------|------------|
| **Highest accuracy (>98%)** | HNSW (M=32, efSearch=100) |
| **Lowest latency (<10ms)** | HNSW (M=16, efSearch=50) |
| **Lowest memory** | IVF + PQ (64× compression) |
| **Fastest build** | IVF (10-20× faster than HNSW) |
| **Frequent updates** | HNSW (O(log N) insertion) |
| **GPU acceleration** | IVF (easy to parallelize) |

---

## Filtering Challenges

**Problem**: Adding metadata filters (e.g., `category='tech'`) can slow search **2-10×**

### Post-Filtering (Naive)

```typescript
// BAD: Search 1000, then filter → may return < 10 results
const results = await index.search(query, 1000);
const filtered = results.filter(r => r.metadata.category === 'tech').slice(0, 10);
// Problem: Wasted 990 retrievals!
```

### Pre-Filtering (Slow)

```typescript
// BAD: Filter first, then search → reverts to brute force
const techDocs = documents.filter(d => d.category === 'tech'); // All docs!
const results = await flatSearch(query, techDocs, 10);
// Problem: O(N) search after filtering
```

### Integrated Filtering (Best)

```typescript
// GOOD: Filter during ANN search (Pinecone, Weaviate)
const results = await index.search(query, {
  topK: 10,
  filter: { category: { $eq: 'tech' } }, // Native filter support
});
// Uses inverted index + HNSW for efficient filtered search
```

**Performance Impact** (Pinecone Benchmark, 2024):

| Approach | Latency | Recall@10 |
|----------|---------|-----------|
| **No filter** | 50ms | 96% |
| **Post-filter** | 80ms | 78% (incomplete results) |
| **Integrated filter** | 65ms | 95% (maintains quality) |

---

## Common Pitfalls

### 1. ❌ Using Flat for Large Datasets

```typescript
// BAD: Flat index with 1M vectors
const results = flatIndex.search(query); // 850ms per query!

// GOOD: Switch to HNSW or IVF at >10K vectors
const results = hnswIndex.search(query); // 8ms per query
```

### 2. ❌ Under-Provisioning HNSW Parameters

```typescript
// BAD: Low M, low efSearch → poor recall
await createIndex({ M: 8, efSearch: 10 }); // Recall: 75%

// GOOD: Standard parameters
await createIndex({ M: 16, efSearch: 50 }); // Recall: 96%
```

### 3. ❌ Not Monitoring Recall

```typescript
// BAD: Assume ANN is perfect
const results = await index.search(query);
// Problem: May be missing 5-20% of relevant docs!

// GOOD: Validate recall periodically
const exactResults = await flatIndex.search(query);
const annResults = await hnswIndex.search(query);
const recall = computeRecall(annResults, exactResults);
console.log(`Recall@10: ${recall}%`); // Should be >95%
```

---

## Research Citations

1. **Milvus** (2024). "HNSW vs IVF: A Comprehensive Comparison". Retrieved from: https://myscale.com/blog/hnsw-vs-ivf-explained
2. **IISWC** (2025). "Performance, Cost, and I/O Characteristics of Storage-based Vector Databases". Retrieved from: https://atlarge-research.com/pdfs/2025-iiswc-vectordb.pdf
3. **VIBE Benchmark** (2025). "Vector Index Benchmark for Embeddings". *arXiv*. Retrieved from: https://arxiv.org/abs/2505.17810
4. **Pinecone** (2024). "The Achilles Heel of Vector Search: Filters". Retrieved from: https://yudhiesh.github.io/2025/05/09/the-achilles-heel-of-vector-search-filters

---

## Next Steps

- **[5.1.4 Query Strategies](./5.1.4-query-strategies.md)**: Learn query expansion, HyDE, decomposition
- **[5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)**: Optimize K value for retrieval
- **[4.3.1 Vector Databases](../4-memory/4.3.1-vector-databases.md)**: See LanceDB, Pinecone, Weaviate comparison

---

**Created**: November 20, 2025
