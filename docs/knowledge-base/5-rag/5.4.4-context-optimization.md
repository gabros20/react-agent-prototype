# 5.4.4 Context Optimization (Window & Injection)

## TL;DR

Context optimization maximizes RAG effectiveness by strategically managing what enters the LLM context window—through intelligent ordering, compression, and injection techniques that improve answer quality by 15-30% while reducing token costs.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.4.1 Naive RAG](./5.4.1-naive-rag.md), [5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)
- **Grounded In**: Lost in the Middle (2024), LongRAG (2024), RAPTOR Research (2024), Context Compression Studies

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-context-window-challenges)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Context optimization addresses the critical gap between retrieval and generation. Even with perfect retrieval, poor context management leads to:

- **Lost information**: Important content buried in the middle of context
- **Token waste**: Irrelevant or redundant content consuming context space
- **Hallucination**: LLM fabricating when context is poorly organized

Optimization techniques ensure retrieved content is maximally useful for generation.

**Key Research Findings** (2024-2025):

- **Lost in the Middle**: LLMs perform worst on information in the middle of long contexts (up to 30% accuracy drop)
- **Context Compression**: Can reduce tokens by 50-70% while maintaining 90%+ answer quality
- **Ordering Matters**: Placing most relevant content first and last improves accuracy by 15-20%
- **LongRAG**: Processing 4K+ token chunks outperforms short chunks on complex queries

**Date Verified**: 2025-12-12

## The Problem: Context Window Challenges

### The Classic Challenge

Naive context injection ignores how LLMs process long contexts:

```
Naive Context Injection:
┌─────────────────────────────────────────────────────────────┐
│ System: You are a helpful assistant.                        │
│                                                              │
│ Context:                                                     │
│ [Doc 1 - somewhat relevant]                                 │
│ [Doc 2 - highly relevant] ← BURIED IN MIDDLE               │
│ [Doc 3 - marginally relevant]                               │
│ [Doc 4 - highly relevant] ← BURIED IN MIDDLE               │
│ [Doc 5 - not very relevant]                                 │
│                                                              │
│ Question: What is X?                                         │
├─────────────────────────────────────────────────────────────┤
│ Result: LLM focuses on beginning/end, misses key info       │
│ Answer quality: 60-70% of optimal                           │
└─────────────────────────────────────────────────────────────┘
```

**Context Management Challenges**:

- ❌ "Lost in the Middle" effect degrades accuracy on middle content
- ❌ Redundant information wastes tokens
- ❌ Irrelevant content dilutes signal
- ❌ No structure for LLM to navigate
- ❌ Token limits force arbitrary truncation

### Why Optimization Matters

```
Optimized Context Injection:
┌─────────────────────────────────────────────────────────────┐
│ System: Answer based on the numbered sources below.         │
│                                                              │
│ Context (compressed, ordered by relevance):                 │
│ [1] [Doc 2 - highly relevant] ← FIRST POSITION             │
│ [2] [Doc 4 - highly relevant] ← SECOND POSITION            │
│ [3] [Doc 1 - compressed summary]                            │
│ [4] [Doc 3 - key points only]                               │
│ └── [Doc 5 removed - below relevance threshold]             │
│                                                              │
│ Question: What is X?                                         │
├─────────────────────────────────────────────────────────────┤
│ Result: LLM processes high-relevance content first          │
│ Answer quality: 90-95% of optimal                           │
└─────────────────────────────────────────────────────────────┘
```

## Core Concept

### What is Context Optimization?

Context optimization applies strategies at three stages:

```
┌─────────────────────────────────────────────────────────────┐
│               CONTEXT OPTIMIZATION PIPELINE                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Retrieved Documents                                         │
│         ↓                                                    │
│  ┌─────────────────────────────────────────┐                │
│  │     1. FILTERING & SELECTION            │                │
│  │  - Relevance threshold (drop low scores) │                │
│  │  - Deduplication (remove redundant)      │                │
│  │  - Diversity sampling (avoid clusters)   │                │
│  └─────────────────┬───────────────────────┘                │
│                    ↓                                         │
│  ┌─────────────────────────────────────────┐                │
│  │     2. COMPRESSION & SUMMARIZATION      │                │
│  │  - Extract key sentences                 │                │
│  │  - Summarize long documents              │                │
│  │  - Remove boilerplate/filler             │                │
│  └─────────────────┬───────────────────────┘                │
│                    ↓                                         │
│  ┌─────────────────────────────────────────┐                │
│  │     3. ORDERING & FORMATTING            │                │
│  │  - Position high-relevance first/last    │                │
│  │  - Add structure (numbers, headers)      │                │
│  │  - Include source metadata               │                │
│  └─────────────────┬───────────────────────┘                │
│                    ↓                                         │
│           Optimized Context                                  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Relevance-First Ordering**: Most relevant content in primacy/recency positions
2. **Aggressive Filtering**: Remove low-relevance content entirely
3. **Smart Compression**: Preserve meaning while reducing tokens
4. **Structural Clarity**: Make it easy for LLM to navigate context

## Implementation Patterns

### Pattern 1: Relevance-Based Ordering

**Use Case**: Mitigating "Lost in the Middle" effect

```typescript
interface RankedDocument {
  content: string;
  relevanceScore: number;
  id: string;
}

type OrderingStrategy = 'relevance_first' | 'interleaved' | 'bookend';

function orderByRelevance(
  documents: RankedDocument[],
  strategy: OrderingStrategy = 'relevance_first'
): RankedDocument[] {
  const sorted = [...documents].sort(
    (a, b) => b.relevanceScore - a.relevanceScore
  );

  switch (strategy) {
    case 'relevance_first':
      // Most relevant first (simple, effective)
      return sorted;

    case 'interleaved':
      // Alternate high/low relevance to maintain attention
      const high = sorted.slice(0, Math.ceil(sorted.length / 2));
      const low = sorted.slice(Math.ceil(sorted.length / 2));
      const interleaved: RankedDocument[] = [];
      for (let i = 0; i < Math.max(high.length, low.length); i++) {
        if (high[i]) interleaved.push(high[i]);
        if (low[i]) interleaved.push(low[i]);
      }
      return interleaved;

    case 'bookend':
      // Most relevant at start AND end (best for long contexts)
      if (sorted.length <= 2) return sorted;
      const first = sorted[0];
      const last = sorted[1];
      const middle = sorted.slice(2);
      return [first, ...middle, last];

    default:
      return sorted;
  }
}

// Research-backed: Place most important content in positions 1, 2, and last
function optimizeForLongContext(
  documents: RankedDocument[]
): RankedDocument[] {
  if (documents.length <= 3) {
    return documents.sort((a, b) => b.relevanceScore - a.relevanceScore);
  }

  const sorted = [...documents].sort(
    (a, b) => b.relevanceScore - a.relevanceScore
  );

  // Top 2 most relevant at the start
  const top2 = sorted.slice(0, 2);
  // Most relevant of remaining at the end
  const third = sorted[2];
  // Rest in the middle
  const middle = sorted.slice(3);

  return [...top2, ...middle, third];
}
```

### Pattern 2: Context Compression

**Use Case**: Reducing token usage while preserving information

```typescript
import { generateText, generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface CompressionOptions {
  maxTokens: number;
  preserveStructure: boolean;
  extractiveRatio: number; // 0-1, how much to extract vs summarize
}

// Extractive compression - select key sentences
async function extractiveCompress(
  document: string,
  query: string,
  maxSentences: number = 5
): Promise<string> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      selectedSentences: z.array(z.string()),
      reasoning: z.string(),
    }),
    prompt: `Extract the ${maxSentences} most relevant sentences from this document for answering the query.

Query: "${query}"

Document:
${document}

Select sentences that directly help answer the query. Preserve exact wording.`,
  });

  return object.selectedSentences.join(' ');
}

// Abstractive compression - summarize content
async function abstractiveCompress(
  document: string,
  query: string,
  maxTokens: number = 200
): Promise<string> {
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    system: `Summarize documents to help answer queries. Be concise but preserve key facts.
Maximum length: ~${maxTokens} tokens.`,
    prompt: `Query: "${query}"

Document to summarize:
${document}

Provide a focused summary relevant to the query.`,
  });

  return text;
}

// Hybrid compression - extract then summarize if needed
async function hybridCompress(
  documents: Array<{ content: string; relevance: number }>,
  query: string,
  options: CompressionOptions
): Promise<string> {
  const results: string[] = [];
  let totalTokens = 0;
  const avgTokensPerDoc = options.maxTokens / documents.length;

  for (const doc of documents) {
    const estimatedTokens = doc.content.split(/\s+/).length * 1.3;

    if (estimatedTokens <= avgTokensPerDoc) {
      // Document fits, use as-is
      results.push(doc.content);
      totalTokens += estimatedTokens;
    } else if (doc.relevance > 0.8) {
      // High relevance - extractive to preserve exact info
      const compressed = await extractiveCompress(
        doc.content,
        query,
        Math.ceil(avgTokensPerDoc / 20) // ~20 tokens per sentence
      );
      results.push(compressed);
      totalTokens += compressed.split(/\s+/).length * 1.3;
    } else {
      // Lower relevance - abstractive summarization OK
      const compressed = await abstractiveCompress(
        doc.content,
        query,
        Math.ceil(avgTokensPerDoc)
      );
      results.push(compressed);
      totalTokens += compressed.split(/\s+/).length * 1.3;
    }

    if (totalTokens >= options.maxTokens) break;
  }

  return results.join('\n\n---\n\n');
}
```

### Pattern 3: Deduplication & Diversity

**Use Case**: Removing redundant content, ensuring diverse coverage

```typescript
import { embed, cosineSimilarity } from 'ai';
import { openai } from '@ai-sdk/openai';

interface Document {
  id: string;
  content: string;
  embedding?: number[];
  relevanceScore: number;
}

// Embedding-based deduplication
async function deduplicateByEmbedding(
  documents: Document[],
  similarityThreshold: number = 0.92
): Promise<Document[]> {
  // Ensure all documents have embeddings
  const docsWithEmbeddings = await Promise.all(
    documents.map(async (doc) => {
      if (doc.embedding) return doc;

      const { embedding } = await embed({
        model: openai.textEmbeddingModel('text-embedding-3-small'),
        value: doc.content,
      });

      return { ...doc, embedding };
    })
  );

  // Sort by relevance to keep best version of duplicates
  const sorted = docsWithEmbeddings.sort(
    (a, b) => b.relevanceScore - a.relevanceScore
  );

  const unique: Document[] = [];

  for (const doc of sorted) {
    const isDuplicate = unique.some((existing) => {
      const similarity = cosineSimilarity(existing.embedding!, doc.embedding!);
      return similarity > similarityThreshold;
    });

    if (!isDuplicate) {
      unique.push(doc);
    }
  }

  return unique;
}

// MMR (Maximal Marginal Relevance) for diversity
async function mmrSelection(
  documents: Document[],
  query: string,
  k: number,
  lambda: number = 0.5 // Balance relevance (1) vs diversity (0)
): Promise<Document[]> {
  // Get query embedding
  const { embedding: queryEmbedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  // Ensure documents have embeddings
  const docsWithEmbeddings = await Promise.all(
    documents.map(async (doc) => {
      if (doc.embedding) return doc;

      const { embedding } = await embed({
        model: openai.textEmbeddingModel('text-embedding-3-small'),
        value: doc.content,
      });

      return { ...doc, embedding };
    })
  );

  const selected: Document[] = [];
  const remaining = [...docsWithEmbeddings];

  while (selected.length < k && remaining.length > 0) {
    let bestIdx = -1;
    let bestScore = -Infinity;

    for (let i = 0; i < remaining.length; i++) {
      const doc = remaining[i];

      // Relevance to query
      const relevance = cosineSimilarity(queryEmbedding, doc.embedding!);

      // Max similarity to already selected (diversity penalty)
      const maxSimToSelected =
        selected.length === 0
          ? 0
          : Math.max(
              ...selected.map((s) =>
                cosineSimilarity(s.embedding!, doc.embedding!)
              )
            );

      // MMR score
      const mmrScore = lambda * relevance - (1 - lambda) * maxSimToSelected;

      if (mmrScore > bestScore) {
        bestScore = mmrScore;
        bestIdx = i;
      }
    }

    if (bestIdx !== -1) {
      selected.push(remaining[bestIdx]);
      remaining.splice(bestIdx, 1);
    }
  }

  return selected;
}
```

### Pattern 4: Structured Context Formatting

**Use Case**: Making context navigable for LLM

```typescript
interface FormattedContext {
  systemPrompt: string;
  context: string;
  userPrompt: string;
}

interface SourceDocument {
  id: string;
  content: string;
  metadata: {
    title?: string;
    source?: string;
    date?: string;
    section?: string;
  };
  relevanceScore: number;
}

function formatContextStructured(
  documents: SourceDocument[],
  query: string,
  options = { includeMetadata: true, numberSources: true }
): FormattedContext {
  const systemPrompt = `You are a helpful assistant that answers questions based on provided sources.

Guidelines:
- Answer based ONLY on the provided sources
- Cite sources using [Source N] notation
- If information is not in the sources, say "I don't have information about that"
- Synthesize information from multiple sources when relevant`;

  const contextParts = documents.map((doc, index) => {
    const sourceNum = options.numberSources ? `[Source ${index + 1}]` : '';
    const metadata = options.includeMetadata && doc.metadata.title
      ? `\nTitle: ${doc.metadata.title}`
      : '';
    const source = options.includeMetadata && doc.metadata.source
      ? `\nFrom: ${doc.metadata.source}`
      : '';

    return `${sourceNum}${metadata}${source}
${doc.content}`;
  });

  const context = `<sources>
${contextParts.join('\n\n---\n\n')}
</sources>`;

  const userPrompt = `Based on the sources above, please answer: ${query}`;

  return { systemPrompt, context, userPrompt };
}

// XML-style structured context (often better for Claude models)
function formatContextXML(
  documents: SourceDocument[],
  query: string
): FormattedContext {
  const systemPrompt = `Answer questions using only the provided documents. Cite using [doc_id] notation.`;

  const contextParts = documents.map(
    (doc) => `<document id="${doc.id}" relevance="${doc.relevanceScore.toFixed(2)}">
<metadata>
${doc.metadata.title ? `<title>${doc.metadata.title}</title>` : ''}
${doc.metadata.source ? `<source>${doc.metadata.source}</source>` : ''}
${doc.metadata.date ? `<date>${doc.metadata.date}</date>` : ''}
</metadata>
<content>
${doc.content}
</content>
</document>`
  );

  const context = `<documents>
${contextParts.join('\n')}
</documents>`;

  return {
    systemPrompt,
    context,
    userPrompt: `<query>${query}</query>`,
  };
}
```

### Pattern 5: Dynamic Context Window Management

**Use Case**: Adapting context size to query complexity

```typescript
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface ContextBudget {
  systemPrompt: number;
  context: number;
  response: number;
  total: number;
}

// Estimate tokens (rough approximation)
function estimateTokens(text: string): number {
  return Math.ceil(text.split(/\s+/).length * 1.3);
}

// Analyze query to determine context needs
async function analyzeContextNeeds(
  query: string
): Promise<{ complexity: 'simple' | 'moderate' | 'complex'; suggestedDocs: number }> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      complexity: z.enum(['simple', 'moderate', 'complex']),
      reasoning: z.string(),
      suggestedDocCount: z.number().min(1).max(20),
    }),
    prompt: `Analyze this query to determine how much context it needs.

Query: "${query}"

Complexity levels:
- simple: Direct factual question, 1-3 sources sufficient
- moderate: Analytical question, 3-7 sources recommended
- complex: Multi-part or comparative, 7-15 sources needed`,
  });

  return {
    complexity: object.complexity,
    suggestedDocs: object.suggestedDocCount,
  };
}

// Allocate context budget based on model limits
function allocateContextBudget(
  modelContextLimit: number,
  queryComplexity: 'simple' | 'moderate' | 'complex'
): ContextBudget {
  const baseAllocations = {
    simple: { system: 0.1, context: 0.5, response: 0.4 },
    moderate: { system: 0.1, context: 0.6, response: 0.3 },
    complex: { system: 0.1, context: 0.7, response: 0.2 },
  };

  const allocation = baseAllocations[queryComplexity];

  return {
    systemPrompt: Math.floor(modelContextLimit * allocation.system),
    context: Math.floor(modelContextLimit * allocation.context),
    response: Math.floor(modelContextLimit * allocation.response),
    total: modelContextLimit,
  };
}

// Fill context up to budget
function fillContextToBudget(
  documents: SourceDocument[],
  budget: number,
  query: string
): SourceDocument[] {
  const result: SourceDocument[] = [];
  let usedTokens = 0;

  // Sort by relevance
  const sorted = [...documents].sort(
    (a, b) => b.relevanceScore - a.relevanceScore
  );

  for (const doc of sorted) {
    const docTokens = estimateTokens(doc.content);

    if (usedTokens + docTokens <= budget) {
      result.push(doc);
      usedTokens += docTokens;
    } else if (result.length === 0) {
      // Always include at least one document, truncated if needed
      const truncatedContent = doc.content.slice(0, budget * 3); // Rough char estimate
      result.push({ ...doc, content: truncatedContent });
      break;
    }
  }

  return result;
}
```

## Research & Benchmarks

### Lost in the Middle Effect (2024)

| Position | Accuracy | Impact |
|----------|----------|--------|
| First 10% | 85% | Primacy effect |
| Middle 40-60% | **55%** | **-30% accuracy** |
| Last 10% | 82% | Recency effect |

### Compression Effectiveness

| Technique | Token Reduction | Quality Retention |
|-----------|-----------------|-------------------|
| Extractive | 40-60% | 92% |
| Abstractive | 60-80% | 85% |
| Hybrid | 50-70% | 90% |

### Context Optimization Impact

| Optimization | Accuracy Improvement | Token Savings |
|--------------|---------------------|---------------|
| Relevance ordering | +15-20% | 0% |
| Deduplication | +5-10% | 20-40% |
| Compression | +0-5% | 50-70% |
| Combined | **+20-30%** | **40-60%** |

### Long Context Performance (LongRAG 2024)

| Chunk Size | Complex Query Accuracy | Simple Query Accuracy |
|------------|------------------------|----------------------|
| 256 tokens | 65% | 82% |
| 512 tokens | 71% | 80% |
| 2K tokens | 78% | 79% |
| **4K tokens** | **83%** | 77% |

## When to Use This Pattern

### ✅ Use When:

1. **Long Context Models**
   - 32K+ context windows
   - Many documents retrieved
   - Lost in middle is a concern

2. **Token Cost Sensitivity**
   - Pay-per-token APIs
   - High query volume
   - Budget constraints

3. **Complex Queries**
   - Multi-hop reasoning
   - Synthesis across documents
   - Comparative analysis

4. **Quality Critical**
   - Accuracy > speed
   - Professional/enterprise use

### ❌ Don't Use When:

1. **Simple Q&A**
   - Single document sufficient
   - Short context needed

2. **Real-Time Requirements**
   - Compression adds latency
   - Sub-100ms needs

3. **Short Documents**
   - Documents already concise
   - Little to optimize

### Decision Matrix

| Scenario | Optimize? | Focus Area |
|----------|-----------|------------|
| Long context + many docs | ✅ Yes | Ordering + Compression |
| Cost-sensitive API use | ✅ Yes | Compression + Filtering |
| Simple factual Q&A | ❌ No | Direct injection |
| Multi-doc synthesis | ✅ Yes | Dedup + MMR |
| Real-time chat | Maybe | Light filtering only |

## Production Best Practices

### 1. Implement Progressive Loading

Load more context only if needed:

```typescript
async function progressiveContextLoad(
  query: string,
  vectorDb: VectorDatabase,
  generateFn: (context: string) => Promise<string>
): Promise<{ answer: string; documentsUsed: number }> {
  const batchSizes = [3, 5, 10]; // Progressive batch sizes

  for (const batchSize of batchSizes) {
    const docs = await vectorDb.search({ query, topK: batchSize });
    const context = formatContext(docs);

    const answer = await generateFn(context);

    // Check if answer indicates insufficient context
    if (!answer.includes("I don't have enough information")) {
      return { answer, documentsUsed: docs.length };
    }
  }

  return {
    answer: "I couldn't find sufficient information to answer this question.",
    documentsUsed: batchSizes[batchSizes.length - 1],
  };
}
```

### 2. Cache Compressed Documents

Pre-compute compressions for common queries:

```typescript
const compressionCache = new LRUCache<string, string>({
  max: 10000,
  ttl: 86400000, // 24 hours
});

async function getCachedCompression(
  docId: string,
  content: string,
  query: string
): Promise<string> {
  // Cache key includes query category, not exact query
  const queryCategory = await categorizeQuery(query);
  const cacheKey = `${docId}:${queryCategory}`;

  if (compressionCache.has(cacheKey)) {
    return compressionCache.get(cacheKey)!;
  }

  const compressed = await abstractiveCompress(content, query);
  compressionCache.set(cacheKey, compressed);
  return compressed;
}
```

### 3. Monitor Context Utilization

Track how context is being used:

```typescript
interface ContextMetrics {
  totalTokens: number;
  usedTokens: number;
  documentCount: number;
  compressionRatio: number;
  relevanceDistribution: number[];
}

function trackContextMetrics(
  original: SourceDocument[],
  optimized: SourceDocument[],
  modelLimit: number
): ContextMetrics {
  const originalTokens = original.reduce(
    (sum, d) => sum + estimateTokens(d.content),
    0
  );
  const optimizedTokens = optimized.reduce(
    (sum, d) => sum + estimateTokens(d.content),
    0
  );

  return {
    totalTokens: modelLimit,
    usedTokens: optimizedTokens,
    documentCount: optimized.length,
    compressionRatio: optimizedTokens / originalTokens,
    relevanceDistribution: optimized.map((d) => d.relevanceScore),
  };
}
```

### 4. A/B Test Ordering Strategies

Experiment with different approaches:

```typescript
type OrderingStrategy = 'relevance' | 'bookend' | 'interleaved';

async function experimentWithOrdering(
  documents: SourceDocument[],
  query: string,
  strategy: OrderingStrategy
): Promise<{ answer: string; strategy: OrderingStrategy }> {
  const ordered = orderDocuments(documents, strategy);
  const context = formatContext(ordered);

  const answer = await generateAnswer(context, query);

  // Log for A/B analysis
  analytics.track('context_ordering_experiment', {
    strategy,
    queryHash: hashQuery(query),
    documentCount: documents.length,
    // Collect user feedback or automated quality metrics
  });

  return { answer, strategy };
}
```

## Key Takeaways

1. **Position Matters**: Place most relevant content at start and end of context
2. **Compress Aggressively**: 50-70% token reduction with minimal quality loss
3. **Deduplicate First**: Remove redundant content before other optimizations
4. **Structure for LLM**: Use numbering, headers, XML tags for navigability
5. **Adapt to Query**: Complex queries need more context; simple queries need less

**Quick Implementation Checklist**:

- [ ] Implement relevance-based ordering (most relevant first)
- [ ] Add deduplication to remove redundant documents
- [ ] Build compression pipeline (extractive for high-relevance, abstractive for lower)
- [ ] Format context with clear structure (numbered sources, metadata)
- [ ] Set up context budget allocation based on query complexity
- [ ] Monitor token usage and context utilization metrics
- [ ] A/B test different ordering strategies

## References

1. **Stanford NLP** (2024). "Lost in the Middle: How Language Models Use Long Contexts". https://arxiv.org/abs/2307.03172
2. **arXiv** (2024). "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs". https://arxiv.org/abs/2406.15319
3. **arXiv** (2024). "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval". https://arxiv.org/abs/2401.18059
4. **LlamaIndex** (2024). "Context Compression in RAG". https://docs.llamaindex.ai/en/stable/examples/retrievers/
5. **Pinecone** (2024). "Optimizing RAG Context Windows". https://www.pinecone.io/learn/context-optimization/

**Related Topics**:

- [5.4.1 Naive RAG](./5.4.1-naive-rag.md)
- [5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)
- [5.3.4 Reranking](./5.3.4-reranking.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
