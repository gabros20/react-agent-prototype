# 5.1.4 - Query Optimization Strategies: HyDE, Reranking, Query Expansion

## TL;DR

**Query optimization bridges the gap between user queries and indexed documents—HyDE generates hypothetical answers to search with (10-20% recall boost), reranking uses cross-encoders to reorder results by relevance (+15-30% precision), and query expansion broadens retrieval coverage.** Combining hybrid search + reranking is the production standard.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.1.3 Index Types](./5.1.3-index-types.md)
- **Grounded In**: Neo4j (2024), LangChain (2024), Cohere (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-query-document-mismatch)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Framework Integration](#framework-integration)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Users ask questions differently than documents are written. Query optimization techniques transform user queries to better match indexed content, improving recall (finding relevant documents) and precision (ranking them correctly).

Three strategies dominate production RAG: **HyDE** (Hypothetical Document Embeddings) generates a fake answer to search with, **Reranking** uses cross-encoders to reorder initial results, and **Query Expansion** broadens search coverage with synonyms or sub-queries. These techniques can be combined for cumulative improvements.

**Key Research Findings (2024-2025)**:

- **HyDE effectiveness**: **10-20% recall improvement** on zero-shot tasks ([Gao et al., 2022](https://arxiv.org/abs/2212.10496))
- **Reranking impact**: Cross-encoders improve precision by **15-30%** over bi-encoders alone ([Cohere, 2024](https://docs.cohere.com/docs/rerank))
- **Hybrid search**: BM25 + vector fusion achieves **5-15% better recall** than vector-only ([Pinecone, 2024](https://www.pinecone.io/learn/hybrid-search/))
- **Combined strategies**: Query expansion + HyDE + reranking shows **14.45% improvement** over single strategies ([MQRF-RAG, 2025](https://dl.acm.org/doi/10.1145/3728199.3728221))

## The Problem: Query-Document Mismatch

### The Classic Challenge

Users and document authors use different vocabulary:

```
User Query: "why is my app slow?"

Document Title: "Performance Optimization Techniques for React Applications"
Document Content: "Reducing bundle size improves initial load time..."

❌ Problem: No keyword overlap
   - Query: "slow" ≠ Document: "performance", "optimization"
   - Query: "app" ≠ Document: "applications"

✅ After Query Optimization:
   HyDE generates: "The app may be slow due to large bundle sizes..."
   → Now searches for "bundle size", "load time" → Matches document!
```

**Problems**:

- ❌ **Vocabulary gap**: Users say "broken", docs say "malfunctioning"
- ❌ **Specificity mismatch**: Vague queries vs technical documents
- ❌ **Intent ambiguity**: "Python" (language? snake? movie?)
- ❌ **Dense retrieval blind spots**: Vector search misses exact keywords

### Why This Matters

Without query optimization, RAG systems miss relevant documents that use different terminology. This causes:
- **Lower recall**: Relevant docs not retrieved
- **Worse answers**: LLM generates from irrelevant context
- **User frustration**: "It should have found that!"

## Core Concept

### Query Optimization Pipeline

```
┌─────────────────────────────────────────────────────────────────┐
│                 QUERY OPTIMIZATION PIPELINE                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  User Query                                                      │
│  "why is my app slow?"                                          │
│         │                                                        │
│         ▼                                                        │
│  ┌─────────────────┐                                            │
│  │ Query Expansion │ → "why is my app slow? performance         │
│  │   (optional)    │    optimization speed latency"             │
│  └─────────────────┘                                            │
│         │                                                        │
│         ▼                                                        │
│  ┌─────────────────┐                                            │
│  │     HyDE        │ → Generate hypothetical answer:            │
│  │ (Hypothetical   │   "Your app may be slow due to large       │
│  │  Document)      │    bundle sizes, unoptimized renders..."   │
│  └─────────────────┘                                            │
│         │                                                        │
│         ▼                                                        │
│  ┌─────────────────┐                                            │
│  │ Hybrid Search   │ → BM25 + Vector search                     │
│  │ (Retrieve 50)   │   Returns top 50 candidates                │
│  └─────────────────┘                                            │
│         │                                                        │
│         ▼                                                        │
│  ┌─────────────────┐                                            │
│  │   Reranking     │ → Cross-encoder scores each doc            │
│  │ (Cohere/BGE)    │   Returns top 10 most relevant             │
│  └─────────────────┘                                            │
│         │                                                        │
│         ▼                                                        │
│  Final Context (top 5-10 docs)                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Key Principles

1. **Recall first, precision second**: Retrieve broadly (50+), then filter to best 10
2. **Hybrid beats single-mode**: BM25 catches keywords, vectors catch semantics
3. **Reranking is cheap precision**: Much cheaper than expanding retrieval
4. **HyDE for zero-shot**: Most effective when no training data available

## Implementation Patterns

### Pattern 1: HyDE (Hypothetical Document Embeddings)

**Use Case**: Zero-shot retrieval, bridging vocabulary gaps, ambiguous queries

HyDE generates a hypothetical answer, then searches with that answer's embedding instead of the query's.

```typescript
import { generateText, embed } from 'ai';
import { openai } from '@ai-sdk/openai';

async function hydeSearch(query: string, vectorDb: VectorDB): Promise<Document[]> {
  // Step 1: Generate hypothetical document
  const { text: hypotheticalDoc } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: `Write a short passage that answers this question:

Question: ${query}

Passage:`,
  });

  // Step 2: Embed the hypothetical document (not the query!)
  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: hypotheticalDoc,
  });

  // Step 3: Search with hypothetical document embedding
  const results = await vectorDb.search(embedding, { topK: 10 });

  return results;
}

// Usage
const results = await hydeSearch('why is my app slow?', vectorDb);
// Finds docs about "performance optimization", "bundle size", etc.
```

**Pros**:

- ✅ **10-20% recall improvement** on zero-shot tasks
- ✅ Bridges vocabulary gap automatically
- ✅ No training data required

**Cons**:

- ❌ Additional LLM call (~100ms latency)
- ❌ Hallucinated hypothetical may mislead search
- ❌ Cost: Extra tokens per query

**When to Use**: New domains without training data, vague user queries

### Pattern 2: Reranking with Cross-Encoders

**Use Case**: Improving precision after initial retrieval, production RAG

Reranking takes initial results and scores each document against the query using a cross-encoder that sees both together.

```typescript
import { CohereClient } from 'cohere-ai';

const cohere = new CohereClient({ token: process.env.COHERE_API_KEY });

async function rerankResults(
  query: string,
  documents: { id: string; text: string }[],
  topN: number = 10
): Promise<{ id: string; text: string; score: number }[]> {
  const response = await cohere.rerank({
    query,
    documents: documents.map((d) => d.text),
    model: 'rerank-english-v3.0',
    topN,
  });

  return response.results.map((r) => ({
    ...documents[r.index],
    score: r.relevanceScore,
  }));
}

// Full pipeline: Retrieve 50 → Rerank → Return top 10
async function searchWithReranking(query: string) {
  // Step 1: Initial broad retrieval
  const candidates = await vectorDb.search(queryEmbedding, { topK: 50 });

  // Step 2: Rerank with cross-encoder
  const reranked = await rerankResults(query, candidates, 10);

  return reranked;
}
```

**Pros**:

- ✅ **15-30% precision improvement**
- ✅ Cross-encoder sees query + document together
- ✅ Cheap: ~$0.10/1K queries with Cohere

**Cons**:

- ❌ Adds ~100-200ms latency
- ❌ Doesn't improve recall (can't find new docs)
- ❌ Additional API dependency

**When to Use**: Always in production RAG (cost-effective precision boost)

### Pattern 3: Hybrid Search (BM25 + Vector)

**Use Case**: Catching both semantic matches and keyword matches

Hybrid search combines lexical (BM25) and semantic (vector) search, then fuses results.

```typescript
async function hybridSearch(
  query: string,
  options: { topK: number; alpha: number } = { topK: 10, alpha: 0.5 }
): Promise<Document[]> {
  // Parallel search
  const [vectorResults, bm25Results] = await Promise.all([
    vectorDb.search(queryEmbedding, { topK: options.topK * 2 }),
    textDb.bm25Search(query, { topK: options.topK * 2 }),
  ]);

  // Reciprocal Rank Fusion (RRF)
  const fused = reciprocalRankFusion(vectorResults, bm25Results, options.alpha);

  return fused.slice(0, options.topK);
}

function reciprocalRankFusion(
  list1: Document[],
  list2: Document[],
  alpha: number,
  k: number = 60
): Document[] {
  const scores = new Map<string, number>();

  list1.forEach((doc, rank) => {
    const rrf = alpha * (1 / (k + rank + 1));
    scores.set(doc.id, (scores.get(doc.id) || 0) + rrf);
  });

  list2.forEach((doc, rank) => {
    const rrf = (1 - alpha) * (1 / (k + rank + 1));
    scores.set(doc.id, (scores.get(doc.id) || 0) + rrf);
  });

  return [...scores.entries()]
    .sort((a, b) => b[1] - a[1])
    .map(([id, score]) => ({ id, score } as Document));
}
```

**Pros**:

- ✅ **5-15% better recall** than vector-only
- ✅ Catches exact keywords (product IDs, error codes)
- ✅ More robust across query types

**Cons**:

- ❌ Requires BM25 index infrastructure
- ❌ Tuning alpha parameter for your domain
- ❌ Slightly more complex pipeline

**When to Use**: Production RAG with mixed query types

### Pattern 4: Query Expansion (Multi-Query)

**Use Case**: Broad coverage, complex questions, research tasks

Generate multiple query variations to retrieve from different angles.

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

async function expandQuery(query: string): Promise<string[]> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      queries: z.array(z.string()).length(3),
    }),
    prompt: `Generate 3 different search queries that would help answer this question from different angles:

Question: ${query}

Generate diverse queries that cover:
1. The literal interpretation
2. Related concepts
3. Potential underlying issues`,
  });

  return [query, ...object.queries]; // Include original
}

async function multiQuerySearch(query: string) {
  const expandedQueries = await expandQuery(query);

  // Search with all queries
  const allResults = await Promise.all(
    expandedQueries.map((q) => vectorDb.search(embed(q), { topK: 10 }))
  );

  // Deduplicate and merge (RRF or simple union)
  const merged = deduplicateResults(allResults.flat());

  return merged.slice(0, 10);
}
```

**Pros**:

- ✅ Broader coverage of relevant documents
- ✅ Handles multi-faceted questions
- ✅ Reduces single-query blind spots

**Cons**:

- ❌ Multiple LLM + embedding calls (cost + latency)
- ❌ May retrieve too broadly (lower precision)
- ❌ Needs deduplication logic

**When to Use**: Complex research queries, question answering over large corpora

## Framework Integration

### AI SDK 6 with Reranking

```typescript
import { generateText, embed } from 'ai';
import { openai } from '@ai-sdk/openai';
import { CohereClient } from 'cohere-ai';

const cohere = new CohereClient({ token: process.env.COHERE_API_KEY });

async function ragWithReranking(query: string) {
  // Step 1: Embed query
  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  // Step 2: Retrieve candidates
  const candidates = await vectorDb.search(embedding, { topK: 50 });

  // Step 3: Rerank
  const reranked = await cohere.rerank({
    query,
    documents: candidates.map((c) => c.text),
    topN: 5,
  });

  const context = reranked.results.map((r) => candidates[r.index].text).join('\n\n');

  // Step 4: Generate answer
  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Answer based on context:

Context:
${context}

Question: ${query}`,
  });

  return text;
}
```

### LangChain-Style Pipeline

```typescript
// Modular pipeline composition
class RAGPipeline {
  constructor(
    private retriever: Retriever,
    private reranker?: Reranker,
    private queryTransformer?: QueryTransformer
  ) {}

  async retrieve(query: string, topK: number = 10): Promise<Document[]> {
    // Optional: Transform query (HyDE, expansion)
    const transformedQuery = this.queryTransformer
      ? await this.queryTransformer.transform(query)
      : query;

    // Retrieve broadly
    const candidates = await this.retriever.retrieve(transformedQuery, topK * 5);

    // Optional: Rerank
    if (this.reranker) {
      return this.reranker.rerank(query, candidates, topK);
    }

    return candidates.slice(0, topK);
  }
}

// Production configuration
const pipeline = new RAGPipeline(
  new HybridRetriever(vectorDb, bm25Index), // Hybrid search
  new CohereReranker(), // Cross-encoder reranking
  new HyDETransformer() // Query transformation
);
```

## Research & Benchmarks

### Strategy Comparison (BEIR Benchmark)

| Strategy | Recall@10 | Precision@10 | Latency | Cost |
|----------|-----------|--------------|---------|------|
| **Vector Only** | 78% | 65% | 50ms | $0.002 |
| **+ HyDE** | 88% | 68% | 150ms | $0.005 |
| **+ Hybrid (BM25)** | 85% | 70% | 60ms | $0.002 |
| **+ Reranking** | 78% | 82% | 150ms | $0.012 |
| **Hybrid + Rerank** | 85% | 88% | 160ms | $0.014 |

### Combined Strategy Results (2025 Research)

| Method | P@5 (FreshQA) | F1 (HotPotQA) |
|--------|---------------|---------------|
| Baseline RAG | 0.42 | 0.38 |
| + HyDE | 0.48 (+14%) | 0.41 (+8%) |
| + RAG Fusion | 0.52 (+24%) | 0.44 (+16%) |
| + MQRF-RAG | 0.58 (+38%) | 0.47 (+24%) |

**Source**: [MQRF-RAG, 2025](https://dl.acm.org/doi/10.1145/3728199.3728221)

## When to Use This Pattern

### ✅ Use HyDE When:

1. **Zero-shot retrieval**
   - No labeled training data
   - New domain without query logs

2. **Vocabulary gap**
   - Users and documents use different terms
   - Technical docs with layman queries

### ✅ Use Reranking When:

1. **Always in production RAG**
   - Cost-effective precision boost
   - Works with any retriever

2. **High-stakes applications**
   - Customer support, medical, legal
   - Precision more important than latency

### ✅ Use Hybrid Search When:

1. **Mixed query types**
   - Some keyword-heavy (error codes, IDs)
   - Some semantic (natural language)

2. **Production robustness**
   - Can't rely on vectors alone
   - Need exact match capability

### ✅ Use Query Expansion When:

1. **Complex research queries**
   - Multi-hop questions
   - Exploratory search

2. **Recall is critical**
   - Can't miss relevant documents
   - Willing to trade precision for coverage

### Decision Matrix

| Your Situation | Recommended Strategy |
|----------------|---------------------|
| Production RAG (default) | Hybrid + Reranking |
| New domain, no data | HyDE + Reranking |
| Keyword-heavy (codes, IDs) | Hybrid search |
| Complex questions | Query expansion |
| Budget constrained | Reranking only |
| Latency critical (<100ms) | Vector + Reranking |

## Production Best Practices

### 1. Retrieve Broadly, Rerank Tightly

```typescript
// Retrieve 50 candidates, rerank to 10
const candidates = await retrieve(query, { topK: 50 });
const reranked = await rerank(query, candidates, { topN: 10 });
// Why: Reranking can't find docs that weren't retrieved
```

### 2. Cache HyDE Results

```typescript
// HyDE is deterministic for same query
const cacheKey = `hyde:${hashQuery(query)}`;
const cached = await redis.get(cacheKey);
if (cached) return JSON.parse(cached);

const hydeDoc = await generateHypothetical(query);
await redis.setex(cacheKey, 3600, JSON.stringify(hydeDoc)); // 1hr TTL
```

### 3. A/B Test Strategies

```typescript
async function search(query: string, userId: string) {
  const variant = getABVariant(userId); // 'control' | 'hyde' | 'hybrid'

  switch (variant) {
    case 'control':
      return vectorSearch(query);
    case 'hyde':
      return hydeSearch(query);
    case 'hybrid':
      return hybridSearch(query);
  }
}

// Track: click-through rate, answer quality, latency
```

### Common Pitfalls

#### ❌ Pitfall: Reranking Without Broad Retrieval

**Problem**: Reranking can only reorder what was retrieved.

```typescript
// BAD: Retrieve 10, rerank 10 (no benefit)
const results = await retrieve(query, { topK: 10 });
const reranked = await rerank(query, results, { topN: 10 });

// GOOD: Retrieve 50, rerank to 10
const candidates = await retrieve(query, { topK: 50 });
const reranked = await rerank(query, candidates, { topN: 10 });
```

#### ❌ Pitfall: HyDE Hallucinations

**Problem**: Hypothetical document may be factually wrong, misleading search.

```typescript
// BAD: Blindly trust HyDE output
const hydeDoc = await generateHypothetical(query);
// hydeDoc: "React was created by Facebook in 2023" (wrong!)

// GOOD: Use HyDE for vocabulary, not facts
const prompt = `Generate a short passage using terms commonly found in
documents that would answer: ${query}
Focus on terminology, not factual claims.`;
```

#### ❌ Pitfall: Over-Optimization Adding Latency

**Problem**: Every technique adds latency; don't stack unnecessarily.

```typescript
// BAD: All techniques on every query (500ms+)
const results = await pipe(query, [expandQuery, hyde, hybridSearch, rerank, rerank2]);

// GOOD: Choose based on query type
const strategy = classifyQuery(query); // simple | complex | keyword
switch (strategy) {
  case 'simple':
    return vectorSearch(query);
  case 'complex':
    return pipe(query, [hyde, hybridSearch, rerank]);
  case 'keyword':
    return bm25Search(query);
}
```

## Key Takeaways

1. **Hybrid + Reranking is the production standard** - 5-15% recall boost + 15-30% precision boost
2. **Retrieve broadly (50+), filter to best (10)** - Reranking needs candidates to work with
3. **HyDE bridges vocabulary gaps** - 10-20% recall improvement on zero-shot tasks
4. **Reranking is cheap precision** - ~$0.10/1K queries with Cohere, always worth it
5. **Combine strategies for cumulative gains** - Query expansion + HyDE + reranking = 38%+ improvement

**Quick Implementation Checklist**:

- [ ] Implement basic vector search first
- [ ] Add BM25 for hybrid search (5-15% recall boost)
- [ ] Add reranking (Cohere/BGE) for precision
- [ ] Consider HyDE for zero-shot domains
- [ ] A/B test strategies on your data
- [ ] Monitor latency vs quality tradeoffs

## References

1. **Gao et al.** (2022). "Precise Zero-Shot Dense Retrieval without Relevance Labels" (HyDE). *arXiv*. https://arxiv.org/abs/2212.10496
2. **Neo4j** (2024). "Advanced RAG Techniques". https://neo4j.com/blog/genai/advanced-rag-techniques/
3. **Cohere** (2024). "Rerank Documentation". https://docs.cohere.com/docs/rerank
4. **Pinecone** (2024). "Hybrid Search". https://www.pinecone.io/learn/hybrid-search/
5. **MQRF-RAG** (2025). "Optimization of RAG Multi-Query Rewrite". *ACM*. https://dl.acm.org/doi/10.1145/3728199.3728221
6. **Superlinked** (2024). "Optimizing RAG with Hybrid Search & Reranking". https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking

**Related Topics**:

- [5.1.3 Index Types](./5.1.3-index-types.md)
- [5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)
- [5.3.3 BM25](./5.3.3-bm25.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-rag--retrieval)
