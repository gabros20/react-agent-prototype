# 5.3.4 Reranking (Cross-Encoder)

## TL;DR

Cross-encoder reranking examines query-document pairs jointly to reorder initial retrieval results, improving precision by 20-35% over bi-encoder retrieval alone—essential for production RAG where initial retrieval returns many candidates.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.3.1 Vector Search](./5.3.1-vector-search.md), [5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)
- **Grounded In**: Cohere Rerank (2024), Pinecone Two-Stage Retrieval, ARAGOG Benchmarks (2024)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-first-stage-retrieval-limitations)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Reranking is a second-stage retrieval technique that uses more sophisticated models to reorder results from initial retrieval. While first-stage retrieval (vector search, BM25) prioritizes speed and recall, reranking prioritizes precision by examining each query-document pair in detail.

The key insight is that bi-encoders (used for initial retrieval) compress documents into fixed vectors independently of the query, losing information. Cross-encoders process query and document together, capturing fine-grained relevance signals that bi-encoders miss.

**Key Research Findings** (2024-2025):

- **20-35% Precision Improvement**: Cross-encoders consistently outperform bi-encoders on relevance
- **28% NDCG@10 Gain**: ZeroEntropy zerank-1 shows significant improvements over baselines
- **Cohere Rerank 4**: Quadrupled context window, added self-learning capabilities (Dec 2024)

**Date Verified**: 2025-12-12

## The Problem: First-Stage Retrieval Limitations

### The Classic Challenge

Bi-encoders must compress all document meaning into a single vector:

```
Bi-Encoder (First Stage):
┌─────────────────┐     ┌─────────────────┐
│     Query       │     │    Document     │
│  "auth errors"  │     │  "Auth guide"   │
└────────┬────────┘     └────────┬────────┘
         ↓                       ↓
    [0.2, -0.3...]          [0.1, -0.2...]
         ↓                       ↓
         └───── Similarity ──────┘
              (independent encoding)

Cross-Encoder (Reranker):
┌─────────────────────────────────────────┐
│     Query + Document (together)         │
│  "auth errors" [SEP] "Auth guide..."    │
└──────────────────┬──────────────────────┘
                   ↓
           Full Attention Between All Tokens
                   ↓
              Relevance Score: 0.89
              (joint encoding)
```

**Bi-Encoder Limitations**:

- ❌ Document compressed without query context
- ❌ Subtle relevance signals lost in compression
- ❌ Cannot model query-document interaction
- ❌ Semantically similar != relevant to this specific query

### Why Reranking Helps

Cross-encoders can capture:

- **Query-specific relevance**: Same doc relevant to query A but not B
- **Fine-grained matching**: Exact phrase matches, negations
- **Context understanding**: "not recommended" vs "recommended"

## Core Concept

### What is Cross-Encoder Reranking?

Cross-encoders score query-document pairs by processing them jointly:

```
Two-Stage Retrieval Pipeline:

Stage 1: First-Stage Retrieval (Fast, High Recall)
┌────────────────────────────────────────────────┐
│ Query → Bi-Encoder/BM25 → Top 100 candidates   │
│ Latency: 10-50ms, Recall: 90%+                 │
└────────────────────────────────────────────────┘
                      ↓
Stage 2: Reranking (Slower, High Precision)
┌────────────────────────────────────────────────┐
│ For each of 100 candidates:                    │
│   Cross-Encoder(query, doc) → relevance score  │
│ Sort by score, return top 5-10                 │
│ Latency: 200-500ms, Precision: +20-35%         │
└────────────────────────────────────────────────┘
```

### Why Cross-Encoders Work Better

```
Query: "How to handle authentication timeout errors?"

Document 1: "Authentication timeout configuration guide"
- Bi-encoder: High similarity (contains "authentication", "timeout")
- Cross-encoder: Medium relevance (configuration, not error handling)

Document 2: "Troubleshooting auth failures and timeouts"
- Bi-encoder: Medium similarity (fewer exact matches)
- Cross-encoder: High relevance (actually about handling errors)
```

### Key Principles

1. **Two-Stage Design**: Fast recall first, precise reranking second
2. **Retrieve More, Rerank Down**: Get 50-100 candidates, return 5-10
3. **Quality vs Speed Trade-off**: Reranking adds 200-500ms latency

## Implementation Patterns

### Pattern 1: Cohere Rerank API

**Use Case**: Production reranking with minimal implementation

```typescript
import { CohereClient } from 'cohere-ai';

interface RerankOptions {
  query: string;
  documents: string[];
  topN: number;
  model?: string;
}

async function cohereRerank(
  options: RerankOptions
): Promise<Array<{ index: number; score: number }>> {
  const cohere = new CohereClient({ token: process.env.COHERE_API_KEY });

  const response = await cohere.rerank({
    query: options.query,
    documents: options.documents,
    topN: options.topN,
    model: options.model ?? 'rerank-english-v3.0',
  });

  return response.results.map((r) => ({
    index: r.index,
    score: r.relevanceScore,
  }));
}

// Full pipeline
async function retrieveAndRerank(
  query: string,
  vectorDb: VectorDatabase,
  options = { retrieveK: 100, rerankK: 5 }
): Promise<SearchResult[]> {
  // Stage 1: Vector search
  const candidates = await vectorDb.search({
    query,
    topK: options.retrieveK,
  });

  // Stage 2: Rerank
  const reranked = await cohereRerank({
    query,
    documents: candidates.map((c) => c.content),
    topN: options.rerankK,
  });

  // Return reranked results
  return reranked.map((r) => ({
    ...candidates[r.index],
    rerankScore: r.score,
  }));
}
```

### Pattern 2: Open-Source Cross-Encoder

**Use Case**: Self-hosted reranking without API costs

```typescript
import { pipeline } from '@xenova/transformers';

let reranker: any = null;

async function loadCrossEncoder() {
  if (!reranker) {
    reranker = await pipeline(
      'text-classification',
      'cross-encoder/ms-marco-MiniLM-L-6-v2'
    );
  }
  return reranker;
}

interface RerankResult {
  index: number;
  score: number;
  content: string;
}

async function crossEncoderRerank(
  query: string,
  documents: string[],
  topK: number
): Promise<RerankResult[]> {
  const model = await loadCrossEncoder();

  // Score each query-document pair
  const scores = await Promise.all(
    documents.map(async (doc, index) => {
      const result = await model(`${query} [SEP] ${doc}`);
      return {
        index,
        score: result[0].score,
        content: doc,
      };
    })
  );

  // Sort by score and return top-K
  return scores
    .sort((a, b) => b.score - a.score)
    .slice(0, topK);
}
```

### Pattern 3: Batch Reranking for Efficiency

**Use Case**: Reducing latency when reranking many candidates

```typescript
interface BatchRerankOptions {
  query: string;
  documents: string[];
  batchSize: number;
  topK: number;
}

async function batchedRerank(
  options: BatchRerankOptions
): Promise<RerankResult[]> {
  const { query, documents, batchSize, topK } = options;

  // Split into batches
  const batches: string[][] = [];
  for (let i = 0; i < documents.length; i += batchSize) {
    batches.push(documents.slice(i, i + batchSize));
  }

  // Process batches in parallel (with concurrency limit)
  const CONCURRENCY = 3;
  const allScores: RerankResult[] = [];

  for (let i = 0; i < batches.length; i += CONCURRENCY) {
    const batchPromises = batches
      .slice(i, i + CONCURRENCY)
      .map((batch, batchIdx) =>
        rerankBatch(query, batch).then((scores) =>
          scores.map((s) => ({
            ...s,
            index: s.index + (i + batchIdx) * batchSize,
          }))
        )
      );

    const batchResults = await Promise.all(batchPromises);
    allScores.push(...batchResults.flat());
  }

  // Sort and return top-K
  return allScores
    .sort((a, b) => b.score - a.score)
    .slice(0, topK);
}
```

### Pattern 4: Conditional Reranking

**Use Case**: Skip reranking when initial results are confident

```typescript
interface ConditionalRerankOptions {
  query: string;
  initialResults: SearchResult[];
  confidenceThreshold: number;
  topK: number;
}

async function conditionalRerank(
  options: ConditionalRerankOptions
): Promise<SearchResult[]> {
  const { query, initialResults, confidenceThreshold, topK } = options;

  // Check if top result is confident enough
  const topScore = initialResults[0]?.similarity ?? 0;
  const secondScore = initialResults[1]?.similarity ?? 0;
  const gap = topScore - secondScore;

  // Skip reranking if clear winner with high confidence
  if (topScore > confidenceThreshold && gap > 0.15) {
    console.log('Skipping rerank - high confidence initial result');
    return initialResults.slice(0, topK);
  }

  // Otherwise, rerank
  const reranked = await cohereRerank({
    query,
    documents: initialResults.map((r) => r.content),
    topN: topK,
  });

  return reranked.map((r) => ({
    ...initialResults[r.index],
    rerankScore: r.score,
    wasReranked: true,
  }));
}
```

## Research & Benchmarks

### Precision Improvements

| Method | Precision@5 | NDCG@10 | Latency |
|--------|-------------|---------|---------|
| Vector only | 68% | 0.65 | 30ms |
| + BM25 Hybrid | 74% | 0.71 | 50ms |
| **+ Reranking** | **89%** | **0.83** | 300ms |

### Reranker Model Comparison (2025)

| Model | NDCG@10 | Latency (100 docs) | Notes |
|-------|---------|-------------------|-------|
| **Cohere rerank-v3** | 0.78 | 200ms | Production-ready |
| **ZeroEntropy zerank-1** | 0.81 | 180ms | +28% over baseline |
| ms-marco-MiniLM-L-6-v2 | 0.72 | 150ms | Open source |
| mxbai-rerank-large | 0.79 | 250ms | Self-hosted option |

### ARAGOG Benchmark Results (2024)

Research findings on RAG optimization techniques:

- **HyDE + Reranking**: Significantly enhanced retrieval precision
- **Cohere Rerank alone**: Did not show notable advantage over baseline in that study
- **LLM Reranking**: High precision but 5-10x slower than cross-encoder

## When to Use This Pattern

### ✅ Use When:

1. **Precision Critical**
   - Wrong answers have high cost
   - Legal, medical, financial domains

2. **Initial Retrieval Returns Many Candidates**
   - Top-100 retrieval common
   - Need to select best 5-10

3. **Latency Budget Available**
   - 200-500ms additional acceptable
   - User-facing with expectations set

### ❌ Don't Use When:

1. **Real-Time Requirements**
   - Sub-100ms latency required
   - High-frequency queries

2. **Small Candidate Sets**
   - Only retrieving 5-10 candidates
   - Reranking adds cost without benefit

3. **Cost Constrained**
   - API reranking costs per query
   - Consider open-source alternatives

### Decision Matrix

| Scenario | Rerank? | Model Choice |
|----------|---------|--------------|
| Production RAG, quality critical | Yes | Cohere/API |
| Budget constrained | Yes | Open-source cross-encoder |
| Real-time chat | Maybe | Conditional reranking |
| Batch processing | Yes | Self-hosted for cost |
| Prototype | No | Vector search sufficient |

## Production Best Practices

### 1. Retrieve More Than You Need

Reranking works best with diverse candidates:

```typescript
const RETRIEVAL_CONFIG = {
  firstStageK: 100,  // Get many candidates
  rerankK: 5,        // Return few, high-quality
};
```

### 2. Set Minimum Relevance Thresholds

Don't return irrelevant results even after reranking:

```typescript
const MIN_RERANK_SCORE = 0.5;

const reranked = await rerank({ query, documents, topN: 10 });
const relevant = reranked.filter((r) => r.score >= MIN_RERANK_SCORE);
```

### 3. Cache Rerank Results

Queries repeat—cache the reranked results:

```typescript
const rerankCache = new LRUCache<string, RerankResult[]>({ max: 1000, ttl: 3600000 });

async function cachedRerank(
  query: string,
  candidateIds: string[]
): Promise<RerankResult[]> {
  const cacheKey = `${query}:${candidateIds.sort().join(',')}`;

  if (rerankCache.has(cacheKey)) {
    return rerankCache.get(cacheKey)!;
  }

  const results = await rerank({ query, documents: getDocsById(candidateIds) });
  rerankCache.set(cacheKey, results);
  return results;
}
```

### 4. Monitor Rerank Impact

Track whether reranking changes results:

```typescript
function logRerankMetrics(
  initialOrder: string[],
  rerankedOrder: string[]
): void {
  const topMoved = initialOrder[0] !== rerankedOrder[0];
  const kendallTau = calculateKendallTau(initialOrder, rerankedOrder);

  metrics.track('rerank_impact', {
    topResultChanged: topMoved,
    orderCorrelation: kendallTau,
    // Low correlation = reranking made significant changes
  });
}
```

## Key Takeaways

1. **Two-Stage is Standard**: Retrieve 50-100, rerank to 5-10
2. **20-35% Precision Gain**: Cross-encoders consistently improve over bi-encoders
3. **Latency Trade-off**: Adds 200-500ms but often worth it for quality
4. **Conditional Reranking**: Skip when initial retrieval is confident
5. **Monitor Impact**: Track how often reranking changes results

**Quick Implementation Checklist**:

- [ ] Set up two-stage pipeline (retrieve 100, rerank 5-10)
- [ ] Choose reranker (Cohere API or open-source)
- [ ] Add minimum relevance threshold
- [ ] Implement caching for repeated queries
- [ ] Monitor rerank impact metrics
- [ ] Consider conditional reranking for latency

## References

1. **Pinecone** (2024). "Rerankers and Two-Stage Retrieval". https://www.pinecone.io/learn/series/rag/rerankers/
2. **VentureBeat** (2024). "Cohere's Rerank 4 Quadruples Context Window". https://venturebeat.com/ai/coheres-rerank-4
3. **ZeroEntropy** (2025). "Ultimate Guide to Choosing the Best Reranking Model". https://www.zeroentropy.dev/articles/ultimate-guide-reranking-model-2025
4. **arXiv** (2024). "ARAGOG: Advanced RAG Output Grading". https://arxiv.org/pdf/2404.01037
5. **LangChain** (2024). "Cross Encoder Reranker". https://python.langchain.com/docs/integrations/document_transformers/cross_encoder_reranker

**Related Topics**:

- [5.3.1 Vector Search](./5.3.1-vector-search.md)
- [5.1.5 Top-K Selection](./5.1.5-top-k-selection.md)
- [5.3.5 Fusion Strategies](./5.3.5-fusion.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
