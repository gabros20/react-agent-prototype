# 5.4.5 RAG Evaluation (Metrics & Testing)

## TL;DR

RAG evaluation requires measuring both retrieval quality (Precision, Recall, MRR, NDCG) and generation quality (faithfulness, relevance, answer correctness)—with production systems targeting NDCG@10 > 0.8 and faithfulness > 0.9 for reliable performance.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.4.1 Naive RAG](./5.4.1-naive-rag.md), [5.3.4 Reranking](./5.3.4-reranking.md)
- **Grounded In**: RAGAS Framework (2024), ARAGOG Benchmark (2024), BEIR Benchmark, Pinecone Evaluation Guide

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-measuring-rag-quality)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

RAG evaluation is uniquely challenging because it requires assessing two interconnected systems: retrieval and generation. Poor retrieval leads to poor answers, but good retrieval with poor generation also fails. Comprehensive evaluation must measure both components and their interaction.

**Key Research Findings** (2024-2025):

- **RAGAS Framework**: Standard metrics for RAG: faithfulness, answer relevancy, context precision/recall
- **NDCG@10 Target**: Production systems should aim for > 0.8 NDCG@10 on retrieval
- **Faithfulness Critical**: 90%+ faithfulness score correlates with user trust
- **LLM-as-Judge**: GPT-4 evaluation correlates 0.85+ with human judgment on RAG quality

**Date Verified**: 2025-12-12

## The Problem: Measuring RAG Quality

### The Classic Challenge

RAG has multiple failure modes that traditional metrics miss:

```
RAG Failure Modes:
┌─────────────────────────────────────────────────────────────┐
│                                                              │
│  1. RETRIEVAL FAILURES                                       │
│     ├─ Wrong documents retrieved (low precision)            │
│     ├─ Relevant docs not retrieved (low recall)             │
│     └─ Relevant docs ranked poorly (low NDCG)               │
│                                                              │
│  2. GENERATION FAILURES                                      │
│     ├─ Answer not grounded in context (hallucination)       │
│     ├─ Answer doesn't address query (irrelevant)            │
│     ├─ Correct retrieval, wrong synthesis (logic error)     │
│     └─ Information in context but missed (extraction fail)  │
│                                                              │
│  3. SYSTEM FAILURES                                          │
│     ├─ Good components, poor integration                    │
│     ├─ Context too long/short for query                     │
│     └─ Latency exceeds acceptable threshold                 │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Evaluation Challenges**:

- ❌ Standard NLP metrics (BLEU, ROUGE) don't measure factual correctness
- ❌ Retrieval metrics alone miss generation quality
- ❌ Human evaluation doesn't scale
- ❌ Ground truth labels expensive to create

### Why Comprehensive Evaluation Matters

```
Complete RAG Evaluation:
┌─────────────────────────────────────────────────────────────┐
│                                                              │
│  Query → [RETRIEVAL] → [GENERATION] → Answer                │
│              ↓               ↓                               │
│         ┌────────┐     ┌──────────┐                         │
│         │Retrieval│     │Generation│                         │
│         │Metrics │     │ Metrics  │                         │
│         └────────┘     └──────────┘                         │
│              ↓               ↓                               │
│         ┌────────────────────────┐                          │
│         │   End-to-End Metrics   │                          │
│         │   (Answer Quality)     │                          │
│         └────────────────────────┘                          │
│                                                              │
│  Retrieval: Precision, Recall, MRR, NDCG                    │
│  Generation: Faithfulness, Relevance, Correctness           │
│  End-to-End: User satisfaction, Task completion             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

## Core Concept

### What is RAG Evaluation?

RAG evaluation measures quality across three dimensions:

```
┌─────────────────────────────────────────────────────────────┐
│                RAG EVALUATION FRAMEWORK                      │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────────────────────────────────┐            │
│  │         RETRIEVAL METRICS                    │            │
│  │                                              │            │
│  │  Precision@K: Relevant docs in top K        │            │
│  │  Recall@K: Found relevant docs / Total      │            │
│  │  MRR: 1/rank of first relevant result       │            │
│  │  NDCG@K: Graded relevance with position     │            │
│  │                                              │            │
│  └─────────────────────────────────────────────┘            │
│                         ↓                                    │
│  ┌─────────────────────────────────────────────┐            │
│  │         GENERATION METRICS                   │            │
│  │                                              │            │
│  │  Faithfulness: Answer grounded in context   │            │
│  │  Answer Relevancy: Addresses the query      │            │
│  │  Context Relevancy: Context useful for Q    │            │
│  │  Context Recall: Context covers ground truth│            │
│  │                                              │            │
│  └─────────────────────────────────────────────┘            │
│                         ↓                                    │
│  ┌─────────────────────────────────────────────┐            │
│  │         END-TO-END METRICS                   │            │
│  │                                              │            │
│  │  Answer Correctness: Matches ground truth   │            │
│  │  Answer Completeness: All aspects covered   │            │
│  │  Latency: Response time                     │            │
│  │  Cost: Tokens/API calls per query           │            │
│  │                                              │            │
│  └─────────────────────────────────────────────┘            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Metrics Explained

**Retrieval Metrics**:

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| **Precision@K** | Relevant in top K / K | What % of retrieved docs are useful? |
| **Recall@K** | Relevant in top K / Total relevant | What % of useful docs did we find? |
| **MRR** | Mean(1/rank of first relevant) | How quickly do we find something useful? |
| **NDCG@K** | DCG@K / Ideal DCG@K | How good is our ranking? (0-1 scale) |

**Generation Metrics (RAGAS)**:

| Metric | What It Measures | Score Range |
|--------|------------------|-------------|
| **Faithfulness** | Answer statements supported by context | 0-1 (higher = better) |
| **Answer Relevancy** | Answer addresses the query | 0-1 |
| **Context Precision** | Retrieved context useful for answer | 0-1 |
| **Context Recall** | Context contains ground truth info | 0-1 |

## Implementation Patterns

### Pattern 1: Retrieval Metrics

**Use Case**: Evaluating retrieval quality before generation

```typescript
interface RetrievalResult {
  docId: string;
  score: number;
  content: string;
}

interface RetrievalGold {
  queryId: string;
  relevantDocIds: string[];
  gradedRelevance?: Map<string, number>; // For NDCG
}

// Precision@K: What fraction of retrieved docs are relevant?
function precisionAtK(
  retrieved: RetrievalResult[],
  gold: RetrievalGold,
  k: number
): number {
  const topK = retrieved.slice(0, k);
  const relevantSet = new Set(gold.relevantDocIds);

  const relevantInTopK = topK.filter((r) => relevantSet.has(r.docId)).length;
  return relevantInTopK / k;
}

// Recall@K: What fraction of relevant docs did we find?
function recallAtK(
  retrieved: RetrievalResult[],
  gold: RetrievalGold,
  k: number
): number {
  const topK = retrieved.slice(0, k);
  const relevantSet = new Set(gold.relevantDocIds);

  const relevantInTopK = topK.filter((r) => relevantSet.has(r.docId)).length;
  return gold.relevantDocIds.length > 0
    ? relevantInTopK / gold.relevantDocIds.length
    : 0;
}

// MRR: Mean Reciprocal Rank
function mrr(retrieved: RetrievalResult[], gold: RetrievalGold): number {
  const relevantSet = new Set(gold.relevantDocIds);

  for (let i = 0; i < retrieved.length; i++) {
    if (relevantSet.has(retrieved[i].docId)) {
      return 1 / (i + 1);
    }
  }
  return 0;
}

// NDCG@K: Normalized Discounted Cumulative Gain
function ndcgAtK(
  retrieved: RetrievalResult[],
  gold: RetrievalGold,
  k: number
): number {
  const topK = retrieved.slice(0, k);
  const relevance = gold.gradedRelevance || new Map(
    gold.relevantDocIds.map((id) => [id, 1])
  );

  // DCG@K
  let dcg = 0;
  for (let i = 0; i < topK.length; i++) {
    const rel = relevance.get(topK[i].docId) || 0;
    dcg += (Math.pow(2, rel) - 1) / Math.log2(i + 2);
  }

  // Ideal DCG@K
  const sortedRelevance = Array.from(relevance.values())
    .sort((a, b) => b - a)
    .slice(0, k);

  let idealDcg = 0;
  for (let i = 0; i < sortedRelevance.length; i++) {
    idealDcg += (Math.pow(2, sortedRelevance[i]) - 1) / Math.log2(i + 2);
  }

  return idealDcg > 0 ? dcg / idealDcg : 0;
}

// Aggregate retrieval evaluation
interface RetrievalEvaluation {
  precision: { at5: number; at10: number };
  recall: { at5: number; at10: number };
  mrr: number;
  ndcg: { at5: number; at10: number };
}

function evaluateRetrieval(
  retrieved: RetrievalResult[],
  gold: RetrievalGold
): RetrievalEvaluation {
  return {
    precision: {
      at5: precisionAtK(retrieved, gold, 5),
      at10: precisionAtK(retrieved, gold, 10),
    },
    recall: {
      at5: recallAtK(retrieved, gold, 5),
      at10: recallAtK(retrieved, gold, 10),
    },
    mrr: mrr(retrieved, gold),
    ndcg: {
      at5: ndcgAtK(retrieved, gold, 5),
      at10: ndcgAtK(retrieved, gold, 10),
    },
  };
}
```

### Pattern 2: RAGAS-Style Generation Metrics

**Use Case**: LLM-based evaluation of generation quality

```typescript
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface RAGASInput {
  query: string;
  context: string[];
  answer: string;
  groundTruth?: string;
}

interface RAGASScores {
  faithfulness: number;
  answerRelevancy: number;
  contextPrecision: number;
  contextRecall: number;
}

// Faithfulness: Is the answer grounded in context?
async function evaluateFaithfulness(
  answer: string,
  context: string[]
): Promise<number> {
  // Step 1: Extract claims from answer
  const { object: claims } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      statements: z.array(z.string()),
    }),
    prompt: `Extract all factual statements from this answer. Each statement should be atomic (one fact).

Answer: "${answer}"

List each statement separately.`,
  });

  if (claims.statements.length === 0) return 1;

  // Step 2: Verify each claim against context
  const contextText = context.join('\n\n');

  const { object: verification } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      verdicts: z.array(
        z.object({
          statement: z.string(),
          supported: z.boolean(),
          evidence: z.string().optional(),
        })
      ),
    }),
    prompt: `For each statement, determine if it is supported by the context.

Context:
${contextText}

Statements to verify:
${claims.statements.map((s, i) => `${i + 1}. ${s}`).join('\n')}

For each statement, indicate if it's supported by the context.`,
  });

  const supportedCount = verification.verdicts.filter((v) => v.supported).length;
  return supportedCount / verification.verdicts.length;
}

// Answer Relevancy: Does the answer address the query?
async function evaluateAnswerRelevancy(
  query: string,
  answer: string
): Promise<number> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      relevancyScore: z.number().min(0).max(1),
      reasoning: z.string(),
      addressedAspects: z.array(z.string()),
      missedAspects: z.array(z.string()),
    }),
    prompt: `Evaluate how well this answer addresses the query.

Query: "${query}"

Answer: "${answer}"

Score from 0 (completely irrelevant) to 1 (fully addresses all aspects).
Identify which aspects of the query are addressed and which are missed.`,
  });

  return object.relevancyScore;
}

// Context Precision: How useful was the retrieved context?
async function evaluateContextPrecision(
  query: string,
  context: string[],
  answer: string
): Promise<number> {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      contextEvaluations: z.array(
        z.object({
          contextIndex: z.number(),
          useful: z.boolean(),
          usefulnessReason: z.string(),
        })
      ),
    }),
    prompt: `Evaluate how useful each context chunk was for answering the query.

Query: "${query}"

Answer given: "${answer}"

Context chunks:
${context.map((c, i) => `[${i + 1}] ${c.slice(0, 500)}...`).join('\n\n')}

For each context chunk, determine if it was useful for generating the answer.`,
  });

  const usefulCount = object.contextEvaluations.filter((e) => e.useful).length;
  return context.length > 0 ? usefulCount / context.length : 0;
}

// Context Recall: Does context cover the ground truth?
async function evaluateContextRecall(
  context: string[],
  groundTruth: string
): Promise<number> {
  // Extract claims from ground truth
  const { object: truthClaims } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      statements: z.array(z.string()),
    }),
    prompt: `Extract all factual statements from this ground truth answer.

Ground Truth: "${groundTruth}"`,
  });

  if (truthClaims.statements.length === 0) return 1;

  // Check which claims are present in context
  const contextText = context.join('\n\n');

  const { object: coverage } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      statementsCovered: z.array(
        z.object({
          statement: z.string(),
          inContext: z.boolean(),
        })
      ),
    }),
    prompt: `Check which statements from the ground truth can be found in the context.

Context:
${contextText}

Statements to find:
${truthClaims.statements.map((s, i) => `${i + 1}. ${s}`).join('\n')}`,
  });

  const coveredCount = coverage.statementsCovered.filter((s) => s.inContext).length;
  return coveredCount / coverage.statementsCovered.length;
}

// Full RAGAS evaluation
async function evaluateRAGAS(input: RAGASInput): Promise<RAGASScores> {
  const [faithfulness, answerRelevancy, contextPrecision, contextRecall] =
    await Promise.all([
      evaluateFaithfulness(input.answer, input.context),
      evaluateAnswerRelevancy(input.query, input.answer),
      evaluateContextPrecision(input.query, input.context, input.answer),
      input.groundTruth
        ? evaluateContextRecall(input.context, input.groundTruth)
        : Promise.resolve(null),
    ]);

  return {
    faithfulness,
    answerRelevancy,
    contextPrecision,
    contextRecall: contextRecall ?? 0,
  };
}
```

### Pattern 3: End-to-End Evaluation Pipeline

**Use Case**: Complete RAG system evaluation

```typescript
interface EvaluationDataset {
  queries: Array<{
    id: string;
    query: string;
    groundTruthAnswer: string;
    relevantDocIds: string[];
  }>;
}

interface RAGSystemUnderTest {
  retrieve: (query: string) => Promise<RetrievalResult[]>;
  generate: (query: string, context: string[]) => Promise<string>;
}

interface EndToEndResults {
  retrieval: {
    avgPrecisionAt5: number;
    avgRecallAt10: number;
    avgMRR: number;
    avgNDCGAt10: number;
  };
  generation: {
    avgFaithfulness: number;
    avgAnswerRelevancy: number;
    avgContextPrecision: number;
    avgContextRecall: number;
  };
  endToEnd: {
    avgAnswerCorrectness: number;
    avgLatency: number;
    errorRate: number;
  };
  perQuery: Map<string, QueryEvaluation>;
}

interface QueryEvaluation {
  queryId: string;
  retrievalMetrics: RetrievalEvaluation;
  ragasScores: RAGASScores;
  answerCorrectness: number;
  latencyMs: number;
  error?: string;
}

async function evaluateRAGSystem(
  system: RAGSystemUnderTest,
  dataset: EvaluationDataset
): Promise<EndToEndResults> {
  const results: QueryEvaluation[] = [];

  for (const testCase of dataset.queries) {
    const startTime = Date.now();

    try {
      // Run retrieval
      const retrieved = await system.retrieve(testCase.query);
      const contexts = retrieved.slice(0, 5).map((r) => r.content);

      // Run generation
      const answer = await system.generate(testCase.query, contexts);

      const latency = Date.now() - startTime;

      // Evaluate retrieval
      const retrievalMetrics = evaluateRetrieval(retrieved, {
        queryId: testCase.id,
        relevantDocIds: testCase.relevantDocIds,
      });

      // Evaluate generation
      const ragasScores = await evaluateRAGAS({
        query: testCase.query,
        context: contexts,
        answer,
        groundTruth: testCase.groundTruthAnswer,
      });

      // Answer correctness (semantic similarity to ground truth)
      const answerCorrectness = await evaluateAnswerCorrectness(
        answer,
        testCase.groundTruthAnswer
      );

      results.push({
        queryId: testCase.id,
        retrievalMetrics,
        ragasScores,
        answerCorrectness,
        latencyMs: latency,
      });
    } catch (error) {
      results.push({
        queryId: testCase.id,
        retrievalMetrics: { precision: { at5: 0, at10: 0 }, recall: { at5: 0, at10: 0 }, mrr: 0, ndcg: { at5: 0, at10: 0 } },
        ragasScores: { faithfulness: 0, answerRelevancy: 0, contextPrecision: 0, contextRecall: 0 },
        answerCorrectness: 0,
        latencyMs: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  // Aggregate results
  const successful = results.filter((r) => !r.error);

  return {
    retrieval: {
      avgPrecisionAt5: avg(successful.map((r) => r.retrievalMetrics.precision.at5)),
      avgRecallAt10: avg(successful.map((r) => r.retrievalMetrics.recall.at10)),
      avgMRR: avg(successful.map((r) => r.retrievalMetrics.mrr)),
      avgNDCGAt10: avg(successful.map((r) => r.retrievalMetrics.ndcg.at10)),
    },
    generation: {
      avgFaithfulness: avg(successful.map((r) => r.ragasScores.faithfulness)),
      avgAnswerRelevancy: avg(successful.map((r) => r.ragasScores.answerRelevancy)),
      avgContextPrecision: avg(successful.map((r) => r.ragasScores.contextPrecision)),
      avgContextRecall: avg(successful.map((r) => r.ragasScores.contextRecall)),
    },
    endToEnd: {
      avgAnswerCorrectness: avg(successful.map((r) => r.answerCorrectness)),
      avgLatency: avg(results.map((r) => r.latencyMs)),
      errorRate: results.filter((r) => r.error).length / results.length,
    },
    perQuery: new Map(results.map((r) => [r.queryId, r])),
  };
}

async function evaluateAnswerCorrectness(
  answer: string,
  groundTruth: string
): Promise<number> {
  const { object } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      correctnessScore: z.number().min(0).max(1),
      factualOverlap: z.number().min(0).max(1),
      completeness: z.number().min(0).max(1),
      reasoning: z.string(),
    }),
    prompt: `Compare the answer to the ground truth and score correctness.

Ground Truth: "${groundTruth}"

Answer to evaluate: "${answer}"

Score:
- correctnessScore: Overall correctness (0-1)
- factualOverlap: How much factual content matches (0-1)
- completeness: How complete is the answer compared to ground truth (0-1)`,
  });

  return object.correctnessScore;
}

function avg(numbers: number[]): number {
  return numbers.length > 0
    ? numbers.reduce((a, b) => a + b, 0) / numbers.length
    : 0;
}
```

### Pattern 4: Automated Test Generation

**Use Case**: Creating evaluation datasets from documents

```typescript
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface GeneratedTestCase {
  query: string;
  groundTruthAnswer: string;
  sourceDocIds: string[];
  difficulty: 'easy' | 'medium' | 'hard';
  queryType: 'factual' | 'analytical' | 'comparative';
}

async function generateTestCases(
  documents: Array<{ id: string; content: string }>,
  count: number
): Promise<GeneratedTestCase[]> {
  const testCases: GeneratedTestCase[] = [];

  // Group documents for multi-doc questions
  const docChunks = chunkArray(documents, 3);

  for (const docGroup of docChunks) {
    if (testCases.length >= count) break;

    const { object } = await generateObject({
      model: openai('gpt-4o'),
      schema: z.object({
        questions: z.array(
          z.object({
            query: z.string(),
            answer: z.string(),
            difficulty: z.enum(['easy', 'medium', 'hard']),
            queryType: z.enum(['factual', 'analytical', 'comparative']),
            sourceDocIndices: z.array(z.number()),
          })
        ),
      }),
      prompt: `Generate diverse test questions from these documents.

Documents:
${docGroup.map((d, i) => `[${i}] ${d.content.slice(0, 2000)}`).join('\n\n---\n\n')}

Generate 3-5 questions per document set:
- 1-2 easy factual questions (single doc, direct answer)
- 1-2 medium analytical questions (require understanding)
- 1 hard comparative/multi-doc question (if applicable)

For each question, provide:
- The question
- The correct answer based on the documents
- Difficulty level
- Query type
- Which document indices contain the answer`,
    });

    for (const q of object.questions) {
      testCases.push({
        query: q.query,
        groundTruthAnswer: q.answer,
        sourceDocIds: q.sourceDocIndices.map((i) => docGroup[i]?.id).filter(Boolean),
        difficulty: q.difficulty,
        queryType: q.queryType,
      });
    }
  }

  return testCases.slice(0, count);
}

function chunkArray<T>(array: T[], size: number): T[][] {
  const chunks: T[][] = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
}
```

### Pattern 5: Continuous Evaluation & Monitoring

**Use Case**: Production monitoring with drift detection

```typescript
interface EvaluationWindow {
  startTime: Date;
  endTime: Date;
  metrics: {
    avgFaithfulness: number;
    avgAnswerRelevancy: number;
    avgNDCGAt10: number;
    errorRate: number;
    p50Latency: number;
    p95Latency: number;
  };
  sampleSize: number;
}

interface DriftAlert {
  metric: string;
  baseline: number;
  current: number;
  percentChange: number;
  severity: 'low' | 'medium' | 'high';
}

class RAGMonitor {
  private baseline: EvaluationWindow | null = null;
  private history: EvaluationWindow[] = [];
  private driftThresholds = {
    faithfulness: { low: 0.05, medium: 0.1, high: 0.15 },
    answerRelevancy: { low: 0.05, medium: 0.1, high: 0.15 },
    ndcg: { low: 0.05, medium: 0.1, high: 0.15 },
    errorRate: { low: 0.02, medium: 0.05, high: 0.1 },
    latency: { low: 0.2, medium: 0.5, high: 1.0 },
  };

  async setBaseline(window: EvaluationWindow): Promise<void> {
    this.baseline = window;
    this.history = [window];
  }

  async evaluateWindow(
    samples: Array<{
      query: string;
      context: string[];
      answer: string;
      latencyMs: number;
      error?: string;
    }>
  ): Promise<{ window: EvaluationWindow; alerts: DriftAlert[] }> {
    const successful = samples.filter((s) => !s.error);

    // Evaluate a sample for efficiency
    const sampleSize = Math.min(50, successful.length);
    const evalSamples = successful.slice(0, sampleSize);

    const scores = await Promise.all(
      evalSamples.map(async (s) => ({
        ragas: await evaluateRAGAS({
          query: s.query,
          context: s.context,
          answer: s.answer,
        }),
        latency: s.latencyMs,
      }))
    );

    const latencies = samples.map((s) => s.latencyMs).sort((a, b) => a - b);

    const window: EvaluationWindow = {
      startTime: new Date(),
      endTime: new Date(),
      metrics: {
        avgFaithfulness: avg(scores.map((s) => s.ragas.faithfulness)),
        avgAnswerRelevancy: avg(scores.map((s) => s.ragas.answerRelevancy)),
        avgNDCGAt10: 0, // Would need retrieval gold labels
        errorRate: samples.filter((s) => s.error).length / samples.length,
        p50Latency: latencies[Math.floor(latencies.length * 0.5)] || 0,
        p95Latency: latencies[Math.floor(latencies.length * 0.95)] || 0,
      },
      sampleSize: samples.length,
    };

    this.history.push(window);

    const alerts = this.detectDrift(window);

    return { window, alerts };
  }

  private detectDrift(current: EvaluationWindow): DriftAlert[] {
    if (!this.baseline) return [];

    const alerts: DriftAlert[] = [];

    // Check faithfulness drift
    const faithDiff =
      (this.baseline.metrics.avgFaithfulness - current.metrics.avgFaithfulness) /
      this.baseline.metrics.avgFaithfulness;

    if (Math.abs(faithDiff) > this.driftThresholds.faithfulness.low) {
      alerts.push({
        metric: 'faithfulness',
        baseline: this.baseline.metrics.avgFaithfulness,
        current: current.metrics.avgFaithfulness,
        percentChange: faithDiff * 100,
        severity: this.getSeverity(Math.abs(faithDiff), 'faithfulness'),
      });
    }

    // Check latency drift
    const latencyDiff =
      (current.metrics.p95Latency - this.baseline.metrics.p95Latency) /
      this.baseline.metrics.p95Latency;

    if (latencyDiff > this.driftThresholds.latency.low) {
      alerts.push({
        metric: 'p95Latency',
        baseline: this.baseline.metrics.p95Latency,
        current: current.metrics.p95Latency,
        percentChange: latencyDiff * 100,
        severity: this.getSeverity(latencyDiff, 'latency'),
      });
    }

    // Check error rate
    const errorDiff = current.metrics.errorRate - this.baseline.metrics.errorRate;

    if (errorDiff > this.driftThresholds.errorRate.low) {
      alerts.push({
        metric: 'errorRate',
        baseline: this.baseline.metrics.errorRate,
        current: current.metrics.errorRate,
        percentChange: (errorDiff / (this.baseline.metrics.errorRate || 0.01)) * 100,
        severity: this.getSeverity(errorDiff, 'errorRate'),
      });
    }

    return alerts;
  }

  private getSeverity(
    change: number,
    metric: keyof typeof this.driftThresholds
  ): 'low' | 'medium' | 'high' {
    const thresholds = this.driftThresholds[metric];
    if (change >= thresholds.high) return 'high';
    if (change >= thresholds.medium) return 'medium';
    return 'low';
  }
}
```

## Research & Benchmarks

### Industry Targets (2024-2025)

| Metric | Minimum | Good | Excellent |
|--------|---------|------|-----------|
| **NDCG@10** | 0.7 | 0.8 | 0.9+ |
| **Precision@5** | 0.6 | 0.75 | 0.85+ |
| **Faithfulness** | 0.8 | 0.9 | 0.95+ |
| **Answer Relevancy** | 0.75 | 0.85 | 0.92+ |
| **Context Recall** | 0.7 | 0.85 | 0.95+ |

### BEIR Benchmark Results

| System | Avg NDCG@10 | Notes |
|--------|-------------|-------|
| BM25 Baseline | 0.422 | Keyword search |
| Dense Retrieval | 0.451 | Single embedding |
| Hybrid (BM25 + Dense) | 0.478 | +6% over dense |
| **+ Reranking** | **0.531** | **+18% over hybrid** |
| **+ Advanced RAG** | **0.589** | **+11% over reranking** |

### LLM-as-Judge Correlation

| Evaluation Method | Human Correlation | Cost |
|-------------------|-------------------|------|
| **GPT-4 Judge** | 0.85-0.90 | Medium |
| GPT-3.5 Judge | 0.75-0.80 | Low |
| BERT Score | 0.65-0.70 | Very Low |
| BLEU/ROUGE | 0.40-0.50 | Very Low |

### Evaluation Speed Benchmarks

| Method | Time per Query | Scalability |
|--------|----------------|-------------|
| Full RAGAS (GPT-4) | 5-10s | Low |
| RAGAS (GPT-3.5) | 2-4s | Medium |
| Embedding similarity | 50-100ms | High |
| Keyword metrics | 10-20ms | Very High |

## When to Use This Pattern

### ✅ Use When:

1. **Production Systems**
   - Need quality guarantees
   - User-facing applications
   - Regulated industries

2. **System Development**
   - Comparing RAG approaches
   - A/B testing changes
   - Regression testing

3. **Continuous Monitoring**
   - Drift detection
   - Performance tracking
   - Alerting on degradation

### ❌ Don't Use When:

1. **Prototyping**
   - Early exploration
   - Quick iterations needed

2. **Cost Constrained**
   - LLM-based evaluation expensive
   - Use simpler proxies

3. **No Ground Truth**
   - Cannot create evaluation sets
   - Use online metrics instead

### Evaluation Strategy by Stage

| Stage | Evaluation Approach |
|-------|---------------------|
| Development | Automated tests, small eval set |
| Testing | Full RAGAS, comprehensive eval set |
| Staging | End-to-end with production-like data |
| Production | Sampling + drift monitoring |

## Production Best Practices

### 1. Create Representative Evaluation Sets

Balance query types and difficulties:

```typescript
interface EvalSetRequirements {
  minQueries: number;
  queryTypeDistribution: Record<string, number>;
  difficultyDistribution: Record<string, number>;
}

const RECOMMENDED_EVAL_SET: EvalSetRequirements = {
  minQueries: 200,
  queryTypeDistribution: {
    factual: 0.4,
    analytical: 0.35,
    comparative: 0.15,
    procedural: 0.1,
  },
  difficultyDistribution: {
    easy: 0.3,
    medium: 0.5,
    hard: 0.2,
  },
};
```

### 2. Separate Retrieval and Generation Evaluation

Diagnose issues precisely:

```typescript
async function diagnoseRAGFailure(
  query: string,
  retrieved: RetrievalResult[],
  answer: string,
  groundTruth: string
): Promise<{
  rootCause: 'retrieval' | 'generation' | 'both' | 'none';
  details: string;
}> {
  // Check if retrieval found relevant docs
  const contextContainsAnswer = await checkContextCoverage(
    retrieved.map((r) => r.content),
    groundTruth
  );

  // Check if generation used context correctly
  const answerMatchesGroundTruth = await evaluateAnswerCorrectness(
    answer,
    groundTruth
  );

  if (!contextContainsAnswer && answerMatchesGroundTruth < 0.5) {
    return {
      rootCause: 'retrieval',
      details: 'Relevant documents not retrieved',
    };
  }

  if (contextContainsAnswer && answerMatchesGroundTruth < 0.5) {
    return {
      rootCause: 'generation',
      details: 'Context was good but generation failed to use it',
    };
  }

  if (!contextContainsAnswer && answerMatchesGroundTruth >= 0.7) {
    return {
      rootCause: 'none',
      details: 'Model answered correctly despite poor retrieval (world knowledge)',
    };
  }

  return {
    rootCause: 'none',
    details: 'System performed correctly',
  };
}
```

### 3. Version Evaluation Sets

Track changes over time:

```typescript
interface VersionedEvalSet {
  version: string;
  createdAt: Date;
  queries: EvaluationDataset['queries'];
  metadata: {
    sourceDocVersions: Record<string, string>;
    generationMethod: string;
    humanReviewedCount: number;
  };
}
```

### 4. Combine Automated and Human Evaluation

Use humans for edge cases:

```typescript
interface HumanEvalTask {
  queryId: string;
  query: string;
  answer: string;
  context: string[];
  autoScores: RAGASScores;
  flagReason: 'low_confidence' | 'edge_case' | 'random_sample';
}

function selectForHumanReview(
  results: QueryEvaluation[],
  sampleRate: number = 0.05
): HumanEvalTask[] {
  const tasks: HumanEvalTask[] = [];

  for (const result of results) {
    // Flag low confidence
    if (result.ragasScores.faithfulness < 0.7) {
      tasks.push({
        queryId: result.queryId,
        // ... populate fields
        flagReason: 'low_confidence',
      });
    }
  }

  // Random sample
  const remaining = results.filter(
    (r) => !tasks.find((t) => t.queryId === r.queryId)
  );
  const sampleCount = Math.ceil(remaining.length * sampleRate);
  const sampled = remaining.slice(0, sampleCount);

  for (const result of sampled) {
    tasks.push({
      queryId: result.queryId,
      // ... populate fields
      flagReason: 'random_sample',
    });
  }

  return tasks;
}
```

## Key Takeaways

1. **Measure Both Components**: Retrieval and generation need separate metrics
2. **NDCG for Retrieval**: Target NDCG@10 > 0.8 for production quality
3. **Faithfulness for Generation**: Target > 0.9 to prevent hallucinations
4. **LLM-as-Judge Works**: GPT-4 correlates 0.85+ with human judgment
5. **Monitor Continuously**: Drift detection catches regressions early

**Quick Implementation Checklist**:

- [ ] Implement retrieval metrics (Precision, Recall, MRR, NDCG)
- [ ] Set up RAGAS-style generation evaluation
- [ ] Create representative evaluation dataset (200+ queries)
- [ ] Build end-to-end evaluation pipeline
- [ ] Add continuous monitoring with drift detection
- [ ] Establish baseline metrics and alert thresholds
- [ ] Plan for periodic human evaluation sampling

## References

1. **arXiv** (2024). "RAGAS: Automated Evaluation of Retrieval Augmented Generation". https://arxiv.org/abs/2309.15217
2. **arXiv** (2024). "ARAGOG: Advanced RAG Output Grading". https://arxiv.org/pdf/2404.01037
3. **BEIR Benchmark** (2024). "Benchmarking IR Evaluation". https://github.com/beir-cellar/beir
4. **Pinecone** (2024). "RAG Evaluation Guide". https://www.pinecone.io/learn/rag-evaluation/
5. **LangChain** (2024). "Evaluating RAG Applications". https://python.langchain.com/docs/guides/evaluation

**Related Topics**:

- [5.4.1 Naive RAG](./5.4.1-naive-rag.md)
- [5.3.4 Reranking](./5.3.4-reranking.md)
- [5.4.3 Agentic RAG](./5.4.3-agentic-rag.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
