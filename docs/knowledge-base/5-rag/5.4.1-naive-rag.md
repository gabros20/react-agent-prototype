# 5.4.1 Naive RAG (Retrieve → Inject → Generate)

## TL;DR

Naive RAG is the foundational pattern—retrieve relevant chunks, inject into prompt, generate response—providing a simple baseline that achieves ~25% answer accuracy but suffers from retrieval quality issues and limited context understanding.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md), [5.3.1 Vector Search](./5.3.1-vector-search.md)
- **Grounded In**: Modular RAG Survey (2024), Prompt Engineering Guide, LangChain Documentation

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-llm-knowledge-limitations)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Naive RAG represents the simplest implementation of Retrieval-Augmented Generation: convert the query to an embedding, find similar documents, concatenate them into the prompt, and generate a response. It's the starting point for any RAG system and remains valuable for simple use cases.

Despite its limitations, Naive RAG provides a crucial baseline for measuring improvements and is often sufficient for straightforward question-answering over well-organized knowledge bases.

**Key Research Findings** (2024-2025):

- **~25% Accuracy**: Naive RAG achieves approximately 25% answer accuracy at best in complex scenarios
- **Simple Implementation**: Can be built in hours with modern frameworks
- **Foundation Pattern**: All advanced RAG techniques build upon this baseline

**Date Verified**: 2025-12-12

## The Problem: LLM Knowledge Limitations

### The Classic Challenge

LLMs have training cutoffs and limited domain knowledge:

```
User: "What's our company's current refund policy?"

Without RAG:
┌─────────────────────────────────────────────────┐
│ LLM Response: "I don't have information about   │
│ your specific company policies. Generally,      │
│ refund policies vary..."                        │
└─────────────────────────────────────────────────┘
❌ Cannot access proprietary knowledge

With Naive RAG:
┌─────────────────────────────────────────────────┐
│ 1. Retrieve: "Refund Policy v2.1: 14-day        │
│    return window for all purchases..."          │
│ 2. Inject: [Context] + Question                 │
│ 3. Generate: "Your company offers a 14-day      │
│    return window for all purchases..."          │
└─────────────────────────────────────────────────┘
✅ Answers from actual knowledge base
```

**LLM Limitations**:

- ❌ Training cutoff means outdated information
- ❌ No access to private/proprietary data
- ❌ Cannot cite sources for claims
- ❌ Hallucinations when knowledge is lacking

### Why RAG Helps

RAG bridges the gap by:

- Providing current, domain-specific information
- Grounding responses in retrievable sources
- Enabling citation and verification
- Reducing hallucination through context

## Core Concept

### What is Naive RAG?

Naive RAG follows a simple three-stage pipeline:

```
┌─────────────────────────────────────────────────────────────┐
│                     NAIVE RAG PIPELINE                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  User Query                                                  │
│      ↓                                                       │
│  ┌─────────────────┐                                        │
│  │  1. RETRIEVAL   │  Query → Embedding → Vector Search     │
│  │                 │  → Top-K similar chunks                │
│  └────────┬────────┘                                        │
│           ↓                                                  │
│  ┌─────────────────┐                                        │
│  │  2. AUGMENT     │  System Prompt + Retrieved Chunks      │
│  │                 │  + User Query → Full Prompt            │
│  └────────┬────────┘                                        │
│           ↓                                                  │
│  ┌─────────────────┐                                        │
│  │  3. GENERATE    │  LLM processes augmented prompt        │
│  │                 │  → Response with context               │
│  └────────┬────────┘                                        │
│           ↓                                                  │
│      Response                                                │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### The Three Stages

**Stage 1: Retrieval**
- Convert query to embedding vector
- Search vector database for similar chunks
- Return top-K most relevant documents

**Stage 2: Augmentation**
- Format retrieved chunks into context
- Combine with system prompt and query
- Create complete prompt for LLM

**Stage 3: Generation**
- LLM processes augmented prompt
- Generates response grounded in context
- Returns answer to user

### Key Principles

1. **Retrieve Before Generate**: Always fetch context first
2. **Simple Pipeline**: Minimize complexity in baseline
3. **Top-K Selection**: Use similarity scores to rank results

## Implementation Patterns

### Pattern 1: Basic Naive RAG

**Use Case**: Minimal viable RAG implementation

```typescript
import { embed, generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

interface RAGResponse {
  answer: string;
  sources: Array<{ id: string; content: string; similarity: number }>;
}

async function naiveRAG(
  query: string,
  vectorDb: VectorDatabase,
  options = { topK: 5 }
): Promise<RAGResponse> {
  // Stage 1: Retrieval
  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  const retrievedChunks = await vectorDb.search({
    vector: embedding,
    topK: options.topK,
  });

  // Stage 2: Augmentation
  const context = retrievedChunks
    .map((chunk, i) => `[Source ${i + 1}]:\n${chunk.content}`)
    .join('\n\n');

  const prompt = `Answer the question based on the following context. If the context doesn't contain the answer, say "I don't have enough information."

Context:
${context}

Question: ${query}

Answer:`;

  // Stage 3: Generation
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt,
  });

  return {
    answer: text,
    sources: retrievedChunks.map((c) => ({
      id: c.id,
      content: c.content.slice(0, 200) + '...',
      similarity: c.similarity,
    })),
  };
}
```

### Pattern 2: Naive RAG with AI SDK v6

**Use Case**: Using AI SDK's native abstractions

```typescript
import { generateText, embed } from 'ai';
import { openai } from '@ai-sdk/openai';

const SYSTEM_PROMPT = `You are a helpful assistant that answers questions based on the provided context.
- Only use information from the context to answer
- If the context doesn't contain the answer, say so
- Cite sources using [Source N] notation`;

async function naiveRAGWithSDK(
  query: string,
  vectorDb: VectorDatabase
): Promise<string> {
  // Retrieve
  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  const chunks = await vectorDb.search({ vector: embedding, topK: 5 });

  // Format context
  const contextBlock = chunks
    .map((c, i) => `<source id="${i + 1}">\n${c.content}\n</source>`)
    .join('\n\n');

  // Generate with context
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    system: SYSTEM_PROMPT,
    prompt: `<context>\n${contextBlock}\n</context>\n\nQuestion: ${query}`,
  });

  return text;
}
```

### Pattern 3: Streaming Naive RAG

**Use Case**: Real-time response streaming for better UX

```typescript
import { streamText, embed } from 'ai';
import { openai } from '@ai-sdk/openai';

async function* streamingNaiveRAG(
  query: string,
  vectorDb: VectorDatabase
): AsyncGenerator<string> {
  // Retrieve (non-streaming)
  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: query,
  });

  const chunks = await vectorDb.search({ vector: embedding, topK: 5 });

  // Format context
  const context = chunks.map((c) => c.content).join('\n\n---\n\n');

  // Stream generation
  const { textStream } = await streamText({
    model: openai('gpt-4o-mini'),
    system: 'Answer based on the provided context.',
    prompt: `Context:\n${context}\n\nQuestion: ${query}`,
  });

  for await (const chunk of textStream) {
    yield chunk;
  }
}

// Usage
const stream = streamingNaiveRAG('What is the refund policy?', vectorDb);
for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

### Pattern 4: Naive RAG with Source Attribution

**Use Case**: Providing citations for generated answers

```typescript
interface AttributedAnswer {
  answer: string;
  citations: Array<{
    text: string;
    sourceId: string;
    sourceTitle: string;
  }>;
}

async function naiveRAGWithCitations(
  query: string,
  vectorDb: VectorDatabase
): Promise<AttributedAnswer> {
  const chunks = await retrieveChunks(query, vectorDb);

  const prompt = `Answer the question using ONLY the provided sources. Include inline citations like [1], [2] etc.

Sources:
${chunks.map((c, i) => `[${i + 1}] ${c.metadata.title}\n${c.content}`).join('\n\n')}

Question: ${query}

Provide your answer with inline citations:`;

  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt,
  });

  // Extract citations from response
  const citationPattern = /\[(\d+)\]/g;
  const citedSources = new Set<number>();
  let match;
  while ((match = citationPattern.exec(text)) !== null) {
    citedSources.add(parseInt(match[1]));
  }

  return {
    answer: text,
    citations: Array.from(citedSources).map((i) => ({
      text: chunks[i - 1].content.slice(0, 100) + '...',
      sourceId: chunks[i - 1].id,
      sourceTitle: chunks[i - 1].metadata.title,
    })),
  };
}
```

## Research & Benchmarks

### Naive RAG Limitations

| Issue | Impact | Cause |
|-------|--------|-------|
| **Low Precision** | Misaligned chunks retrieved | Semantic similarity ≠ relevance |
| **Low Recall** | Missing relevant chunks | Single-query retrieval |
| **Context Fragmentation** | Incomplete answers | Chunks lack full context |
| **Hallucination** | Incorrect information | Insufficient context handling |

### Performance Benchmarks

| Metric | Naive RAG | Advanced RAG | Delta |
|--------|-----------|--------------|-------|
| Answer Accuracy | ~25% | 40-60% | +15-35% |
| Retrieval Recall | 70-80% | 85-95% | +15% |
| Hallucination Rate | 15-25% | 5-10% | -10-15% |
| Latency | 500-1000ms | 1000-2000ms | +500ms |

### Common Failure Modes

1. **Vocabulary Mismatch**: Query uses different terms than documents
2. **Multi-hop Questions**: Answer requires information from multiple sources
3. **Negation Handling**: "What is NOT covered by the policy?"
4. **Temporal Questions**: "What changed in the latest update?"

## When to Use This Pattern

### ✅ Use When:

1. **Simple Q&A Systems**
   - FAQ bots, documentation search
   - Single-hop questions with direct answers

2. **Prototyping and Baselines**
   - Initial implementation to validate approach
   - Baseline for measuring improvements

3. **Well-Organized Knowledge**
   - Clean, structured documents
   - Questions map directly to content

4. **Low Complexity Requirements**
   - Precision requirements are moderate
   - Latency is more important than accuracy

### ❌ Don't Use When:

1. **Complex Queries**
   - Multi-hop reasoning required
   - Synthesis across multiple documents

2. **High Accuracy Required**
   - Medical, legal, financial domains
   - Wrong answers have consequences

3. **Poor Document Quality**
   - Unstructured, noisy content
   - Need preprocessing/enrichment

### Decision Matrix

| Scenario | Use Naive RAG? | Alternative |
|----------|----------------|-------------|
| FAQ Chatbot | ✅ Yes | - |
| Research Assistant | ❌ No | Advanced RAG |
| Documentation Search | ✅ Yes | Add reranking |
| Complex Analysis | ❌ No | Agentic RAG |
| Prototype/MVP | ✅ Yes | Upgrade later |

## Production Best Practices

### 1. Set Clear Expectations

Tell the LLM how to handle missing information:

```typescript
const SYSTEM_PROMPT = `You are a helpful assistant.
- Answer ONLY based on the provided context
- If information is missing, say "I don't have information about that"
- Never make up information
- Cite sources when possible`;
```

### 2. Filter Low-Similarity Results

Don't include irrelevant context:

```typescript
const MIN_SIMILARITY = 0.7;

const relevantChunks = retrievedChunks.filter(
  (chunk) => chunk.similarity >= MIN_SIMILARITY
);

if (relevantChunks.length === 0) {
  return {
    answer: "I don't have relevant information to answer this question.",
    sources: [],
  };
}
```

### 3. Format Context Clearly

Use structured formatting for better LLM understanding:

```typescript
function formatContext(chunks: Chunk[]): string {
  return chunks
    .map((chunk, i) =>
      `<document index="${i + 1}" source="${chunk.metadata.source}">
${chunk.content}
</document>`
    )
    .join('\n\n');
}
```

### 4. Monitor and Iterate

Track metrics to identify when to upgrade:

```typescript
interface RAGMetrics {
  queryLatency: number;
  retrievalCount: number;
  averageSimilarity: number;
  userSatisfaction?: number;
}

function logRAGMetrics(metrics: RAGMetrics): void {
  // Track for optimization decisions
  analytics.track('naive_rag_query', metrics);
}
```

## Key Takeaways

1. **Start Simple**: Naive RAG is the right starting point for most projects
2. **~25% Baseline**: Expect moderate accuracy—upgrade when needed
3. **Three Stages**: Retrieve → Augment → Generate is the foundation
4. **Filter Results**: Don't include low-similarity chunks
5. **Know Limitations**: Multi-hop, complex queries need advanced patterns

**Quick Implementation Checklist**:

- [ ] Set up vector database with embedded documents
- [ ] Implement basic retrieve → augment → generate pipeline
- [ ] Add minimum similarity threshold filtering
- [ ] Include source attribution in responses
- [ ] Set up metrics to track when to upgrade
- [ ] Test with representative queries

## References

1. **arXiv** (2024). "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks". https://arxiv.org/html/2407.21059v1
2. **Prompt Engineering Guide** (2024). "Retrieval Augmented Generation (RAG)". https://www.promptingguide.ai/research/rag
3. **Pinecone** (2024). "Advanced RAG Techniques". https://www.pinecone.io/learn/advanced-rag-techniques/
4. **RAGFlow** (2024). "Agentic RAG - Definition and Implementation". https://ragflow.io/blog/agentic-rag
5. **Superteams** (2024). "How to Implement Naive RAG, Advanced RAG, and Modular RAG". https://www.superteams.ai/blog/how-to-implement-rag

**Related Topics**:

- [5.4.2 Advanced RAG](./5.4.2-advanced-rag.md)
- [5.4.3 Agentic RAG](./5.4.3-agentic-rag.md)
- [5.3.1 Vector Search](./5.3.1-vector-search.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
