# 5.2.5 Chunk Size Trade-offs

## TL;DR

Chunk size creates a fundamental precision-recall trade-off: smaller chunks (128-256 tokens) improve retrieval precision but lose context, while larger chunks (512-1024 tokens) preserve context but dilute relevance—optimal size depends on query type and embedding model.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-12
- **Prerequisites**: [5.2.1 Fixed-Size Chunks](./5.2.1-fixed-size.md), [5.1.1 Embedding Documents](./5.1.1-embedding-documents.md)
- **Grounded In**: Chroma Research (2024), NVIDIA Chunking Benchmarks (2024), Multi-Dataset Analysis (2025)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-no-universal-optimal-size)
- [Core Concept](#core-concept)
- [Implementation Patterns](#implementation-patterns)
- [Research & Benchmarks](#research--benchmarks)
- [When to Use This Pattern](#when-to-use-this-pattern)
- [Production Best Practices](#production-best-practices)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Chunk size is one of the most impactful yet often underestimated parameters in RAG systems. The choice creates a fundamental trade-off between precision (how relevant each retrieved chunk is) and recall (how much useful information each chunk contains).

The optimal size varies by use case: factoid queries need small, precise chunks (256-512 tokens), while analytical queries benefit from larger, context-rich chunks (512-1024 tokens). Additionally, different embedding models exhibit distinct sensitivities to chunk size.

**Key Research Findings** (2024-2025):

- **Model Sensitivity**: Stella benefits from larger chunks (512-1024), Snowflake excels with smaller chunks (arXiv 2025)
- **Query Type Impact**: Factoid queries: 256-512 tokens; Analytical queries: 1024+ tokens (NVIDIA 2024)
- **Sweet Spot**: 400-512 tokens provides balanced performance across most benchmarks (Chroma 2024)

**Date Verified**: 2025-12-12

## The Problem: No Universal Optimal Size

### The Classic Challenge

Different query types need different chunk sizes:

```
Query 1: "What is the CEO's name?"
→ Needs: Small, precise chunk
→ Problem with large chunks: Answer buried in irrelevant context

Query 2: "Explain the company's growth strategy"
→ Needs: Large, context-rich chunk
→ Problem with small chunks: Strategy fragmented across multiple chunks
```

### The Precision-Recall Trade-off

```
Small Chunks (128-256 tokens)      Large Chunks (512-1024 tokens)
┌─────────────────────────────┐    ┌─────────────────────────────┐
│ ✅ High precision           │    │ ✅ Rich context             │
│ ✅ Focused embeddings       │    │ ✅ Complete ideas           │
│ ❌ Lost context             │    │ ❌ Diluted relevance        │
│ ❌ Fragmented answers       │    │ ❌ Noisy embeddings         │
└─────────────────────────────┘    └─────────────────────────────┘

            Precision                        Recall
              High ◄─────────────────────────► High
               │                               │
        Small Chunks                    Large Chunks
```

### Why This Matters

Incorrect chunk size causes:

- **Factoid queries fail**: Large chunks bury specific facts in noise
- **Analytical queries fail**: Small chunks fragment complex arguments
- **Token waste**: Large chunks consume context window with irrelevant text
- **Poor embeddings**: Chunks that are too small or large create suboptimal vector representations

## Core Concept

### What Determines Optimal Size?

```
                    Optimal Chunk Size
                           │
     ┌─────────────────────┼─────────────────────┐
     ↓                     ↓                     ↓
Query Type           Content Type          Embedding Model
     │                     │                     │
├─ Factoid: 256-512   ├─ Dense: smaller    ├─ Stella: larger
├─ Analytical: 1024+  ├─ Sparse: larger    ├─ Snowflake: smaller
└─ Mixed: 400-512     └─ Code: function    └─ OpenAI: flexible
```

### Key Trade-off Dimensions

| Dimension | Small Chunks | Large Chunks |
|-----------|--------------|--------------|
| **Precision** | Higher | Lower |
| **Recall** | Lower | Higher |
| **Context** | Fragmented | Complete |
| **Embedding Quality** | Focused but narrow | Broad but diluted |
| **Token Efficiency** | Better | Worse |
| **Retrieval Speed** | More chunks to search | Fewer chunks |

### Key Principles

1. **Match Query Complexity**: Factoid vs analytical queries need different sizes
2. **Consider Content Density**: Technical docs vs prose have different information density
3. **Test Your Model**: Embedding models have different optimal ranges

## Implementation Patterns

### Pattern 1: Query-Adaptive Chunk Size

**Use Case**: Different chunk sizes for different query types

```typescript
interface ChunkingConfig {
  factoid: { chunkSize: number; overlap: number };
  analytical: { chunkSize: number; overlap: number };
  default: { chunkSize: number; overlap: number };
}

const CONFIG: ChunkingConfig = {
  factoid: { chunkSize: 256, overlap: 25 },
  analytical: { chunkSize: 1024, overlap: 100 },
  default: { chunkSize: 512, overlap: 50 },
};

function classifyQuery(query: string): 'factoid' | 'analytical' | 'default' {
  // Factoid indicators: who, what, when, where, specific names/dates
  const factoidPatterns = /^(who|what|when|where|which)\b|specific|exact|name of/i;

  // Analytical indicators: why, how, explain, compare, analyze
  const analyticalPatterns = /^(why|how|explain|compare|analyze|describe)\b|strategy|overview/i;

  if (factoidPatterns.test(query)) return 'factoid';
  if (analyticalPatterns.test(query)) return 'analytical';
  return 'default';
}

async function adaptiveSearch(
  query: string,
  indices: Map<string, VectorIndex>
): Promise<SearchResult[]> {
  const queryType = classifyQuery(query);
  const index = indices.get(queryType) || indices.get('default')!;

  return index.search(query);
}
```

**Trade-off**: Requires maintaining multiple indices (3x storage) but optimizes for query type.

### Pattern 2: Multi-Resolution Indexing

**Use Case**: Index same content at multiple chunk sizes

```typescript
interface MultiResolutionIndex {
  small: VectorIndex;   // 256 tokens - factoid queries
  medium: VectorIndex;  // 512 tokens - general queries
  large: VectorIndex;   // 1024 tokens - analytical queries
}

async function indexDocument(
  doc: Document,
  indices: MultiResolutionIndex
): Promise<void> {
  const configs = [
    { index: indices.small, size: 256, overlap: 25 },
    { index: indices.medium, size: 512, overlap: 50 },
    { index: indices.large, size: 1024, overlap: 100 },
  ];

  for (const config of configs) {
    const chunks = chunkDocument(doc, config.size, config.overlap);
    await config.index.upsert(chunks);
  }
}

async function multiResolutionSearch(
  query: string,
  indices: MultiResolutionIndex
): Promise<SearchResult[]> {
  // Search all resolutions in parallel
  const [small, medium, large] = await Promise.all([
    indices.small.search(query, { topK: 5 }),
    indices.medium.search(query, { topK: 5 }),
    indices.large.search(query, { topK: 5 }),
  ]);

  // Merge and deduplicate results
  return fuseResults([small, medium, large]);
}
```

**Trade-off**: 3x storage but provides resolution flexibility at query time.

### Pattern 3: Content-Type Specific Sizing

**Use Case**: Different chunk sizes based on document characteristics

```typescript
interface ContentChunkConfig {
  code: { size: number; strategy: 'function' | 'class' | 'fixed' };
  documentation: { size: number; strategy: 'section' | 'paragraph' };
  conversational: { size: number; strategy: 'turn' | 'fixed' };
  default: { size: number; strategy: 'fixed' };
}

const CONTENT_CONFIG: ContentChunkConfig = {
  code: { size: 512, strategy: 'function' },        // Keep functions intact
  documentation: { size: 512, strategy: 'section' }, // Respect structure
  conversational: { size: 256, strategy: 'turn' },  // Per conversation turn
  default: { size: 512, strategy: 'fixed' },
};

function detectContentType(doc: Document): keyof ContentChunkConfig {
  if (doc.path.match(/\.(ts|js|py|go|rs)$/)) return 'code';
  if (doc.path.match(/\.(md|rst|adoc)$/)) return 'documentation';
  if (doc.metadata?.type === 'transcript') return 'conversational';
  return 'default';
}

function chunkByContentType(doc: Document): Chunk[] {
  const contentType = detectContentType(doc);
  const config = CONTENT_CONFIG[contentType];

  switch (config.strategy) {
    case 'function':
      return chunkByFunctions(doc.content, config.size);
    case 'section':
      return chunkBySections(doc.content, config.size);
    case 'turn':
      return chunkByConversationTurns(doc.content, config.size);
    default:
      return fixedSizeChunk(doc.content, config.size);
  }
}
```

### Pattern 4: Dynamic Chunk Size Selection

**Use Case**: Runtime optimization based on retrieval performance

```typescript
interface ChunkPerformanceMetrics {
  chunkSize: number;
  precision: number;
  recall: number;
  userSatisfaction: number;
  sampleSize: number;
}

class AdaptiveChunker {
  private metrics: Map<number, ChunkPerformanceMetrics> = new Map();
  private readonly chunkSizes = [256, 384, 512, 768, 1024];

  async selectOptimalSize(queryType: string): Promise<number> {
    const relevantMetrics = this.getMetricsForQueryType(queryType);

    if (relevantMetrics.length < 100) {
      // Not enough data, use balanced default
      return 512;
    }

    // Select size with best F1 score (balanced precision/recall)
    return relevantMetrics.reduce((best, current) => {
      const currentF1 = 2 * (current.precision * current.recall) /
                        (current.precision + current.recall);
      const bestF1 = 2 * (best.precision * best.recall) /
                     (best.precision + best.recall);
      return currentF1 > bestF1 ? current : best;
    }).chunkSize;
  }

  recordMetrics(
    chunkSize: number,
    wasRelevant: boolean,
    wasComplete: boolean
  ): void {
    // Update running metrics for this chunk size
    const existing = this.metrics.get(chunkSize) || {
      chunkSize,
      precision: 0,
      recall: 0,
      userSatisfaction: 0,
      sampleSize: 0,
    };

    // Incremental update logic...
    this.metrics.set(chunkSize, updated);
  }
}
```

## Research & Benchmarks

### Chunk Size Recommendations by Query Type

| Query Type | Recommended Size | Rationale |
|------------|------------------|-----------|
| **Factoid** (names, dates, facts) | 256-512 tokens | Precise matching |
| **Analytical** (explanations, comparisons) | 1024+ tokens | Full context needed |
| **Mixed/General** | 400-512 tokens | Balanced trade-off |

### Embedding Model Sensitivity (arXiv 2025)

| Model | Optimal Chunk Size | Key Finding |
|-------|-------------------|-------------|
| **Stella** | 512-1024 tokens | +5-8% recall@1 with larger chunks |
| **Snowflake** | 128-256 tokens | Excels at fine-grained entity matching |
| **OpenAI ada-002** | 256-512 tokens | Balanced across sizes |
| **text-embedding-3-small** | 256-768 tokens | Flexible, slight preference for medium |

### Benchmark Results (Chroma 2024)

| Chunk Size | Recall | Precision | IoU |
|------------|--------|-----------|-----|
| 200 tokens | 86.1% | **8.0%** | **8.0%** |
| 400 tokens | **91.3%** | 6.3% | 6.0% |
| 800 tokens | 89.5% | 4.2% | 3.8% |
| 1600 tokens | 85.2% | 2.8% | 2.5% |

**Key Finding**: 400 tokens provides best recall, but 200 tokens wins on precision and efficiency (IoU).

### Content Type Guidelines

| Content Type | Recommended Size | Notes |
|--------------|------------------|-------|
| **FAQ Systems** | 128-256 tokens | One Q&A per chunk |
| **Technical Docs** | 256-512 tokens | Preserve code examples |
| **Research Papers** | 512-1024 tokens | Keep arguments intact |
| **Legal Contracts** | 1024+ tokens | Context-heavy clauses |
| **Chat Transcripts** | 256 tokens | Per conversation turn |

## When to Use This Pattern

### ✅ Use Smaller Chunks (128-256) When:

1. **Factoid Queries Dominate**
   - FAQ systems, lookup databases
   - Users asking specific questions

2. **Content Is Information-Dense**
   - Reference materials, APIs
   - Each sentence contains distinct facts

3. **Precision Over Recall**
   - Better to miss than retrieve irrelevant
   - Context window constraints

### ✅ Use Larger Chunks (512-1024) When:

1. **Analytical Queries Dominate**
   - Research assistants, analysis tools
   - Users need comprehensive answers

2. **Content Has Long Dependencies**
   - Legal documents, academic papers
   - Arguments span multiple paragraphs

3. **Recall Over Precision**
   - Missing context is costly
   - Sufficient context window available

### Decision Matrix

| Scenario | Chunk Size | Overlap |
|----------|------------|---------|
| FAQ Bot | 256 | 10% |
| Code Search | 512 (function-aware) | 15% |
| Document QA | 512 | 15% |
| Research Assistant | 1024 | 20% |
| Legal Analysis | 1024+ | 25% |

## Production Best Practices

### 1. Start with 512 Tokens as Baseline

Industry consensus for balanced performance:

```typescript
const DEFAULT_CONFIG = {
  chunkSize: 512,
  overlap: Math.round(512 * 0.15), // 77 tokens
  minChunkSize: 100,
};
```

### 2. Benchmark Against Your Data

Run experiments before committing:

```typescript
async function benchmarkChunkSizes(
  testQueries: TestQuery[],
  chunkSizes: number[]
): Promise<BenchmarkResults> {
  const results: BenchmarkResults = {};

  for (const size of chunkSizes) {
    const index = await createIndex(documents, { chunkSize: size });

    results[size] = {
      precision: await measurePrecision(index, testQueries),
      recall: await measureRecall(index, testQueries),
      avgChunksRetrieved: await measureRetrievalCount(index, testQueries),
      latency: await measureLatency(index, testQueries),
    };
  }

  return results;
}
```

### 3. Consider Storage vs Performance

| Chunk Size | Chunks per 100K tokens | Storage Factor |
|------------|------------------------|----------------|
| 256 | ~390 | 1.0x |
| 512 | ~195 | 1.0x |
| 1024 | ~98 | 1.0x |
| Multi-resolution (3 sizes) | ~683 | 3.0x |

### 4. Monitor and Iterate

Track retrieval quality in production:

```typescript
interface RetrievalMetrics {
  queryType: string;
  chunkSize: number;
  retrievedChunks: number;
  userRatedRelevant: number;
  answerWasComplete: boolean;
}

function logRetrievalMetrics(metrics: RetrievalMetrics): void {
  // Log to analytics for chunk size optimization
  analytics.track('rag_retrieval', metrics);
}
```

## Key Takeaways

1. **No Universal Optimal**: Best chunk size depends on query type, content, and embedding model
2. **256-512 for Factoid**: Specific questions need precise, focused chunks
3. **512-1024 for Analytical**: Complex questions need context-rich chunks
4. **400-512 Default**: Balanced starting point for mixed workloads
5. **Test Your Use Case**: Benchmark against your actual queries and content

**Quick Implementation Checklist**:

- [ ] Start with 512 tokens as baseline
- [ ] Classify your dominant query types
- [ ] Benchmark 3-4 chunk sizes against test queries
- [ ] Consider multi-resolution indexing for mixed workloads
- [ ] Monitor precision/recall in production
- [ ] Iterate based on user feedback

## References

1. **arXiv** (2025). "Rethinking Chunk Size for Long-Document Retrieval: A Multi-Dataset Analysis". https://arxiv.org/html/2505.21700v2
2. **Chroma Research** (2024). "Evaluating Chunking Strategies for Retrieval". https://research.trychroma.com/evaluating-chunking
3. **NVIDIA** (2024). "Finding the Best Chunking Strategy for Accurate AI Responses". https://developer.nvidia.com/blog/finding-best-chunking-strategy
4. **Milvus** (2024). "What is the optimal chunk size for RAG applications?". https://milvus.io/ai-quick-reference/what-is-optimal-chunk-size-for-rag
5. **Weaviate** (2024). "Chunking Strategies to Improve RAG Performance". https://weaviate.io/blog/chunking-strategies-for-rag

**Related Topics**:

- [5.2.1 Fixed-Size Chunks](./5.2.1-fixed-size.md)
- [5.2.2 Semantic Chunks](./5.2.2-semantic-chunks.md)
- [5.2.3 Overlapping Windows](./5.2.3-overlapping-windows.md)

**Layer Index**: [Layer 5: RAG & Retrieval](../AI_KNOWLEDGE_BASE_TOC.md#layer-5-retrieval--rag)
