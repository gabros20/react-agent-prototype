# 3.4.1 - Max Steps Limits

## Overview

**Max steps limits** are a critical safety mechanism for preventing runaway agents that consume excessive API costs while stuck in unproductive loops. This pattern sets a hard upper bound on the number of iterations an agent can execute, forcing termination when the limit is reached.

While simple to implement, max steps limits are a **symptom detector** rather than a root cause solver. Production systems should combine step limits with convergence detection, stuck loop prevention, and adaptive reasoning strategies for robust agent control.

**Key Research Findings (2024-2025)**:

- **85% of agent failures** are due to unclear goals and ambiguous stopping conditions (Doinamerica, Sep 2025)
- **Default limits**: Most frameworks use 10-25 steps (LangChain: 15, Vercel AI SDK: 10, AutoGen: 25)
- **Cost impact**: Without limits, stuck agents can consume $10-100+ in a single loop
- **Adaptive approaches**: REFRAIN framework reduces tokens by 20-55% through dynamic stopping (Oct 2025)
- **Optimal range**: 5-10 steps for simple tasks, 15-30 for complex, 50+ for research/debugging

**Date Verified**: November 19, 2025

---

## Why Max Steps Limits Matter

### The Runaway Agent Problem

**Example: Endless Wikipedia Scrolling**

```typescript
// User request: "Find the population of Tokyo"

// Agent execution (without limits):
// Step 1: web_search("Tokyo population")
// Step 2: open_page("https://en.wikipedia.org/wiki/Tokyo")
// Step 3: scroll_down() ‚Üí No population found yet
// Step 4: scroll_down() ‚Üí Still looking...
// Step 5: scroll_down() ‚Üí Keep scrolling...
// Step 6: scroll_down() ‚Üí ...
// Step 7: scroll_down() ‚Üí ...
// ... (continues indefinitely until manual cancellation)

// Cost: $0.05 per step √ó 100+ steps = $5+ wasted
// Time: 300 seconds (5 minutes) of stuck behavior
// User experience: Terrible
```

**Root causes** (from research):
1. **Unclear success criteria**: Agent doesn't know what "done" looks like
2. **Missing validation**: No checks if action achieved its goal
3. **Tool confusion**: Agent doesn't understand tool's actual effect
4. **Reasoning loops**: Agent repeats same thought pattern

### Production Impact

| Scenario                     | Without Limits      | With Limits (15 steps) | Savings         |
| ---------------------------- | ------------------- | ---------------------- | --------------- |
| **Stuck scroll loop**        | $5.00 (100 steps)   | $0.75 (15 steps)       | **$4.25 (85%)** |
| **Search refinement loop**   | $3.50 (70 steps)    | $0.75 (15 steps)       | **$2.75 (79%)** |
| **Invalid API retry loop**   | $10.00 (200 steps)  | $0.75 (15 steps)       | **$9.25 (93%)** |

**ROI**: Max steps limits prevent catastrophic cost overruns with minimal implementation effort.

---

## Implementation Patterns

### 1. Hard Limit (Simplest)

```typescript
// ‚úÖ GOOD: Simple hard limit with AI SDK
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const runAgentWithLimit = async (prompt: string) => {
  try {
    const { text, steps } = await generateText({
      model: openai('gpt-4o'),
      prompt,
      tools: {
        // Your tools here
      },
      maxSteps: 10, // Hard limit
    });
    
    console.log(`Completed in ${steps} steps`);
    return text;
  } catch (error) {
    if (error.message.includes('max steps')) {
      console.error('Agent exceeded max steps limit');
      return 'Task could not be completed within step limit. Please simplify your request.';
    }
    throw error;
  }
};

// Usage
await runAgentWithLimit('Research the history of AI and summarize key milestones');
```

**Pros**:
- ‚úÖ Simple to implement (one parameter)
- ‚úÖ Guaranteed termination
- ‚úÖ Cost protection

**Cons**:
- ‚ùå Fixed limit may be too low for complex tasks or too high for simple ones
- ‚ùå No distinction between productive and unproductive steps
- ‚ùå Abrupt failure with no partial results

### 2. Task-Based Adaptive Limits

```typescript
// ‚úÖ BETTER: Adaptive limits based on task complexity
interface TaskConfig {
  maxSteps: number;
  taskType: 'simple' | 'medium' | 'complex' | 'research';
  description: string;
}

const TASK_CONFIGS: Record<string, TaskConfig> = {
  simple: {
    maxSteps: 5,
    taskType: 'simple',
    description: 'Single API call or database query',
  },
  medium: {
    maxSteps: 15,
    taskType: 'medium',
    description: 'Multi-step operations with 2-5 tools',
  },
  complex: {
    maxSteps: 30,
    taskType: 'complex',
    description: 'Complex workflows with branching logic',
  },
  research: {
    maxSteps: 50,
    taskType: 'research',
    description: 'Open-ended research with exploration',
  },
};

const classifyTask = async (prompt: string): Promise<TaskConfig> => {
  // Use LLM to classify task complexity
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      taskType: z.enum(['simple', 'medium', 'complex', 'research']),
      reasoning: z.string(),
    }),
    prompt: `Classify the complexity of this task: "${prompt}"
             
             Simple: Single lookup or query
             Medium: 2-5 steps with clear path
             Complex: Multi-step with branching
             Research: Open-ended exploration`,
  });
  
  return TASK_CONFIGS[object.taskType];
};

const runAdaptiveAgent = async (prompt: string) => {
  // Classify task to determine appropriate limit
  const config = await classifyTask(prompt);
  
  console.log(`Task classified as ${config.taskType}, using ${config.maxSteps} max steps`);
  
  const { text, steps } = await generateText({
    model: openai('gpt-4o'),
    prompt,
    tools: { /* ... */ },
    maxSteps: config.maxSteps,
  });
  
  console.log(`Completed in ${steps}/${config.maxSteps} steps`);
  
  // Warn if we hit the limit
  if (steps >= config.maxSteps) {
    console.warn('Agent hit max steps limit - task may be incomplete');
  }
  
  return text;
};

// Usage
await runAdaptiveAgent('What is 2+2?'); // ‚Üí simple (5 steps)
await runAdaptiveAgent('Create a blog post about AI ethics'); // ‚Üí medium (15 steps)
await runAdaptiveAgent('Research and compare top 5 AI frameworks'); // ‚Üí research (50 steps)
```

**Pros**:
- ‚úÖ Tailored limits for different task complexities
- ‚úÖ Better balance between safety and capability
- ‚úÖ Predictable cost per task type

**Cons**:
- ‚ùå Classification overhead ($0.0001 per task)
- ‚ùå May misclassify some tasks
- ‚ùå Still has abrupt termination

### 3. Progressive Limits with Checkpoints

```typescript
// ‚úÖ BEST: Progressive limits with graceful degradation
interface ProgressiveConfig {
  softLimit: number;  // Warn user, allow continuation
  hardLimit: number;  // Force stop
  checkpointEvery: number;
}

const runProgressiveAgent = async (
  prompt: string,
  config: ProgressiveConfig = {
    softLimit: 10,
    hardLimit: 20,
    checkpointEvery: 5,
  }
) => {
  let currentStep = 0;
  let partialResults: string[] = [];
  let shouldContinue = true;
  
  const onStepFinish = async (step: any) => {
    currentStep++;
    
    // Save checkpoint
    if (currentStep % config.checkpointEvery === 0) {
      partialResults.push(step.text || 'Step completed');
      console.log(`Checkpoint ${currentStep}: Saved progress`);
    }
    
    // Soft limit: Ask user if they want to continue
    if (currentStep === config.softLimit) {
      console.warn(`Soft limit reached (${config.softLimit} steps). Task may be complex.`);
      // In production: prompt user with option to continue or stop
      // shouldContinue = await askUserToContinue();
    }
    
    // Hard limit: Force stop
    if (currentStep >= config.hardLimit) {
      console.error(`Hard limit reached (${config.hardLimit} steps). Forcing stop.`);
      return { shouldContinue: false, partialResults };
    }
    
    return { shouldContinue: true };
  };
  
  try {
    const { text, steps } = await generateText({
      model: openai('gpt-4o'),
      prompt,
      tools: { /* ... */ },
      maxSteps: config.hardLimit,
      onStepFinish,
    });
    
    return {
      success: true,
      result: text,
      steps,
      partialResults,
    };
  } catch (error) {
    // Return partial results on failure
    return {
      success: false,
      result: `Task incomplete after ${currentStep} steps. Partial results:\n${partialResults.join('\n')}`,
      steps: currentStep,
      partialResults,
    };
  }
};

// Usage
const result = await runProgressiveAgent('Complex multi-step task', {
  softLimit: 10,   // Warn at 10 steps
  hardLimit: 25,   // Stop at 25 steps
  checkpointEvery: 5, // Save progress every 5 steps
});

if (!result.success && result.partialResults.length > 0) {
  console.log('Agent stopped, but here are partial results:', result.partialResults);
}
```

**Pros**:
- ‚úÖ Graceful degradation with partial results
- ‚úÖ User control at soft limit
- ‚úÖ Checkpointing prevents total loss
- ‚úÖ Two-tier safety (soft + hard limits)

**Cons**:
- ‚ùå More complex implementation
- ‚ùå Requires UI for user prompts in production

---

## Optimal Limit Selection

### Research-Based Guidelines

**From "Agent Stopped Due to Max Iterations: Fixes That Work" (Sep 2025)**:

| Task Type                    | Recommended Limit | Reasoning                                    |
| ---------------------------- | ----------------- | -------------------------------------------- |
| **Simple Q&A**               | 3-5 steps         | Single lookup or calculation                 |
| **Data retrieval**           | 5-10 steps        | Fetch ‚Üí Transform ‚Üí Return                   |
| **CMS operations**           | 8-15 steps        | Search ‚Üí Read ‚Üí Update ‚Üí Verify              |
| **Debugging**                | 15-25 steps       | Investigate ‚Üí Hypothesize ‚Üí Test ‚Üí Fix      |
| **Research**                 | 25-50 steps       | Explore ‚Üí Gather ‚Üí Synthesize ‚Üí Validate    |
| **Code generation**          | 30-60 steps       | Plan ‚Üí Write ‚Üí Test ‚Üí Fix ‚Üí Refine          |

### Task Complexity Heuristics

```typescript
// Automatic limit calculation based on task features
const calculateOptimalLimit = (task: {
  toolsAvailable: number;
  expectedBranching: number; // How many decision points?
  dataComplexity: 'low' | 'medium' | 'high';
  requiresExploration: boolean;
}): number => {
  let baseLimit = 10;
  
  // More tools = potentially more steps needed
  baseLimit += Math.min(task.toolsAvailable * 0.5, 10);
  
  // Branching adds complexity
  baseLimit += task.expectedBranching * 3;
  
  // Data complexity multiplier
  const complexityMultiplier = {
    low: 1.0,
    medium: 1.5,
    high: 2.0,
  };
  baseLimit *= complexityMultiplier[task.dataComplexity];
  
  // Exploration requires more steps
  if (task.requiresExploration) {
    baseLimit *= 2;
  }
  
  // Cap at reasonable maximum
  return Math.min(Math.round(baseLimit), 60);
};

// Example usage
const cmsTaskLimit = calculateOptimalLimit({
  toolsAvailable: 12,
  expectedBranching: 2,  // Search ‚Üí Read, may need Update
  dataComplexity: 'medium',
  requiresExploration: false,
});
// Result: ~15 steps

const researchTaskLimit = calculateOptimalLimit({
  toolsAvailable: 8,
  expectedBranching: 5,
  dataComplexity: 'high',
  requiresExploration: true,
});
// Result: ~50 steps
```

---

## Framework Comparison

### Default Limits Across Frameworks

| Framework          | Default Max Steps | Configurable? | Notes                                  |
| ------------------ | ----------------- | ------------- | -------------------------------------- |
| **Vercel AI SDK**  | 10                | ‚úÖ Yes        | `maxSteps` parameter                   |
| **LangChain**      | 15                | ‚úÖ Yes        | `max_iterations` in agent config       |
| **AutoGen**        | 25                | ‚úÖ Yes        | `max_consecutive_auto_reply`           |
| **CrewAI**         | 20                | ‚úÖ Yes        | `max_iter` per agent                   |
| **LlamaIndex**     | Unlimited         | ‚ö†Ô∏è Manual     | Must implement custom stopping logic   |

### AI SDK v6 Implementation (Your System)

```typescript
// File: server/agent/orchestrator.ts
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export const AGENT_CONFIG = {
  maxSteps: 15,           // Production default
  model: 'gpt-4o',
  temperature: 0.2,       // Lower for consistent reasoning
  maxRetries: 3,
};

export const runAgent = async (
  prompt: string,
  context: AgentContext,
  options: { maxSteps?: number } = {}
) => {
  const maxSteps = options.maxSteps ?? AGENT_CONFIG.maxSteps;
  
  try {
    const result = await generateText({
      model: openai(AGENT_CONFIG.model),
      temperature: AGENT_CONFIG.temperature,
      prompt,
      tools: ALL_TOOLS,
      maxSteps,
      experimental_context: context,
    });
    
    // Log completion stats
    console.log(`Agent completed in ${result.steps}/${maxSteps} steps`);
    
    // Warn if approaching limit
    if (result.steps >= maxSteps * 0.8) {
      console.warn(`Agent used ${result.steps} steps, close to limit of ${maxSteps}`);
    }
    
    return result;
  } catch (error) {
    if (error.code === 'MAX_STEPS_EXCEEDED') {
      throw new Error(
        `Agent exceeded maximum steps (${maxSteps}). Task may be too complex. ` +
        `Consider: 1) Simplifying the request, 2) Breaking into smaller tasks, ` +
        `3) Increasing max steps if justified.`
      );
    }
    throw error;
  }
};
```

---

## Monitoring and Alerts

### Production Telemetry

```typescript
// Track step usage across all requests
interface StepUsageMetrics {
  requestId: string;
  taskType: string;
  stepsUsed: number;
  maxStepsAllowed: number;
  utilizationPct: number; // stepsUsed / maxStepsAllowed
  hitLimit: boolean;
  cost: number;
  duration: number;
}

const metricsCollector = {
  data: [] as StepUsageMetrics[],
  
  record(metric: StepUsageMetrics) {
    this.data.push(metric);
    
    // Alert if limit hit
    if (metric.hitLimit) {
      console.error(`‚ö†Ô∏è Agent hit max steps limit`, {
        requestId: metric.requestId,
        taskType: metric.taskType,
        steps: metric.stepsUsed,
      });
    }
    
    // Alert if high utilization
    if (metric.utilizationPct >= 0.9 && !metric.hitLimit) {
      console.warn(`‚ö†Ô∏è Agent approaching limit (${metric.utilizationPct}%)`, {
        requestId: metric.requestId,
        steps: `${metric.stepsUsed}/${metric.maxStepsAllowed}`,
      });
    }
  },
  
  getStats() {
    const total = this.data.length;
    const hitLimit = this.data.filter(m => m.hitLimit).length;
    const avgUtilization = this.data.reduce((sum, m) => sum + m.utilizationPct, 0) / total;
    const avgSteps = this.data.reduce((sum, m) => sum + m.stepsUsed, 0) / total;
    
    return {
      totalRequests: total,
      limitHitRate: (hitLimit / total * 100).toFixed(1) + '%',
      avgUtilization: (avgUtilization * 100).toFixed(1) + '%',
      avgStepsUsed: avgSteps.toFixed(1),
    };
  },
};

// Usage in your agent wrapper
const monitoredRunAgent = async (prompt: string, context: AgentContext) => {
  const requestId = generateId();
  const startTime = Date.now();
  const maxSteps = 15;
  
  const result = await runAgent(prompt, context, { maxSteps });
  
  const duration = Date.now() - startTime;
  const cost = estimateCost(result.usage);
  
  metricsCollector.record({
    requestId,
    taskType: classifyTaskType(prompt),
    stepsUsed: result.steps,
    maxStepsAllowed: maxSteps,
    utilizationPct: result.steps / maxSteps,
    hitLimit: result.steps >= maxSteps,
    cost,
    duration,
  });
  
  return result;
};

// Weekly stats review
console.log('Agent Step Usage Stats (Last 7 Days):', metricsCollector.getStats());
// Output:
// {
//   totalRequests: 1542,
//   limitHitRate: '3.2%',
//   avgUtilization: '52.1%',
//   avgStepsUsed: '7.8'
// }
```

### Dashboard Metrics

**Key metrics to track**:

1. **Limit Hit Rate**: % of requests that hit max steps (target: <5%)
2. **Average Utilization**: stepsUsed / maxSteps (target: 40-70%)
3. **Step Distribution**: Histogram of steps used
4. **Cost per Task Type**: Track spending by complexity
5. **Time to Completion**: Correlate steps with latency

**Alert thresholds**:
- üö® Critical: Limit hit rate >10% (limits too low)
- ‚ö†Ô∏è Warning: Avg utilization <30% (limits too high, wasting potential)
- ‚ö†Ô∏è Warning: Avg utilization >80% (limits too tight, tasks struggling)

---

## Best Practices

### DO ‚úÖ

1. **Set conservative defaults**: Start with 10-15 steps, increase only if needed
2. **Monitor utilization**: Track how often limits are hit and adjust
3. **Provide partial results**: Return checkpoint data even if limit is hit
4. **Log reasoning**: Help debug why agent used so many steps
5. **Task-specific limits**: Different limits for different task types
6. **Two-tier limits**: Soft warning limit + hard stop limit
7. **Cost protection**: Max steps prevents runaway costs
8. **User feedback**: Tell users when their task exceeded limits

### DON'T ‚ùå

1. **Don't set arbitrarily high limits**: "1000 steps just to be safe" = $50+ costs
2. **Don't ignore limit hits**: They indicate deeper problems (unclear goals, stuck loops)
3. **Don't treat limits as success criteria**: Hitting limit means something went wrong
4. **Don't have one-size-fits-all**: Simple vs complex tasks need different limits
5. **Don't hide limit errors**: User should know their task was incomplete
6. **Don't forget to checkpoint**: Without checkpoints, hitting limit = total failure
7. **Don't disable limits in production**: Even for "trusted" users
8. **Don't skip telemetry**: Need data to tune limits over time

---

## Troubleshooting High Step Usage

### Diagnostic Checklist

When agents consistently hit max steps:

1. **Check goal clarity**: Is the success condition well-defined?
   ```typescript
   // ‚ùå BAD: Vague goal
   "Make the homepage better"
   
   // ‚úÖ GOOD: Specific goal
   "Add a hero section with title 'Welcome' and tagline 'Your AI Partner'"
   ```

2. **Review tool descriptions**: Are they clear about what each tool does?
   ```typescript
   // ‚ùå BAD: Ambiguous description
   description: 'Updates a page'
   
   // ‚úÖ GOOD: Clear description
   description: 'Updates page content and metadata. Returns updated page object or error if page not found.'
   ```

3. **Add validation**: Does each tool validate its success?
   ```typescript
   // ‚úÖ GOOD: Post-action validation
   const updatePage = async (pageId: string, content: string) => {
     await db.update({ id: pageId, content });
     
     // Validate update succeeded
     const updated = await db.findOne({ id: pageId });
     if (updated.content !== content) {
       throw new Error('Update failed: content mismatch');
     }
     
     return { success: true, page: updated };
   };
   ```

4. **Check for loops**: Is agent repeating same actions?
   ```typescript
   // Add loop detection
   const actionHistory: string[] = [];
   
   const onStepFinish = (step: any) => {
     const actionKey = `${step.toolName}:${JSON.stringify(step.args)}`;
     actionHistory.push(actionKey);
     
     // Detect repeated actions
     const lastThree = actionHistory.slice(-3);
     if (lastThree.every(a => a === actionKey)) {
       console.error('Loop detected: Same action repeated 3 times');
       throw new Error('Agent stuck in loop');
     }
   };
   ```

---

## Summary

### Key Takeaways

1. **Max steps are essential**: Prevent runaway costs and infinite loops
2. **Start conservative**: 10-15 steps for most tasks, adjust based on data
3. **Make them adaptive**: Different limits for different task complexities
4. **Track utilization**: Monitor if limits are too low or too high
5. **Provide feedback**: Tell users why agent stopped and what was completed
6. **Combine with other patterns**: Max steps + convergence detection + stuck prevention
7. **Checkpoint progress**: Don't lose all work when hitting limit

### Limit Selection Quick Reference

```typescript
const STEP_LIMITS = {
  simple: 5,       // Single action
  standard: 10,    // Default for most tasks
  medium: 15,      // Multi-step with some branching
  complex: 25,     // Workflows with many steps
  research: 40,    // Open-ended exploration
  maximum: 60,     // Absolute cap (review if needed)
};
```

---

## Research Citations

1. **Doinamerica** - "Agent stopped due to max iterations: fixes that work" (Sep 2025)  
   https://doinamerica.com/fr/agent-stopped-max-iterations-fix/

2. **LangChain** - "Max Iterations Documentation" (Sep 2025)  
   https://js.langchain.com/v0.1/docs/modules/agents/how_to/max_iterations/

3. **REFRAIN Paper** - "Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought" (Sep 2025)  
   https://arxiv.org/html/2510.10103v1

4. **Stopping Agents** - "Optimal Stopping in Conversations" (2025)  
   https://stoppingagents.com/

5. **MetricCoders** - "Defining Stopping Criteria in LLMs: A Practical Guide" (Mar 2025)  
   https://www.metriccoders.com/post/defining-stopping-criteria-in-large-language-models

---

**Next Steps**:

- Read [3.4.2 - Convergence Detection](./3.4.2-convergence.md) for identifying task completion
- Read [3.4.3 - Stuck Detection](./3.4.3-stuck-detection.md) for preventing infinite loops
- Read [3.4.5 - Early Exit Strategies](./3.4.5-early-exit.md) for optimizing step usage
