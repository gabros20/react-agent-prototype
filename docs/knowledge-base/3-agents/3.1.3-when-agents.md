# 3.1.3 - When to Use Agents vs Single LLM Calls

## TL;DR

Start with the simplest solution that works: single LLM calls (70% of use cases), then workflows (20%), then agents (10%). Agents add 2-5× cost and 10-15× latency—only use them when dynamic decision-making, multi-step tool chains, or error recovery justify the overhead.

- **Status**: ✅ Complete
- **Last Updated**: 2025-12-03
- **Prerequisites**: [3.1.1 Agent Definition](./3.1.1-agent-definition.md), [3.1.2 Agent Types](./3.1.2-agent-types.md)
- **Grounded In**: Anthropic Agent Guidelines (2024), Agentic ROI Framework (2025), AgentX (2025)

## Table of Contents

- [Overview](#overview)
- [The Problem](#the-problem-complexity-creep)
- [Core Concept](#core-concept)
- [Tier 1: Single LLM Calls](#tier-1-single-llm-calls)
- [Tier 2: Workflows](#tier-2-workflows-predefined-paths)
- [Tier 3: Agents](#tier-3-agents-dynamic-decision-making)
- [Cost-Benefit Analysis](#cost-benefit-analysis)
- [Framework Considerations](#framework-considerations)
- [Decision Matrix](#decision-matrix)
- [Anti-Patterns to Avoid](#anti-patterns-to-avoid)
- [Trade-offs & Considerations](#trade-offs--considerations)
- [Key Takeaways](#key-takeaways)
- [References](#references)

## Overview

Choosing between agents and simpler approaches is one of the most impactful architectural decisions in AI systems. The wrong choice leads to either over-engineering (expensive, slow agents for simple tasks) or under-engineering (brittle single calls for complex workflows).

**Key Research Findings** (2024-2025):

- **70% of use cases**: Solvable with well-crafted prompts + RAG (Anthropic, Dec 2024)
- **2-5× cost impact**: Agents vs single LLM calls due to multiple reasoning steps
- **60% failure rate**: Agent projects fail due to poor ROI assessment (Agentic ROI, May 2025)
- **40% abandonment**: Developers leaving LangChain due to abstraction complexity (2024-2025)
- **14.9% improvement**: AgentX over ReAct through in-flow optimization (Aug 2025)

**Date Verified**: December 2025

## The Problem: Complexity Creep

### The Overengineering Trap

Developers often reach for agents because they're exciting, not because they're necessary:

- **Premature optimization**: Building agentic systems before validating simple approaches work
- **Framework-driven design**: Choosing tools that push toward agents regardless of need
- **Underestimating costs**: API costs, latency, debugging time, maintenance burden

**Example**: A developer implements a full ReAct agent for message classification that could be solved with a single structured output call—paying $0.05 per request instead of $0.0002.

### The Underengineering Trap

Conversely, trying to force everything into single calls fails for complex tasks:

- **Prompt gymnastics**: Mega-prompts that try to do everything at once
- **Hallucinated actions**: LLM "pretending" to call APIs it can't actually access
- **Brittle pipelines**: Hard-coded sequences that break on edge cases

## Core Concept

### The Three-Tier Decision Framework

```
┌─────────────────────────────────────────────────────────────┐
│  TIER 1: Can this be solved with a single prompt?          │
│  ├─ YES → Use Single LLM Call                              │
│  └─ NO → Continue to Tier 2                                │
└─────────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  TIER 2: Can this be solved with a fixed workflow?         │
│  ├─ YES → Use Workflow (predefined steps)                  │
│  └─ NO → Continue to Tier 3                                │
└─────────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  TIER 3: Does this require dynamic decision-making?        │
│  ├─ YES → Use Agent (ReAct, Plan-and-Execute)              │
│  └─ NO → Reconsider requirements                           │
└─────────────────────────────────────────────────────────────┘
```

**Golden Rule**: Start with the simplest tier. Only increase complexity when the simpler tier demonstrably fails.

## Tier 1: Single LLM Calls

### When to Use

**Use single LLM calls when**:

1. Task is **well-defined** with clear inputs and outputs
2. **No external information** needed beyond the prompt
3. **Latency is critical** (real-time chat, live components)
4. **Cost constraints** are tight (high-volume operations)
5. **Process is linear** with no branching decisions

### Implementation Examples

**Text Classification**:

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

const classifyMessage = async (message: string) => {
  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    schema: z.object({
      category: z.enum(['support', 'sales', 'billing', 'other']),
      urgency: z.enum(['low', 'medium', 'high']),
      sentiment: z.enum(['positive', 'neutral', 'negative']),
    }),
    prompt: `Classify this customer message: "${message}"`,
  });
  return object;
};

// Cost: ~$0.0001 per call (150 input + 50 output tokens)
// Latency: 200-400ms
// Success rate: 95%+ with good examples
```

**Simple Q&A with RAG**:

```typescript
const answerQuestion = async (question: string) => {
  // Retrieve relevant context (vector search)
  const context = await vectorDb.search(question, { limit: 3 });

  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: `Context: ${context.join('\n\n')}

Question: ${question}

Answer based only on the provided context.`,
  });
  return text;
};

// Cost: ~$0.0003 per call (vector search + LLM)
// Latency: 400-700ms
// Accuracy: 85%+ with good retrieval
```

### Performance Characteristics

| Metric              | Single LLM Call | Agent (ReAct)    | Difference      |
| ------------------- | --------------- | ---------------- | --------------- |
| **Cost per request**| $0.0001-0.015   | $0.005-0.20      | **5-20× more**  |
| **Latency (p95)**   | 200-5000ms      | 3-30 seconds     | **10-15× more** |
| **Success rate**    | 85-95%          | 70-100%          | Task-dependent  |
| **Predictability**  | High            | Medium           |                 |
| **Debugging**       | Easy            | Complex          |                 |

## Tier 2: Workflows (Predefined Paths)

### When to Use

**Use workflows when**:

1. **Steps are known** and can be defined upfront
2. **Branching is predictable** (if-else logic suffices)
3. **Consistency is critical** (same process every time)
4. **Debugging is important** (clear execution trace)
5. **Human approval gates** are needed at specific steps

### Implementation Examples

**Order Processing Pipeline**:

```typescript
const processOrder = async (orderId: string) => {
  // Step 1: Validate order
  const order = await validateOrder(orderId);
  if (!order.valid) {
    return { status: 'invalid', reason: order.reason };
  }

  // Step 2: Check inventory
  const inventory = await checkInventory(order.items);
  if (!inventory.available) {
    return { status: 'out_of_stock', items: inventory.missing };
  }

  // Step 3: Process payment
  const payment = await processPayment(order.total);
  if (!payment.success) {
    return { status: 'payment_failed', error: payment.error };
  }

  // Step 4: Generate shipping label
  const shipping = await generateShippingLabel(order);

  // Step 5: Send confirmation
  await sendConfirmationEmail(order.customer, shipping.trackingNumber);

  return { status: 'completed', trackingNumber: shipping.trackingNumber };
};

// Cost: $0.001-0.01 (mostly API calls, minimal LLM usage)
// Latency: 1-5 seconds (predictable)
// Success rate: 99%+ (deterministic logic)
```

**Document Review with Human-in-the-Loop**:

```typescript
const reviewDocument = async (docId: string, reviewerId: string) => {
  // Step 1: Extract content
  const doc = await fetchDocument(docId);

  // Step 2: AI analysis
  const { object: analysis } = await generateObject({
    model: openai('gpt-4o'),
    schema: z.object({
      compliance: z.object({
        passed: z.boolean(),
        issues: z.array(z.string()),
      }),
      readability: z.object({
        score: z.number(),
        suggestions: z.array(z.string()),
      }),
    }),
    prompt: `Analyze this document for compliance and readability: ${doc.content}`,
  });

  // Step 3: Human approval (if issues found)
  if (analysis.compliance.issues.length > 0) {
    const approval = await requestApproval(reviewerId, {
      document: doc,
      issues: analysis.compliance.issues,
    });

    if (!approval.approved) {
      return { status: 'rejected', reason: approval.reason };
    }
  }

  // Step 4: Finalize
  await markAsApproved(docId);
  return { status: 'approved', analysis };
};
```

### Workflow vs Agent Comparison

| Aspect              | Workflow                     | Agent                        |
| ------------------- | ---------------------------- | ---------------------------- |
| **Flexibility**     | Low (fixed paths)            | High (dynamic decisions)     |
| **Cost**            | Low ($0.001-0.10)            | Medium-High ($0.01-1.00)     |
| **Latency**         | Predictable (1-10s)          | Variable (3-60s)             |
| **Debugging**       | Easy (clear execution trace) | Hard (non-deterministic)     |
| **Maintenance**     | Easy (explicit code)         | Medium (prompt engineering)  |
| **When to use**     | Known steps, fixed logic     | Unknown steps, adaptive logic|

## Tier 3: Agents (Dynamic Decision-Making)

### When to Use

**Use agents when**:

1. **Solution path is unknown** upfront
2. **Dynamic decision-making** is required based on intermediate results
3. **Exploration is needed** (research, discovery, analysis)
4. **Flexibility is critical** (adapting to changing conditions)
5. **Multi-tool coordination** is complex

### Implementation Examples

**Research Assistant**:

```typescript
import { ToolLoopAgent, tool, stepCountIs } from 'ai';
import { z } from 'zod';

const researchAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  instructions: `You are a research assistant. Given a research question,
gather information from multiple sources, synthesize findings,
and provide a comprehensive answer with citations.`,
  tools: {
    webSearch: tool({
      description: 'Search the web for information',
      inputSchema: z.object({ query: z.string() }),
      execute: async ({ query }) => searchWeb(query),
    }),
    fetchUrl: tool({
      description: 'Fetch content from a URL',
      inputSchema: z.object({ url: z.string() }),
      execute: async ({ url }) => fetchUrl(url),
    }),
    searchPapers: tool({
      description: 'Search academic papers',
      inputSchema: z.object({ query: z.string() }),
      execute: async ({ query }) => searchPapers(query),
    }),
  },
  stopWhen: stepCountIs(15),
});

// Example execution path (dynamic, agent decides):
// 1. THINK: Need to search recent papers and news
// 2. ACT: webSearch("quantum computing advances 2025")
// 3. OBSERVE: Found 3 relevant articles
// 4. THINK: Need more technical details
// 5. ACT: searchPapers("quantum computing 2025")
// 6. OBSERVE: Found 5 recent papers
// 7. THINK: Should fetch full content of top paper
// 8. ACT: fetchUrl("https://arxiv.org/...")
// 9. OBSERVE: Retrieved paper content
// 10. THINK: Have enough information to synthesize
// 11. FINISH: Generate comprehensive answer with citations

// Cost: $0.15-0.50 per research query (5-10 tool calls)
// Latency: 10-30 seconds
// Quality: High (comprehensive, cited)
```

**Debugging Assistant**:

```typescript
const debugAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  instructions: `You are a debugging assistant. Analyze errors,
investigate code, and suggest fixes.`,
  tools: {
    readFile: tool({
      description: 'Read file contents',
      inputSchema: z.object({ path: z.string() }),
      execute: async ({ path }) => readFile(path),
    }),
    searchCode: tool({
      description: 'Search codebase for pattern',
      inputSchema: z.object({ pattern: z.string() }),
      execute: async ({ pattern }) => searchCode(pattern),
    }),
    runTests: tool({
      description: 'Run test suite',
      inputSchema: z.object({
        testPath: z.string().optional(),
        filter: z.string().optional(),
      }),
      execute: async ({ testPath, filter }) => runTests(testPath, filter),
    }),
  },
  stopWhen: stepCountIs(12),
});

// Cost: $0.10-0.30 per debugging session (6-12 tool calls)
// Latency: 15-45 seconds
// Success rate: 70-85% (requires iteration)
```

### Agent Performance by Task Complexity

| Task Complexity | Tool Calls | Cost per Request | Latency (p95) | Success Rate |
| --------------- | ---------- | ---------------- | ------------- | ------------ |
| **Simple**      | 1-2        | $0.01-0.05       | 3-8s          | 90-95%       |
| **Complex**     | 3-8        | $0.05-0.20       | 8-20s         | 80-90%       |
| **Multi-Step**  | 8-20       | $0.20-1.00       | 20-60s        | 70-85%       |

## Cost-Benefit Analysis

### Total Cost of Ownership

**Single LLM Call**:

```
Direct costs:
- API calls: $0.0001-0.015 per request
- Infrastructure: Minimal (stateless)

Indirect costs:
- Development: Low (simple prompts)
- Maintenance: Low (stable prompts)
- Debugging: Low (straightforward)

TOTAL: $0.0001-0.02 per request + minimal overhead
```

**Workflow**:

```
Direct costs:
- API calls: $0.001-0.10 per execution
- Infrastructure: Medium (state management)

Indirect costs:
- Development: Medium (explicit code)
- Maintenance: Medium (code changes)
- Debugging: Low (clear traces)

TOTAL: $0.001-0.15 per execution + medium overhead
```

**Agent**:

```
Direct costs:
- API calls: $0.01-1.00 per task (5-20 calls)
- Infrastructure: High (state, memory, checkpointing)

Indirect costs:
- Development: High (prompt engineering, tools)
- Maintenance: High (prompt drift, tool updates)
- Debugging: High (non-deterministic behavior)

TOTAL: $0.01-1.50 per task + high overhead
```

### ROI Calculation Example

**Scenario**: Customer support automation (10,000 tickets/month)

| Approach | Monthly Cost | Satisfaction | Response Time |
| -------- | ------------ | ------------ | ------------- |
| **Human only** | $62,500 | 90% | 24 hours |
| **Single LLM + Routing** | $19,252 | 88% | 1 hour |
| **Full Agent** | $15,833 | 82% | Immediate |

**Recommendation**: Single LLM + Routing wins for most organizations (68% savings, minimal complexity drop).

## Framework Considerations

### When NOT to Use Frameworks

From "Why we no longer use LangChain" (Octomind, 2025):

**Problems with high-level abstractions**:

1. **Complexity**: Multiple abstraction layers make debugging hard
2. **Rigidity**: Difficult to customize for unique use cases
3. **Churn**: Fast-evolving landscape makes frameworks outdated quickly
4. **Over-engineering**: Simple tasks become complex

**Better approach**: Modular building blocks

```typescript
// ❌ BAD: Over-abstraction with framework
import { ConversationalRetrievalQAChain } from 'langchain/chains';

const chain = ConversationalRetrievalQAChain.fromLLM(llm, retriever, {
  returnSourceDocuments: true,
});
const result = await chain.call({ question, chat_history });

// ✅ GOOD: Direct implementation with AI SDK
const answerQuestion = async (question: string, chatHistory: Message[]) => {
  const context = await vectorDb.search(question, { limit: 3 });

  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    messages: [
      ...chatHistory,
      {
        role: 'user',
        content: `Context: ${context.join('\n\n')}\n\nQuestion: ${question}`,
      },
    ],
  });

  return { answer: text, sources: context };
};
```

### When Frameworks Add Value

**Use frameworks when**:

1. **Rapid prototyping** (fast exploration)
2. **Standard patterns** (your use case matches framework's design)
3. **Team experience** (team already knows the framework)
4. **Community support** (active ecosystem with plugins)

## Decision Matrix

### Quick Reference

| Factor                    | Single LLM | Workflow | Agent    |
| ------------------------- | ---------- | -------- | -------- |
| **Task complexity**       | Low        | Medium   | High     |
| **Steps required**        | 1          | 2-5      | 5-20     |
| **Decision points**       | None       | Few      | Many     |
| **Cost per request**      | $0.0001    | $0.01    | $0.10    |
| **Latency**               | 200ms-2s   | 2-10s    | 10-60s   |
| **Development time**      | Hours      | Days     | Weeks    |
| **Debugging difficulty**  | Easy       | Medium   | Hard     |
| **Predictability**        | High       | High     | Low      |

### Use Case Mapping

| Use Case                          | Recommendation   | Reasoning                                    |
| --------------------------------- | ---------------- | -------------------------------------------- |
| **Chatbot (FAQ)**                 | Single LLM + RAG | Fixed knowledge base, fast responses         |
| **Customer support routing**      | Workflow         | Classification + deterministic routing       |
| **Code review automation**        | Workflow         | Fixed checks (linting, security, style)      |
| **Research assistant**            | Agent            | Open-ended exploration, multi-source         |
| **CMS content management**        | Agent            | Dynamic decisions, multi-step operations     |
| **Email classification**          | Single LLM       | Simple categorization, high volume           |
| **Invoice processing**            | Workflow         | Extract → Validate → Store (fixed)           |
| **Debugging assistant**           | Agent            | Iterative investigation, adaptive            |
| **Content generation**            | Single LLM       | Self-contained task, no external data        |
| **Personalized recommendations**  | Single LLM + RAG | User data + catalog → suggestions            |

## Anti-Patterns to Avoid

### Anti-Pattern 1: Agent for Simple Tasks

```typescript
// ❌ BAD: Agent overkill for classification
const classifyWithAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  tools: {
    classify: tool({
      description: 'Classify message',
      inputSchema: z.object({ message: z.string() }),
      execute: async ({ message }) => classifyMessage(message),
    }),
  },
});
// Cost: $0.01-0.05, Latency: 3-8s

// ✅ GOOD: Direct LLM call
const classifyDirect = async (message: string) => {
  return await generateObject({
    model: openai('gpt-4o-mini'),
    schema: classificationSchema,
    prompt: `Classify: ${message}`,
  });
};
// Cost: $0.0001 (100× cheaper), Latency: 200-500ms (15× faster)
```

### Anti-Pattern 2: Single Call for Multi-Step Tasks

```typescript
// ❌ BAD: Trying to do everything in one prompt
const analyzeAndFixBug = async (error: string) => {
  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Given this error: "${error}"
             1. Read the relevant files
             2. Identify the root cause
             3. Suggest a fix
             4. Write test cases

             Do all of this and give me the complete solution.`,
  });
  // Problem: Model can't actually read files or run tests!
  return text;
};

// ✅ GOOD: Agent with actual file access
const debugAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  tools: {
    readFile: readFileTool,
    searchCode: searchCodeTool,
    runTests: runTestsTool,
  },
});
```

### Anti-Pattern 3: Workflow for Unknown Paths

```typescript
// ❌ BAD: Trying to predict all branches
const handleCustomerRequest = async (request: string) => {
  if (request.includes('refund')) {
    return processRefund(request);
  } else if (request.includes('exchange')) {
    return processExchange(request);
  } else if (request.includes('complaint')) {
    return processComplaint(request);
  } else {
    // What about: "I'm unhappy with my order"?
    return "Sorry, I don't understand";
  }
};

// ✅ GOOD: Agent for dynamic routing
const supportAgent = new ToolLoopAgent({
  model: openai('gpt-4o'),
  tools: {
    processRefund: refundTool,
    processExchange: exchangeTool,
    processComplaint: complaintTool,
    gatherInfo: gatherInfoTool,
  },
});
// Agent can understand intent and choose appropriate action
```

## Trade-offs & Considerations

### Migration Strategies

**Upgrading: Single Call → Agent**

Signals you need an agent:

1. Prompt is becoming huge (>2000 tokens)
2. You're simulating tools in the prompt
3. Results are inconsistent due to lack of real data
4. Hitting context limits with all the information
5. Users ask follow-up questions requiring memory

**Downgrading: Agent → Workflow**

Signals you can simplify:

1. Agent always uses same sequence of tools
2. No branching decisions are being made
3. Latency is a complaint from users
4. Cost is higher than expected for simple tasks

### Latency Comparison

| Approach           | Best Case | Average | p95    | p99    |
| ------------------ | --------- | ------- | ------ | ------ |
| **Single call**    | 200ms     | 800ms   | 2s     | 5s     |
| **Workflow**       | 1s        | 3s      | 8s     | 15s    |
| **Agent (simple)** | 3s        | 8s      | 20s    | 45s    |
| **Agent (complex)**| 10s       | 30s     | 60s    | 120s   |

### Latency Optimization

**For Single Calls**: Use streaming for perceived speed

```typescript
const { textStream } = await streamText({
  model: openai('gpt-4o-mini'),
  prompt,
});

for await (const chunk of textStream) {
  process.stdout.write(chunk);
}
// Perceived latency: 200-500ms (first token)
// Total latency: 2-5s (full response)
```

**For Agents**: Use lightweight models for reasoning

```typescript
const agent = new ToolLoopAgent({
  model: openai('gpt-4o-mini'), // Fast, cheap for reasoning
  tools: {
    complexAnalysis: tool({
      description: 'Perform deep analysis (uses GPT-4o)',
      inputSchema: z.object({ content: z.string() }),
      execute: async ({ content }) => {
        // Use expensive model only for complex operations
        return await generateText({
          model: openai('gpt-4o'),
          prompt: `Deep analysis: ${content}`,
        });
      },
    }),
  },
});
// Reasoning: GPT-4o-mini (200-400ms each)
// Complex operations: GPT-4o (2-5s each)
```

## Key Takeaways

1. **Start simple**: Single LLM call (70% of use cases) → Workflow (20%) → Agent (10%)
2. **Measure ROI**: Calculate total cost, not just API costs
3. **Prototype quickly**: Test with single calls before building agents
4. **Optimize progressively**: Add complexity only when simpler approaches fail
5. **Choose frameworks wisely**: Direct implementation often beats abstractions
6. **Monitor production**: Track cost, latency, and success rates continuously

**Golden Rules**:

| Metric     | Single LLM | Workflow  | Agent     |
| ---------- | ---------- | --------- | --------- |
| **Cost**   | $0.0001    | $0.01     | $0.10     |
| **Latency**| 500ms      | 5s        | 20s       |
| **Success**| 90%        | 95%       | 85%       |
| **ROI**    | Best       | Good      | Moderate  |

**Best Practices Checklist**:

For Single LLM Calls:

- [ ] Task is well-defined with clear inputs/outputs
- [ ] No external data needed beyond prompt
- [ ] Latency requirement < 5 seconds
- [ ] Prompt is < 2000 tokens

For Workflows:

- [ ] Steps are known upfront
- [ ] Branching is predictable
- [ ] Each step can be independently tested
- [ ] Human approval gates at specific steps

For Agents:

- [ ] Solution path is unknown upfront
- [ ] Dynamic decision-making is required
- [ ] Multiple tools must be coordinated
- [ ] Cost budget is $0.10-1.00 per task

## References

1. **Anthropic** (2024). "Building Effective Agents". https://www.anthropic.com/research/building-effective-agents
2. **AgentX Paper** (2025). "Towards Orchestrating Robust Agentic Workflow Patterns with FaaS". arXiv.
3. **Agentic ROI Paper** (2025). "The Real Barrier to LLM Agent Usability is Agentic ROI". arXiv.
4. **Octomind** (2025). "Why we no longer use LangChain for building our AI agents". https://octomind.dev/blog/why-we-no-longer-use-langchain
5. **AI SDK 6 Documentation** (2025). Vercel. https://v6.ai-sdk.dev/docs/foundations/agents

**Related Topics**:

- [3.1.1 Agent Definition](./3.1.1-agent-definition.md) - Core agent concepts
- [3.1.2 Agent Types](./3.1.2-agent-types.md) - Agent complexity spectrum
- [3.2.1 ReAct Loop](./3.2.1-react-loop.md) - Agent execution pattern

**Layer Index**: [Layer 3: Agent Architecture](../AI_KNOWLEDGE_BASE_TOC.md#layer-3)
