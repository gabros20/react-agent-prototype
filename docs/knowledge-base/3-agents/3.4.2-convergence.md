# 3.4.2 - Convergence Detection

> **Layer**: Agent Architecture | **Topic**: Loop Control
> **Status**: Verified | **Last Updated**: December 2025
> **Prerequisites**: [3.4.1 Max Steps Limits](./3.4.1-max-steps.md)
> **Grounded In**: ReflAct, HALT-CoT, AI SDK v6 `stopWhen` conditions

## TL;DR

**Convergence detection identifies when an agent has successfully completed its task and should stop.** Unlike stuck detection (which catches failures) and max steps (which enforces limits), convergence detection recognizes **positive completion signals** that indicate the agent achieved its goal.

**Key Takeaways**
- **96.5% of tasks converge within 3 iterations** with proper verification
- **Multi-criteria convergence** outperforms single-signal approaches by 40%
- **Goal-state verification** achieves 93.3% success rate (ReflAct, May 2025)
- **Entropy-based stopping** reduces tokens by 15-30% while maintaining accuracy
- **AI SDK v6** uses `stopWhen` combinators for custom convergence conditions

## Why It Matters

**The Problem**

Without convergence detection, agents waste resources:

```typescript
// ❌ WITHOUT CONVERGENCE DETECTION
// Task: "Create a page titled 'About Us'"

// Step 1: cms_createPage({ title: 'About Us' }) → Success!
// Step 2: cms_getPage('about-us') → Verify it exists... yes!
// Step 3: Agent keeps "thinking" → unnecessary
// Step 4: Agent tries to add more content → not asked
// Step 5-15: Agent continues working on already-completed task

// Problem: Task was done at step 2, but agent used 15 steps
// Cost: $0.75 wasted (13 unnecessary steps)
```

**Root causes:**
1. **No clear completion signal** - Agent doesn't know when to stop
2. **Missing verification** - No confirmation goal was achieved
3. **Over-optimization** - Agent keeps "improving" beyond requirements

**Research Evidence**

| Finding | Source | Year |
|---------|--------|------|
| **96.5%** of tasks converge within 3 iterations | Plan Verification | 2025 |
| **ReflAct** achieves 93.3% success rate (27.7% improvement) | arXiv | May 2025 |
| **Entropy-based stopping** reduces tokens by 15-30% | HALT-CoT | Jul 2025 |
| **Multi-criteria** outperforms single-signal by 40% | Industry research | 2025 |

## Core Concepts

### Convergence vs. Other Stopping Mechanisms

```
┌─────────────────────────────────────────────────────────────┐
│                    STOPPING MECHANISMS                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  CONVERGENCE (Positive)                                     │
│  └─ Task complete, goal achieved                            │
│                                                              │
│  STUCK DETECTION (Negative)                                 │
│  └─ Unproductive loops, failures                            │
│                                                              │
│  MAX STEPS (Limit)                                          │
│  └─ Exceeded iteration limit                                │
│                                                              │
│  EARLY EXIT (Adaptive)                                      │
│  └─ Sufficient progress, efficiency                         │
│                                                              │
└─────────────────────────────────────────────────────────────┘

Best Practice: Use all four together for robust agent control
```

### Convergence Signals

```
Signal Type              Reliability     Use Case
───────────────────────────────────────────────────
Explicit Finish Tool     High           All tasks
Goal-State Verification  Very High      Execution tasks
Information Sufficiency  Medium         Research tasks
Entropy-Based           Medium          Answer generation
No Tool Calls           Low             Simple queries
```

## Implementation Patterns

### Explicit Completion Marker

```typescript
import { tool } from 'ai';
import { z } from 'zod';

export const task_finish = tool({
  description: `Call this when you have completed the task and have a final answer.
                Only call this when you are certain the task is complete.`,

  inputSchema: z.object({
    answer: z.string().describe('The final answer or summary of work completed'),
    confidence: z.number().min(0).max(1).describe('Confidence in completion (0-1)'),
    workSummary: z.string().optional().describe('Summary of actions taken'),
  }),

  execute: async ({ answer, confidence, workSummary }) => {
    return { finished: true, answer, confidence, workSummary };
  },
});
```

**Usage with AI SDK v6:**

```typescript
import { generateText, stepCountIs, hasToolCall } from 'ai';

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: userMessage,
  tools: { ...allTools, task_finish },

  // Stop when finish tool is called OR max steps reached
  stopWhen: [hasToolCall('task_finish'), stepCountIs(15)],

  experimental_context: context,
});

// Check how agent stopped
const finishCall = result.steps
  .flatMap(s => s.toolCalls || [])
  .find(tc => tc.toolName === 'task_finish');

if (finishCall) {
  console.log('Agent completed with confidence:', finishCall.args.confidence);
}
```

### Goal-State Verification

```typescript
interface TaskGoal {
  type: 'create' | 'update' | 'delete' | 'retrieve' | 'analyze';
  target: string;
  successCriteria: SuccessCriterion[];
}

interface SuccessCriterion {
  field: string;
  operator: 'equals' | 'contains' | 'exists' | 'matches';
  expectedValue: any;
}

export class GoalStateVerifier {
  async verifyConvergence(
    goal: TaskGoal,
    ctx: AgentContext
  ): Promise<ConvergenceResult> {
    const results = await Promise.all(
      goal.successCriteria.map(criterion =>
        this.checkCriterion(criterion, goal.target, ctx)
      )
    );

    const allMet = results.every(r => r.met);
    const confidence = results.reduce((sum, r) => sum + r.confidence, 0) / results.length;

    return { converged: allMet, confidence, criteriaResults: results };
  }

  private async checkCriterion(
    criterion: SuccessCriterion,
    target: string,
    ctx: AgentContext
  ): Promise<CriterionResult> {
    const currentState = await ctx.cmsService.getPage(target, 'full');

    if (!currentState) {
      return { met: false, confidence: 1.0, details: `Target "${target}" not found` };
    }

    const value = currentState[criterion.field as keyof typeof currentState];
    let met = false;

    switch (criterion.operator) {
      case 'equals':
        met = value === criterion.expectedValue;
        break;
      case 'contains':
        met = String(value).includes(criterion.expectedValue);
        break;
      case 'exists':
        met = value !== null && value !== undefined;
        break;
      case 'matches':
        met = new RegExp(criterion.expectedValue).test(String(value));
        break;
    }

    return {
      met,
      confidence: 1.0,
      details: `${criterion.field} ${criterion.operator} ${criterion.expectedValue}: ${met ? 'PASS' : 'FAIL'}`,
    };
  }
}
```

### Multi-Criteria Convergence

```typescript
interface ConvergenceScore {
  signal: string;
  score: number;
  weight: number;
  details: string;
}

export class CompositeConvergenceDetector {
  private goalVerifier: GoalStateVerifier;
  private readonly threshold: number;

  constructor(threshold: number = 0.75) {
    this.threshold = threshold;
    this.goalVerifier = new GoalStateVerifier();
  }

  async checkConvergence(
    goal: TaskGoal,
    ctx: AgentContext,
    step: any
  ): Promise<CompositeResult> {
    const scores: ConvergenceScore[] = [];

    // 1. Goal-state verification (50% weight)
    const goalResult = await this.goalVerifier.verifyConvergence(goal, ctx);
    scores.push({
      signal: 'goal_state',
      score: goalResult.converged ? 1.0 : 0.0,
      weight: 0.5,
      details: `${goalResult.criteriaResults.filter(c => c.met).length}/${goalResult.criteriaResults.length} criteria met`,
    });

    // 2. Explicit finish signal (30% weight)
    const finishCalled = step.toolCalls?.some((c: any) => c.toolName === 'task_finish');
    scores.push({
      signal: 'explicit_finish',
      score: finishCalled ? 1.0 : 0.0,
      weight: 0.3,
      details: finishCalled ? 'Finish tool called' : 'No finish signal',
    });

    // 3. Agent confidence (20% weight)
    const agentConfidence = finishCalled
      ? step.toolCalls.find((c: any) => c.toolName === 'task_finish')?.args?.confidence || 0.5
      : 0.5;
    scores.push({
      signal: 'agent_confidence',
      score: agentConfidence,
      weight: 0.2,
      details: `Agent confidence: ${agentConfidence}`,
    });

    // Calculate weighted score
    const totalWeight = scores.reduce((sum, s) => sum + s.weight, 0);
    const overallScore = scores.reduce((sum, s) => sum + (s.score * s.weight), 0) / totalWeight;

    return { converged: overallScore >= this.threshold, overallScore, breakdown: scores };
  }
}
```

## Framework Integration

### AI SDK v6 Pattern

```typescript
import { generateText, stepCountIs, hasToolCall } from 'ai';
import { openai } from '@ai-sdk/openai';

// Custom stop condition with convergence detection
const result = await generateText({
  model: openai('gpt-4o'),
  prompt: userMessage,
  tools: { ...allTools, task_finish },

  // Multiple convergence conditions
  stopWhen: [
    stepCountIs(20),
    hasToolCall('task_finish'),
  ],

  experimental_context: context,

  // Optional: Track convergence progress
  experimental_onStepFinish: async (step) => {
    const detector = new CompositeConvergenceDetector();
    const result = await detector.checkConvergence(goal, context, step);

    if (result.converged) {
      console.log('Convergence detected:', result.overallScore);
    }
  },
});
```

### Production Integration

```typescript
export interface ConvergenceConfig {
  method: 'explicit' | 'goal_state' | 'multi_criteria';
  threshold: number;
  goal?: TaskGoal;
}

export async function runAgentWithConvergence(
  prompt: string,
  context: AgentContext,
  config: ConvergenceConfig = { method: 'multi_criteria', threshold: 0.75 }
): Promise<ConvergenceResult> {
  const detector = new CompositeConvergenceDetector(config.threshold);
  let convergenceResult: CompositeResult | null = null;

  const result = await generateText({
    model: openai('gpt-4o'),
    prompt,
    tools: { ...getAllTools(), task_finish },
    stopWhen: [stepCountIs(20), hasToolCall('task_finish')],
    experimental_context: context,

    experimental_onStepFinish: async (step) => {
      if (config.goal) {
        convergenceResult = await detector.checkConvergence(config.goal, context, step);

        if (convergenceResult.converged) {
          context.logger?.info('Convergence detected', {
            score: convergenceResult.overallScore,
            breakdown: convergenceResult.breakdown,
          });
        }
      }
    },
  });

  return {
    success: true,
    text: result.text,
    steps: result.steps.length,
    convergedEarly: convergenceResult?.converged || false,
    convergenceScore: convergenceResult?.overallScore,
  };
}
```

## Testing Strategy

### Unit Testing Convergence Detection

```typescript
import { describe, it, expect, vi } from 'vitest';
import { GoalStateVerifier } from '../goal-verifier';

describe('GoalStateVerifier', () => {
  const createMockContext = () => ({
    cmsService: { getPage: vi.fn() },
  });

  it('detects convergence when all criteria met', async () => {
    const ctx = createMockContext();
    ctx.cmsService.getPage.mockResolvedValue({
      title: 'About Us',
      content: 'Welcome to our hero section',
      status: 'published',
    });

    const verifier = new GoalStateVerifier();
    const result = await verifier.verifyConvergence(
      {
        type: 'create',
        target: 'about-page',
        successCriteria: [
          { field: 'title', operator: 'exists', expectedValue: true },
          { field: 'content', operator: 'contains', expectedValue: 'hero section' },
          { field: 'status', operator: 'equals', expectedValue: 'published' },
        ],
      },
      ctx as any
    );

    expect(result.converged).toBe(true);
    expect(result.confidence).toBe(1.0);
  });

  it('returns partial convergence for partial criteria', async () => {
    const ctx = createMockContext();
    ctx.cmsService.getPage.mockResolvedValue({
      title: 'About Us',
      content: 'Some other content',
      status: 'draft',
    });

    const verifier = new GoalStateVerifier();
    const result = await verifier.verifyConvergence(
      {
        type: 'create',
        target: 'about-page',
        successCriteria: [
          { field: 'title', operator: 'exists', expectedValue: true },
          { field: 'content', operator: 'contains', expectedValue: 'hero section' },
          { field: 'status', operator: 'equals', expectedValue: 'published' },
        ],
      },
      ctx as any
    );

    expect(result.converged).toBe(false);
    expect(result.criteriaResults.filter(c => c.met)).toHaveLength(1);
  });
});
```

## Common Pitfalls

### ❌ Avoid

| Pitfall | Problem | Solution |
|---------|---------|----------|
| Single signal reliance | Premature finish calls | Use multi-criteria |
| No state verification | Trust agent's claim without proof | Always verify goal state |
| Ignoring false positives | Agent claims done but isn't | Track and investigate |
| Same criteria for all tasks | Different tasks need different signals | Configure per task type |
| No logging | Can't tune thresholds | Log every convergence check |
| Too strict threshold | Never converges | Start at 0.75, adjust down |

### ✅ Best Practices

1. **Use multiple signals** - Combine explicit + goal-state + information sufficiency
2. **Verify objectively** - Don't trust agent's judgment alone
3. **Track false positives** - Monitor premature convergence cases
4. **Task-specific tuning** - Different tasks need different criteria
5. **Return partial results** - Even if not fully converged
6. **Log convergence attempts** - Track why agent thought it was done

## Quick Reference

### AI SDK v6 Convergence Patterns

```typescript
import { generateText, stepCountIs, hasToolCall } from 'ai';

// Pattern 1: Explicit finish tool
stopWhen: [hasToolCall('task_finish'), stepCountIs(15)]

// Pattern 2: Combined (recommended)
stopWhen: [
  stepCountIs(20),
  hasToolCall('task_finish'),
]

// Convergence strategy by task type
const STRATEGIES = {
  execution: { method: 'goal_state', threshold: 0.9 },
  research: { method: 'multi_criteria', threshold: 0.75 },
  qa: { method: 'explicit', threshold: 0.8 },
};
```

### Target Metrics

| Metric | Target | Notes |
|--------|--------|-------|
| Early Convergence Rate | >50% | Saves costs |
| False Positive Rate | <5% | High accuracy |
| Avg Steps to Convergence | 60-70% of max | Efficiency |

## Related Topics

- [3.4.1 - Max Steps Limits](./3.4.1-max-steps.md) - Hard bounds on iterations
- [3.4.3 - Stuck Detection](./3.4.3-stuck-detection.md) - Preventing infinite loops
- [3.4.5 - Early Exit Strategies](./3.4.5-early-exit.md) - Optimizing step usage

## Research & Resources

**Academic Papers**
- Plan Verification - "Plan Verification for LLM-Based Embodied Task Agents" (2025) - [arxiv.org](https://arxiv.org/html/2509.02761v1)
- ReflAct - "World-Grounded Decision Making via Goal-State Reflection" (May 2025) - [arxiv.org](https://arxiv.org/abs/2505.15182)
- HALT-CoT - "Model-Agnostic Early Stopping for Chain-of-Thought" (Jul 2025) - [openreview.net](https://openreview.net/pdf?id=CX5c7C1CZa)
- VeriLA - "Human-Centered Evaluation Framework for LLM Agent Failures" (Mar 2025) - [arxiv.org](https://arxiv.org/pdf/2503.12651)

**Framework Documentation**
- AI SDK v6 - "Stop Conditions and Multi-Step Calls" (2025) - [v6.ai-sdk.dev](https://v6.ai-sdk.dev/docs/ai-sdk-core/generating-text#multi-step-calls)
