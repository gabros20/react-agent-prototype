# 3.4.5 - Early Exit Strategies (Adaptive Stopping)

## Overview

**Early exit strategies** enable agents to stop reasoning or acting before reaching max steps when sufficient progress has been made. Unlike convergence detection (which identifies task completion) and stuck detection (which catches failures), early exit strategies **optimize efficiency** by recognizing when additional steps yield diminishing returns.

This pattern is critical for production systems where cost and latency directly impact user experience and business ROI.

**Key Research Findings (2024-2025)**:

- **20-55% token reduction** with adaptive stopping while maintaining/improving accuracy (REFRAIN, Oct 2025)
- **35-61% sequence length reduction** with S-GRPO reinforcement learning (May 2025)
- **15-30% token savings** with entropy-based stopping (HALT-CoT, Jul 2025)
- **2-3× reasoning length reduction** with adaptive effort control (e1 model, Oct 2025)
- **Stop-RAG achieves 4.7× efficiency** vs fixed iterations in multi-hop QA (Oct 2025)

**Date Verified**: November 19, 2025

---

## When to Apply Early Exit

### Decision Matrix

| Scenario                          | Use Early Exit? | Reasoning                                    |
| --------------------------------- | --------------- | -------------------------------------------- |
| **Research with 20+ sources**     | ✅ YES          | Diminishing returns after ~10 sources        |
| **Multi-hop Q&A**                 | ✅ YES          | Sufficient info often found in 2-3 hops      |
| **Code generation with tests**    | ✅ YES          | Stop when tests pass, don't over-optimize    |
| **Critical financial transaction**| ❌ NO           | Must complete all validation steps           |
| **Simple lookup (1-2 steps)**     | ❌ NO           | Already fast, exit overhead not worth it     |
| **Creative writing (exploration)**| ⚠️ MAYBE        | May want full iteration for quality          |

**General rule**: Use early exit when:
1. Task has **clear intermediate checkpoints**
2. **Diminishing returns** are likely (research, exploration)
3. **Cost/latency optimization** is important
4. **Quality threshold** can be defined (not perfection)

---

## Early Exit Strategies

### 1. Answer Entropy (HALT-CoT Method)

**Best for**: Q&A, reasoning tasks where answer converges

```typescript
// From "HALT-CoT: Model-Agnostic Early Stopping for CoT"
// https://openreview.net/pdf?id=CX5c7C1CZa

import { generateObject } from 'ai';

class EntropyEarlyExit {
  private readonly ENTROPY_THRESHOLD = 0.3; // Low entropy = confident answer
  private readonly CHECK_EVERY_N_STEPS = 2; // Check every 2 reasoning steps
  
  async shouldExit(
    currentStep: number,
    currentAnswer: string
  ): Promise<{ exit: boolean; confidence: number; reasoning: string }> {
    // Only check periodically to save API calls
    if (currentStep % this.CHECK_EVERY_N_STEPS !== 0) {
      return { exit: false, confidence: 0, reasoning: 'Not time to check yet' };
    }
    
    // Sample multiple answers to measure consistency
    const samples = await this.sampleAnswers(currentAnswer, 5);
    
    // Calculate Shannon entropy
    const entropy = this.calculateEntropy(samples);
    const confidence = 1 - entropy;
    
    // Exit if entropy below threshold (high confidence)
    const exit = entropy < this.ENTROPY_THRESHOLD;
    
    return {
      exit,
      confidence,
      reasoning: exit 
        ? `High confidence (entropy: ${entropy.toFixed(3)})` 
        : `Still uncertain (entropy: ${entropy.toFixed(3)})`,
    };
  }
  
  private async sampleAnswers(baseAnswer: string, n: number): Promise<string[]> {
    const samples: string[] = [];
    
    for (let i = 0; i < n; i++) {
      const { object } = await generateObject({
        model: openai('gpt-4o-mini'),
        temperature: 0.8, // Higher temp for diversity
        schema: z.object({ answer: z.string() }),
        prompt: `Given reasoning so far: "${baseAnswer}"
                 What is your answer? Be concise.`,
      });
      samples.push(object.answer.toLowerCase().trim());
    }
    
    return samples;
  }
  
  private calculateEntropy(answers: string[]): number {
    // Count frequency of each answer
    const counts = new Map<string, number>();
    for (const ans of answers) {
      counts.set(ans, (counts.get(ans) || 0) + 1);
    }
    
    // Calculate probabilities
    const probs = Array.from(counts.values()).map(count => count / answers.length);
    
    // Shannon entropy: H = -Σ(p * log2(p))
    const entropy = -probs.reduce((sum, p) => 
      sum + (p > 0 ? p * Math.log2(p) : 0),
      0
    );
    
    // Normalize to [0, 1]
    const maxEntropy = Math.log2(answers.length);
    return entropy / maxEntropy;
  }
}

// Usage
const earlyExit = new EntropyEarlyExit();

const onStepFinish = async (step: any) => {
  if (step.text) {
    const result = await earlyExit.shouldExit(step.stepNumber, step.text);
    
    if (result.exit) {
      console.log(`✓ Early exit (step ${step.stepNumber}): ${result.reasoning}`);
      console.log(`  Confidence: ${(result.confidence * 100).toFixed(1)}%`);
      return { shouldContinue: false };
    }
  }
  
  return { shouldContinue: true };
};
```

**Results from HALT-CoT paper**:
- GSM8K: 92% accuracy with 25% token savings
- StrategyQA: 69.5% accuracy with 30% token savings
- Model-agnostic: works with GPT-4, Claude, Llama

**Pros**:
- ✅ Works across any model
- ✅ No training required
- ✅ Significant cost savings

**Cons**:
- ❌ Requires sampling multiple answers (overhead)
- ❌ Only for answer-generation tasks
- ❌ Not suitable for execution/action tasks

### 2. Adaptive Test-Time Compute (Learning When to Plan)

**Best for**: Planning tasks where some actions need more compute than others

```typescript
// From "Learning When to Plan" paper
// https://arxiv.org/abs/2509.03581

interface PlanningContext {
  stateComplexity: number; // 0-1 score
  recentSuccessRate: number; // 0-1 score
  stepsRemaining: number;
}

class AdaptivePlanningExit {
  private readonly COMPLEXITY_THRESHOLD = 0.7; // Plan only if complex
  private readonly SUCCESS_THRESHOLD = 0.8; // Skip planning if doing well
  
  shouldPlan(context: PlanningContext): { plan: boolean; reasoning: string } {
    // Skip planning if state is simple
    if (context.stateComplexity < this.COMPLEXITY_THRESHOLD) {
      return {
        plan: false,
        reasoning: 'State is simple, direct action sufficient',
      };
    }
    
    // Skip planning if recent actions successful
    if (context.recentSuccessRate > this.SUCCESS_THRESHOLD) {
      return {
        plan: false,
        reasoning: 'Recent actions successful, continue current strategy',
      };
    }
    
    // Plan if state is complex and struggling
    return {
      plan: true,
      reasoning: `Complex state (${context.stateComplexity.toFixed(2)}) and low success rate (${context.recentSuccessRate.toFixed(2)})`,
    };
  }
  
  estimateStateComplexity(state: any): number {
    // Heuristics for complexity
    let complexity = 0.0;
    
    // More objects = more complex
    const objectCount = state.objects?.length || 0;
    complexity += Math.min(objectCount / 10, 0.4); // Max 0.4 from objects
    
    // More relationships = more complex
    const relationshipCount = state.relationships?.length || 0;
    complexity += Math.min(relationshipCount / 5, 0.3); // Max 0.3 from relations
    
    // Goal distance = more complex
    const goalDistance = state.goalDistance || 0;
    complexity += Math.min(goalDistance / 20, 0.3); // Max 0.3 from distance
    
    return Math.min(complexity, 1.0);
  }
}

// Usage
const adaptivePlanning = new AdaptivePlanningExit();

const executeStep = async (state: any) => {
  const complexity = adaptivePlanning.estimateStateComplexity(state);
  const recentSuccessRate = calculateRecentSuccessRate(state.history);
  
  const decision = adaptivePlanning.shouldPlan({
    stateComplexity: complexity,
    recentSuccessRate,
    stepsRemaining: MAX_STEPS - currentStep,
  });
  
  if (decision.plan) {
    console.log(`Planning: ${decision.reasoning}`);
    await generatePlan(state);
  } else {
    console.log(`Skipping planning: ${decision.reasoning}`);
    await executeDirectAction(state);
  }
};
```

**Results from paper**:
- More sample-efficient in Crafter environment
- Agents can be guided by human-written plans
- Achieves complex objectives with less compute

**Pros**:
- ✅ Allocates compute where needed
- ✅ Faster for simple states
- ✅ Better sample efficiency

**Cons**:
- ❌ Requires complexity estimation heuristics
- ❌ May skip planning when needed (false negatives)
- ❌ Needs tuning per environment

### 3. Reinforcement Learning with Decaying Rewards (S-GRPO)

**Best for**: Training agents to exit early via RL

```typescript
// From "Serial-Group Decaying-Reward Policy Optimization"
// https://arxiv.org/abs/2505.07686

class DecayingRewardEarlyExit {
  private readonly BASE_REWARD = 1.0;
  private readonly DECAY_RATE = 0.1; // 10% penalty per extra step
  
  calculateReward(
    isCorrect: boolean,
    exitStep: number,
    maxSteps: number
  ): number {
    if (!isCorrect) {
      return 0; // No reward for wrong answer
    }
    
    // Reward decreases with each step
    // Early exit = higher reward
    // Formula: R = BASE_REWARD * (1 - DECAY_RATE * exitStep / maxSteps)
    const decayFactor = 1 - (this.DECAY_RATE * exitStep / maxSteps);
    return this.BASE_REWARD * Math.max(decayFactor, 0.1); // Min 10% reward
  }
  
  // Example: Train agent to prefer early exits
  async trainEpisode(prompt: string, maxSteps: number) {
    const trajectories: { exitStep: number; answer: string; reward: number }[] = [];
    
    // Try exiting at different steps
    for (let exitStep = 3; exitStep <= maxSteps; exitStep += 2) {
      const result = await this.runWithEarlyExit(prompt, exitStep);
      const isCorrect = this.verifyAnswer(result.answer);
      const reward = this.calculateReward(isCorrect, exitStep, maxSteps);
      
      trajectories.push({
        exitStep,
        answer: result.answer,
        reward,
      });
    }
    
    // Find best exit point
    const best = trajectories.reduce((max, t) => 
      t.reward > max.reward ? t : max
    );
    
    console.log(`Best exit step: ${best.exitStep} (reward: ${best.reward.toFixed(2)})`);
    return best;
  }
  
  private async runWithEarlyExit(prompt: string, maxSteps: number) {
    // Run agent for exactly maxSteps
    const result = await generateText({
      model: openai('gpt-4o'),
      prompt,
      tools: ALL_TOOLS,
      maxSteps,
    });
    
    return { answer: result.text };
  }
  
  private verifyAnswer(answer: string): boolean {
    // Implement answer verification logic
    return true; // Placeholder
  }
}
```

**Results from S-GRPO paper**:
- 35.4-61.1% reduction in sequence length
- 0.72-6.08% accuracy improvement
- Works across GSM8K, MATH-500 benchmarks

**Pros**:
- ✅ Learns optimal exit points
- ✅ Improves both speed and accuracy
- ✅ Generalizes to new problems

**Cons**:
- ❌ Requires training phase (not zero-shot)
- ❌ Complex to implement
- ❌ Needs reward function definition

### 4. Progress-Based Early Exit

**Best for**: Multi-step execution tasks (your CMS agent)

```typescript
// Track progress toward goal and exit when "good enough"
class ProgressBasedEarlyExit {
  private readonly SUFFICIENT_PROGRESS_THRESHOLD = 0.85; // 85% complete
  private readonly CHECK_FREQUENCY = 3; // Every 3 steps
  
  async shouldExit(
    currentStep: number,
    goal: TaskGoal,
    context: AgentContext
  ): Promise<{ exit: boolean; progress: number; reasoning: string }> {
    // Only check periodically
    if (currentStep % this.CHECK_FREQUENCY !== 0) {
      return { exit: false, progress: 0, reasoning: 'Not time to check' };
    }
    
    // Calculate progress toward goal
    const progress = await this.calculateProgress(goal, context);
    
    // Exit if sufficient progress made
    const exit = progress >= this.SUFFICIENT_PROGRESS_THRESHOLD;
    
    return {
      exit,
      progress,
      reasoning: exit
        ? `Sufficient progress (${(progress * 100).toFixed(1)}%)`
        : `Still working (${(progress * 100).toFixed(1)}% complete)`,
    };
  }
  
  private async calculateProgress(
    goal: TaskGoal,
    context: AgentContext
  ): number {
    // Check each success criterion
    const results = await Promise.all(
      goal.successCriteria.map(criterion =>
        this.checkCriterion(criterion, goal.target, context)
      )
    );
    
    // Progress = % of criteria met
    const metCount = results.filter(r => r.met).length;
    return metCount / results.length;
  }
  
  private async checkCriterion(
    criterion: any,
    target: string,
    context: AgentContext
  ): Promise<{ met: boolean }> {
    // Verify criterion (simplified)
    const state = await context.fetchState(target);
    
    if (!state) return { met: false };
    
    // Check criterion
    switch (criterion.operator) {
      case 'exists':
        return { met: state[criterion.field] != null };
      case 'equals':
        return { met: state[criterion.field] === criterion.expectedValue };
      case 'contains':
        return { met: String(state[criterion.field]).includes(criterion.expectedValue) };
      default:
        return { met: false };
    }
  }
}

// Usage with your CMS agent
const progressExit = new ProgressBasedEarlyExit();

const goal: TaskGoal = {
  type: 'create',
  target: 'about-page',
  successCriteria: [
    { field: 'title', operator: 'exists', expectedValue: true },
    { field: 'content', operator: 'contains', expectedValue: 'hero section' },
    { field: 'status', operator: 'equals', expectedValue: 'published' },
  ],
};

const onStepFinish = async (step: any, context: AgentContext) => {
  const result = await progressExit.shouldExit(step.stepNumber, goal, context);
  
  if (result.exit) {
    console.log(`✓ Early exit: ${result.reasoning}`);
    return { shouldContinue: false };
  }
  
  console.log(`Progress: ${(result.progress * 100).toFixed(1)}%`);
  return { shouldContinue: true };
};
```

**Pros**:
- ✅ Objective progress measurement
- ✅ Prevents over-optimization
- ✅ Saves steps when "good enough"

**Cons**:
- ❌ Requires verifiable criteria
- ❌ May exit before perfection
- ❌ Overhead of checking progress

---

## Stop-RAG: Value-Based Retrieval Control

**Specialized for iterative RAG systems**

```typescript
// From "Stop-RAG: Value-Based Retrieval Control"
// https://www.arxiv.org/abs/2510.14337

interface RAGState {
  query: string;
  retrievedDocs: Document[];
  currentAnswer: string;
  iteration: number;
}

class StopRAGController {
  private readonly VALUE_THRESHOLD = 0.8; // Stop if retrieval value < 0.8
  
  async shouldRetrieveMore(state: RAGState): Promise<{ retrieve: boolean; reasoning: string }> {
    // Estimate value of next retrieval
    const value = await this.estimateRetrievalValue(state);
    
    const retrieve = value >= this.VALUE_THRESHOLD;
    
    return {
      retrieve,
      reasoning: retrieve
        ? `High value (${value.toFixed(2)}), retrieve more`
        : `Low value (${value.toFixed(2)}), current answer sufficient`,
    };
  }
  
  private async estimateRetrievalValue(state: RAGState): Promise<number> {
    // Factors that decrease retrieval value:
    // 1. Current answer confidence
    // 2. Diminishing returns from previous retrievals
    // 3. Document diversity saturation
    
    // Simplified heuristic
    const answerConfidence = await this.assessAnswerConfidence(state.currentAnswer);
    const diminishingReturns = 1 / (1 + state.iteration * 0.5); // Decays with iterations
    const documentDiversity = this.calculateDocumentDiversity(state.retrievedDocs);
    
    // Value = weighted combination
    const value = 
      0.4 * (1 - answerConfidence) + // More retrieval if low confidence
      0.4 * diminishingReturns +     // Less retrieval as iterations increase
      0.2 * documentDiversity;       // More retrieval if docs are diverse
    
    return Math.min(value, 1.0);
  }
  
  private async assessAnswerConfidence(answer: string): Promise<number> {
    // Use LLM to assess confidence
    const { object } = await generateObject({
      model: openai('gpt-4o-mini'),
      schema: z.object({
        confidence: z.number().min(0).max(1),
      }),
      prompt: `Rate your confidence in this answer (0-1): "${answer}"`,
    });
    
    return object.confidence;
  }
  
  private calculateDocumentDiversity(docs: Document[]): number {
    // Simple diversity: unique sources
    const uniqueSources = new Set(docs.map(d => d.source));
    return Math.min(uniqueSources.size / 10, 1.0); // Normalize to [0, 1]
  }
}

// Usage in RAG loop
const stopRAG = new StopRAGController();

const ragLoop = async (query: string) => {
  let state: RAGState = {
    query,
    retrievedDocs: [],
    currentAnswer: '',
    iteration: 0,
  };
  
  while (state.iteration < 5) { // Max 5 iterations
    // Decide if should retrieve more
    const decision = await stopRAG.shouldRetrieveMore(state);
    
    console.log(`Iteration ${state.iteration}: ${decision.reasoning}`);
    
    if (!decision.retrieve) {
      console.log('✓ Early exit from RAG loop');
      break;
    }
    
    // Retrieve and update state
    const newDocs = await retrieveDocuments(query);
    state.retrievedDocs.push(...newDocs);
    state.currentAnswer = await generateAnswer(query, state.retrievedDocs);
    state.iteration++;
  }
  
  return state.currentAnswer;
};
```

**Results from Stop-RAG paper**:
- 4.7× efficiency vs fixed iterations
- Outperforms confidence-based approaches
- Works with multi-hop Q&A

---

## Production Integration

### Unified Early Exit System

```typescript
// File: server/agent/early-exit.ts

export type EarlyExitStrategy = 
  | 'entropy' 
  | 'progress' 
  | 'adaptive_planning' 
  | 'stop_rag'
  | 'none';

export interface EarlyExitConfig {
  strategy: EarlyExitStrategy;
  checkEveryNSteps: number;
  threshold: number; // Strategy-specific threshold
  enabled: boolean;
}

export const DEFAULT_EARLY_EXIT_CONFIG: EarlyExitConfig = {
  strategy: 'progress',
  checkEveryNSteps: 3,
  threshold: 0.85, // 85% progress triggers exit
  enabled: true,
};

export class EarlyExitManager {
  private controller: any; // Strategy-specific controller
  
  constructor(private config: EarlyExitConfig) {
    this.controller = this.createController(config.strategy);
  }
  
  async shouldExit(
    step: any,
    context: AgentContext
  ): Promise<{ exit: boolean; reason: string; metrics: any }> {
    if (!this.config.enabled) {
      return { exit: false, reason: 'Early exit disabled', metrics: {} };
    }
    
    // Check periodically
    if (step.stepNumber % this.config.checkEveryNSteps !== 0) {
      return { exit: false, reason: 'Not time to check', metrics: {} };
    }
    
    // Delegate to strategy controller
    const result = await this.controller.evaluate(step, context, this.config);
    
    if (result.exit) {
      console.log(`✓ Early exit triggered: ${result.reason}`);
      console.log(`  Metrics:`, result.metrics);
    }
    
    return result;
  }
  
  private createController(strategy: EarlyExitStrategy) {
    switch (strategy) {
      case 'entropy':
        return new EntropyEarlyExit();
      case 'progress':
        return new ProgressBasedEarlyExit();
      case 'adaptive_planning':
        return new AdaptivePlanningExit();
      case 'stop_rag':
        return new StopRAGController();
      case 'none':
        return { evaluate: async () => ({ exit: false, reason: 'Disabled' }) };
      default:
        throw new Error(`Unknown strategy: ${strategy}`);
    }
  }
}

// Integration with agent
export const runAgentWithEarlyExit = async (
  prompt: string,
  context: AgentContext,
  config: EarlyExitConfig = DEFAULT_EARLY_EXIT_CONFIG
) => {
  const earlyExit = new EarlyExitManager(config);
  let exitedEarly = false;
  let exitReason = '';
  
  const onStepFinish = async (step: any) => {
    const result = await earlyExit.shouldExit(step, context);
    
    if (result.exit) {
      exitedEarly = true;
      exitReason = result.reason;
      return { shouldContinue: false };
    }
    
    return { shouldContinue: true };
  };
  
  const result = await generateText({
    model: openai('gpt-4o'),
    prompt,
    tools: ALL_TOOLS,
    maxSteps: 20,
    onStepFinish,
    experimental_context: context,
  });
  
  return {
    ...result,
    exitedEarly,
    exitReason,
    stepsSaved: exitedEarly ? (20 - result.steps) : 0,
  };
};
```

---

## Cost-Benefit Analysis

### Savings Calculation

```typescript
interface EarlyExitROI {
  totalRequests: number;
  earlyExitCount: number;
  avgStepsSaved: number;
  costPerStep: number; // $0.001 typical
  totalSavings: number;
  implementationCost: number; // One-time
  netROI: number;
}

function calculateEarlyExitROI(
  monthlyRequests: number,
  earlyExitRate: number, // 0.5 = 50% exit early
  avgStepsSaved: number, // e.g., 5 steps
  costPerStep: number = 0.001
): EarlyExitROI {
  const earlyExitCount = monthlyRequests * earlyExitRate;
  const totalStepsSaved = earlyExitCount * avgStepsSaved;
  const totalSavings = totalStepsSaved * costPerStep;
  const implementationCost = 500; // Hours to implement
  
  // ROI over 6 months
  const sixMonthSavings = totalSavings * 6;
  const netROI = sixMonthSavings - implementationCost;
  
  return {
    totalRequests: monthlyRequests,
    earlyExitCount,
    avgStepsSaved,
    costPerStep,
    totalSavings,
    implementationCost,
    netROI,
  };
}

// Example: 10,000 requests/month
const roi = calculateEarlyExitROI(10000, 0.5, 5);
console.log(`Monthly savings: $${roi.totalSavings.toFixed(2)}`);
console.log(`6-month net ROI: $${roi.netROI.toFixed(2)}`);
// Output:
// Monthly savings: $25.00
// 6-month net ROI: $-350.00 (break-even at ~20 months)
```

**When early exit is worth it**:
- High request volume (>50K/month)
- Expensive models (GPT-4, Claude Opus)
- Long-running tasks (>10 steps average)
- High early-exit rate (>40%)

---

## Best Practices

### DO ✅

1. **Check periodically**: Every 2-3 steps, not every step (overhead)
2. **Track metrics**: Monitor exit rate, steps saved, accuracy impact
3. **Tune thresholds**: Different tasks need different exit criteria
4. **Combine with convergence**: Early exit for efficiency, convergence for correctness
5. **Provide feedback**: Tell user when/why agent exited early
6. **A/B test**: Compare with/without early exit to validate savings
7. **Start conservative**: Higher thresholds initially, lower as confidence grows
8. **Log false exits**: Track cases where early exit degraded quality

### DON'T ❌

1. **Don't exit too aggressively**: Quality > cost savings
2. **Don't skip critical steps**: Some tasks require full execution
3. **Don't forget verification**: Validate answer before exiting
4. **Don't use for short tasks**: Overhead not worth it for 1-3 step tasks
5. **Don't ignore user preferences**: Some users want thorough over fast
6. **Don't apply uniformly**: Different tasks need different strategies
7. **Don't forget to measure**: Track actual savings and quality impact
8. **Don't exit on critical operations**: Financial, security, safety tasks

---

## Summary

### Key Takeaways

1. **20-55% cost savings** with proper early exit strategies
2. **Multi-modal strategies**: Different tasks need different exit criteria
3. **Quality vs. speed trade-off**: Must balance thoroughness with efficiency
4. **Periodic checking**: Check every 2-3 steps to minimize overhead
5. **Track ROI**: Monitor savings vs. quality impact
6. **Research-backed**: Multiple papers validate effectiveness

### Strategy Selection Guide

```typescript
const EARLY_EXIT_STRATEGY_GUIDE = {
  // Q&A tasks
  qa: {
    strategy: 'entropy',
    checkEveryNSteps: 2,
    threshold: 0.3, // Low entropy
    expectedSavings: '15-30%',
  },
  
  // Multi-step execution (CMS)
  execution: {
    strategy: 'progress',
    checkEveryNSteps: 3,
    threshold: 0.85, // 85% progress
    expectedSavings: '20-40%',
  },
  
  // Research/RAG
  research: {
    strategy: 'stop_rag',
    checkEveryNSteps: 1, // Check each retrieval
    threshold: 0.8, // Retrieval value
    expectedSavings: '30-50%',
  },
  
  // Planning tasks
  planning: {
    strategy: 'adaptive_planning',
    checkEveryNSteps: 1,
    threshold: 0.7, // Complexity
    expectedSavings: '25-45%',
  },
};
```

---

## Research Citations

1. **REFRAIN** - "Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought" (Oct 2025)  
   https://arxiv.org/html/2510.10103v1

2. **Learning When to Plan** - "Efficiently Allocating Test-Time Compute for LLM Agents" (Sep 2025)  
   https://arxiv.org/abs/2509.03581

3. **S-GRPO** - "Serial-Group Decaying-Reward Policy Optimization" (May 2025)  
   https://arxiv.org/abs/2505.07686

4. **HALT-CoT** - "Model-Agnostic Early Stopping via Answer Entropy" (Jul 2025)  
   https://openreview.net/pdf?id=CX5c7C1CZa

5. **Stop-RAG** - "Value-Based Retrieval Control for Iterative RAG" (Oct 2025)  
   https://www.arxiv.org/abs/2510.14337

6. **e1 Model** - "Learning Adaptive Control of Reasoning Effort" (Oct 2025)  
   https://arxiv.org/abs/2510.27042

7. **Runaway Agent** - "Early-Exit Behavior in Embodied Environments" (May 2025)  
   https://arxiv.org/abs/2505.17616

---

**Next Steps**:

- Read [3.4.2 - Convergence Detection](./3.4.2-convergence.md) for task completion signals
- Read [3.4.3 - Stuck Detection](./3.4.3-stuck-detection.md) for failure prevention
- Read [11.4.1 - Cost Optimization](../11-production/11.4.1-token-reduction.md) for production strategies
